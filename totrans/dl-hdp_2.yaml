- en: Chapter 2.  Distributed Deep Learning for Large-Scale Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章 分布式深度学习与大规模数据
- en: '|   | *"In God we trust, all others must bring data"* |   |'
  id: totrans-1
  prefs: []
  type: TYPE_TB
  zh: '|   | *“我们信仰上帝，其他的都必须提供数据”* |   |'
- en: '|   | --*W. Edwards Deming* |'
  id: totrans-2
  prefs: []
  type: TYPE_TB
  zh: '|   | --*W. Edwards Deming* |'
- en: In this exponentially growing digital world, big data and deep learning are
    the two hottest technical trends. Deep learning and big data are two interrelated
    topics in the world of data science, and in terms of technological growth, both
    are critically interconnected and equally significant.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个指数增长的数字世界中，大数据和深度学习是两大最热门的技术趋势。深度学习和大数据是数据科学领域中两个相互关联的主题，在技术发展方面，它们密切相连，且同样重要。
- en: Digital data and cloud storage follow a generic law, termed as Moore's law [50],
    which roughly states that the world's data are doubling every two years; however,
    the cost of storing that data decreases at approximately the same rate. This profusion
    of data generates more features and verities, hence, to extract all the valuable
    information out of it, better deep learning models should be built.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 数字数据和云存储遵循一种通用法则，称为摩尔定律[50]，大致表示全球数据每两年翻一番；然而，存储这些数据的成本大致以相同的速度下降。这种数据的激增带来了更多的特征和多样性，因此，为了从中提取所有有价值的信息，应该构建更好的深度学习模型。
- en: This voluminous availability of data helps to bring huge opportunities for multiple
    sectors. Moreover, big data, with its analytic part, has produced lots of challenges
    in the field of data mining, harnessing the data, and retrieving the hidden information
    out of it. In the field of Artificial Intelligence, deep learning algorithms provide
    their best output with large-scale data during the learning process. Therefore,
    as data are growing faster than ever before, deep learning also plays a crucial
    part in delivering all the big data analytic solutions.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这种海量数据的可用性为多个行业带来了巨大的机会。此外，带有分析部分的大数据，在数据挖掘、数据获取和从中提取隐藏信息的领域中也带来了许多挑战。在人工智能领域，深度学习算法在学习过程中能够在大规模数据下提供最佳输出。因此，随着数据增长速度空前加快，深度学习在提供大数据分析解决方案中也发挥着至关重要的作用。
- en: This chapter will give an insight into how deep learning models behave with
    big data, and reveal the associated challenges. The later part of the chapter
    will introduce Deeplearning4j, an open source distributed framework, with a provision
    for integration with Hadoop and Spark, used to deploy deep learning for large-scale
    data. The chapter will provide examples to show how basic deep neural networks
    can be implemented with Deeplearning4j, and its integration to Apache Spark and
    Hadoop YARN.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将深入探讨深度学习模型在大数据中的表现，并揭示相关的挑战。章末将介绍Deeplearning4j，一个开源分布式框架，提供与Hadoop和Spark的集成，用于大规模数据的深度学习部署。本章还将提供示例，展示如何使用Deeplearning4j实现基本的深度神经网络，以及它与Apache
    Spark和Hadoop YARN的集成。
- en: 'The following are the important topics that will be covered in this chapter:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下重要主题：
- en: Deep learning for massive amounts of data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面向海量数据的深度学习
- en: Challenges of deep learning for big data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习在大数据中的挑战
- en: Distributed deep learning and Hadoop
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式深度学习与Hadoop
- en: 'Deeplearning4j: An open source distributed framework for deep learning'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deeplearning4j：一个用于深度学习的开源分布式框架
- en: Setting up Deeplearning4j on Hadoop YARN
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Hadoop YARN上设置Deeplearning4j
- en: Deep learning for massive amounts of data
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 面向海量数据的深度学习
- en: In this Exa-Byte scale era, the data are increasing at an exponential rate.
    This growth of data are analyzed by many organizations and researchers in various
    ways, and also for so many different purposes. According to the survey of **International
    Data Corporation** (**IDC**), the Internet is processing approximately 2 Petabytes
    of data every day [51]. In 2006, the size of digital data was around 0.18 ZB,
    whereas this volume has increased to 1.8 ZB in 2011\. Up to 2015, it was expected
    to reach up to 10 ZB in size, and by 2020, its volume in the world will reach
    up to approximately 30 ZB to 35 ZB. The timeline of this data mountain is shown
    in *Figure 2.1*. These immense amounts of data in the digital world are formally
    termed as big data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个Exa-Byte级别的时代，数据以指数级的速度增长。许多组织和研究人员以各种方式分析这种数据增长，且目的各异。根据**国际数据公司**（**IDC**）的调查，互联网每天处理约2
    Petabytes的数据[51]。2006年，数字数据的大小约为0.18 ZB，而这一数据量在2011年增加到了1.8 ZB。到2015年，预计这一数据量将达到10
    ZB，而到2020年，全球数据量将达到大约30 ZB至35 ZB。这一数据山脉的时间线如*图2.1*所示。数字世界中这些庞大的数据量正式被称为大数据。
- en: '|   | *"The world of Big Data is on fire"* |   |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '|   | *“大数据的世界正在燃烧”* |   |'
- en: '|   | --*The Economist, Sept 2011* |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '|   | --*《经济学人》，2011年9月* |'
- en: '![Deep learning for massive amounts of data](img/image_02_001-1.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![海量数据的深度学习](img/image_02_001-1.jpg)'
- en: 'Figure 2.1: Figure shows the increasing trend of data for a time span of around
    20 years'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1：图表展示了约20年时间跨度内数据的增长趋势
- en: Facebook has almost 21 PB in 200M objects [52], whereas Jaguar ORNL has more
    than 5 PB data. These stored data are growing so rapidly that Exa-Byte scale storage
    systems are likely to be used by 2018 to 2020.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook的存储量几乎为21PB，存储了2亿个对象[52]，而Jaguar ORNL的存储量超过5PB。这些存储的数据增长速度如此之快，以至于预计在2018到2020年间，Exa-Byte级别的存储系统将被使用。
- en: This explosion of data certainly poses an immediate threat to the traditional
    data-intensive computations, and points towards the need for some distributed
    and scalable storage architecture for querying and analysis of the large-scale
    data. A generic line of thought for big data is that raw data is extremely complex,
    sundry, and increasingly growing. An ideal Big dataset consists of a vast amount
    of unsupervised raw data, and with some negligible amount of structured/categorized
    data. Therefore, while processing these amounts of non-stationary structured data,
    the conventional data-intensive computations often fail. As a result, big data,
    having unrestricted diversity, requires sophisticated methods and tools, which
    could be implemented to extract patterns and analyze the large-scale data. The
    growth of big data has mostly been caused by an increasing computational processing
    power and the capability of the modern systems to store data at lower cost.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的爆炸性增长无疑对传统的数据密集型计算构成了直接威胁，并指向了需要某种分布式和可扩展的存储架构，用于查询和分析大规模数据。关于大数据的一个普遍观点是，原始数据非常复杂、种类繁多，并且不断增长。一个理想的大数据集包含大量的无监督原始数据，并且只有少量的结构化或分类数据。因此，在处理这些非静态结构化数据时，传统的数据密集型计算通常会失败。因此，具有无限多样性的大数据需要复杂的方法和工具，以便提取模式并分析大规模数据。大数据的增长主要是由计算处理能力的提升和现代系统以更低成本存储数据的能力推动的。
- en: 'Considering all these features of big data, it can be broken into four distinct
    dimensions, often referred to as the four Vs: **Volume**, **Variety**, **Velocity**,
    and **Veracity**. Following *figure 2.2* shows the different characteristics of
    big data by providing all the 4Vs of data:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到大数据的所有特征，可以将其分为四个不同的维度，通常称为四个V：**数据量（Volume）**、**数据种类（Variety）**、**数据速度（Velocity）**和**数据真实性（Veracity）**。下图*图2.2*展示了大数据的不同特征，并提供了所有4V的数据：
- en: '![Deep learning for massive amounts of data](img/B05883_02_02-1.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![海量数据的深度学习](img/B05883_02_02-1.jpg)'
- en: 'Figure 2.2: Figure depicts the visual representation of 4Vs of big data'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2：图表展示了大数据4V的可视化表现
- en: In this current data-intensive technology era, the velocity of the data, the
    escalating rate at which the data are collected and obtained is as significant
    as the other parameters of the big data, that is, **Volume** and **Variety**.
    With the given pace, at which this data is getting generated, if it is not collected
    and analyzed sensibly, there is a huge risk of important data loss. Although,
    there is an option to retain this rapid-moving data into bulk storage for batch
    processing at a later period, the genuine importance in tackling this high velocity
    data lies in how quickly an organization can convert the raw data to a structured
    and usable format. Specifically, time-sensitive information such as flight fare,
    hotel fare, or some e-commerce product's price, and so on would become obsolete
    if the data is not immediately retained and processed in a systemic manner. The
    parameter veracity in big data is concerned with the accuracy of the results obtained
    after the data analysis. As data turns more complex each day, sustaining trust
    in the hidden information of big data throws a significant challenge.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据密集型的技术时代，数据的速度、数据收集和获取的增长速度与大数据的其他参数一样重要，即**数据量（Volume）**和**数据种类（Variety）**。随着数据生成速度的加快，如果不加以合理收集和分析，就有可能丧失重要的数据。尽管可以将这些快速生成的数据存储到大容量存储中，供以后批量处理，但处理这些高速度数据的真正关键在于组织能够多快地将原始数据转化为结构化且可用的格式。具体而言，像航班票价、酒店费用或某些电子商务产品的价格等时间敏感信息，如果不立即以系统化的方式保存和处理，将会变得过时。大数据中的真实性（Veracity）参数涉及数据分析后得到的结果的准确性。随着数据变得越来越复杂，保持对大数据中隐藏信息的信任，带来了重大挑战。
- en: To extract and analyze such critically complex data, a better, well-planned
    model is desired. In ideal cases, a model should perform better dealing with big
    data compared to data with small sizes. However, this is not always the case.
    Here, we will show one example to discuss more on this point.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提取和分析如此复杂的数据，需要一个更好、更周密的模型。在理想情况下，与小数据集相比，模型在处理大数据时应该表现更好。然而，这并不总是如此。接下来，我们将通过一个例子进一步讨论这个问题。
- en: As illustrated in *Figure 2.3*, with a small size dataset, the performance of
    the best algorithm is *n%* better than the worst one. However, as the size of
    the dataset increases (big data), the performance also enhances exponentially
    to some *k % >> n %*. Such kind of traces can well be found from [53], which clearly
    shows the effect of a large-scale training dataset in the performance of the model.
    However, it would be completely misleading that with any of the simplest models,
    one can achieve the best performance only using Big dataset.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 2.3*所示，在小数据集的情况下，最好的算法比最差的算法要*多 n%*。然而，随着数据集大小的增加（大数据），性能也会呈指数级增强，达到*多 k
    % >> n %*。从[53]中可以清晰地看到，大规模训练数据集对模型性能的影响。然而，认为仅使用大数据就能通过任何最简单的模型达到最佳性能，这种说法是完全误导的。
- en: From [53] we can see that algorithm 1 is basically a Naive Bayes model, algorithm
    2 belongs to a memory-based model, and algorithm 3 corresponds to Winnow. The
    following graph shows, with a small dataset, that the performance of Winnow is
    less that the memory-based one. Whereas when dealing with Big dataset, both the
    Naive Bayes and Winnow show better performance than the memory-based model. So,
    looking at the *Figure 2.3*, it would be really difficult to infer on what basis
    any one of these simple models work better in an environment of large dataset.
    An intuitive explanation for the relatively poor performance of the memory-based
    method with large datasets is that the algorithm suffered due to the latency of
    loading a huge amount of data to its memory. Hence, it is purely a memory related
    issue, and only using big data would not resolve that. Therefore, a primary reason
    for the performance should be how sophisticated the models are. Hence, the importance
    of deep learning model comes into play.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从[53]中我们可以看到，算法1基本上是一个朴素贝叶斯模型，算法2属于基于记忆的模型，而算法3对应的是Winnow模型。下图显示，在小数据集的情况下，Winnow的表现不如基于记忆的模型。而在处理大数据集时，朴素贝叶斯和Winnow的表现都优于基于记忆的模型。因此，查看*图
    2.3*时，很难推测出这些简单模型在大数据环境中哪个表现更好。一个直观的解释是，基于记忆的方法在处理大数据时表现较差，原因在于加载大量数据到内存时产生的延迟。因此，这纯粹是一个与内存相关的问题，仅仅使用大数据并不能解决这个问题。因此，性能的主要原因应是模型的复杂性。这也说明了深度学习模型的重要性。
- en: Note
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Big data. Small Minds. No Progress! Big data. Big Brains. Breakthrough! [54]
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据。小智慧。没有进步！大数据。大智慧。突破！[54]
- en: Deep learning stands in contrast to big data. Deep learning has triumphantly
    been implemented in various industry products and widely practiced by various
    researchers by taking advantage of this large-scale digital data. Famous technological
    companies such as Facebook, Apple, and Google collect and analyze this voluminous
    amount of data on a daily basis, and have been bellicosely going forward with
    various deep learning related projects over the last few years.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习与大数据有所不同。深度学习通过利用大规模的数字数据，已成功地应用于各种行业产品，并被各类研究人员广泛实践。像Facebook、Apple和Google等著名科技公司每天都会收集并分析大量数据，并且在过去几年里，他们在多个深度学习相关项目中不断取得进展。
- en: Google deploys deep learning algorithms on the massive unstructured data collected
    from various sources including Google's Street view, image search engine, Google's
    translator, and Android's voice recognition.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Google在从各种来源收集的大量非结构化数据上部署深度学习算法，包括Google的街景、图像搜索引擎、Google翻译以及Android的语音识别。
- en: '![Deep learning for massive amounts of data](img/image_02_003.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习用于海量数据](img/image_02_003.jpg)'
- en: 'Figure 2.3: Variation of percentage of accuracy of different types of algorithms
    with increasing size of datasets'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3：不同类型算法在数据集大小增加时准确率的变化
- en: Apple's Siri, a virtual personal assistant for iPhones, provides a bulk of different
    services, such as sport news, weather reports, answers to users' questions, and
    so on. The entire application of Siri is based on deep learning, which collects
    data from different Apple services and obtains its intelligence. Other industries,
    mainly Microsoft and IBM, are also using deep learning as their major domain to
    deal with this massive amount of unstructured data. IBM's brain-like computer,
    Watson, and Microsoft's Bing search engine primarily use deep learning techniques
    to leverage the big data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 苹果的Siri是iPhone的虚拟个人助手，提供各种不同的服务，如体育新闻、天气预报、用户问题的回答等。Siri的整个应用基于深度学习，收集来自不同Apple服务的数据并获取其智能。其他行业，主要是微软和IBM，也在使用深度学习作为其处理这海量非结构化数据的主要领域。IBM的类大脑计算机沃森和微软的必应搜索引擎主要利用深度学习技术来挖掘大数据。
- en: Current deep learning architectures comprise of millions or even billions of
    data points. Moreover, the scale at which the data is growing prevents the model
    from the risk of overfitting. The rapid increase in computation power too has
    made the training of advanced models much easier.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的深度学习架构包含数百万甚至数十亿个数据点。此外，数据增长的规模使得模型避免了过拟合的风险。计算能力的快速增长也使得训练先进模型变得更加容易。
- en: '*Table 2.1* shows how big data is practiced with popular deep learning models
    in recent research to get maximum information out of data:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*表2.1* 显示了如何在最近的研究中使用流行的深度学习模型来最大化数据的提取信息：'
- en: '| **Models** | **Computing power** | **Datasets** | **Average running time**
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **计算能力** | **数据集** | **平均运行时间** |'
- en: '| Convolutional Neural Network[55] | Two NVIDIA GTX 580 3 GB GPUs. | Roughly
    90 cycles through the training set of 1.2 million high resolution images. | Five
    to six days. |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 卷积神经网络 [55] | 两个NVIDIA GTX 580 3 GB GPU | 大约90轮训练，使用120万张高分辨率图片 | 五到六天 |'
- en: '| Deep Belief Network [41] | NVIDIA GTX 280 1 GB GPU. | 1 million images. |
    Approximately one day. |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 深度置信网络 [41] | NVIDIA GTX 280 1 GB GPU | 100万张图片 | 大约一天 |'
- en: '| Sparse autoencoder[ 66] | 1000 CPU having 16000 cores each. | 10 million
    200*200 pixel images. | Approximately three days. |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏自编码器 [66] | 1000个CPU，每个拥有16000个核心 | 1000万张200*200像素的图片 | 大约三天 |'
- en: 'Table 2.1: Recent research progress of large-scale deep learning models. Partial
    information taken from [55]'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1：大规模深度学习模型的最新研究进展。部分信息摘自[55]
- en: Deep learning algorithms, with the help of a hierarchical learning approach,
    are basically used to extract meaningful generic representations from the input
    raw data. Basically, at a higher level, more complex and abstract representations
    of the data are learnt from the previous layers and the less abstracted data of
    the multi-level learning model. Although deep learning can also learn from massive
    amounts of labelled (categorized) data, the models generally look attractive when
    they can learn from unlabeled/uncategorized data [56], and hence, help in generating
    some meaningful patterns and representation of the big unstructured data.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在分层学习方法的帮助下，深度学习算法主要用于从输入的原始数据中提取有意义的通用表示。基本上，在更高的层次上，从前一层和多层学习模型的低抽象数据中学习到更复杂、抽象的数据表示。尽管深度学习也可以从大量标注（分类）数据中学习，但当模型能够从未标注/未分类数据中学习时，它们通常更具吸引力[56]，从而帮助生成大规模非结构化数据的有意义模式和表示。
- en: 'While dealing with large-scale unsupervised data, deep learning algorithms
    can extract the generic patterns and relationships among the data points in a
    much better way than the shallow learning architectures. The following are a few
    of the major characteristics of deep learning algorithms, when trained with large-scale
    unlabeled data:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理大规模无监督数据时，深度学习算法可以比浅层学习架构更好地提取数据点之间的通用模式和关系。以下是深度学习算法在用大规模未标注数据训练时的一些主要特征：
- en: From the higher level of abstractions and representation, semantics and relational
    knowledge of the big data can be obtained from the deep learning models
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从更高层次的抽象和表示中，可以从深度学习模型中获得大数据的语义和关系知识。
- en: Even a simple linear model can perform effectively with the knowledge obtained
    from excessively complex and more abstract representations of the huge dataset
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使是一个简单的线性模型，也可以通过从过于复杂和更抽象的巨大数据集表示中获得的知识，进行有效的表现。
- en: This huge variety of data representation from the unsupervised data opens its
    door for learning other data types such as textual, audio, video, image, and the
    like
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自无监督数据的这种巨大数据表示种类为学习其他数据类型（如文本、音频、视频、图像等）打开了大门。
- en: Therefore, it can be surely concluded that deep learning will become an essential
    ingredient for providing big data sentiment analysis, predictive analysis, and
    so on, particularly with the enhanced processing power and advancement in the
    **graphics processing unit** (**GPU**) capacity. The aim of this chapter is not
    to extensively cover big data, but to represent the relationship between big data
    and deep learning. The subsequent sections will introduce the key concepts, applications,
    and challenges of deep learning while working with large-scale uncategorized data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，可以肯定地得出结论，深度学习将成为提供大数据情感分析、预测分析等的关键要素，特别是随着处理能力的增强和**图形处理单元**（**GPU**）容量的提升。本章的目的是不是广泛讨论大数据，而是展示大数据与深度学习之间的关系。接下来的章节将介绍深度学习在处理大规模未分类数据时的关键概念、应用和挑战。
- en: Challenges of deep learning for big data
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在大数据中的挑战
- en: The potential of big data is certainly noteworthy. However, to fully extract
    valuable information at this scale, we would require new innovations and promising
    algorithms to address many of these related technical problems. For example, to
    train the models, most of the traditional machine learning algorithms load the
    data in memory. But with a massive amount of data, this approach will surely not
    be feasible, as the system might run out of memory. To overcome all these gritty
    problems, and get the most out of the big data with the deep learning techniques,
    we will require brain storming.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据的潜力无疑是值得注意的。然而，要在如此规模下充分提取有价值的信息，我们需要新的创新和有前景的算法来解决这些相关的技术问题。例如，为了训练模型，大多数传统的机器学习算法会将数据加载到内存中。但对于海量数据，这种方法显然不可行，因为系统可能会耗尽内存。为了克服这些棘手的问题，并通过深度学习技术最大化大数据的价值，我们将需要集思广益。
- en: Although, as discussed in the earlier section, large-scale deep learning has
    achieved many accomplishments in the past decade, this field is still in a growing
    phase. Big data is constantly raising limitations with its 4Vs. Therefore, to
    tackle all of those, many more advancements in the models need to take place.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管正如前面一节讨论的那样，大规模深度学习在过去十年里取得了许多成就，但这个领域仍处于成长阶段。大数据不断地通过其4V特性提出新的限制。因此，为了应对这些挑战，模型中还需要进行更多的进步。
- en: Challenges of deep learning due to massive volumes of data (first V)
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 由于海量数据，深度学习面临的挑战（第一个V）
- en: The volume of large-scale data imposes a great challenge to deep learning. With
    very high dimensionality (attributes), a large number of examples (input) and
    large varieties of classifications (outputs), big data often increases the complexity
    of the model, as well as the running-time complexity of the algorithm. The mountain
    of data makes the training of deep learning algorithms almost impossible using
    centralized storage and its limited processing ability. To provide a cushion to
    this challenge, pushed by the huge volume of data, distributed frameworks with
    parallelized servers should be used. The upgraded deep network models have started
    to use clusters of CPUs and GPUs to enhance the training speed, without compromising
    the algorithm's accuracy. Various new strategies have been evolved for model parallelism
    and data parallelism.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模数据的体量对深度学习构成了巨大的挑战。随着维度（属性）非常高，示例（输入）数量庞大以及分类（输出）种类繁多，大数据常常增加模型的复杂性，同时也增加了算法的运行时间复杂度。大量数据使得使用集中存储及其有限的处理能力几乎不可能进行深度学习算法的训练。为了应对由数据量巨大带来的挑战，应该使用分布式框架和并行化的服务器。这些升级后的深度网络模型已经开始使用CPU和GPU集群来加速训练速度，同时不妥协算法的准确性。模型并行性和数据并行性已经发展出了各种新的策略。
- en: In these types, the models or data are split into blocks, which can fit with
    the in-memory data, and then be distributed to various nodes with forward and
    backward propagations [57]. Deeplearning4j, a Java-based distributed tool for
    deep learning, uses data parallelism for this purpose, and will be explained in
    the next section.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些类型中，模型或数据被拆分成可以适应内存数据的块，然后分发到各个节点进行前向和反向传播[57]。基于Java的深度学习分布式工具Deeplearning4j使用数据并行性来实现这一目标，下一节将详细介绍。
- en: High volumes of data are always associated with noisy labels and data incompleteness.
    This poses a major challenge during the training of large-scale deep learning.
    A huge proportion of the big data is contained by the unlabeled or unstructured
    data, where the noisy labels predominantly exist. To overcome this issue, some
    manual curation of the datasets is required to a significant extent. For example,
    all the search engines are used to collect the data over the last one year span.
    For this data, we need some sort of filtering, particularly to remove redundancy
    and the low-value data. Advanced deep learning methods are essential to handle
    such noisy, redundant data. Also, the associated algorithms should be able to
    tolerate these disarray datasets. One can also implement some more efficient cost
    function and updated training strategy to fully overcome the effect of noisy labels.
    Moreover, the use of semi-supervised learning [58] [59] could help to enhance
    the solution associated with this noisy data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 大量数据总是伴随着标签噪声和数据不完整性，这在大规模深度学习训练中构成了一个重大挑战。大部分大数据由无标签或非结构化数据构成，其中噪声标签主要存在于此。为了克服这个问题，数据集需要进行一定程度的人工整理。例如，所有搜索引擎都用于收集过去一年间的数据。对于这些数据，我们需要进行某种形式的筛选，特别是去除冗余和低价值数据。先进的深度学习方法对于处理这类噪声和冗余数据至关重要。此外，相关算法应能够容忍这些杂乱的数据集。还可以实现更高效的代价函数和更新的训练策略，以完全克服噪声标签的影响。此外，使用半监督学习[58]
    [59]有助于增强与这些噪声数据相关的解决方案。
- en: Challenges of deep learning from a high variety of data (second V)
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习面临的来自高种类数据的挑战（第二个V）
- en: This is the second dimension of big data, which represents all types of formats,
    with different distributions and from numerous sources. The exponentially growing
    data come from heterogeneous sources, which include a mammoth collection of audio
    streams, images, videos, animations, graphics, and unstructured texts from various
    log files. These varieties of data possess different characteristics and behavior.
    Data integration could be the only way to deal with such situations. As stated
    in [Chapter 1](ch01.html "Chapter 1. Introduction to Deep Learning") , *Introduction
    to Deep Learning*, deep learning has the ability to represent learning from structured/unstructured
    data. Deep learning can carry out unsupervised learning in a hierarchical fashion,
    which is training performed one level at a time, and the higher level features
    are defined by the immediate lower levels. This property of deep learning can
    be used to address the data integration problem. The natural solution of this
    could be to learn the data representation from each individual data sources, and
    then integrate the learned features at the subsequent levels.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这是大数据的第二个维度，代表所有类型的格式，具有不同的分布并来自众多来源。指数级增长的数据来自异质来源，其中包括大量的音频流、图像、视频、动画、图形以及来自各种日志文件的非结构化文本。这些不同种类的数据具有不同的特性和行为。数据集成可能是解决此类情况的唯一途径。正如[第1章](ch01.html
    "第1章 深度学习简介")《深度学习简介》中所述，深度学习有能力从结构化/非结构化数据中进行学习。深度学习可以以分层的方式执行无监督学习，即逐层训练，更高层次的特征由紧接着的较低层次定义。深度学习的这一特性可以用来解决数据集成问题。一个自然的解决方案是从每个单独的数据源中学习数据表示，然后在随后的层次中集成学到的特征。
- en: There have already been a few experiments [60] [61], which have successfully
    demonstrated that deep learning can easily be used for the heterogeneous data
    sources for its significant gains in system performance. However, there are still
    many unanswered questions which deep learning has to address in the upcoming years.
    Currently, most of the deep learning models are mainly tested on bi-modalities
    (data from only two sources), but will the system performance be enhanced while
    dealing with multiple modalities? It might happen that multiple sources of data
    will offer conflicting information; in those cases, how will the model be able
    to nullify such conflicts and integrate the data in a constructive and fruitful
    way? Deep learning seems perfectly appropriate for the integration of various
    sources of data with multiple modalities, on account of its capability of learning
    intermediate representations and the underlying factors associated with a variety
    of data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有一些实验[60] [61]成功证明，深度学习可以轻松应用于异构数据源，显著提高系统性能。然而，深度学习在未来几年仍需解决许多未解之谜。目前，大多数深度学习模型主要在双模态（仅来自两个源的数据）上进行测试，但在处理多模态数据时，系统性能是否会得到提升呢？可能出现多种数据源提供相互冲突的信息；在这种情况下，模型如何能够消除这些冲突，并以一种建设性且富有成效的方式整合数据呢？由于深度学习能够学习中间表示和与各种数据相关的潜在因素，因此它似乎非常适合于多模态的各种数据源的集成。
- en: Challenges of deep learning from a high velocity of data (third V)
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习面临的高速度数据挑战（第三个V）
- en: The extreme velocity at which data is growing poses an enormous challenge to
    the deep learning technique. For data analytics, data created at this speed should
    also be processed in a timely manner. Online learning is one of the solutions
    to learning from this high velocity data [62-65]. However, online learning uses
    a sequential learning strategy, where the entire dataset should be kept in-memory,
    which becomes extremely difficult for traditional machines. Although the conventional
    neural network has been modified for online learning [67-71], there is still so
    much scope for progress in this field for deep learning. As an alternate approach
    to online learning, the stochastic gradient descent approach [72], [73] is also
    applied for deep learning. In this type, one training example with the known label
    is fed to the next label to update the model parameters. Further, to speed up
    learning, the updates can also be performed on a small batch basis [74]. This
    mini batch can provide a good balance between running time and the computer memory.
    In the next section, we will explain why mini batch data is most important for
    distributed deep learning.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增长的极端速度给深度学习技术带来了巨大挑战。对于数据分析，按此速度创建的数据也应该及时处理。在线学习是从这种高速数据中学习的解决方案之一[62-65]。然而，在线学习使用的是一种顺序学习策略，其中整个数据集需要保存在内存中，这对于传统机器来说极为困难。尽管传统神经网络已经针对在线学习进行了修改[67-71]，但在深度学习领域仍有很大的进展空间。作为在线学习的替代方法，随机梯度下降法[72]，[73]也被应用于深度学习。在这种方法中，一个带有已知标签的训练样本被馈送到下一个标签，以更新模型参数。此外，为了加速学习，更新也可以基于小批量进行[74]。这种小批量能够在运行时间和计算机内存之间提供良好的平衡。在下一部分，我们将解释为什么小批量数据对分布式深度学习至关重要。
- en: One more big challenge related to this high velocity of data is that this data
    is extremely changeable in nature. The distribution of data happens too frequently
    over time. Ideally, the data that changes over time is split into chunks taken
    from small time durations. The basic idea is that the data remains stationary
    for some time, and also possesses some major degree of correlation [75] [76].
    Therefore, the deep learning algorithms of big data should have the feature of
    learning the data as a stream. Algorithms which can learn from those non-stationary
    data are really crucial for deep learning.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 与这种高速度数据相关的另一个重大挑战是，这些数据在性质上极为变化无常。数据的分布在时间上变化过于频繁。理想情况下，随着时间的推移变化的数据被划分为来自小时间段的块。基本的思想是数据在一段时间内保持静止，并且具有一定程度的相关性[75]
    [76]。因此，大数据的深度学习算法应具备将数据作为流来学习的特性。能够从这些非平稳数据中学习的算法对于深度学习至关重要。
- en: Challenges of deep learning to maintain the veracity of data (fourth V)
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习面临的数据真实性挑战（第四个V）
- en: Data veracity, imprecise, or uncertain data, is sometime overlooked, though
    it is equally consequential as the other 3Vs of big data. With the immense variety
    and velocity of big data, an organization can no longer rely on the traditional
    models to measure the accuracy of data. Unstructured data, by definition, contains
    a huge amount of imprecise and uncertain data. For example, social media data
    is excessively uncertain in nature. Although there are tools that can automate
    the normalization and cleansing of data, they are mostly in the pre-industrial
    stage.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的真实性、不精确性或不确定性有时会被忽视，尽管它与大数据的其他3Vs一样具有重要的影响。随着大数据的巨大多样性和速度，组织再也无法依赖传统模型来衡量数据的准确性。根据定义，非结构化数据包含大量的不精确和不确定数据。例如，社交媒体数据本质上是不确定的。尽管有一些工具可以自动化数据的规范化和清理，但它们大多处于前工业化阶段。
- en: Distributed deep learning and Hadoop
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式深度学习与Hadoop
- en: 'From the earlier sections of this chapter, we already have enough insights
    on why and how the relationship of deep learning and big data can bring major
    changes to the research community. Also, a centralized system is not going to
    help this relationship substantially with the course of time. Hence, distribution
    of the deep learning network across multiple servers has become the primary goal
    of the current deep learning practitioners. However, dealing with big data in
    a distributed environment is always associated with several challenges. Most of
    those are explained in-depth in the previous section. These include dealing with
    higher dimensional data, data with too many features, amount of memory available
    to store, processing the massive Big datasets, and so on. Moreover, Big datasets
    have a high computational resource demand on CPU and memory time. So, the reduction
    of processing time has become an extremely significant criterion. The following
    are the central and primary challenges in distributed deep learning:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从本章的前面部分，我们已经获得了足够的洞察力，了解深度学习与大数据之间的关系如何以及为何能给研究领域带来重大变化。而且，随着时间的推移，集中式系统不会对这种关系产生实质性帮助。因此，将深度学习网络分布到多个服务器上已成为当前深度学习从业者的主要目标。然而，在分布式环境中处理大数据总是伴随着多个挑战。大多数这些挑战在前一节中已被深入解释。这些挑战包括处理高维数据、特征过多的数据、可用内存的存储量、大规模大数据集的处理等等。此外，大数据集对CPU和内存的计算资源需求很高。因此，减少处理时间已成为一个极其重要的标准。以下是分布式深度学习中的核心和主要挑战：
- en: How can we keep chunks of dataset in the primary memory of the nodes?
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何将数据集的块保存在节点的主内存中？
- en: How can we maintain coordination among the chunks of data, so that later they
    can be moved together to result in the final outcome?
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何保持数据块之间的协调，以便它们能够一起被移动并最终得出结果？
- en: How can we make distributed and parallel processing extremely scheduled and
    coordinated?
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何使分布式和并行处理变得极其有序和协调？
- en: How can we achieve an orchestral search process across the dataset to achieve
    high performance?
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何在数据集上实现一种管弦乐式的搜索过程，从而获得高效能？
- en: There are multiple ways of using distributed deep learning with big datasets.
    However, when we talk about big data, the framework that is performing tremendously
    well in defending most of the challenges from the past half decade is the Hadoop
    framework [77-80]. Hadoop allows for parallel and distributed processing. It is
    undoubtedly the most popular and widely used framework, and it can store and process
    the data mountain more efficiently compared to the other traditional frameworks.
    Almost all the major technology companies, such as Google, Facebook, and so on
    use Hadoop to deploy and process their data in a sophisticated fashion. Most of
    the software designed at Google, which requires the use of an ocean of data, uses
    Hadoop. The primary advantage of Hadoop is the way it stores and processes enormous
    amount of data across thousands of commodity servers, bringing some well-organized
    results [81]. From our general understanding of deep learning, we can relate that
    deep learning surely needs that sort of distributed computing power to produce
    some wondrous outcomes from the input data. The Big dataset can be split into
    chunks and distributed across multiple commodity hardware for parallel training.
    Further more, the complete stage of a deep neural network can be split into subtasks,
    and then those subtasks can be processed in parallel.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hadoop has turned out to be the point of convergence for all the data lakes.
    The need to shif deep learning to the data, which is already residing in Hadoop,
    has become quintessential.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop operates on the concept that *moving computation is cheaper than moving
    data* [86] [87]. Hadoop allows for the distributed processing of large-scale datasets
    across clusters of commodity servers. It also provides efficient load balancing,
    has a very high degree of fault tolerance, and is highly horizontally scalable
    with minimal effort. It can detect and tolerate failures in the application layers,
    and hence, is suitable for running on commodity hardware. To achieve the high
    availability of data, Hadoop, by default, keeps a replication factor of three,
    with a copy of each block placed on two other separate machines. So, if a node
    fails, the recovery can be done instantly from the other two nodes. The replication
    factor of Hadoop can be easily increased based on how valuable the data is and
    other associated requirements on the data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop was initially built mainly for processing the batch tasks, so it is mostly
    suitable for deep learning networks, where the main task is to find the classification
    of large-scale data. The selection of features to learn how to classify the data
    is mainly done on a large batch of datasets.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop is extremely configurable, and can easily be optimized as per the user's
    requirements. For example, if a user wants to keep more replicas of the data for
    better reliability, he can increase the replication factor. However, an increase
    in the number of replicas will eventually increase the storage requirements. Here
    we will not be explaining more about the features and configuration of data, rather
    we will mostly discuss the part of Hadoop which will be used extensively in distributed
    deep neural networks.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: In the new version of Hadoop, the parts which we will mainly use in this book
    are HDFS, Map-Reduce, and **Yet Another Resource Negotiator** (**YARN**). YARN
    has already dominated Hadoop's Map-Reduce (explained in the next part) in a large
    manner. YARN currently has the responsibility to assign the works to the Data
    nodes (data server) of Hadoop. **Hadoop Distributed File System** (**HDFS**),
    on the other hand, is a distributed file system, which is distributed across all
    the Data nodes under a centralized meta-data server called NameNode. To achieve
    high-availability, in the later version, a secondary NameNode was integrated to
    Hadoop framework, the purpose of which is to have a copy of the metadata from
    primary NameNode after certain checkpoints.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Map-Reduce
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Map-Reduce paradigm [83] is a distributed programming model developed by
    Google in 2004, and is associated with processing huge datasets with a parallel
    and distributed algorithm on a cluster of machines. The entire Map-Reduce application
    is useful with large-scale datasets. Basically, it has two primary components,
    one is called Map and the other is called Reduce, along with a few intermediate
    stages like shuffling, sorting and partitioning. In the map phase, the large input
    job is broken down into smaller ones, and each of the jobs is distributed to different
    cores. The operation(s) are then carried out on every small job placed on those
    machines. The Reduce phase accommodates all the scattered and transformed output
    into one single dataset.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'Explaining the concept of Map-Reduce in detail is beyond the scope of this
    chapter; interested readers can go through *"Map-Reduce: Simplified data processing
    on large clusters*" [83] to get an in-depth knowledge of this.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Iterative Map-Reduce
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The deep learning algorithms are iterative in nature - the models learn from
    the optimization algorithms, which go through multiple steps so that it leads
    to a point of minimal error. For these kinds of models the Map-Reduce application
    does not seem to work as efficiently as it does for other use-cases.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Iterative Map-Reduce, a next generation YARN framework (unlike the traditional
    Map-Reduce) does multiple iterations on the data, which passes through only once.
    Although the architecture of Iterative Map-Reduce and Map-Reduce is dissimilar
    in design, the high level of understanding of both the architectures is simple.
    Iterative Map-Reduce is nothing but a sequence of Map-Reduce operations, where
    the output of the first Map-Reduce operation becomes the input to the next operation
    and so on. In the case of deep learning models, the map phase places all the operations
    of a particular iteration on each node of the distributed systems. It then distributes
    that massive input dataset to all the machines in the cluster. The training of
    the models is performed on each node of the cluster.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Before sending the aggregated new model back to each of the machines, the reduce
    phase takes all the outputs collected from the map phase and calculates the average
    of the parameters. The same operations are iterated over and over again by the
    Iterative Reduce algorithm until the learning process completes and the errors
    minimize to almost zero.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2.4* compares the high-level functionalities of the two methods. The
    left image shows the block diagram of Map-Reduce, while on the right, we have
    the close-up of Iterative Map-Reduce. Each ''Processor'' is a working deep network,
    which is learning on small chunks of the larger dataset. In the ''Superstep''
    phase, the averaging of the parameters is done before the entire model is redistributed
    to the whole cluster as shown in the following diagram:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![Iterative Map-Reduce](img/B05883_02_04.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4: Difference of functionalities in Map-Reduce and parallel iterative
    reduce'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Yet Another Resource Negotiator (YARN)
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The primary idea of YARN is to dissociate job scheduling and resource management
    from the data processing. So the data can continue to process in the system in
    parallel with the Map-Reduce batch jobs. YARN possesses a central resource manager,
    which mostly manages the Hadoop system resources according to the need. The node
    manager (specific to nodes) is responsible for managing and monitoring the processing
    of individual nodes of the cluster. This processing is dedicatedly controlled
    by an ApplicationMaster, which monitors the resources from the central resource
    manager, and works with the node manager to monitor and execute the tasks. The
    following figure gives an overview of the architecture of YARN:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '![Yet Another Resource Negotiator (YARN)](img/B05883_02_05-1.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5: An overview of the high-level architecture of YARN'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: All these components of Hadoop are primarily used in distributed deep learning
    to overcome all the challenges stated earlier. The following subsection shows
    the criteria that need to be satisfied for better performance of distributed deep
    learning.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Important characteristics for distributed deep learning design
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'the following are the important characteristics of distributed deep learning
    design:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '**Small batch processing**: In distributed deep learning, the network must
    intake and process data quickly in parallel. To process and provide results more
    accurately, every node of the cluster should receive small chunks of data of approximately
    10 elements at a time.'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For example, say the master node of YARN is coordinating 20 worker nodes for
    a Big dataset of 200 GB. The master node will split the dataset into 10 GB of
    20 small batches of data, allocating one small batch to each worker. The workers
    will process the data in parallel, and send the results back to the master as
    soon as it finishes the computing. All these outcomes will be aggregated by the
    master node, and the average of the results will be finally redistributed to the
    individual workers.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Deep learning networks perform well with small batches of nearly 10, rather
    than working with 100 or 200 large batches of data. Small batches of data empower
    the networks to learn from different orientations of the data in-depth, which
    later on recompiles to give a broader knowledge to the model.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: On the other hand, if the batch size is too large, the network tries to learn
    quickly, which maximizes the errors. Conversly, smaller batch size slows down
    the speed of learning, and results in the possibility of divergence as the network
    approaches towards the minimum error rate.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Parameter Averaging**: Parameter averaging is a crucial operation for the
    training of distributed deep network. In a network, parameters are generally the
    weight and biases of the node layers. As mentioned in the small batch processing
    section, once training is completed for several workers, they will pass different
    sets of parameters back to the master. With every iteration, the parameters are
    averaged, updated, and sent back to the master for further operations.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The sequential process of parameter averaging can be outlined as follows:'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The master configures the initial network and sets the different hyperparameters
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the configuration of the training master, the Big dataset is split
    into chunks of several smaller datasets
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For each split of the training dataset, until the error rate approaches towards
    zero, perform the following:'
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The master distributes the parameter from the master to each individual worker
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Each worker starts the training of the model with its dedicated chunk of dataset
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The average of the parameters is calculated and returned back to the master.
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The training completes, and the master will have one copy of the training network
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parameter averaging offers the following two important advantages in case of
    distributed training:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It enables parallelism by generating simultaneous results.
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It helps to prevent over-fitting by distributing the given dataset into multiple
    datasets of smaller sizes. The network then learns the average result, rather
    than just aggregating the results from different smaller batches.
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 2.6* shows a combined diagrammatic overview of the small batch processing
    and parameter averaging operation:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![Important characteristics for distributed deep learning design](img/B05883_02_06-1.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.6: Figure shows the high level architecture of a distributed deep
    learning architecture'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Deeplearning4j - an open source distributed framework for deep learning
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Deeplearning4j** (**DL4J**) [82] is an open source deep learning framework
    which is written for JVM, and mainly used for commercial grade. The framework
    is written entirely in Java, and thus, the name ''4j'' is included. Because of
    its use with Java, Deeplearning4j has started to earn popularity with a much wider
    audience and range of practitioners.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: This framework is basically composed of a distributed deep learning library
    that is integrated with Hadoop and Spark. With the help of Hadoop and Spark, we
    can very easily distribute the model and Big datasets, and run multiple GPUs and
    CPUs to perform parallel operations. Deeplearning4j has primarily shown substantial
    success in performing pattern recognition in images, sound, text, time series
    data, and so on. Apart from that, it can also be applied for various customer
    use cases such as facial recognition, fraud detection, business analytics, recommendation
    engines, image and voice search, and predictive maintenance with the sensor data.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'The following *Figure 2.7* shows a generic high-level architectural block diagram
    of Deeplearning4j:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![Deeplearning4j - an open source distributed framework for deep learning](img/B05883_02_07-1.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.7: High level architectural block diagram of Deeplearning4j [82]'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Major features of Deeplearning4j
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deeplearning4j comes with various attractive features, which completely distinguishes
    it from other existing deep learning tools like Theano, Torch, and so on.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed architecture**: Training in Deeplearning4j can be performed in
    two ways - with distributed, multi-threaded deep-learning, or with traditional,
    normal single-threaded deep-learning techniques. The training is carried out in
    clusters of commodity nodes. Therefore, Deeplearning4j is able to process any
    amount of data quickly. The neural networks are trained in parallel using the
    iterative reduce method, which works on Hadoop YARN and Spark. It also integrates
    with Cuda kernels to conduct pure GPU operations, and works with distributed GPUs.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deeplearning4j operations can be run on Hadoop YARN or Spark as a job. In Hadoop,
    Iterative Reduce workers work on every block of HDFS, and synchronously process
    the data in parallel. As the processing completes, they push the transformed parameters
    back to their master, where the average of the parameters are taken and the model
    of each worker's node is updated.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: In Deeplearning4j, the distributed runtimes are interchangeable, where they
    act like a directory in a huge modular architecture, which can be swapped in or
    out.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '**Data parallelism**: There are two ways in which the neural networks can be
    trained in a distributed manner: one is data parallelism, and the other is model
    parallelism. Deeplearning4j follows data parallelism for training. In data parallelism,
    we can split the large dataset into chunks of smaller datasets, and distribute
    those to parallel models running on different servers to train in parallel.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific computing capability for the JVM**: For scientific computing in
    Java and Scala, Deeplearning4j includes an N-dimensional array class using **N-Dimensional
    Arrays for Java** (**ND4J**). The functionality of ND4J is much faster than what
    Numpy provides to Python, and its mostly written in C++. It''s effectively based
    on a library for matrix manipulation and linear algebra in a production environment.
    Most of the routines of ND4J are designed to run fast with minimum RAM requirements.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vectorization tool for machine learning**: For vectorization of various file
    formats and data types, Canova has been merged with Deeplearning4j. Canova performs
    vectorization using an input/output system similar to how Hadoop uses Map-Reduce.
    Canova is primarily designed to vectorize text, CSVs, images, sounds, videos,
    and so on from the **command line interface** (**CLI**).'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary of functionalities of Deeplearning4j
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are summary of functionalities of Deeplearning4j:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Deeplearning4j can be claimed as the most complete, production-ready, open source
    deep learning library ever built
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compared to Theano-based tools, it has many more features specially designed
    for deep networks
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deeplearning4j is very easy to use; even non-specialists can apply its conventions
    to solve computationally intensive problems
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tools provide a wide range of applicability, hence, the networks work equally
    well with image, sound, text, and time-series
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is completely distributed and can run multiple GPUs in parallel, unlike Theano
    [84], which is not distributed, and Torch7 [85], which has not automated its distribution
    like DL4J
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up Deeplearning4j on Hadoop YARN
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deeplearning4j primarily works on networks having multiple layers. To get started
    working with Deeplearning4j, one needs to get accustomed with the prerequisites,
    and how to install all the dependent software. Most of the documentation can be
    easily found on the official website of Deeplearning4j at [https://deeplearning4j.org/](https://deeplearning4j.org/) [88].
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: In this section of the chapter, we will help you to get familiar with the code
    of Deeplearning4j. Initially, we will show the implementation of a simple operation
    of a multilayer neural network with Deeplearning4j. The later part of the section
    will discuss distributed deep learning with Deeplearning4j library. Deeplearning4j
    trains distributed deep neural network on multiple distributed GPUs using Apache
    Spark. The later part of this section will also introduce the setup of Apache
    Spark for Deeplearning4j.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Getting familiar with Deeplearning4j
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This part will mainly introduce the 'Hello World' programs of deep learning
    with deeplearning4j. We will explain the basic functions of the library with the
    help of two simple deep learning problems.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: In Deeplearning4j, `MultiLayerConfiguration`, a class of the library can be
    considered as the base of the building block, which is responsible for organizing
    the layers and the corresponding hyperparameters of a neural network. This class
    can be considered as the core building block of Deeplearning4j for neural networks.
    Throughout the book, we will use this class to configure different multilayer
    neural networks.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hyperparameters are the main backbone to determine the learning process of a
    neural network. They mostly include how to initialize the weights of the models,
    how many times they should be updated, the learning rate of the model, which optimization
    algorithms to use, and so on.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: In the first example, we will show how to classify data patterns for the multilayer
    perceptron classifier with the help of Deeplearning4j.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the sample training dataset that will be used in this program:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Initially, we need to initialize the various hyperparameters of the networks.
    The following piece of code will set the ND4J environment for the program:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Number of epochs is set to `30`:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following piece of code will load the training data to the network:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As the training data is loaded next we load the test data into the model with
    the following code:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Organization of all the layers of the network model as well as setting up the
    hyperparameters can be done with the following piece of code:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we have loaded the training and test dataset, the initialization of the
    model can be done by calling the `init()` method. This will also start the training
    of the model from the given inputs:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To check the output after a certain internal, let''s print the scores every
    `5` parameter updates:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, the network is trained by calling the `.fit()` method:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'So the training of the model is done. In the next part, the data points will
    be plotted and the corresponding accuracy of the data will be calculated as shown
    in the following code:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following code will store all the training data in an array before plotting
    those in the graph:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Running the test data through the network and generating the prediction can
    be done with the following code:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: When the preceding code is executed, it will run for approximately 5-10 seconds,
    depending upon your system configuration. During that time, you can check the
    console, which will display the updated score of training for your model.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'A piece of evaluation is displayed as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, the program will output the different statistics of the training for
    the model using Deeplearning4j as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the background, we can visualize the plotting of the data, which will give
    an impression of what the planet Saturn looks like. In the next part, we will
    show how to integrate Hadoop YARN and Spark with Deeplearning4j. The following
    *Figure 2.8* shows the output of the program in graphical representation:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting familiar with Deeplearning4j](img/image_02_008.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.8: The scattered data points are plotted when the preceding program
    is executed. The data points give an impression of the planet Saturn'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Integration of Hadoop YARN and Spark for distributed deep learning
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To use Deeplearning4j on Hadoop, we need to include the `deeplearning-hadoop`
    dependency as shown in the following code:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Similarly, for Spark, we have to include the `deeplearning-spark` dependency
    as shown in the following code:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Explaining the detailed functionalities of Apache Spark is beyond the scope
    of this book. Interested readers can catch up on the same at [http://spark.apache.org/](http://spark.apache.org/)
    .
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Rules to configure memory allocation for Spark on Hadoop YARN
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As already stated in the previous section, Apache Hadoop YARN is a cluster
    resource manager. When Deeplearning4j submits a training job to a YARN cluster
    via Spark, it is the responsibility of YARN to manage the allocation of resources
    such as CPU cores, amount of memory consumed by each executor, and so on. However,
    to extract the best performance from Deeplearning4j on YARN, some careful memory
    configuration is desired. This is done as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: The executer JVM memory amount needs to be specified using `spark.executor.memory`.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The YARN container memory overhead needs to be specified using `spark.yarn.executor.memoryOverhead`.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sum of `spark.executor.memory` and `spark.yarn.executor.memoryOverhead`
    must always be less than the amount of memory allocated to a container by the
    YARN.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ND4j and JavaCPP should know the allotment of the off-heap memory; this can
    be done using the `org.bytedeco.javacpp.maxbytes` system property.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`org.bytedeco.javacpp.maxbytes` must be less than `spark.yarn.executor.memoryOverhead`.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The current version of Deeplearning4j uses parameter averaging to perform distributed
    training of the neural network. The following operation is performed exactly the
    way it is described in the parameter averaging part of the earlier section:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To list all the files from HDFS so as to run the code on different nodes, run
    the following code:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: A complete code for how to set up Spark with YARN and HDFS will be provided
    along with the code bundle. For simplicity, only part of the code is shown here
    for the purpose of understanding.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will show an example to demonstrate how to use Spark and load the data
    into memory with Deeplearning4j. We will use a basic DataVec example to show some
    pre-processing operation on some CSV data.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'The sample dataset will look as like the following:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The problem statement of the program is as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Remove some unnecessary columns
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filter out data, and keep only examples with values `USA` and `MX` for the `MerchantCountryCode`
    column
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace the invalid entries in the `TransactionAmountUSD` column
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parse the data string, and collect the hour of day from it to create a new `HourOfDay`
    column
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following part will define the operations that we want to perform on the
    dataset:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In unstructured data, the datasets are generally noisy, and so we need to take
    care of some of the invalid data. In case of negative dollar value, the program
    will replace those to `0.0`. We will keep the positive dollar amounts intact.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, to format the `DateTime` format as per the problem statement, the following
    piece of code is used:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'A different schema is created after execution of the all these operations as
    follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The following piece of code will set Spark to perform all the operations:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To take the data directly from HDFS, one has to pass `hdfs://{the filepath
    name}`:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The input data are parsed using `CSVRecordReader()` method as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The pre-defined transformation of Spark is performed as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'As mentioned, to save the data back to HDFS, just putting the file path after
    `hdfs://` will do:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'When the program is executed with Spark using Deeplearning4j, we will get the
    following output:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The following is the output:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Similar to this example, lots of other datasets can be processed in a customized
    way in Spark. From the next chapter, we will show the Deeplearning4j codes for
    specific deep neural networks. The implementation of Apache Spark and Hadoop YARN
    is a generic procedure, and will not change according to neural network. Readers
    can use that code to deploy the deep network code in cluster or locally, based
    on their requirements.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In contrast to the traditional machine learning algorithms, deep learning models
    have the capability to address the challenges imposed by a massive amount of input
    data. Deep learning networks are designed to automatically extract complex representation
    of data from the unstructured data. This property makes deep learning a precious
    tool to learn the hidden information from the big data. However, due to the velocity
    at which the volume and varieties of data are increasing day by day, deep learning
    networks need to be stored and processed in a distributed manner. Hadoop, being
    the most widely used big data framework for such requirements, is extremely convenient
    in this situation. We explained the primary components of Hadoop that are essential
    for distributed deep learning architecture. The crucial characteristics of distributed
    deep learning networks were also explained in depth. Deeplearning4j, an open source
    distributed deep learning framework, integrates with Hadoop to achieve the mentioned
    indispensable requirement. Deeplearning4j is entirely written in Java, can process
    data faster in a distributed manner with iterative Map-Reduce, and can address
    many problems imposed by the large-scale data. We have provided two sample examples
    to let you know about basic Deeplearning4j codes and syntax. We have also provided
    some code snippets for Spark configuration with integration with Hadoop YARN and
    Hadoop Distributed File System.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter of this book will introduce convolutional neural network, a
    popular deep learning network. The chapter will discuss the method convolution
    and how it can be used to build an advanced neural network mainly for image processing
    and image recognition. The chapter will then provide information on how a convolutional
    neural network can be implemented using Deeplearning4j.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
