["```py\nval priceDataFileName: String = \"bitstampUSD_1-min_data_2012-01-01_to_2017-10-20.csv\"\n\nval spark = SparkSession\n    .builder()\n    .master(\"local[*]\")\n    .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\")\n    .appName(\"Bitcoin Preprocessing\")\n    .getOrCreate()\n\nval data = spark.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(priceDataFileName)\ndata.show(10)\n>>>\n```", "```py\nprintln((data.count(), data.columns.size))\n```", "```py\n(3045857, 8)\n```", "```py\nval dataWithDelta = data.withColumn(\"Delta\", data(\"Close\") - data(\"Open\"))\n```", "```py\nimport org.apache.spark.sql.functions._\nimport spark.sqlContext.implicits._\n\nval dataWithLabels = dataWithDelta.withColumn(\"label\", when($\"Close\" - $\"Open\" > 0, 1).otherwise(0))\nrollingWindow(dataWithLabels, 22, outputDataFilePath, outputLabelFilePath)\n```", "```py\nval dropFirstCount: Int = 612000\n\ndef rollingWindow(data: DataFrame, window: Int, xFilename: String, yFilename: String): Unit = {\n var i = 0\n val xWriter = new BufferedWriter(new FileWriter(new File(xFilename)))\n val yWriter = new BufferedWriter(new FileWriter(new File(yFilename)))\n val zippedData = data.rdd.zipWithIndex().collect()\n    System.gc()\n val dataStratified = zippedData.drop(dropFirstCount)//slice 612K\n\n while (i < (dataStratified.length - window)) {\n val x = dataStratified\n                .slice(i, i + window)\n                    .map(r => r._1.getAs[Double](\"Delta\")).toList\n val y = dataStratified.apply(i + window)._1.getAs[Integer](\"label\")\n val stringToWrite = x.mkString(\",\")\n        xWriter.write(stringToWrite + \"n\")\n        yWriter.write(y + \"n\")\n        i += 1\n\n if (i % 10 == 0) {\n            xWriter.flush()\n            yWriter.flush()\n            }\n        }\n    xWriter.close()\n    yWriter.close()\n}\n```", "```py\nval outputDataFilePath: String = \"output/scala_test_x.csv\"\nval outputLabelFilePath: String = \"output/scala_test_y.csv\"\n```", "```py\n{\n    \"Response\":\"Success\",\n    \"Type\":100,\n    \"Aggregated\":false,\n    \"Data\":\n    [{\"time\":1510774800,\"close\":7205,\"high\":7205,\"low\":7192.67,\"open\":7198,                                             \"volumefrom\":81.73,\"volumeto\":588726.94},\n        {\"time\":1510774860,\"close\":7209.05,\"high\":7219.91,\"low\":7205,\"open\":7205,                                 \"volumefrom\":16.39,\"volumeto\":118136.61},\n        ... (other price data)\n        ],\n    \"TimeTo\":1510776180,\n    \"TimeFrom\":1510774800,\n    \"FirstValueInArray\":true,\n    \"ConversionType\":{\"type\":\"force_direct\",\"conversionSymbol\":\"\"}\n}\n```", "```py\nimport javax.inject.Inject\nimport play.api.libs.json.{JsResult, Json}\nimport scala.concurrent.Future\nimport play.api.mvc._\nimport play.api.libs.ws._\nimport processing.model.CryptoCompareResponse\n\nclass RestClient @Inject() (ws: WSClient) {\n def getPayload(url : String): Future[JsResult[CryptoCompareResponse]] = {\n        val request: WSRequest = ws.url(url)\n        val future = request.get()\n        implicit val context = play.api.libs.concurrent.Execution.Implicits.defaultContext\n        future.map {\n            response => response.json.validate[CryptoCompareResponse]\n            }\n        }\n    }\n```", "```py\ncase class CryptoCompareResponse(Response : String,\n    Type : Int,\n    Aggregated : Boolean,\n    Data : List[OHLC],\n    FirstValueInArray : Boolean,\n    TimeTo : Long,\n    TimeFrom: Long)\n\nobject CryptoCompareResponse {\n    implicit val cryptoCompareResponseReads = Json.reads[CryptoCompareResponse]\n    }\n```", "```py\ncase class OHLC(time: Long,\n    open: Double,\n    high: Double,\n    low: Double,\n    close: Double,\n    volumefrom: Double,\n    volumeto: Double)\n\n    object OHLC {\n    implicit val implicitOHLCReads = Json.reads[OHLC]\n        }\n```", "```py\nval spark = SparkSession\n        .builder()\n        .master(\"local[*]\")\n        .config(\"spark.sql.warehouse.dir\", \"\"/home/user/spark/\")\n        .appName(\"Bitcoin Preprocessing\")\n        .getOrCreate()\n```", "```py\nval xSchema = StructType(Array(\n    StructField(\"t0\", DoubleType, true),\n    StructField(\"t1\", DoubleType, true),\n    StructField(\"t2\", DoubleType, true),\n    StructField(\"t3\", DoubleType, true),\n    StructField(\"t4\", DoubleType, true),\n    StructField(\"t5\", DoubleType, true),\n    StructField(\"t6\", DoubleType, true),\n    StructField(\"t7\", DoubleType, true),\n    StructField(\"t8\", DoubleType, true),\n    StructField(\"t9\", DoubleType, true),\n    StructField(\"t10\", DoubleType, true),\n    StructField(\"t11\", DoubleType, true),\n    StructField(\"t12\", DoubleType, true),\n    StructField(\"t13\", DoubleType, true),\n    StructField(\"t14\", DoubleType, true),\n    StructField(\"t15\", DoubleType, true),\n    StructField(\"t16\", DoubleType, true),\n    StructField(\"t17\", DoubleType, true),\n    StructField(\"t18\", DoubleType, true),\n    StructField(\"t19\", DoubleType, true),\n    StructField(\"t20\", DoubleType, true),\n    StructField(\"t21\", DoubleType, true))\n    )\n```", "```py\nimport spark.implicits._\nval y = y_tmp.withColumn(\"y\", 'y.cast(IntegerType))\nimport org.apache.spark.sql.functions._\n\nval x_id = x.withColumn(\"id\", monotonically_increasing_id())\nval y_id = y.withColumn(\"id\", monotonically_increasing_id())\nval data = x_id.join(y_id, \"id\")\n```", "```py\nval featureAssembler = new VectorAssembler()\n        .setInputCols(Array(\"t0\", \"t1\", \"t2\", \"t3\",\n                            \"t4\", \"t5\", \"t6\", \"t7\",\n                            \"t8\", \"t9\", \"t10\", \"t11\",\n                            \"t12\", \"t13\", \"t14\", \"t15\",\n                            \"t16\", \"t17\", \"t18\", \"t19\",\n                            \"t20\", \"t21\"))\n        .setOutputCol(\"features\")\n```", "```py\nval Array(trainingData,testData) = dataWithLabels.randomSplit(Array(0.75, 0.25), 123)\n```", "```py\nval gbt = new GBTClassifier()\n        .setLabelCol(\"label\")\n        .setFeaturesCol(\"features\")\n        .setMaxIter(10)\n        .setSeed(123)\n```", "```py\nval pipeline = new Pipeline()\n            .setStages(Array(featureAssembler, gbt))\n```", "```py\nval rocEvaluator = new BinaryClassificationEvaluator()\n        .setLabelCol(\"label\")\n        .setRawPredictionCol(\"rawPrediction\")\n        .setMetricName(\"areaUnderROC\")\n```", "```py\nval cv = new CrossValidator()\n        .setEstimator(pipeline)\n        .setEvaluator(rocEvaluator)\n        .setEstimatorParamMaps(paramGrid)\n        .setNumFolds(numFolds)\n        .setSeed(123)\nval cvModel = cv.fit(trainingData)\n```", "```py\nval predictions = cvModel.transform(testData)\n```", "```py\nval roc = rocEvaluator.evaluate(predictions)\n```", "```py\nval gbtModel = cvModel.bestModel.asInstanceOf[PipelineModel]\ngbtModel.save(rootDir + \"__cv__gbt_22_binary_classes_\" + System.nanoTime() / 1000000 + \".model\")\n```", "```py\nimport org.apache.spark.{ SparkConf, SparkContext }\nimport org.apache.spark.ml.{ Pipeline, PipelineModel }\n\nimport org.apache.spark.ml.classification.{ GBTClassificationModel, GBTClassifier, RandomForestClassificationModel, RandomForestClassifier}\nimport org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator, MulticlassClassificationEvaluator}\nimport org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorAssembler, VectorIndexer}\nimport org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\nimport org.apache.spark.sql.types.{DoubleType, IntegerType, StructField, StructType}\nimport org.apache.spark.sql.SparkSession\n\nobject TrainGradientBoostedTree {\n    def main(args: Array[String]): Unit = {\n        val maxBins = Seq(5, 7, 9)\n        val numFolds = 10\n        val maxIter: Seq[Int] = Seq(10)\n        val maxDepth: Seq[Int] = Seq(20)\n        val rootDir = \"output/\"\n        val spark = SparkSession\n            .builder()\n            .master(\"local[*]\")\n            .config(\"spark.sql.warehouse.dir\", \"\"/home/user/spark/\")\n            .appName(\"Bitcoin Preprocessing\")\n            .getOrCreate()\n\n        val xSchema = StructType(Array(\n            StructField(\"t0\", DoubleType, true),\n            StructField(\"t1\", DoubleType, true),\n            StructField(\"t2\", DoubleType, true),\n            StructField(\"t3\", DoubleType, true),\n            StructField(\"t4\", DoubleType, true),\n            StructField(\"t5\", DoubleType, true),\n            StructField(\"t6\", DoubleType, true),\n            StructField(\"t7\", DoubleType, true),\n            StructField(\"t8\", DoubleType, true),\n            StructField(\"t9\", DoubleType, true),\n            StructField(\"t10\", DoubleType, true),\n            StructField(\"t11\", DoubleType, true),\n            StructField(\"t12\", DoubleType, true),\n            StructField(\"t13\", DoubleType, true),\n            StructField(\"t14\", DoubleType, true),\n            StructField(\"t15\", DoubleType, true),\n            StructField(\"t16\", DoubleType, true),\n            StructField(\"t17\", DoubleType, true),\n            StructField(\"t18\", DoubleType, true),\n            StructField(\"t19\", DoubleType, true),\n            StructField(\"t20\", DoubleType, true),\n            StructField(\"t21\", DoubleType, true)))\n\n        val ySchema = StructType(Array(StructField(\"y\", DoubleType,\n        true)))\n        val x = spark.read.format(\"csv\").schema(xSchema).load(rootDir +\n        \"scala_test_x.csv\")\n        val y_tmp =\n        spark.read.format(\"csv\").schema(ySchema).load(rootDir +\n        \"scala_test_y.csv\")\n\n        import spark.implicits._\n        val y = y_tmp.withColumn(\"y\", 'y.cast(IntegerType))\n\n        import org.apache.spark.sql.functions._\n        //joining 2 separate datasets in single Spark dataframe\n        val x_id = x.withColumn(\"id\", monotonically_increasing_id())\n        val y_id = y.withColumn(\"id\", monotonically_increasing_id())\n        val data = x_id.join(y_id, \"id\")\n        val featureAssembler = new VectorAssembler()\n            .setInputCols(Array(\"t0\", \"t1\", \"t2\", \"t3\", \"t4\", \"t5\", \n                                \"t6\", \"t7\", \"t8\", \"t9\", \"t10\", \"t11\", \n                                \"t12\", \"t13\", \"t14\", \"t15\", \"t16\",\n                                \"t17\",\"t18\", \"t19\", \"t20\", \"t21\"))\n            .setOutputCol(\"features\")\n        val encodeLabel = udf[Double, String] { case \"1\" => 1.0 case\n                                                \"0\" => 0.0 }\n        val dataWithLabels = data.withColumn(\"label\",\n                                encodeLabel(data(\"y\")))\n\n        //123 is seed number to get same datasplit so we can tune\n        params\n        val Array(trainingData, testData) =\n        dataWithLabels.randomSplit(Array(0.75, 0.25), 123)\n        val gbt = new GBTClassifier()\n            .setLabelCol(\"label\")\n            .setFeaturesCol(\"features\")\n            .setMaxIter(10)\n            .setSeed(123)\n        val pipeline = new Pipeline()\n            .setStages(Array(featureAssembler, gbt))\n        // ***********************************************************\n        println(\"Preparing K-fold Cross Validation and Grid Search\")\n        // ***********************************************************\n        val paramGrid = new ParamGridBuilder()\n            .addGrid(gbt.maxIter, maxIter)\n            .addGrid(gbt.maxDepth, maxDepth)\n            .addGrid(gbt.maxBins, maxBins)\n            .build()\n        val cv = new CrossValidator()\n            .setEstimator(pipeline)\n            .setEvaluator(new BinaryClassificationEvaluator())\n            .setEstimatorParamMaps(paramGrid)\n            .setNumFolds(numFolds)\n            .setSeed(123)\n        // ************************************************************\n        println(\"Training model with GradientBoostedTrees algorithm\")\n        // ************************************************************\n        // Train model. This also runs the indexers.\n        val cvModel = cv.fit(trainingData)\n        cvModel.save(rootDir + \"cvGBT_22_binary_classes_\" +\n        System.nanoTime() / 1000000 + \".model\")\n        println(\"Evaluating model on train and test data and\n        calculating RMSE\")\n        // **********************************************************************\n        // Make a sample prediction\n        val predictions = cvModel.transform(testData)\n\n        // Select (prediction, true label) and compute test error.\n        val rocEvaluator = new BinaryClassificationEvaluator()\n            .setLabelCol(\"label\")\n            .setRawPredictionCol(\"rawPrediction\")\n            .setMetricName(\"areaUnderROC\")\n        val roc = rocEvaluator.evaluate(predictions)\n        val prEvaluator = new BinaryClassificationEvaluator()\n            .setLabelCol(\"label\")\n            .setRawPredictionCol(\"rawPrediction\")\n            .setMetricName(\"areaUnderPR\")\n        val pr = prEvaluator.evaluate(predictions)\n        val gbtModel = cvModel.bestModel.asInstanceOf[PipelineModel]\n        gbtModel.save(rootDir + \"__cv__gbt_22_binary_classes_\" +\n        System.nanoTime()/1000000 +\".model\")\n\n        println(\"Area under ROC curve = \" + roc)\n        println(\"Area under PR curve= \" + pr)\n        println(predictions.select().show(1))\n        spark.stop()\n    }\n}\n```", "```py\n>>> \nArea under ROC curve = 0.6045355104779828\nArea under PR curve= 0.3823834607704922\n```", "```py\nclass JobModule extends AbstractModule with AkkaGuiceSupport {\n    def configure(): Unit = {\n        //configuring launch of price-fetching Actor\n        bindActor[SchedulerActor](\"scheduler-actor\")\n        bind(classOf[Scheduler]).asEagerSingleton()\n    }\n}\n```", "```py\nplay.modules.enabled += \"modules.jobs.JobModule\"\n```", "```py\nclass Scheduler @Inject()\n    (val system: ActorSystem, @Named(\"scheduler-actor\") val schedulerActor: ActorRef, configuration:     Configuration)(implicit ec: ExecutionContext) {\n    //constants.frequency is set in conf/application.conf file\n    val frequency = configuration.getInt(\"constants.frequency\").get\n    var actor = system.scheduler.schedule(\n    0.microseconds, //initial delay: whether execution starts immediately after app launch\n    frequency.seconds, //every X seconds, specified above\n    schedulerActor,\n    \"update\")\n}\n```", "```py\ndef constructUrl(exchange: String): String =\n{\n \"https://min-api.cryptocompare.com/data/histominute?fsym=BTC&tsym=USD&limit=23&aggregate=1&e=\" + exchange\n }\n```", "```py\nfinal val predictionActor = system.actorOf(Props(new PredictionActor(configuration, db)))\nfinal val traderActor = system.actorOf(Props(new TraderActor(ws)))\n```", "```py\noverride def receive: Receive = {\n```", "```py\ncase _ =>\n    val futureResponse=restClient.getPayload(constructUrl(exchange))\n```", "```py\ncase class CryptoCompareResponse(Response: String, Type: Int, Aggregated: Boolean, Data: List[OHLC],     FirstValueInArray: Boolean, TimeTo: Long,TimeFrom: Long)\n```", "```py\nobject CryptoCompareResponse {\n implicit val cryptoCompareResponseReads = Json.reads[CryptoCompareResponse]\n            }\n```", "```py\ncase class OHLC(time: Long, open: Double, \n                high: Double, \n                low: Double, \n                close: Double, \n                volumefrom: Double, \n                volumeto: Double)\n```", "```py\nval batch = BatchSql(\n        \"\"\"|INSERT INTO PRICE_STAGING(TIMESTAMP,EXCHANGE,PRICE_OPEN,PRICE_CLOSED,VOLUME_BTC,             \n            VOLUME_USD)| VALUES({timestamp}, {exchange}, {priceOpen}, {priceClosed}, {volumeBTC},                   {volumeUSD})\"\"\".stripMargin,transformedPriceDta.head,transformedPriceDta.tail:_*)\nval res: Array[Int] = batch.execute() // array of update count\nval reInsert = SQL(\n        \"\"\"\n          |INSERT INTO PRICE(TIMESTAMP, EXCHANGE, PRICE_OPEN, PRICE_CLOSED, VOLUME_BTC, VOLUME_USD)\n          |SELECT  TIMESTAMP, EXCHANGE, PRICE_OPEN, PRICE_CLOSED, VOLUME_BTC, VOLUME_USD\n          |FROM PRICE_STAGING AS s\n          |WHERE NOT EXISTS (\n          |SELECT *\n          |FROM PRICE As t\n          |WHERE t.TIMESTAMP = s.TIMESTAMP\n          |)\n        \"\"\".stripMargin).execute()\n      Logger.debug(\"reinsert \" + reInsert)\n```", "```py\n(predictionActor ? CryptoCompareDTOToPredictionModelTransformer.tranform(cryptoCompareResponse)).mapTo[CurrentDataWithShortTermPrediction].map {\n```", "```py\npredictedWithCurrent =>\ntraderActor ! predictedWithCurrent}\n```", "```py\n# This is the main configuration file for the application.\n# Secret key\n# The secret key is used to secure cryptographics functions.\n# If you deploy your application to several instances be sure to use the same key!\napplication.secret=\"%APPLICATION_SECRET%\"\n# The application languages\napplication.langs=\"en\"\n# Global object class\n# Define the Global object class for this application.\n# Default to Global in the root package.sb\n# application.global=Global\n# Router\n# Define the Router object to use for this application.\n# This router will be looked up first when the application is starting up,\n# so make sure this is the entry point.\n# Furthermore, it's assumed your route file is named properly.\n# So for an application router like `my.application.Router`,\n# you may need to define a router file `conf/my.application.routes`.\n# Default to Routes in the root package (and conf/routes)\n# application.router=my.application.Routes\n# Database configuration\n# You can declare as many datasources as you want.\n# By convention, the default datasource is named `default`\nrootDir = \"<path>/Bitcoin_price_prediction/\"\ndb.default.driver = org.h2.Driver\ndb.default.url = \"jdbc:h2: \"<path>/Bitcoin_price_prediction/DataBase\"\ndb.default.user = user\ndb.default.password = \"\"\nplay.evolutions.db.default.autoApply = true\n# Evolutions\n# You can disable evolutions if needed\n# evolutionplugin=disabled\n# Logger\n# You can also configure logback (http://logback.qos.ch/),\n# by providing an application-logger.xml file in the conf directory.\n# Root logger:\nlogger.root=ERROR\n# Logger used by the framework:\nlogger.play=INFO\n# Logger provided to your application:\nlogger.application=DEBUG\n#Enable JobModule to run scheduler\nplay.modules.enabled += \"modules.jobs.JobModule\"\n#Frequency in seconds to run job. Might make sense to put 30 seconds, for recent data\nconstants.frequency = 30\nml.model_version = \"gbt_22_binary_classes_32660767.model\"\n```", "```py\nrootDir = \"<path>/ Bitcoin_price_prediction\"\n```", "```py\ndb.default.url = \"jdbc:h2: \"<path>/Bitcoin_price_prediction/DataBase\"\n```", "```py\nml.model_version = \"gbt_22_binary_classes_32660767.model\"\n```", "```py\nobject PredictionActor {\n    def props = Props[PredictionActor]\n    case class PriceData(timeFrom: Long,\n                        timeTo: Long, \n                        priceDelta: (Long, Double)*)\n        }\n```", "```py\nval models: List[(Transformer, String)] =\n            SubDirectoryRetriever.getListOfSubDirectories(modelFolder)\n            .map(modelMap => (PipelineModel.load(modelMap(\"path\")),modelMap(\"modelName\")))\n        .toList\n```", "```py\noverride def receive: Receive = {\n    case data: PriceData =>\n        val priceData = shrinkData(data, 1, 22)\n        val (predictedLabelForUnknownTimestamp, details) =             \n            predictionService.predictPriceDeltaLabel(priceData,productionModel)\n```", "```py\nsender() ! CurrentDataWithShortTermPrediction(predictedLabelForUnknownTimestamp, data)\n```", "```py\nmodels.foreach { mlModel =>\n    val (predictedLabel, details) =predictionService.predictPriceDeltaLabel(shrinkData(data, 0, 21),     mlModel._1)\n    val actualDeltaPoint = data.priceDelta.toList(22)\n```", "```py\nstoreShortTermBinaryPredictionIntoDB( mlModel._2, actualDeltaPoint._1,\npredictedLabel, actualDeltaPoint._2)\n```", "```py\noverride def receive: Receive = {\n    case data: CurrentDataWithShortTermPrediction =>\n        Logger.debug(\"received short-term prediction\" + data)\n        data.prediction match {\n            case \"0\" => notifySellShortTerm()\n            case \"1\" => notifyHoldShortTerm()\n    }\n```", "```py\noverride def predictPriceDeltaLabel(priceData: PriceData, mlModel: org.apache.spark.ml.Transformer): (String, Row) = {\n        val df = transformPriceData(priceData)\n        val prediction = mlModel.transform(df)\n        val predictionData = prediction.select(\"probability\", \"prediction\", \"rawPrediction\").head()\n        (predictionData.get(1).asInstanceOf[Double].toInt.toString, predictionData)\n        }\n```", "```py\nSELECT MODEL, SUM(TPR) as TPR, SUM(FPR) as FPR, SUM(TNR) as TNR, \n    SUM(FNR) as FNR, COUNT(*) as TOTAL FROM (SELECT *,\n    case when PREDICTED_LABEL='1' and ACTUAL_PRICE_DELTA > 0\n        then 1 else 0 end as TPR,\n    case when PREDICTED_LABEL='1' and ACTUAL_PRICE_DELTA <=0\n        then 1 else 0 end as FPR,\n    case when PREDICTED_LABEL='0' and ACTUAL_PRICE_DELTA <=0\n        then 1 else 0 end as TNR,\n    case when PREDICTED_LABEL='0' and ACTUAL_PRICE_DELTA > 0\n        then 1 else 0 end as FNR\nFROM SHORT_TERM_PREDICTION_BINARY)\nGROUP BY MODEL\n```", "```py\nlibraryDependencies ++= Seq(jdbc, evolutions,\n \"com.typesafe.play\" %% \"anorm\" % \"2.5.1\",\n cache, ws, specs2 % Test, ws)\n\nunmanagedResourceDirectories in Test <+= baseDirectory(_ / \"target/web/public/test\")\nresolvers += \"scalaz-bintray\" at \"https://dl.bintray.com/scalaz/releases\"\n\nresolvers ++= Seq(\n     \"apache-snapshots\" at \"http://repository.apache.org/snapshots/\")\n    routesGenerator := InjectedRoutesGenerator\n    val sparkVersion = \"2.2.0\"\n    libraryDependencies += \"org.apache.spark\" %% \"spark-mllib\" % sparkVersion\n    libraryDependencies += \"org.apache.hadoop\" % \"hadoop-mapreduce-client-core\" % \"2.7.2\"\n    libraryDependencies += \"org.apache.hadoop\" % \"hadoop-common\" % \"2.7.2\"\n    libraryDependencies += \"commons-io\" % \"commons-io\" % \"2.4\"\n    libraryDependencies += \"org.codehaus.janino\" % \"janino\" % \"3.0.7\" //fixing     \"java.lang.ClassNotFoundException: de.unkrig.jdisasm.Disassembler\" exception\n\n    libraryDependencies ++= Seq(\n     \"com.typesafe.slick\" %% \"slick\" % \"3.1.1\",\n     \"org.slf4j\" % \"slf4j-nop\" % \"1.6.4\"\n)\n```"]