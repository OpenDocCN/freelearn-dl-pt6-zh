["```py\nimport theano\nimport theano.tensor as T\n\n```", "```py\nx = T.dscalar('x')\n\n```", "```py\ny = x ** 2\ndy = T.grad(y, x)\n\n```", "```py\nf = theano.function([x], dy)\n\n```", "```py\nprint f(1)  # => 2.0\nprint f(2)  # => 4.0\n\n```", "```py\nx = T.dvector('x')\ny = T.sum(x ** 2)\n\n```", "```py\nW = theano.shared(value=W_values, name='W', borrow=True)\nb = theano.shared(value=b_values, name='b', borrow=True)\n```", "```py\nlin_output = T.dot(input, self.W) + self.b\nself.output = (\n    lin_output if activation is None\n    else activation(lin_output)\n)\n```", "```py\nself.W = theano.shared(\n    value=numpy.zeros(\n        (n_in, n_out),\n        dtype=theano.config.floatX\n    ),\n    name='W',\n    borrow=True\n)\n\nself.b = theano.shared(\n    value=numpy.zeros(\n        (n_out,),\n        dtype=theano.config.floatX\n    ),\n    name='b',\n    borrow=True\n)\n```", "```py\nself.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n```", "```py\nself.y_pred = T.argmax(self.p_y_given_x, axis=1)\n```", "```py\ndef negative_log_likelihood(self, y):\n    return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n```", "```py\nclass MLP(object):\n    def __init__(self, rng, input, n_in, n_hidden, n_out):\n        # self.hiddenLayer = HiddenLayer(...)\n        # self.logRegressionLayer = LogisticRegression(...)\n\n        # L1 norm\n        self.L1 = (\n             abs(self.hiddenLayer.W).sum()\n             + abs(self.logRegressionLayer.W).sum()\n        )\n\n        # square of L2 norm\n        self.L2_sqr = (\n           (self.hiddenLayer.W ** 2).sum()\n            + (self.logRegressionLayer.W ** 2).sum()\n        )\n\n        # negative log likelihood of MLP\n        self.negative_log_likelihood = (\n           self.logRegressionLayer.negative_log_likelihood\n        )\n\n        # the parameters of the model\n        self.params = self.hiddenLayer.params + self.logRegressionLayer.params\n```", "```py\ncost = (\n    classifier.negative_log_likelihood(y)\n    + L1_reg * classifier.L1\n    + L2_reg * classifier.L2_sqr\n)\n```", "```py\ngparams = [T.grad(cost, param) for param in classifier.params]\n```", "```py\nupdates = [\n    (param, param - learning_rate * gparam)\n    for param, gparam in zip(classifier.params, gparams)\n]\n```", "```py\ntrain_model = theano.function(\n    inputs=[index],\n    outputs=cost,\n    updates=updates,\n    givens={\n        x: train_set_x[index * batch_size: (index + 1) * batch_size],\n        y: train_set_y[index * batch_size: (index + 1) * batch_size]\n    }\n)\n```", "```py\nwhile (epoch < n_epochs) and (not done_looping):\n    epoch = epoch + 1\n        for minibatch_index in xrange(n_train_batches):\n           minibatch_avg_cost = train_model(minibatch_index)\n```", "```py\nimport tensorflow as tf\n\n```", "```py\nimport input_data\n\n```", "```py\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n```", "```py\nx_placeholder = tf.placeholder(\"float\", [None, 784])\nlabel_placeholder = tf.placeholder(\"float\", [None, 10])\n```", "```py\ndef inference(x_placeholder):\n\n    W = tf.Variable(tf.zeros([784, 10]))\n    b = tf.Variable(tf.zeros([10]))\n\n    y = tf.nn.softmax(tf.matmul(x_placeholder, W) + b)\n\n    return y\n```", "```py\ndef loss(y, label_placeholder):\n    cross_entropy = - tf.reduce_sum(label_placeholder * tf.log(y))\n\n    return cross_entropy\n```", "```py\ndef training(loss):\n    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n\n    return train_step\n```", "```py\ninit = tf.initialize_all_variables()\nsess.run(init)\n```", "```py\nfor i in range(1000):\n    batch_xs, batch_ys = mnist.train.next_batch(100)\n    feed_dict = {x_placeholder: batch_xs, label_placeholder: batch_ys}\n\n    sess.run(train_step, feed_dict=feed_dict)\n```", "```py\ndef res(y, label_placeholder, feed_dict):\n    correct_prediction = tf.equal(\n        tf.argmax(y, 1), tf.argmax(label_placeholder, 1)\n    )\n\n    accuracy = tf.reduce_mean(\n        tf.cast(correct_prediction, \"float\")\n    )\n\n   print sess.run(accuracy, feed_dict=feed_dict)\n```", "```py\n$ tensorboard --logdir=<ABOSOLUTE_PATH>/data\n\n```", "```py\nx_placeholder = tf.placeholder(\"float\", [None, 784], name=\"input\")\nlabel_placeholder = tf.placeholder(\"float\", [None, 10], name=\"label\")\n```", "```py\ndef inference(x_placeholder):\n    with tf.name_scope('inference') as scope:\n        W = tf.Variable(tf.zeros([784, 10]), name=\"W\")\n        b = tf.Variable(tf.zeros([10]), name=\"b\")\n\n        y = tf.nn.softmax(tf.matmul(x_placeholder, W) + b)\n\n    return y\n\ndef loss(y, label_placeholder):\n    with tf.name_scope('loss') as scope:\n        cross_entropy = - tf.reduce_sum(label_placeholder * tf.log(y))\n\n        tf.scalar_summary(\"Cross Entropy\", cross_entropy)\n\n    return cross_entropy\n\ndef training(loss):\n    with tf.name_scope('training') as scope:\n        train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n\n    return train_step\n```", "```py\nsummary_step = tf.merge_all_summaries()\ninit = tf.initialize_all_variables()\n\nsummary_writer = tf.train.SummaryWriter('data', graph_def=sess.graph_def)\n```", "```py\nsummary = sess.run(summary_step, feed_dict=feed_dict)\nsummary_writer.add_summary(summary, i)\n```", "```py\n$ ./data/mnist/get_mnist.sh\n$ ./examples/mnist/train_lenet.sh\n\n```", "```py\n#!/usr/bin/env sh\n\n./build/tools/caffe train --solver=examples/mnist/lenet_solver.prototxt\n```", "```py\nlayer {\n  name: \"conv1\"\n  type: \"Convolution\"\n  bottom: \"data\"\n  top: \"conv1\"\n  param {\n    lr_mult: 1\n  }\n  param {\n    lr_mult: 2\n  }\n  convolution_param {\n    num_output: 20\n    kernel_size: 5\n    stride: 1\n    weight_filler {\n      type: \"xavier\"\n    }\n    bias_filler {\n      type: \"constant\"\n    }\n  }\n}\n```"]