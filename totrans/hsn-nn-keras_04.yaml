- en: Signal Processing - Data Analysis with Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having acquired substantial knowledge on neural networks, we are now ready to
    perform our first operation using them. We will start with processing signals,
    and see how a neural network is fed data. You will be mesmerized at how increasing
    the levels and complexity of neurons can actually make a problem look simple.
    We will then look at how language can be processed. We will make several predictions
    using datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Processing signals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Images as numbers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feeding a neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of tensors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compiling the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing weight regularization in Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weight regularization experiments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing dropout regularization in Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The internet movie reviews dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plotting a single training instance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One-hot encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vectorizing features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vectorizing labels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Callbacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing model predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature-wise normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross validation with the scikit-learn API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing signals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There may only be four fundamental forces in our universe, but they are all
    signals. By signal, we mean any kind of feature representations we may have of
    a real-world phenomenon. Our visual world, for example, is full of signals that
    indicate motion, color, and shapes. These are very dynamic signals, and it is
    a miracle that biology is able to process these stimuli so accurately, even if
    we do say so ourselves. Of course, in the grander scheme of things, realizing
    that nature has had hundreds of millions of years to perfect this recipe may humble
    us, if only a little. For now, we can admire the marvel that is the human visual
    cortex, which is equipped with 140 million densely interconnected neurons. In
    fact, an entire series of layers (V1 – V5) exist through which information propagates
    as we engage in progressively more complex image processing tasks. The eye itself,
    using rods and cones to detect different patterns of light intensity and colors,
    does an excellent job of piecing together electromagnetic radiation and converting
    it into electrical impulses through photo transduction.
  prefs: []
  type: TYPE_NORMAL
- en: When we look at an image, our visual cortex is actually interpreting the specific
    configuration of electromagnetic signals that the eye is converting into electrical
    signals and feeding it. When we listen to music, our eardrum, or myringa, simply
    converts and amplifies a successive pattern of vibrational signals so that our
    auditory cortex may process it. Indeed, it appears that the neural mechanisms
    in the brain are extremely efficient at abstracting and representing patterns
    that are present in different real-world signals. In fact, neuroscientists have
    even found that some mammalian brains have the capacity to be rewired in a manner
    that permits different cortices to process types of data that they were originally
    never intended to encounter. Most notably, scientists found that rewiring the
    auditory cortex of ferrets allowed these creatures to process visual signals from
    the auditory regions of the brain, allowing them to *see* using very different
    neurons that they previously employed for the task of audition. Many scientists
    cite such studies to put forth the case that the brain may be using a master algorithm,
    which is capable of handling any form of data, and turning it into efficient representations
    of the world around it.
  prefs: []
  type: TYPE_NORMAL
- en: As intriguing as this is, it naturally raises a thousand more questions about
    neural learning than it answers, and we sadly do not have the scope to address
    all of them in this book. Suffice it to say, whatever algorithm—or sets of algorithms—that
    lets our brain achieve such efficient representations of the world around us,
    are naturally of great interest to neurologists, deep learning engineers, and
    the rest of the scientific community alike.
  prefs: []
  type: TYPE_NORMAL
- en: Representational learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw earlier with our perceptron experiments on the TensorFlow Playground,
    sets of artificial neurons seem to be capable of learning fairly simple patterns.
    This is nothing remotely close to the sort of complex representations we humans
    can perform and wish to predict. However, we can see that, even in their nascent
    simplicity, these networks seem to be able to adapt to the sort of data we provide
    them with, at times even outperforming other statistical predictive models. So,
    what's going on here that is so different than previous approaches to teach machines
    to do things for us?
  prefs: []
  type: TYPE_NORMAL
- en: It can be very useful to teach a computer what skin cancer looks like, simply
    by showing it the vast number of medically relevant features that we may have.
    Indeed, this is what our approach has been toward machines thus far. We would
    hand-engineer features so that our machines could easily digest them and generate
    relevant predictions. But why stop there? Why not just show the computer what
    skin cancer actually *looks* like? Why not show it millions of images and let
    *it* figure out what is relevant? Indeed, that's exactly what we try to do when
    we speak of deep learning. As opposed to traditional **Machine Learning** (**ML**)
    algorithms, where we represent the data in an explicitly processed representation
    for the machine to learn, we take a different approach with neural networks. Here,
    what we actually wish to achieve is for the network to learn these representations
    on its own.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following diagram, a network achieves this by learning simple
    representations and using them to define more and more complex representations
    in successive layers, until the ultimate layer learns to represent output classes
    accurately:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/884630bc-15a4-4cfe-980e-51bbe6ed2471.png)'
  prefs: []
  type: TYPE_IMG
- en: This approach, as it turns out, can be quite useful for teaching your computer
    to detect complex movement patterns and facial expressions, just as we humans
    do. Say you want it to accept packages on your behalf when you're away, or perhaps
    detect any potential robbers trying to break in to your house. Similarly, what
    if we wanted our computer to schedule our appointments, find potentially lucrative
    stocks on the market, and keep us up to date according to what we find interesting?
    Doing so involves processing complex image, video, audio, text, and time-series
    data, all of which come in complex dimensional representations, and cannot be
    modeled by just a few neurons. So, how do we work with the neural learning system,
    akin to what we saw in the last chapter? How do we make neural networks learn
    the complex and hierarchical patterns in eyes, faces, and other real-world objects?
    Well, the obvious answer is that we make them bigger. But, as we will see, this
    brings in complexities of its own. Long story short, the more learnable parameters
    you put in a network, the higher the chance that it will memorize some random
    patterns, and hence will not generalize well. Ideally, you want a configuration
    of neurons that perfectly fits the learning job at hand, but this is almost impossible
    to determine a priori without performing experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding random memorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another answer can be to manipulate not only the overall number of neurons,
    but also the degree of interconnectivity among those neurons. We can do this through
    techniques such as *dropout regularization* and *weighted parameter*, as we will
    see soon enough. So far, we have already seen the various computations that can
    be performed through each neuron as data propagates through a network. We also
    saw how the brain leverages hundreds of millions of densely interconnected neurons
    to get the job done. But, naturally, we can't just scale up our networks by arbitrarily
    adding more and more neurons. Long story short, simulating a neural structure
    close to the brain is likely to require thousands of **petaflops** (a unit of
    computing speed equal to one thousand million million (10^(15)) floating-point
    operations per second) of computing power. Maybe this will be possible in the
    near future, with the aforementioned paradigm of massively parallelized computing,
    along with other advances in software and hardware technologies. For now, though,
    we have to think of clever ways to train our network so that it can find the most
    efficient representations without wasting precious computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: Representing signals with numbers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will see how we can layer sequences of neurons to progressively
    represent more and more complex patterns. We will also see how concepts such as
    regularization and batched learning are essential in getting the most out of a
    training session. We will learn to process different types of real-world data
    in the form of images, texts, and time-series dependent information.
  prefs: []
  type: TYPE_NORMAL
- en: Images as numbers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For tasks such as these, we need deep networks with multiple hidden layers if
    we are hoping to learn any representative features for our output classes. We
    also need a nice dataset to practice our understanding and familiarize ourselves
    with the tools we will be using to design our intelligent systems. Hence, we come
    to our first hands-on neural network task as we introduce ourselves to the concepts
    of computer vision, image processing, and hierarchical representation learning.
    Our task at hand is to teach computers to read numbers not as 0 and 1s, as they
    already do, but more in the manner of how we would read digits that are composed
    by our own kin. We are speaking of handwritten digits, and for this task, we will
    be using the iconic MNIST dataset, the true *hello world* of deep learning datasets.
    For our first example, there are good theoretical and practical reasons behind
    our choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'From a theoretical perspective, we need to understand how we can use layer
    neurons to progressively learn more complex patterns, like our own brain does.
    Since our brain has had about 2,000 to 2,500 years worth of training data, it
    has gotten quite good at identifying complex symbols such as handwritten digits.
    In fact, we normally perceive this as an absolutely effortless task, since we
    learn how to distinguish between such symbols from as early as preschool. But
    this is actually quite a daunting task. Consider the vast variations in how each
    of these digits may be written by different humans, and yet our brains are still
    able to classify these digits, as if it were much ado about nothing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc8d8dcd-ce9c-4f0e-a1d9-180cbafdab03.png)'
  prefs: []
  type: TYPE_IMG
- en: While exhaustively coding explicit rules would drive any programmer insane,
    as we look at the preceding image, our own brain intuitively notice some patterns
    in the data. For example, it picks up on how both **2** and **3** have a half-loop
    at the top of them, and how **1**, **4**, and **7** have a straight downward line.
    It also perceives how a **4** is actually one downward line, one semi-downward
    line, and another horizontal line in between the others. Due to this, we are able
    to easily break down a complex pattern into smaller patterns. This is specifically
    easy to do with handwritten digits, as we just saw. Therefore, our task will be
    to see how we can construct a deep neural network and hope for each of our neurons
    to capture simple patterns from our data, such as line segments, and then progressively
    construct more complex patterns in deeper layers using the simple patterns we
    learned in the previous layers. We will do this to learn about the accurate combinations
    of representations that correspond to our output classes.
  prefs: []
  type: TYPE_NORMAL
- en: Practically speaking, the MNIST dataset has been studied for about two decades
    by many pioneers in the field of deep learning. We have gained a good wealth of
    knowledge out of this dataset, making it ideal for exploring concepts such as
    layer representations, regularization, and overfitting, among others. As soon
    as we understand how to train and test a neural network, we can repurpose it for
    more exciting tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Feeding a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Essentially, all of the data that enters and propagates through the network
    is represented by a mathematical structure known as a **tensor**. This applies
    to audio data, images, video, and any other data we can think of, to feed our
    data-hungry network. In mathematics ([https://en.wikipedia.org/wiki/Mathematics](https://en.wikipedia.org/wiki/Mathematics)),
    a tensor is defined as an abstract and arbitrary geometric ([https://en.wikipedia.org/wiki/Geometry](https://en.wikipedia.org/wiki/Geometry))
    entity that maps aggregations of vectors in a multi-linear ([https://en.wikipedia.org/wiki/Linear_map](https://en.wikipedia.org/wiki/Linear_map))
    manner to a resulting tensor. In fact, vectors and scalars are considered simpler
    forms of tensors. In Python, tensors are defined with three specific properties,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rank**: Specifically, this denotes the number axes. A matrix is said to have
    the rank 2, as it represents a two-dimensional tensor. In Python libraries, this
    is often indicated as `ndim`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shape**: The shape of a tensor can be checked by calling the shape property
    on a NumPy *n*-dimensional array (which is how a tensor is represented in Python).
    This will return a tuple of integers, indicating the number of dimensions a tensor
    has along each axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Content**: This refers to the type of data that''s stored in the tensor,
    and can be checked by calling the `type()` method on a tensor of interest. This
    will return data types such as float32, uint8, float64, and so on, except for
    string values, which are first converted into vector representations before being
    represented as a tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is a tensor graph. Don''t worry about the complex diagram—we
    will look at what it means later:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e3eaf25-2ec1-4b81-b958-4b51b38399a4.png)'
  prefs: []
  type: TYPE_IMG
- en: Examples of tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The illustration we previously saw was that of a three dimensional tensor,
    yet tensors can appear in many forms. In the following section, we will overview
    some tensors of different ranks, starting with a tensor of rank zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalar**: Values simply denote a single numeric value on its own. This can
    also be described as a tensor of dimension 0\. An example of this is processing
    a single grayscale pixel of an image through a network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vector**: A bunch of scalars or an array of numbers is called a **vector**,
    or a tensor of rank 1\. A 1D tensor is said to have exactly one axis. An example
    of this is processing a single flattened image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Matrix**: An array of vectors is a matrix, or 2D tensor. A matrix has two
    axes (often referred to as rows and columns). You can visually interpret a matrix
    as a rectangular grid of numbers. An example of this is processing a single grayscale
    image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Three-dimensional tensor**: By packing several matrices into a new array,
    you get a 3D tensor, which is visually interpretable as a cube of numbers. An
    example of this is processing a dataset of grayscale images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Four-dimensional tensor**: By packing 3D tensors in an array, you can create
    a 4D tensor, and so on. An example of this is processing a dataset of colored
    images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Five-dimensional tensor**: These are created by packing 4D tensors in an
    array. An example of this is processing a dataset of videos.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality of data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, consider the tensor of a shape (400, 600, 3). This is a common input shape
    that refers to a three-dimensional tensor that's used to represent a color image
    of 400 x 600 pixels. Since the MNIST dataset uses binary grayscale pixel values,
    we only deal with matrices of 28 x 28 pixels when representing an image. Here,
    each image is a tensor of dimension two, and the whole dataset can be represented
    by a tensor of dimension three. In a color image, each pixel value actually has
    three numbers, representing the amount of red, green, and blue light intensity
    represented by that pixel. Hence, with colored images, the two-dimensional matrices
    that are used to represent an image now scale up to three-dimensional tensors.
    Such a tensor is denoted by a tuple of (*x*, *y*, 3), where *x* and *y* represent
    the pixel dimensions of the image. Hence, a dataset of color images can be represented
    by a four-dimensional tensor, as we will see in later examples. For now, it is
    useful to know that we can use NumPy *n*-dimensional arrays to represent, reshape,
    manipulate, and store tensors in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Making some imports
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, let''s get started, shall we? We will perform some simple experiments by
    leveraging all the concepts we have learned about in the previous chapters, and
    perhaps also encounter some new ones while on the job. We will use Keras, as well
    as the TensorFlow API, allowing us to also explore the eager execution paradigm.
    Our first task will be to implement a simple version of the multi-layered perceptron.
    This version is known as the **feedforward neural network**, and is a basic architecture
    that we can use to further explore some simple image classification examples.
    Obeying customary deep learning tradition, we will begin our first classification
    task by using the MNIST dataset for handwritten digits. This dataset has 70,000
    grayscale images of digits between 0 and 9\. The large size of this dataset is
    ideal, as machines require about 5,000 images per class to be able to come close
    to human-level performance at visual recognition tasks. The following code imports
    the libraries we will be using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Keras's sequential API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you may well know, each Python library often comes with a core data abstraction
    that defines the data structure that the library is able to manipulate to perform
    computations. NumPy has its arrays, while pandas has its DataFrames. The core
    data structure of Keras is a model, which is essentially a manner to organize
    layers of interconnected neurons. We will start with the simplest type of model:
    the sequential model ([https://keras.io/getting-started/sequential-model-guide/](https://keras.io/getting-started/sequential-model-guide/)).
    This is available as a linear stack of layers through the sequential API. More
    complex architectures also allow us to review the functional API, which is used
    to build custom layers. We will cover these later. The following code imports
    the sequential model, as well as some of the layers we will be using to build
    our first network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Loading the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s load in the data and split it up. Thankfully, MNIST is one of the
    core datasets that''s already implemented in Keras, allowing a nice one-liner
    import, which also lets us split up our data in training and test sets. Of course,
    real-world data is not that easy to port and split up. A lot of useful tools exist
    for this purpose in `Keras.utils`, which we will cover briefly later, but also
    encourage you to explore. Alternatively, other **ML** libraries such as scikit-learn
    come with some handy tools (such as `train_test_split`, `MinMaxScaler`, and `normalizer`,
    to name a few methods), which, as their names indicate, let you split up, scale,
    and normalize your data as often required to optimize neural network training.
    Let''s import and load the datasets, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Checking the dimensions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we need to check out what our data looks like. We will do this by checking
    its type, then shape, and finally by plotting our individual observations using
    `matplotlib.pyplot`, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Plotting the points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This will plot a figure similar to what''s shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc711508-0aac-4fb2-8243-f1056e15fef0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, our training set has 60,000 images, with each image represented
    by a 28 x 28 matrix. When we represent our whole dataset, we are just representing
    a tensor of three dimensions (60,000 x 28 x 28). Now, let''s rescale our pixel
    values, which usually lie between 0 and 225\. Rescaling these values to values
    between 0 and 1 makes it a lot easier for our network to perform computations
    and learn predictive features. We encourage you to carry out experiments with
    and without normalization so that you can assess the difference in predictive
    power:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following plot is acquired:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/369416be-0620-432c-8a0f-b9f1e2d10161.png)'
  prefs: []
  type: TYPE_IMG
- en: Building a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we can move on and build our predictive model. But before jumping into the
    interesting code, we must know the theory that surrounds a few important things.
  prefs: []
  type: TYPE_NORMAL
- en: Introducting Keras layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The core building blocks of a neural network model in Keras is its layers.
    Layers are basically data-processing filters that *contort* the data they are
    fed into more useful representations. As we will see, prominent architectures
    of neural networks mostly vary in the manner in which layers are designed, and
    the interconnection of neurons among them. The inventor of Keras, Francois Chollet,
    describes this architecture as performing a *progressive distillation* on our
    data. Let''s see how this works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We define our model by initializing an instance of a blank model with no layers.
    Then, we add our first layer, which always expects an input dimension corresponding
    to the size of the data you want it to ingest. In our case, we want the model
    to ingest sets of 28 x 28 pixels, as we defined previously. The extra comma we
    added refers to how many examples the network will see at a time, as we will soon
    see. We also call the `Flatten()` method on our input matrix. All this does is
    convert each 28 x 28 image matrix into a single vector of 784-pixel values, each
    corresponding to its own input neuron.
  prefs: []
  type: TYPE_NORMAL
- en: We continue adding the layers until we get to our output layer, which has a
    number of output neurons corresponding to the number of our output classes—in
    this case, the 10 digits between 0 and 9\. Do note that only the input layer needs
    to specify an input dimension of data entering it, as the progressive hidden layers
    are able to perform automatic shape inference (and only the first, because the
    following layers can do automatic shape inference).
  prefs: []
  type: TYPE_NORMAL
- en: Initializing weights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We also have the option to initialize the neurons on each layer with specific
    weights. This is not a prerequisite, as they will be automatically initialized
    with small random numbers if not specified otherwise. Weight initialization practices
    are actually a whole separate sub-field of study in neural networks. It is prominently
    noted that the careful initialization of the network can significantly speed up
    the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the `kernel_initializer` and `bais_initializer` parameters to set
    both the weights and biases of each layer, respectively. Remember that these very
    weights will represent the knowledge that''s acquired by our network, which is
    why ideal initializations can significantly boost its learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'A comprehensive review of the different parameter values is beyond the scope
    of this chapter. We may encounter some use cases where tweaking these parameters
    is beneficial later on (refer to chapter optimization). Some values for the `kernel_initializer`
    parameter include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`glorot_uniform`: The weights are drawn from samples of uniform distributions
    between `-limit` and `limit`. Here, the term `limit` is defined as `sqrt(6 / (fan_in
    + fan_out))`. The term `fan_in` simply denotes the number of input units in the
    weight tensor, while `fan_out` is the number of output units in the weight tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`random_uniform`: The weights are randomly initialized with small uniform values
    ranging between -0.05 and 0.05.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`random_normal`: The weights are initialized for obeying a Gaussian distribution[1],
    with a mean of 0 and a standard deviation of 0.05.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`zero`: The layer weights are initialized at zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras activations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the moment, our network is composed of a flattened input layer, followed
    by a sequence of two dense layers, which are fully connected layers of neurons.
    The first two layers employ a **Rectified Linear Unit** (**ReLU**) activation
    function, which plots out a bit differently than the sigmoid we saw in [Chapter
    2](e8898caf-ebe4-4c1d-b262-5b814276be04.xhtml), *A* *Deeper Dive into Neural Networks*.
    In the following diagram, you can see how some of the different activation functions
    that are provided by Keras plot out. Remember, picking between them requires an
    intuitive understanding of the possible decision boundaries that may help with
    or hinder the partitioning your feature space. Using the appropriate activation
    function in conjunction with ideally initialized biases can be of paramount importance
    in some scenarios, but trivial in others. It is always advisable to experiment,
    leaving no stone unturned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17d76c00-abee-4ac8-96d2-e2378f0f609b.png)'
  prefs: []
  type: TYPE_IMG
- en: The fourth (and last) layer in our model is a 10-way Softmax layer. In our case,
    this means it will return an array of ten probability scores, all of which will
    add up to 1\. Each score will be the probability that the current digit image
    belongs to one of our output classes. Hence, for any given input, a layer with
    the Softmax activation computes and returns the class probability of that input,
    with respect to each of our output classes.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing your model visually
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Going back to our model, let''s summarize the output of what we are about to
    train. You can do this in Keras by using the `summary()` method on the model,
    which is actually a shortcut for the longer `utility` function (and hence harder
    to remember), which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this, you can actually visualize the shapes of the individual layers
    of the neural network, as well as the parameters in each layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, contrary to the perceptron we saw in [Chapter 2](e8898caf-ebe4-4c1d-b262-5b814276be04.xhtml),
    *A* *Deeper Dive into Neural Networks*, this extremely simple model already has
    51, 600 trainable parameters that are capable of scaling its learning almost exponentially
    compared to its ancestor.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we will compile our Keras model. Compilation basically refers to the
    manner in which your neural network will learn. It lets you have hands-on control
    of implementing the learning process, which is done by using the `compile` method
    that''s called on our `model` object. The method takes at least three arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we describe the following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A** `loss` **function**: This simply measures our performance on the training
    data, compared to the true output labels. Due to this, the `loss` function can
    be used as an indication of our model''s errors. As we saw earlier, this metric
    is actually a function that determines how far our model''s predictions are from
    the actual labels of the output classes. We saw the **Mean Squared Error** (**MSE**)
    `loss` function in [Chapter 2](e8898caf-ebe4-4c1d-b262-5b814276be04.xhtml), *A*
    *Deeper Dive into Neural Networks*, of which many variations exist. These `loss`
    functions are implemented in Keras, depending on the nature of our **ML** task.
    For example, if you wish to perform a binary classification (two output neurons
    representing two output classes), you are better off choosing binary cross-entropy.
    For more than two categories, you may try categorical cross-entropy, or sparse
    categorical cross-entropy. The former is used when your output labels are one-hot
    encoded, whereas the latter is used when your output classes are numerical categorical
    variables. For regression problems, we often advise the MSE `loss` function. When
    dealing with sequence data, as we will later, then **Connectionist Temporal Classification**
    (**CTC**) is deemed a more appropriate type of `loss` function. Other flavors
    of loss may differ in the manner they measure the distance between predictions
    and actual output labels (for example, `cosine_proximity` uses a cosine measure
    of distance), or the choice of probability distribution to model the predicted
    values (for example, the **Poisson loss function** is perhaps better if you are
    dealing with count data).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**An** `optimizer`: An intuitive way to think of an optimizer is that it tells
    the network how to get to a global minimum loss. This includes the goal you want
    to optimize, as well as the size of the step it will take in the direction of
    your goal. Technically, the optimizer is often described as the mechanism that''s
    employed by the network to self-update, which is does by using the data it is
    fed and the `loss` function. Optimization algorithms are used to update weights
    and biases that are the internal parameters of a model in the process of error
    reduction. There are actually two distinct types of optimization functions: functions
    with constant learning rates (such as **Stochastic Gradient Decent** (**SGD**))
    and functions with adaptive learning rates (such as Adagrad, Adadelta, RMSprop,
    and Adam). The latter of the two are known for implementing heuristic-based and
    pre-parameterized learning rate methods. Consequentially, using adaptive learning
    rates can lead to less work in tuning the hyperparameters of your model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metrics`: This simply denotes the evaluation benchmark we monitor during training
    and testing. Accuracy is most commonly used, but you may design and implement
    a custom metric through Keras, if you so choose. The main functional difference
    between the loss and accuracy score that''s shown by the metric is that the accuracy
    measure is not involved in the training process at all, whereas loss is used directly
    in the training process by our optimizer to backpropagate the errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fitting the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `fit` parameter initiates the training session, and hence should be thought
    of as synonymous to training our model. It takes your training features, their
    corresponding training labels, the number of times the model sees your data, and
    the number of learning examples your model sees per training iteration as training
    measures, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: You can also have additional arguments to shuffle your data, create validation
    splits, or give custom weights to output classes. Shuffling training data before
    each epoch can be useful, especially to ensure that your model does not learn
    any random non-predictive sequences in our data, and hence simply overfit the
    training set. To shuffle your data, you have to set the Boolean value of the shuffle
    argument to **True**. Finally, custom weights can be particularly useful if you
    have underrepresented classes in your dataset. Setting a higher weight is equivalent
    to telling your model, *Hey, you, pay more attention to these examples here*.
    To set custom weights, you have to provide the `class_weight` argument with a
    dictionary that maps class indices to custom weights corresponding to your output
    classes, in order of the indices that are provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an overview of the key architectural decisions you will face
    when compiling a model. These decisions relate to the training process you instruct
    your model to undergo:'
  prefs: []
  type: TYPE_NORMAL
- en: '`epochs`: This argument must be defined as an integer value, corresponding
    to the number of times your model will iterate through the entire dataset. Technically,
    the model is not trained for a number of iterations given by epochs, but merely
    until the epoch of index epochs is reached. You want to set this parameter *just
    right*, depending on the nature of complexity you want your model to represent.
    Setting it too low will lead to simplistic representations that are used for inference,
    whereas setting it too high will make your model overfit on your training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size`: The `batch_size` defines the number of samples that will be propagated
    through the network per training iteration. Intuitively, this can be thought of
    as the number of examples the network sees at a time while learning. Mathematically,
    this is simply the number of training instances the network will see before updating
    the model weights. So far, we have been updating our model weights at each training
    example (with a `batch_size` of 1), but this can quickly become a computational
    and memory management burden. This becomes especially cumbersome in instances
    where your dataset is too big to even load into memory. Setting a `batch_size`
    helps prevent this. Neural networks also train faster in mini-batches. In fact,
    batch size even has an impact on the accuracy of our gradient estimate during
    the backpropagation process, as shown in the following diagram. The same network
    is trained using three different batch sizes. Stochastic denotes random, or a
    batch size of one. As you can see, the direction of the stochastic and mini-batch
    gradients (green) fluctuates much more in comparison to the steady direction of
    the larger full-batch gradient (blue):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/5ba338bd-2549-4d34-9bc5-a36c8b994344.png)'
  prefs: []
  type: TYPE_IMG
- en: The **number of** **iterations** (which don't need to be explicitly defined)
    simply denotes the number of passes, where each pass contains the number of training
    examples denoted by the `batch_size`. To be clear, by one pass, we mean a forward
    filtering of data through our layers, as well as the backpropagation of errors.
    Suppose that we set our batch size to 32\. One iteration encompasses our model
    by viewing 32 training examples, then updating its weights accordingly. In a dataset
    of 64 examples with a batch size of 32, it will take only two iterations for your
    model to cycle through it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we have called the `fit` method on our training samples to initiate
    the learning process, we will observe the output, which simply displays the estimated
    training time, loss (in errors), and accuracy per epoch on our training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In only five full runs through our data, we achieve an accuracy of 0.96 (96.01%)
    during training. Now, we must verify whether our model is truly learning what
    we want it to learn by testing it on our secluded test set, which our model hasn''t
    seen so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating model performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whenever we evaluate a network, we are actually interested in our accuracy of
    classifying images in the test set. This remains true for any ML model, as our
    accuracy on the training set is not a reliable indicator of our model's generalizability.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, the test set accuracy is 95.78%, which is marginally lower than
    our training set accuracy of 96%. This is a classic case of overfitting, where
    our model seems to have captured irrelevant noise in our data to predict the training
    images. Since that inherent noise is different on our randomly selected test set,
    our network couldn't rely on the useless representations it had previously picked
    up on, and so performed poorly during testing. As we will see throughout this
    book, when testing neural networks, it is important to ensure that it has learnt
    correct and efficient representations of our data. In other words, we need to
    ensure that our network is not overfitting on our training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the way, you can always visualize your predictions on the test set by printing
    out the label with the highest probability value for the given test subject and
    plotting the said test subject using Matplotlib. Here, we are printing out the
    label with maximum probability for test subject `110`. Our model thinks it is
    an `8`. By plotting the subject, we see that our model is right in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ae3e1bf-72b5-43c4-8cf9-f8680081245d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once satisfied, you can save and load your model for later use, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, what can you do to prevent a model from learning misleading or irrelevant
    patterns from the training data? Well, with neural networks, the best solution
    is to almost always get more training data. A model that's trained on more data
    will indeed allow your model to have better out-of-set predictivity. Of course,
    getting more data is not always that simple, or even possible. When this is the
    case, you have several other techniques at your disposal to achieve similar effects.
    One of them is to constrain your model in terms of the amount of information that
    it may store. As we saw in the *behind enemy lines* example in [Chapter 1](e54db312-2f54-4eab-a2c2-91b5a38d13f2.xhtml),
    *Overview of Neural Networks*, it is useful to find the most efficient representations
    of information, or representations with the lowest entropy. Similarly, if we can
    only afford our model the ability to memorize a small number of patterns, we are
    actually forcing it to find the most efficient representations that generalize
    better on other data that our model may encounter later on. This process of improving
    model generalizability through reducing overfitting is known as **regularization**,
    and will be go over it in more detail before we use it in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Adjusting network size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we speak of a network''s size, we simply mean the number of trainable
    parameters within the network. These parameters are defined by the number of layers
    in the network, as well as the number of neurons per each layer. Essentially,
    a network''s size is a measure of its complexity. We mentioned how having too
    large a network size can be counterproductive and lead to overfitting. An intuitive
    way to think about this is that we should favor simpler representations over complex
    ones, as long as they achieve the same ends—sort of a *lex parsimoniae*, if you
    will. The engineers who design such learning systems are indeed deep thinkers.
    The intuition here is that you could probably have various representations of
    your data, depending on your network''s depth and number of neurons per layer,
    but we will favor simpler configurations and only progressively scale a network
    if required, to prevent it from using any extra learning capacity to memorize
    randomness. However, letting our model have too few parameters may well cause
    it to underfit, leaving it oblivious to the underlying trends we are trying to
    capture in our data. Through experimentation, you can find a network size that
    fits just right, depending on your use case. We force our network to be efficient
    in representing our data, allowing it to generalize better out of our training
    data. Beneath, we show a few experiments that are performed while varying the
    size of the network. This lets us compare how our loss on the validation set differs
    per epoch. As we will see, larger models are quicker to diverge away from the
    minimum loss values, and they will start to overfit on our training data almost
    instantly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae411f1a-470f-40de-9077-037c56a5be17.png)'
  prefs: []
  type: TYPE_IMG
- en: Size experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will perform some short experiments by varying the size of our network
    and gauging our performance. We will train six simple neural networks on Keras,
    each progressively larger than the other, to observe how these separate networks
    learn to classify handwritten digits. We will also present some of the results
    from the experiments. All of these models were trained with a constant batch size
    (`batch_size=100`), the `adam` optimizer, and `sparse_categorical_crossentropy`
    as a `loss` function, for the purpose of this experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following fitting graph shows how increasing our neural network''s complexity
    (in terms of size) impacts our performance on the training and test sets of our
    data. Note that we are always aiming for a model that minimizes the difference
    between training and test accuracy/loss, as this indicates the minimum amount
    of overfitting. Intuitively, this simply shows us how much our networks learning
    benefits if we allocate it more neurons. By observing the increase in accuracy
    on the test set, we can see that adding more neurons does help our network to
    better classify images that it has never encountered before. This can be noticed
    until the *sweet spot*, which is where the training and test values are the closest
    to each other. Eventually, however, increases in complexity will lead to diminishing
    returns. In our case, our model seems to overfit the least at a dropout rate around
    0.5, after which the accuracy of the training and test sets start to diverge:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6d1ddb0-1ab8-46e3-936e-8690f653ea00.png)   ![](img/a153a0d8-4f37-47d0-bcfa-ef6e702f760d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To replicate these results by increasing the size of our network, we can tweak
    both the breadth (number of neurons per layer) and the depth of the network (number
    of layers in network). Adding depth to your network is done in Keras by adding
    layers to your initialized model by using `model.add()`. The `add` method takes
    the type of layer (for example, `Dense()`), as an argument. The `Dense` function
    takes the number of neurons to be initialized in that specific layer, along with
    the activation function to be employed for said layer, as arguments. The following
    is an example of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Regularizing the weights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another way to make sure that your network doesn''t pick up on irrelevant features
    is through regularizing the weights of our model. This simply allows us to put
    a constraint on the complexity of the network by limiting its layer weights to
    only take small values. All this does is make the distribution of layer weights
    more regular. How do we do this? By simply adding a cost to the `loss` function
    of our network. This cost actually represents a penalization for neurons that
    have larger weights. Conventionally, we implement this cost in three ways, namely
    L1, L2, and elastic net regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: '**L1 regularization**: We add a cost that is proportional to the absolute value
    of our weighted coefficients.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**L2 regularization**: We add a cost that is proportional to the square of
    the value of the weighted coefficients. This is also known as **weight decay**,
    as the weights exponentially decay to zero if no other update is scheduled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Elastic net regularization**: This regularization method allows us to capture
    the complexity of our model by using a combination of both L1 and L2 regularization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using dropout layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, adding dropout neurons to layers is a technique that's widely used
    to regularize neural networks and prevent them from overfitting. Here, we, quite
    literally, drop out some neurons from our model at random. Why? Well, this results
    in a two-fold utility. Firstly, the contributions these neurons had for the activations
    of neurons further down our network are randomly ignored during a forward pass
    of data through our network. Also, any weight adjustments during the process of
    backpropagation are not applied to the neuron. While seemingly bizarre, there
    is good intuition behind this. Intuitively, neuron weights are adjusted at each
    backward pass to specialize a specific feature in your training data. But specialization
    breeds dependence. What often ends up happening is the surrounding neurons start
    relying on the specialization of a certain neuron in the vicinity, instead of
    doing some representational work themselves. This dependence pattern is often
    denoted as complex co-adaptation, a term that was coined by **Artificial Intelligence**
    (**AI**) researchers. One among them was Geoffrey Hinton, who was the original
    co-author of the backpropagation paper and is prominently referred to as the godfather
    of deep learning. Hinton playfully describes this behavior of complex coadaptation
    as *conspiracies* between neurons, stating that he was inspired by a fraud prevention
    system at his bank. This bank continuously rotated its employees so that whenever
    Hinton paid the bank a visit, he would always encounter a different person behind
    the desk.
  prefs: []
  type: TYPE_NORMAL
- en: Thinking about dropout intuitively
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For those of you who are familiar with Leonardo Dicaprio's movie *Catch me if
    you can*, you'll recall how Leonardo charmed the bank workers by asking them on
    dates and buying them treats, only so he could defraud the bank by cashing in
    his fake airline checks. In fact, due to the frequent fraternization of the employees
    and DiCaprio's character, the bank workers were paying more attention to irrelevant
    features such as DiCaprio's charm. What they should have actually been paying
    attention was the fact that DiCaprio was cashing out his monthly salary checks
    more than three times each month. Needless to say, businesses don't usually behave
    so generously. Dropping out some neurons is synonymous to rotating them to ensure
    that none of them get lazy and let a sleazy Leonardo defraud your network.
  prefs: []
  type: TYPE_NORMAL
- en: When we apply a dropout to a layer, we simply drop some of the outputs it would
    have otherwise given. Suppose a layer produces the vector [3, 5, 7, 8, 1] as an
    output for a given input. Adding a dropout rate of (0.4) to this layer would simply
    convert this output to [0, 5, 7, 0, 1]. All we did was initialize 40% of the scalars
    in our vector as zero.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout only occurs during training. During testing, layers with dropouts have
    their outputs scaled down by the factor of the dropout rate that was previously
    used. This is actually done to adjust for the fact that more neurons are active
    during testing than training, as a result of the dropout mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing weight regularization in Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have visited the theories behind three specific ways that allow us
    to improve our model's generalizability on unseen data. Primarily, we can vary
    our network size to ensure it has no extra learning capacity. We can also penalize
    inefficient representations by initializing weighted parameters. Finally, we can
    add dropout layers to prevent our network from getting lazy. As we noted previously,
    seeing is believing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s implement our understanding using the MNIST dataset and some Keras
    code. As we saw previously, to change the network size, you are simply required
    to change the number of neurons per layer. This can be done in Keras during the
    process of adding layers, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Weight regularization experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Simply put, regularizers let us apply penalties to layer parameters during
    optimization. These penalties are incorporated in to the `loss` function that
    the network optimizes. In Keras, we regularize the weights of a layer by passing
    a `kernel_regularizer` instance to a layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'As we mentioned previously, we add L2 regularization to both our layers, each
    with an alpha value of (0.0001). The alpha value of a regularizer simply refers
    to the transformation that''s being applied to each coefficient in the weight
    matrix of the layer, before it is added to the total loss of our network. In essence,
    the alpha value is used to multiply each coefficient in our weight matrix with
    it (in our case, 0.0001). The different regularizers in Keras can be found in
    `keras.regularizers`. The following diagram shows how regularization impacts validation
    loss per epoch on two models that are the same size. One observes that our regularized
    model is much less prone to overfitting, since the validation loss does not significantly
    increase as a function of time. On the model without regularization, we can clearly
    see that this is not the case, and after about seven epochs, the model starts
    overfitting, and so performs worse on the validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4c91389-24a7-4a3b-b6ba-d1febaf38000.png)'
  prefs: []
  type: TYPE_IMG
- en: Implementing dropout regularization in Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In Keras, adding a dropout layer is also very simple. All you are required
    to do is use the `model.add()` parameter again, and then specify a dropout layer
    (instead of the dense layer that we''ve been using so far) to be added. The `Dropout`
    parameter in Keras takes a float value that refers to the fraction of neurons
    whose predictions will be dropped. A very low dropout rate might not provide the
    robustness we are looking for, while a high dropout rate simply means we have
    a network prone to amnesia, incapable of remembering any useful representations.
    Once again, we strive for a dropout value that is just right; conventionally,
    the dropout rate is set between 0.2 and 0.4:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Dropout regularization experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are two experiments that we performed using a network of the
    same size, with different dropout rates, to observe the differences in performance.
    We started with a dropout rate of 0.1, and progressively scaled to 0.6 to see
    how this affected our performance in recognizing handwritten digits. As we can
    see in the following diagram, scaling our dropout rate seems to reduce overfitting,
    as the model''s superficial accuracy on the training set progressively drops.
    We can see that both our training and test accuracy converges near the dropout
    rate of 0.5, after which they exhibit divergent behavior. This simply tells us
    that the network seems to overfit the least when a dropout layer of rate 0.5 is
    added:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc7da382-5d59-4ffe-90e8-5960e26cd6ff.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/b9f13e80-67a8-47eb-b72b-275a841d8dbc.png)'
  prefs: []
  type: TYPE_IMG
- en: Complexity and time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, you have seen some of the most prominent tricks in our repertoire to reduce
    overfitting through regularization. In essence, regularization is just a manner
    of controlling the complexity of our network. Complexity control is not just useful
    to restrict your network from memorizing randomness; it also brings more direct
    benefits. Inherently, more complex networks are computationally expensive. They
    will take longer to train, and hence consume more of your resources. While this
    makes an insignificant difference when dealing with the task at hand, this difference
    is still quite noticeable. In the following diagram is a time complexity chart.
    This is a useful way of visualizing training time as a function of network complexity.
    We can see that an increase in our network''s complexity seems to have a quasi-exponential
    effect on the increase in average time taken per training iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/44b86b50-9754-4361-a6c5-6b9c9b979971.png)'
  prefs: []
  type: TYPE_IMG
- en: A summary of MNIST
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in our journey, you were introduced to the fundamental learning mechanisms
    and processes that govern a neural network's functionality. You learned that neural
    networks need tensor representations of input data to be able to process it for
    predictive use cases. You also learned how different types of data that are found
    in our world, such as images, videos, text, and so on, can be represented as tensors
    of *n*-dimensions. Furthermore, you saw how to implement a sequential model in
    Keras, which essentially lets you build sequential layers of interconnected neurons.
    You used this model structure to construct a simple feedforward neural network
    for the task of classifying handwritten digits with the MNIST dataset. In doing
    so, you learned about the key architectural decisions to consider at each stage
    of model development.
  prefs: []
  type: TYPE_NORMAL
- en: During model construction, the main decisions pertain to defining the correct
    input size of your data, choosing a relevant activation function per layer, and
    defining the number of output neurons in your last layer, according to the number
    of output classes in your data. During the compilation process, you got to choose
    the optimization technique, `loss` function, and a metric to monitor your training
    progress. Then, you initiated the training session of your newly minted model
    by using the `.fit()` parameter, and passing the model the final two architectural
    decisions to be made before initiating the training procedure. These decisions
    pertained to the batch size of your data to be seen at a time, and the total number
    of epochs to train the model for.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, you saw how to test your predictions, and learned about the pivotal
    concept of regularization. We concluded this classification task by experimenting
    with regularization techniques to modify our model''s size, layer weights, and
    add dropout layers, which in turn helped us improve the generalizability of our
    model to unseen data. Lastly, we saw that increasing model complexity is unfavourable
    unless explicitly required due to the nature of our task:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise x**: Initialize different weighted parameters and see how this affects
    model performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exercise y**: Initialize different weights per layer and see how this affects
    model performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen how we can train a simple feedforward neural network on
    Keras for an image classification task. We also saw how we can mathematically
    represent image data as a high-dimensional geometric shape, namely a tensor. We
    saw that a higher-order tensor is simply composed of tensors of a smaller order.
    Pixels group up to represent an image, which in turn group up to represent an
    entire dataset. Essentially, whenever we want to employ the learning mechanism
    of neural networks, we have a way to represent our training data as a tensor.
    But what about language? How can we represent human thought, with all of its intricacies,
    as we do through language? You guessed it—we will use numbers once again. We will
    simply translate our texts, which are composed of sentences, which themselves
    are composed of words, into the universal language of mathematics. This is done
    through a process known as **vectorization**, which we will explore first-hand
    during our task of classifying the sentiment of movie reviews by using the **internet
    movie database** (**IMDB**) dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As our computing power improved over the years, we started applying computational
    techniques to domains that were previously frequented only by linguists and qualitative
    academics. It turns out that tasks that were initially considered too time-consuming
    to pursue became ideal for computers to optimize as processors increased in potency.
    This led to an explosion of computer-assisted text analysis, not only in academia,
    but also in the industry. Tasks such as computer-assisted sentiment analysis can
    be specifically beneficial for various use cases. This can be used if you're a
    company trying to track your online customer reviews, or an employer wanting to
    do some identity management on social media platforms. In fact, even political
    campaigns increasingly consult services that monitor public sentiments and conduct
    opinion mining on a large variety of political topics. This helps politicians
    prepare their campaign points and understand the general aura of opinions that
    are held by people. While such use of technology can be quite controversial, it
    can vastly help organizations understand their flaws in products, services, and
    marketing strategies, while catering to their audience in a more relevant manner.
  prefs: []
  type: TYPE_NORMAL
- en: The internet movie reviews dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The simplest form of sentiment analysis task deals with categorizing whether
    a piece of text represents a positive or negative opinion. This is often referred
    to as a *polar* or *binary sentiment classification task*, where 0 refers to a
    negative sentiment and 1 refers to a positive sentiment. We can, of course, have
    more complex sentiment models (perhaps using the big-five personality metrics
    we saw in [Chapter 1](e54db312-2f54-4eab-a2c2-91b5a38d13f2.xhtml), *Overview of
    Neural Networks*), but for the time being, we will concentrate on this simple
    yet conceptually loaded binary example. The example in question refers to classifying
    movie reviews from the Internet Movie Database or IMDB.
  prefs: []
  type: TYPE_NORMAL
- en: 'The IMDB dataset consists of 50,000 binary reviews, which are evenly split
    into positive and negative opinions. Each review consists of a list of integers,
    where each integer represents a word in that review. Once again, the guardians
    of Keras have thoughtfully included this dataset for practice, and hence can be
    found in Keras under `keras.datasets`. We encourage you to enjoy this importing
    data using Keras, as we won''t be doing so in future exercises (nor will you be
    able to do it in the real world):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Loading the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we did previously, we load our dataset by defining our training instances
    and labels, as well as our test instances and labels. We are able to use the `load_data`
    parameter on `imdb` to load in our pre-processed data into a 50/50 train–test
    split. We can also indicate the number of most frequently occurring words we want
    to keep in our dataset. This helps us control the inherent complexity of our task
    as we work with review vectors of reasonable sizes. It is safe to assume that
    rare words occurring in reviews would have to do more with the specific subject
    matter of a given movie, and so they have little influence on the *sentiment*
    of that review in question. Due to this, we will limit the number of words to
    12,000.
  prefs: []
  type: TYPE_NORMAL
- en: Checking the shape and type
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can check the number of reviews per split by checking the `.shape` parameter
    of `x_train`, which is essentially a NumPy array of *n*-dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Plotting a single training instance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we can see, there are 25,000 training and test samples. We can also plot
    out an individual training sample to see how we can represent a single review.
    Here, we can see that each review simply contains a list of integers, where each
    integer corresponds to a word in a dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Decoding the reviews
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you''re curious (which we are), we can of course map out the exact words
    that these numbers correspond to so that we can read what the review actually
    says. To do this, we must back up our labels. While this step is not essential,
    it is useful if we want to visually verify our network''s predictions later on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to recover the words corresponding to the integers representing
    a review, which we saw earlier. The dictionary of words that were used to encode
    these reviews is included with the IMDB dataset. We will simply recover them as
    the `word_index` variable and reverse their order of storage. This basically allows
    us to map each integer index to its corresponding word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The following function takes two arguments. The first one (`n`) denotes an integer
    referring to the n^(th) review in a set. The second argument defines whether the
    n^(th) review is taken from our training or test data. Then, it simply returns
    the string version of the review we specify.
  prefs: []
  type: TYPE_NORMAL
- en: 'This allows us to read out what a reviewer actually wrote. As we can see, in
    our function, we are required to adjust the position of indices, which are offset
    by three positions. This is simply how the designers of the IMDB dataset chose
    to implement their coding scheme, and so this is not of practical relevance for
    other tasks. The offset of the three positions in question occurs because positions
    0, 1, and 2 are occupied by indices for padding, denoting the start of a sequence,
    and denoting unknown values, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this function, we can decode review number five from our training set,
    as shown in the following code. It turns out that this is a negative review, as
    denoted by its training label, and inferred by its content. Note that the question
    marks are simply an indication of unknown values. Unknown values can occur inherently
    in the review (due to the use of emojis, for example) or due to the restrictions
    we have imposed (that is, if a word is not in the top 12,000 most frequent words
    that were used in the corpus, as stated earlier):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Preparing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well then, what are we waiting for? We have a series of numbers representing
    each movie review, with their corresponding label, indicating (1) for positive
    or (0) for negative. This sounds like a classic structured dataset, so why not
    start feeding it to a network? Well, it's not that simple. Earlier, we mentioned
    that neural networks have a very specific diet. They are almost exclusively *Tensor-vores*,
    and so feeding them a list of integers won't do us much good. Instead, we must
    represent our dataset as a tensor of *n*-dimensions before we attempt to pass
    it on to our network for training. At the moment, you will notice that each of
    our movie reviews is represented by a separate list of integers. Naturally, each
    of these lists are of different sizes, as some reviews are smaller than others.
    Our network, on the other hand, requires the input features to be of the same
    size. Hence, we have to find a way to *pad* our reviews so that each of them represents
    a vector of the same length.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since we know that the maximum number of unique words in our entire corpus
    is 12,000, we can assume that the longest possible review can only be 12,000 in
    length. Hence, we can make each review a vector of length 12,000, containing binary
    values. How does this work? Suppose we have a review of two words: *bad* and *movie*.
    A list containing these words in our dataset may look like [6, 49]. Instead, we
    can represent this same review as a 12,000-dimensional vector populated with 0s,
    except for the indices of 6 and 49, which would instead be 1s. What you''re essentially
    doing is creating 12,000 dummy features to represent each review. Each of these
    dummy features represents the presence or absence of any of the 12,000 words in
    a given review. This approach is also known as **one-hot encoding**. It is commonly
    used to encode features and categorical labels alike in various deep learning
    scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: Vectorizing features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following function will take our training data of 25,000 lists of integers,
    where each list is a review. In return, it spits out one-hot encoded vectors for
    each of the integer lists it received from our training set. Then, we simply redefine
    our training and test features by using this function to transform our integer
    lists into a 2D tensor of one-hot encoded review vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see the result of our transformations by checking the type and shape
    of our training features and labels. You can also check what one individual vector
    looks like, as shown in the following code. We can see that each of our reviews
    is now a vector of length `12000`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Vectorizing labels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can also vectorize our training labels, which simply helps our network handle
    our data better. You can think of vectorization as an efficient way to represent
    information to computers. Just like humans are not very good at performing computations
    using Roman numerals, computers are notoriously worse off when dealing with unvectorized
    data. In the following code, we are transforming our labels into NumPy arrays
    that contain 32-bit floating-point arithmetic values of either 0.0 or 1.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we have our tensor, ready to be consumed by a neural network. This
    2D tensor is essentially 25,000 stacked vectors, each with its own label. All
    that is left to do is build our network.
  prefs: []
  type: TYPE_NORMAL
- en: Building a network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first architectural constraints that you must consider while building a
    network with dense layers are its the depth and width. Then, you need to define
    an input layer with the appropriate shape, and successively choose from different
    activation functions to use per layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we did for our MNIST example, we simply import the sequential model and
    the dense layer structure. Then we proceed by initializing an empty sequential
    model and progressively add hidden layers until we reach the output layer. Do
    note that our input layer always requires a specific input shape, which for us
    corresponds to the 12,000 - dimensional one-hot encoded vectors that we will be
    feeding it. In our current model, the output layer only has one neuron, which
    will ideally fire if the sentiment in a given review is positive; otherwise, it
    won''t. We will choose **Rectified Linear Unit** (**ReLU**) activation functions
    for our hidden layers and a sigmoid activation function for the ultimate layer.
    Recall that the sigmoid activation function simply squished probability values
    between 0 and 1, making it quite ideal for our binary classification task. The
    ReLU activation function simply helps us zero out negative values, and hence can
    be considered a good default to begin with in many deep learning tasks. In summary,
    we have chosen a model with three densely interconnected hidden layers, containing
    18, 12, and 4 neurons, respectively, as well as an output layer with 1 neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Compiling the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we can compile our freshly built model, as is deep learning tradition. Recall
    that in the compilation process, the two key architectural decisions are the choice
    of the `loss` function, as well as the optimizer. The `loss` function simply helps
    us measure how far our model is from the actual labels at each iteration, whereas
    the optimizer determines how we converge to the ideal predictive weights for our
    model. In [Chapter 10](cd18f9ea-65ed-4ebd-af06-0403d3774be1.xhtml), *Contemplating
    Present and Future Developments*, we will review advanced optimizers and their
    relevance in various data processing tasks. For now, we will show how you can
    manually adjust the learning rate of an optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have chosen a very small learning rate of 0.001 on the **Root Mean Square**
    (**RMS**) prop for demonstrative purposes. Recall that the size of the learning
    rate simply determines the size of the step we want out network to take in the
    direction of the correct output at each training iteration. As we mentioned previously,
    a big step can cause our network to *walk over* the global minima in the loss
    hyperspace, whereas a small learning rate can cause your model to take ages to
    converge to a minimum loss value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Fitting the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our previous MNIST example, we went over the least number of architectural
    decisions to get our code running. This lets us cover a deep learning workflow
    quite quickly, but at the expense of efficiency. You will recall that we simply
    used the `fit` parameter on our model and passed it our training features and
    labels, along with two integers denoting the epochs to train the model for, and
    the batch size per training iteration. The former simply defines how many times
    our data runs through the model, while the latter defines how many learning examples
    our model will see at a time before updating its weights. These are the two paramount
    architectural considerations that must be defined and adapted to the case at hand.
    However, there are several other useful arguments that the `fit` parameter may
    take.
  prefs: []
  type: TYPE_NORMAL
- en: Validation data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may be wondering why we train our model blindly for an arbitrary number
    of iterations and then test it on our holdout data. Wouldn't it be more efficient
    to gauge our model to some unseen data after each epoch, just to see how well
    we are doing? This way, we are able to assess exactly when our model starts to
    overfit, and hence end the training session and save some expensive hours of computing.
    We could show our model the test set after each epoch, without updating its weights,
    purely to see how well it does on our test data after that epoch. Since we do
    not update our model weights at each test run, we don't risk our model overfitting
    on the test data. This allows us to get a genuine understanding of how generalizable
    our model is *during* the training process, and not after. To test your model
    on a validation split, you can simply pass the validation features and labels
    as a parameter, as you did with your training data, to the `fit` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we have simply used the test features and labels as our validation
    data. In a rigorous deep learning scenario with high stakes, you may well choose
    to have separate test and validation sets, where one is used for validation during
    training, and the other is reserved for later assessments before you deploy your
    model to production. This is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Now, when you execute the preceding cell, you will see the training session
    initiate. Moreover, at the end of each training epoch, you will see that our model
    takes a brief pause to compute the accuracy and loss on the validation set, which
    is then displayed. Then, without updating its weights after this validation pass,
    the model proceeds to the next epoch for another training round. The preceding
    model will run for 20 epochs, where each epoch will iterate over our 25,000 *training*
    examples in batches of 100, updating the model weights after each batch. Note
    that in our case, the model weights are updated 250 times per epoch, or 5,000
    times during the preceding training session of 20 epochs. So, now we can better
    assess when our model starts to memorize random features of our training set,
    but how do we actually interrupt the training session at this point? Well, you
    may have noticed that instead of just executing `model.fit()`, we defined it as
    `network_metadata`. As it happens, the `fit()` parameter actually returns a history
    object containing the relevant training statistics of our model, which we are
    interested in recovering. This history object is recorded by something called
    a **callback** in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Callbacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A `callback` is basically a Keras library function that can interact with our
    model during the training session to check on its internal state and save relevant
    training statistics for later scrutiny. While quite a few callback functions exist
    in `keras.callbacks`, we will introduce a few that are crucial. For those of you
    who are more technically oriented, Keras even lets you construct custom callbacks.
    To use a callback, you simply pass it to the `fit` parameter using the keyword
    argument `callbacks`. Note that the history callback is automatically applied
    to every Keras model, and so it does not need to be specified as long as you define
    the fitting process as a variable. This lets you recover the associated history
    object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Importantly, if you initiated a training session previously in your Jupyter
    Notebook, then calling the `fit()` parameter on the model will continue training
    the same model. Instead, you want to reinitialize a blank model, before proceeding
    with another training run. This can be done by simply rerunning the cells where
    you previously defined and compiled your sequential model. Then, you may proceed
    by implementing a callback by passing it to the `fit()` parameter by using the
    `callbacks` keyword argument, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Early stopping and history callbacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the preceding cell, we used a callback known as **early stopping**. This
    callback allows us to monitor a specific training metric. Our choices are between
    our accuracy or loss on the training set or on the validation set, which are all
    stored in a dictionary pertaining to our model''s history:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Choosing a metric to monitor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ideal choice is always *validation loss* or *validation accuracy*, as these
    metrics best represent the out of set predictability of our model. This is simply
    due to the fact that we only update our model weights during a training pass,
    and not a validation pass. Choosing our *training accuracy* or *loss* as a metric
    (as in the following code) is suboptimal in the sense that you are benchmarking
    your model by its own definition of a benchmark. To put this in a different way,
    your model might keep reducing its loss and increasing in accuracy, but it is
    doing so by rote memorization—not because it is learning general predictive rules
    as we want it to. As we can see in the following code, by monitoring our *training*
    loss, our model continues to decrease loss on the training set, even though the
    loss on the validation set actually starts increasing shortly after the very first
    epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15feadd5-d291-4653-8c98-d90b611a6984.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We used Matplotlib to plot out the preceding graph. Similarly, you can clear
    out the previous loss graph and plot out a new accuracy graph of our training
    session, as shown in the following code. If we had used validation accuracy as
    a metric to track our early stopping callback, our training session would have
    ended after the *first* epoch, as *this* is the point in time where our model
    appears to be the most generalizable to unseen data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a902e7f2-2276-4c1a-ad14-22e84927d07d.png)'
  prefs: []
  type: TYPE_IMG
- en: Accessing model predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the MNIST example, we used the *Softmax* activation function as our last
    layer. You may recall that the layer generated an array of 10 probability scores,
    adding up to 1 for a given input. Each of those 10 scores referred to the likelihood
    of the image being presented to our network corresponding to one of the output
    classes (that is, it is 90% sure it sees a 1, and 10% sure it sees a 7, for example).
    This approach made sense for a classification task with 10 categories. In our
    sentiment analysis problem, we chose a sigmoid activation function, because we
    are dealing with binary categories. Using the sigmoid here simply forces our network
    to output a prediction between 0 and 1 for any given instance of data. Hence,
    a value closer to 1 means that our network believes that the given piece of information
    is more likely to be a positive review, whereas a value closer to zero states
    our network''s conviction of having found a negative review. To view our model''s
    predictions, we simply define a variable called `predictions` by using the `predict()`
    parameter on our trained model and passing it our test set. Now we can check our
    network predictions on a given example from this set, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, it appears that our network is quite confident that review `5`
    from our test set is a positive review. Not only can we check whether this is
    indeed the case by checking the label stored in `y_test[5]`, we can also decode
    the review itself due to the decoder function we built earlier. Let''s put our
    network''s prediction to the test by decoding review `5` and checking its label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: It turns out our network is right. This is an example of a complex linguistic
    pattern that requires a higher-level understanding of linguistic syntax, real-world
    entities, relational logic, and the propensity for humans to blabber aimlessly.
    Yet, with only 12 neurons, our network has seemingly understood the underlying
    sentiment that's encoded in this piece of information. It makes a prediction with
    a high degree of certainty (99.99%), despite the presence of words such as *disgusting*,
    which are very likely to appear in negative reviews.
  prefs: []
  type: TYPE_NORMAL
- en: Probing the predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s examine another review. To probe our predictions, we will make a few
    functions that will help us visualize our results better. Such a gauging function
    can also be used if you want to restrict your model''s predictions to instances
    where it is most certain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We will make two functions to help us better visualize our network''s errors,
    while also limiting our predictive accuracy to upper and lower bounds. We will
    use the first function to arbitrarily define good predictions as instances where
    the network has a probability score above 0.7, and bad instances where the score
    is below 0.4, for a *positive review*. We simply reverse this scheme for the negative
    reviews (a good prediction score for a *negative review* is below 0.4 and a bad
    one is above 0.7). We also leave a middle ground between 40 and 70%, labeled as
    uncertain predictions so that we can better understand the reason behind its accurate
    and inaccurate guesses. The second function is designed for simplicity, taking
    an integer value that refers to the *n*th review you want to probe and verify
    as input, and returning an assessment of what the network thinks, the actual probability
    score, as well as what the review in question reads. Let''s use these newly forged
    functions to probe yet another review:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, our network seems to be quite sure that review `22` from our
    test set is negative. It has generated a probability score of 0.169\. You could
    also interpret this score as that our network believed with 16.9% confidence that
    this review is positive, and so it must be negative (since these are the only
    two classes we used to train our network). It turns out that our network got this
    one wrong. Reading the review, you will notice that the reviewer actually expresses
    their appreciation for what they deemed to be an undervalued movie. Note that
    the tone is quite ambiguous at the beginning, with words like *silly* and *fall
    flat*. However, contextual valence shifters later on in the sentence allow our
    biological neural networks to determine that the review actually expresses a positive
    sentiment. Sadly, our artificial network does not seem to have caught up with
    this particular pattern. Let''s continue our exploratory analysis using yet another
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that our network is not too sure about the review, even though
    it has actually guessed the correct sentiment in the review, with a probability
    score of 0.59, which is closer to 1 ( positive) than 0 (negative). To us, this
    review clearly appears positive—even a bit promotionally pushy. It is intuitively
    unclear why our network is not certain of the sentiment. Later in this book, we
    will learn how to visualize word embeddings using our network layers. For now,
    let''s continue our probing with one last example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, our network gets it right again. In fact, our network is 99.9% sure
    that this is a positive review. While reading the review, you''ll notice that
    it has actually done a decent job, as the review contains words like *boring*,
    *average*, and suggestive language such as *mouth shut*, which could all easily
    be present in other negative reviews, potentially misleading our network. As we
    can see, we conclude this probing session by providing a short function that you
    can play around with by randomly checking your network''s predictions for a given
    number of reviews. We then print out our network''s predictions for two randomly
    chosen reviews from our test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Summary of IMDB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now you should have a better idea of how to go about processing natural language
    texts and dialogues through a simple feedforward neural network. In this subsection
    of our journey, you learned how to execute a binary sentiment classification task
    using a feedforward neural network. In doing so, you learned how to pad and vectorize
    your natural language data, preparing it for processing with neural networks.
    You also went over the key architectural changes that are involved in binary classification,
    such as using an output neuron and the sigmoid activation function on the last
    layer of our network. You also saw how you can leverage a validation split in
    your data to get an idea of how your model performs on unseen data after each
    training epoch. Moreover, you learned how to indirectly interact with your model
    during the training process by using Keras callbacks. Callbacks can be useful
    for a variety of use cases, ranging from saving your model at a certain checkpoint
    or terminating the training session when a desired metric has reached a certain
    point. We can use the history callback to visualize training statistics, and we
    can use the early stopping callback to designate a moment to terminate the current
    training session. Finally, you saw how you can probe your network''s predictions
    per review to better understand what kind of mistakes it makes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**: Improve performance with regularization, as we did in the MNIST
    example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting continuous variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have performed two classification tasks using neural networks. For
    our first task, we classified handwritten digits. For our second task, we classified
    sentiments in movie reviews. But what if we wanted to predict a continuous value
    instead of a categorical value? What if we wanted to predict how likely an event
    may occur, or the future price of a given object? For such a task, examples such
    as predicting prices in a given market may come to mind. Hence, we will conclude
    this chapter by coding another simple feedforward network by using the Boston
    Housing Prices dataset.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset resembles most real-world datasets that data scientists and machine
    learning practitioners would come across. You are given 13 features that refer
    to a specific geographical area located in Boston. With these features, the task
    at hand is to predict the median price of houses. The features themselves include
    various indicators ranging from residential and industrial activity, level of
    toxic chemicals in the air, property tax, access to education, and other socio-economic
    indicators that are associated with location. The data was collected during the
    mid-1970s, and seems to have brought along some bias from the time. You will notice
    that some features seem very nuanced and perhaps even inappropriate. Features
    such as feature number 12 can be very controversial to use in machine learning
    projects. You must always consider the higher-level implications when using a
    certain source or type of data. It is your duty as a machine learning practitioner
    to ensure that your model does not introduce or reinforce any sort of societal
    bias, or contribute in any way to disparities and discomfort for people. Remember,
    we are in the business of using technology to alleviate human burden, not add
    to it.
  prefs: []
  type: TYPE_NORMAL
- en: Boston Housing Prices dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned in the previous section, this dataset contains 13 training features
    that are represented on an observed geographical region.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dependent variable that we are interested in predicting is the housing price
    per location, which is denoted as a continuous variable that denotes house prices
    in thousands of dollars.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, each of our observations can be represented as a vector of dimension
    13, with a corresponding scalar label. In the following code, we are plotting
    out the second observation in our training set, along with its corresponding label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Exploring the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This dataset is a much smaller dataset in comparison to the ones we''ve dealt
    with so far. We can only see 404 training observations and `102` test observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also generate a dictionary containing the description of our features
    so that we can understand what each of them actually encodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s create a pandas `DataFrame` and have a look at the first five observations
    in our training set. We will simply pass out the training data, along with the
    previously defined column names, as arguments to the pandas `DataFrame` constructor.
    Then, we will use the `.head()` parameter on our newly forged `.DataFrame` object
    to get a nice display, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Feature-wise normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can see that each feature in our observation seems to be on a different
    scale. Some values range in the hundreds, while others are between 1 and 12, or
    even binary. While neural networks may still ingest unscaled features, it almost
    exclusively prefers to deal with features on the same scale. In practice, a network
    can learn from heterogeneously scaled features, but it may take much longer to
    do so without any guarantee of finding an ideal minimum on the loss landscape.
    To allow our network to learn in an improved way for this dataset, we must homogenize
    our data through the process of feature-wise normalization. We can achieve this
    by subtracting the feature-specific mean and dividing it by the feature-specific
    standard deviation for each feature in our dataset. Note that in live-deployed
    models (for the stock exchange, for example), such a scaling measure is impractical,
    as the means and standard deviation values may keep on changing, depending on
    new, incoming data. In such scenarios, other normalization and standardization
    techniques (such as log normalization, for example) are better to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Building the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main architectural difference in this regression model, as opposed to the
    previous classification models we built, is to do with the way we construct the
    last layer of this network. Recall that in a classic scalar regression problem,
    such as the one at hand, we aim to predict a continuous variable. To implement
    this, we avoid using an activation function in our last layer, and use only one
    output neuron.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason we forego an activation function is because we do not want to constrain
    the range that the output values of this layer may take. Since we are implementing
    a purely linear layer, our network is able to learn to predict a scalar continuous
    value, just as we want it to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Compiling the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main architectural difference during compilation here is to do with the
    `loss` function and metric we choose to implement. We will use the MSE `loss`
    function to penalize higher prediction errors, while monitoring our model''s training
    progress with the **Mean Absolute Error** (**MAE**) metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: As we saw previously, the MSE function measures the average of the squares of
    our network's prediction errors. Simply put, we are simply measuring the average
    squared difference between the estimated and actual house price labels. The squared
    term emphasizes the spread of our prediction errors by penalizing the errors that
    are further away from the mean. This approach is especially helpful with regression
    tasks where small error values still have a significant impact on predictive accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, our housing price labels range between 5 and 50, measured in thousands
    of dollars. Hence, an absolute error of 1 actually means a difference of $1,000
    in prediction. Thus, taking using an absolute error-based `loss` function might
    not give the best feedback mechanism to the network.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the choice of MAE as a metric is ideal to measure our training
    progress itself. Visualizing squared errors, as it turns out, is not very intuitive
    to us humans. It is better to simply see the absolute errors in our models' predictions,
    as it is visually more informative. Our choice of metric has no actual impact
    on the training mechanism of the model—it is simply providing us with a feedback
    statistic to visualize how good or bad our model is doing during the training
    session. The MAE metric itself is essentially a measure of difference between
    two continuous variables.
  prefs: []
  type: TYPE_NORMAL
- en: Plotting training and test errors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following graph, we can see that the average error is about 2.5 (or
    $2,500 dollars). While this may be a small variance when predicting the prices
    on houses that cost $50,000, it starts to matter if the house itself costs $5,000:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bb27c237-7146-42c8-935c-ec9d9825c23c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, let''s predict some housing prices using data from the test set. We
    will use a scatter plot to plot the predictions and actual labels of our test
    set. In the following graph, we can see a line of best fit, along with the data
    points. Our model seems to capture the general trend in our data, despite having
    some outlandish predictions for some points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6f365c3-5ccb-4e46-b5ac-1c14c9d5819b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Moreover, we can plot a histogram that shows the distribution of our prediction
    errors. It appears that our model seems to do pretty well on most counts, but
    has some trouble predicting certain values, while overshooting and undershooting
    for a small number of observations, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ab62f15f-c7ec-4917-9000-a2143ca576fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Validating your approach using k-fold validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We noted earlier how our dataset is significantly smaller than the ones we
    previously dealt with. This leads to several complications while training and
    testing. Primarily, splitting the data into training and test samples, as we did,
    left us with only 100 validation samples. This is hardly enough for us to assuredly
    deploy our model, even if we wanted to. Furthermore, our test scores may change
    a lot depending on which segment of the data ended up in the test set. Hence,
    to reduce our reliance on any particular segment of our data for testing our model,
    we adopted a common machine learning approach known as **k-fold cross validation**.
    Essentially, we split our data into *n* number of smaller partitions and used
    the same number of neural networks to train on each of those smaller partitions
    of our data. Hence, a k-fold cross validation with five folds will split up our
    entire training data of 506 samples into five splits of 101 samples (and one with
    102). Then, we use five different neural networks, each of which trains on four
    splits out of the five data splits and tests itself on the remaining split of
    data. Then, we simply average the predictions from our five models to generate
    a single estimation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32ce8785-1897-46ee-8e63-c0c4a34e6a87.png)'
  prefs: []
  type: TYPE_IMG
- en: Cross validation with scikit-learn API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The advantage of cross validation over repeated random sub-sampling is that
    all of the observations are used for both training and validation, and each observation
    is used for validation exactly once.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows you how to implement a five-fold cross validation
    in Keras, where we use the entire dataset (training and testing together) and
    print out the averaged predictions of a network on each of the cross validation
    runs. As we can see, this is achieved by training the model on four random splits
    and testing it on the remaining split, per each cross validation run. We use the
    scikit-learn API wrapper provided by Keras and leverage the Keras regressor, along
    with sklearn''s standard scaler, k-fold cross-validator creator, and score evaluator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'You will notice that we constructed a function named `baseline_model()` to
    build our network. This is a useful way of constructing networks in many scenarios,
    but here it helps us feed the model object to the `KerasRegressor` function that
    we are using from the scikit-learn API wrapper that Keras provides. As many of
    you may well be aware, scikit-learn has been the go-to Python library for ML,
    with all sorts of pre-processing, scaling, normalizing, and algorithmic implementations.
    The Keras creators have implemented a scikit-learn wrapper to enable a certain
    degree of cross functionality between these libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We will take advantage of this cross functionality to perform our k-fold cross
    validation, as we did previously. Firstly, we will initialize a random number
    generator with a constant random seed. This simply gives us consistency in initializing
    our model weights, helping us to ensure that we can compare future models consistently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: We will create a list of estimators to pass to the sklearn transformation pipeline,
    which is useful to scale and process our data in sequence. To scale our values
    this time, we simply use the `StandardScaler()` preprocessing function from sklearn
    and append it to our list. We also append the Keras wrapper object to the same
    list. This Keras wrapper object is actually a regression estimator called `KerasRegressor`,
    and takes the model function we created, along with the desired number of batch
    size and training epochs as arguments. **Verbose** simply means how much feedback
    you want to see during the training process. By setting it to `0`, we ask our
    model to train silently.
  prefs: []
  type: TYPE_NORMAL
- en: Note that these are the same parameters that you would otherwise pass along
    to the `.fit()` function of the model, as we did earlier to initiate our training
    sessions.
  prefs: []
  type: TYPE_NORMAL
- en: Running the preceding code gives us an estimate of the average performance of
    our network for the five cross-validation runs we executed. The `results` variable
    stores the MSE scores of our network for each run of the cross validator. We then
    print out the mean and standard deviation (average variance) of MSEs over all
    five runs. Notice that we multiplied our mean value by `-1`. This is simply an
    implementational issue, as the unified scoring API of scikit-learn always maximizes
    a given score. However, in our case, we are trying to minimize our MSE. Hence,
    scores that need to be minimized are negated so that the unified scoring API can
    work correctly. The score that is returned is the negative version of the actual
    MSE.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how we can perform a regression task with neural networks.
    This involved some simple architectural changes to our previous classification
    models, pertaining to model construction (one output layer with no activation
    function) and the choice of `loss` function (MSE). We also tracked the MAE as
    a metric, since squared errors are not very intuitive to visualize. Finally, we
    plotted out our model's predictions versus the actual prediction labels using
    a scatter plot to better visualize how well the network did. We also used a histogram
    to understand the distribution of prediction errors in our model.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we introduced the methodology of k-fold cross validation, which is
    preferred over explicit train test splits of our data, in cases where we deal
    with very few data observations. What we did instead of splitting our data into
    a training and test split was split it into a *k* number of smaller partitions.
    Then, we generated a single estimate of predictions by using the same number of
    models as our data subsets. Each of these models were trained on a *k*-1 number
    of data partitions and tested on the remaining one data partition, after which
    their prediction scores were averaged. Doing so prevents our reliance on any particular
    split of our data for testing, and hence we get a more generalizable prediction
    estimate.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about **Convolutional Neural Networks** (**CNNs**).
    We will implement CNNs and detect objects using them. We will also solve some
    image recognition problems.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implement three different functions, each returning a network varying in size
    (depth and width). Use each of these functions and perform a k-fold cross validation.
    Assess which size fits best.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiment with MAE and MSE `loss` functions, and note the difference during
    training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiment with different `loss` functions and note the differences during training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiment with different regularization techniques and note the differences
    during training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
