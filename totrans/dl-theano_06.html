<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;6.&#xA0;Locating with Spatial Transformer Networks"><div class="book" id="21PMQ2-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06" class="calibre1"/>Chapter 6. Locating with Spatial Transformer Networks</h1></div></div></div><p class="calibre8">In this chapter, the NLP field is left to come back to images, and get an example of application of recurrent neural networks to images. In <a class="calibre1" title="Chapter 2. Classifying Handwritten Digits with a Feedforward Network" href="part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 2</a>, <span class="strong"><em class="calibre12">Classifying Handwritten Digits with a Feedforward Network</em></span> we addressed the case of image classification, consisting of predicting the class of an image. Here, we'll address object localization, a common task in computer vision as well, consisting of predicting the bounding box of an object in the image.</p><p class="calibre8">While <a class="calibre1" title="Chapter 2. Classifying Handwritten Digits with a Feedforward Network" href="part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 2</a>, <span class="strong"><em class="calibre12">Classifying Handwritten Digits with a Feedforward Network</em></span> solved the classification task with neural nets built with linear layers, convolutions, and non-linarites, the spatial transformer is a new module built on very specific equations dedicated to the localization task.</p><p class="calibre8">In order to locate multiple objects in the image, spatial transformers are composed with recurrent networks. This chapter takes the opportunity to show how to use prebuilt recurrent <a id="id258" class="calibre1"/>networks in <span class="strong"><strong class="calibre2">Lasagne</strong></span>, a library on top of Theano that brings extra modules, and helps you develop your neural networks very fast with pre-built components, while not changing the way you build and handle nets with Theano.</p><p class="calibre8">To sum up, the list of topics is composed of:</p><div class="book"><ul class="itemizedlist"><li class="listitem">An introduction to the Lasagne library </li><li class="listitem">Spatial transformer networks</li><li class="listitem">Classification network with spatial transformers</li><li class="listitem">Recurrent modules with Lasagne</li><li class="listitem">Recurrent read of digits</li><li class="listitem">Unsupervised training with hinge loss functions</li><li class="listitem">Region-based object localization neural nets</li></ul></div></div>

<div class="book" title="Chapter&#xA0;6.&#xA0;Locating with Spatial Transformer Networks">
<div class="book" title="MNIST CNN model with Lasagne"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch06lvl1sec61" class="calibre1"/>MNIST CNN model with Lasagne</h1></div></div></div><p class="calibre8">The Lasagne <a id="id259" class="calibre1"/>library has packaged layers and tools to handle neural nets easily. Let's first install the latest version of Lasagne:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">pip</strong></span> install --upgrade https://github.com/Lasagne/Lasagne/archive/master.zip</pre></div><p class="calibre8">Let us <a id="id260" class="calibre1"/>reprogram the MNIST model from <a class="calibre1" title="Chapter 2. Classifying Handwritten Digits with a Feedforward Network" href="part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 2</a>, <span class="strong"><em class="calibre12">Classifying Handwritten Digits with a Feedforward Network</em></span> with Lasagne:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">def </strong></span>model(l_input, <span class="strong"><strong class="calibre2">input_dim</strong></span>=28, <span class="strong"><strong class="calibre2">num_units</strong></span>=256, <span class="strong"><strong class="calibre2">num_classes</strong></span>=10, <span class="strong"><strong class="calibre2">p</strong></span>=.5):


    network = lasagne.layers.Conv2DLayer(
            l_input, <span class="strong"><strong class="calibre2">num_filters</strong></span>=32, <span class="strong"><strong class="calibre2">filter_size</strong></span>=(<span class="strong"><strong class="calibre2">5</strong></span>, <span class="strong"><strong class="calibre2">5</strong></span>),
            <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=lasagne.nonlinearities.rectify,
            <span class="strong"><strong class="calibre2">W</strong></span>=lasagne.init.GlorotUniform())

    network = lasagne.layers.MaxPool2DLayer(network, <span class="strong"><strong class="calibre2">pool_size</strong></span>=(2, 2))

    network = lasagne.layers.Conv2DLayer(
            network, <span class="strong"><strong class="calibre2">num_filters</strong></span>=32, <span class="strong"><strong class="calibre2">filter_size</strong></span>=(5, 5),
            <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=lasagne.nonlinearities.rectify)

    network = lasagne.layers.MaxPool2DLayer(network, <span class="strong"><strong class="calibre2">pool_size</strong></span>=(2, 2))

    <span class="strong"><strong class="calibre2">if</strong></span> num_units <span class="strong"><strong class="calibre2">&gt;</strong></span> 0:
        network = lasagne.layers.DenseLayer(
                lasagne.layers.dropout(network, <span class="strong"><strong class="calibre2">p</strong></span>=p),
                <span class="strong"><strong class="calibre2">num_units</strong></span>=num_units,
                <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=lasagne.nonlinearities.rectify)

    <span class="strong"><strong class="calibre2">if</strong></span> (num_units &gt; <span class="strong"><strong class="calibre2">0</strong></span>) and (num_classes &gt; <span class="strong"><strong class="calibre2">0</strong></span>):
        network = lasagne.layers.DenseLayer(
                lasagne.layers.dropout(network, <span class="strong"><strong class="calibre2">p</strong></span>=p),
                <span class="strong"><strong class="calibre2">num_units</strong></span>=num_classes,
                <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=lasagne.nonlinearities.softmax)

    <span class="strong"><strong class="calibre2">return</strong></span> network</pre></div><p class="calibre8">The layers are <code class="email">layer0_input</code>, <code class="email">conv1_out</code>, <code class="email">pooled_out</code>, <code class="email">conv2_out</code>, <code class="email">pooled2_out</code>, <code class="email">hidden_output</code>. They are built with pre-built modules, such as, <code class="email">InputLayer</code>, <code class="email">Conv2DLayer</code>, <code class="email">MaxPool2DLayer</code>, <code class="email">DenseLayer</code>, dropout non-linearities such as rectify or softmax, and initialization such as <code class="email">GlorotUniform</code>.</p><p class="calibre8">To <a id="id261" class="calibre1"/>connect the network graph composed of modules <a id="id262" class="calibre1"/>with the input symbolic <code class="email">var</code> and get the output <code class="email">var</code>, use the following code:</p><div class="informalexample"><pre class="programlisting">input_var = T.tensor4('inputs')
l_input = lasagne.layers.InputLayer(shape=(None, 1, 28, 28), <span class="strong"><strong class="calibre2">input_var</strong></span>=input_var)
network = mnist_cnn.model(l_input)
prediction = lasagne.layers.get_output(network)</pre></div><p class="calibre8">Or use this code:</p><div class="informalexample"><pre class="programlisting">l_input = lasagne.layers.InputLayer(shape=(None, 1, 28, 28))
network = mnist_cnn.model(l_input)

input_var = T.tensor4('inputs')
prediction = lasagne.layers.get_output(network, input_var)</pre></div><p class="calibre8">A very convenient feature is that you can print the output shape of any module:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">print</strong></span>(l_input.output_shape)</pre></div><p class="calibre8">Lasagne's <code class="email">get_all_params</code> method lists the parameters of the model:</p><div class="informalexample"><pre class="programlisting">params = lasagne.layers.get_all_params(network, <span class="strong"><strong class="calibre2">trainable</strong></span>=True)
<span class="strong"><strong class="calibre2">for</strong></span> p <span class="strong"><strong class="calibre2">in</strong></span> params:
    <span class="strong"><strong class="calibre2">print</strong></span> p.name</pre></div><p class="calibre8">Lastly, Lasagne comes with different learning rules, such as <code class="email">RMSprop</code>, <code class="email">Nesterov</code> <code class="email">Momentum</code>, <code class="email">Adam</code>, and <code class="email">Adagrad</code>:</p><div class="informalexample"><pre class="programlisting">target_var = T.ivector('targets')
loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)
loss = loss.mean()

updates <span class="strong"><strong class="calibre2">= </strong></span>lasagne.updates.nesterov_momentum(
        loss, params, <span class="strong"><strong class="calibre2">learning_rate</strong></span>=0.01, <span class="strong"><strong class="calibre2">momentum</strong></span>=0.9)

train_fn = theano.function([input_var, target_var], loss, <span class="strong"><strong class="calibre2">updates</strong></span>=updates)</pre></div><p class="calibre8">All other things remain unchanged.</p><p class="calibre8">To test our MNIST model, download the MNIST dataset:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">wget</strong></span> http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz -P /sharedfiles</pre></div><p class="calibre8">Train <a id="id263" class="calibre1"/>an MNIST classifier for digit classification:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">python</strong></span> 1-train-mnist.py</pre></div><p class="calibre8">The <a id="id264" class="calibre1"/>model parameters are saved in <code class="email">model.npz</code>. The accuracy is again above 99%.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="A localization network"><div class="book" id="22O7C2-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec62" class="calibre1"/>A localization network</h1></div></div></div><p class="calibre8">In <span class="strong"><strong class="calibre2">Spatial Transformer Networks</strong></span> (<span class="strong"><strong class="calibre2">STN</strong></span>), instead of applying the network directly to the <a id="id265" class="calibre1"/>input image signal, the idea is to add a module <a id="id266" class="calibre1"/>to preprocess the image and crop it, rotate it, and scale it to fit the object, to assist in classification:</p><div class="mediaobject"><img src="../images/00089.jpeg" alt="A localization network" class="calibre9"/><div class="caption"><p class="calibre29">Spatial Transformer Networks</p></div></div><p class="calibre10"> </p><p class="calibre8">For that purpose, STNs use a localization network to predict the affine transformation parameters and process the input:</p><div class="mediaobject"><img src="../images/00090.jpeg" alt="A localization network" class="calibre9"/><div class="caption"><p class="calibre29">Spatial transformer networks</p></div></div><p class="calibre10"> </p><p class="calibre8">In Theano, differentiation through the affine transformation is automatic, we simply have to connect the localization net with the input of the classification net through the affine transformation.</p><p class="calibre8">First, we <a id="id267" class="calibre1"/>create a localization network not very far from the MNIST CNN model, to predict six parameters of the affine transformation:</p><div class="informalexample"><pre class="programlisting">l_in <span class="strong"><strong class="calibre2">=</strong></span> lasagne.layers.InputLayer((None, dim, dim))
l_dim <span class="strong"><strong class="calibre2">=</strong></span> lasagne.layers.DimshuffleLayer(l_in, (0, 'x', 1, 2))
l_pool0_loc <span class="strong"><strong class="calibre2">=</strong></span> lasagne.layers.MaxPool2DLayer(l_dim, <span class="strong"><strong class="calibre2">pool_size</strong></span>=(2, 2))
l_dense_loc = mnist_cnn.model(l_pool0_loc, <span class="strong"><strong class="calibre2">input_dim</strong></span>=dim, <span class="strong"><strong class="calibre2">num_classes</strong></span>=0)

b <span class="strong"><strong class="calibre2">=</strong></span> np.zeros((2, 3), <span class="strong"><strong class="calibre2">dtype</strong></span>=theano.config.floatX)
b[0, 0] = 1.0
b[1, 1] = 1.0

l_A_net = lasagne.layers.DenseLayer(
    l_dense_loc,
    <span class="strong"><strong class="calibre2">num_units</strong></span>=6,
    <span class="strong"><strong class="calibre2">name</strong></span>='A_net',
    <span class="strong"><strong class="calibre2">b</strong></span>=b.flatten(),
    <span class="strong"><strong class="calibre2">W</strong></span>=lasagne.init.Constant(0.0),
    <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=lasagne.nonlinearities.identity)</pre></div><p class="calibre8">Here, we simply add to the input array, with <code class="email">DimshuffleLayer</code>, a channel dimension that will have only value 1. Such a dimension add is named a broadcast.</p><p class="calibre8">The pooling layer resizes the input image to <span class="strong"><em class="calibre12">50x50</em></span>, which is enough to determine the position of the digit.</p><p class="calibre8">The <a id="id268" class="calibre1"/>localization layer weight is initiated with zeros, except for the bias, which is initiated to the Identity affine parameters; the STN modules will not have any impact at the beginning and the full input image will be transmitted.</p><p class="calibre8">To crop given the affine parameters:</p><div class="informalexample"><pre class="programlisting">l_transform <span class="strong"><strong class="calibre2">=</strong></span> lasagne.layers.TransformerLayer(
    <span class="strong"><strong class="calibre2">incoming</strong></span>=l_dim,
    <span class="strong"><strong class="calibre2">localization_network</strong></span>=l_A_net,
    <span class="strong"><strong class="calibre2">downsample_factor</strong></span>=args.downsample)</pre></div><p class="calibre8">The <code class="email">down_sampling_factor</code> enables us to define the size of the output image with respect to the input. In this case, it is three, meaning the image will be <span class="strong"><em class="calibre12">33x33</em></span>—not very far from our MNIST digit size of <span class="strong"><em class="calibre12">28x28</em></span>. Lastly, we simply add our MNIST CNN model to classify the output:</p><div class="informalexample"><pre class="programlisting">l_out <span class="strong"><strong class="calibre2">=</strong></span> mnist_cnn.model(l_transform, <span class="strong"><strong class="calibre2">input_dim</strong></span>=dim, <span class="strong"><strong class="calibre2">p</strong></span>=sh_drp, <span class="strong"><strong class="calibre2">num_units</strong></span>=400)</pre></div><p class="calibre8">To test the classifier, let us create images of <span class="strong"><em class="calibre12">100x100</em></span> pixels, with some distortions and one digit:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">python</strong></span> create_mnist_sequence.py --nb_digits=1</pre></div><p class="calibre8">Plot the first three images (corresponding to 1, 0, 5):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">python</strong></span> plot_data.py mnist_sequence1_sample_8distortions_9x9.npz</pre></div><div class="mediaobject"><img src="../images/00091.jpeg" alt="A localization network" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Let's run the command to train the model:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">python</strong></span> 2-stn-cnn-mnist.py</pre></div><p class="calibre8">Here <a id="id269" class="calibre1"/>again, the accuracy gets above 99% when the digit is alone without distortions, which is typically not possible with the simple MNIST CNN model alone, and above 96.9% with distortions.</p><p class="calibre8">The command to plot the crops is:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">python</strong></span> plot_crops.py res_test_2.npz</pre></div><p class="calibre8">It gives us the following result:</p><div class="mediaobject"><img src="../images/00092.jpeg" alt="A localization network" class="calibre9"/></div><p class="calibre10"> </p><div class="mediaobject"><img src="../images/00093.jpeg" alt="A localization network" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">And <a id="id270" class="calibre1"/>with distortions:</p><div class="mediaobject"><img src="../images/00094.jpeg" alt="A localization network" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">STN can be thought of as a module to include in any network, at any place between two layers. To improve the classification results further, adding multiple STNs between different layers of a classification network helps get better results.</p><p class="calibre8">Here <a id="id271" class="calibre1"/>is an example of a network with two branches inside the network, each with its SPN that will, when unsupervised, try to catch different parts of the image to classify it:</p><div class="mediaobject"><img src="../images/00095.jpeg" alt="A localization network" class="calibre9"/><div class="caption"><p class="calibre29">(Spatial transformer networks paper, Jaderberg et al., 2015)</p></div></div><p class="calibre10"> </p></div>

<div class="book" title="A localization network">
<div class="book" title="Recurrent neural net applied to images"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch06lvl2sec22" class="calibre1"/>Recurrent neural net applied to images</h2></div></div></div><p class="calibre8">The idea <a id="id272" class="calibre1"/>is to use recurrency to read multiple digits, instead of just one:</p><div class="mediaobject"><img src="../images/00096.jpeg" alt="Recurrent neural net applied to images" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">In order to read multiple digits, we simply replace the localization feedforward network with a recurrent network that will output multiple affine transformations corresponding to each digit:</p><div class="mediaobject"><img src="../images/00097.jpeg" alt="Recurrent neural net applied to images" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">From the <a id="id273" class="calibre1"/>previous example, we replace the fully connected layer with the GRU layer:</p><div class="informalexample"><pre class="programlisting">l_conv2_loc <span class="strong"><strong class="calibre2">=</strong></span> mnist_cnn.model(l_pool0_loc, <span class="strong"><strong class="calibre2">input_dim</strong></span>=dim, <span class="strong"><strong class="calibre2">p</strong></span>=sh_drp, <span class="strong"><strong class="calibre2">num_units</strong></span>=0)

<span class="strong"><strong class="calibre2">class</strong></span> Repeat(<span class="strong"><strong class="calibre2">lasagne.layers.Layer</strong></span>):
    <span class="strong"><strong class="calibre2">def __init__(</strong></span>self, incoming, n, **kwargs):
        super(Repeat, self).<span class="strong"><strong class="calibre2">__init__</strong></span>(incoming, **kwargs)
        self.n = n

    <span class="strong"><strong class="calibre2">def</strong></span> get_output_shape_for(self, input_shape):
        <span class="strong"><strong class="calibre2">return</strong></span> tuple([input_shape[0], self.n] + list(input_shape[1:]))

    <span class="strong"><strong class="calibre2">def</strong></span> get_output_for(self, input, <span class="strong"><strong class="calibre2">**</strong></span>kwargs):
        tensors = [input]*self.n
        stacked = theano.tensor.stack(*tensors)
        dim = [1, 0] + range(2, input.ndim+1)
        <span class="strong"><strong class="calibre2">return</strong></span> stacked.dimshuffle(dim)

l_repeat_loc = Repeat(l_conv2_loc, <span class="strong"><strong class="calibre2">n</strong></span>=num_steps)
l_gru = lasagne.layers.GRULayer(l_repeat_loc, <span class="strong"><strong class="calibre2">num_units</strong></span>=num_rnn_units,
<span class="strong"><strong class="calibre2">unroll_scan</strong></span>=True)

l_shp <span class="strong"><strong class="calibre2">=</strong></span> lasagne.layers.ReshapeLayer(l_gru, (-1, num_rnn_units))  </pre></div><p class="calibre8">This outputs a tensor of dimension (None, 3, 256), where the first dimension is the batch size, 3 is the number of steps in the GRU, and 256 is the hidden layer size. On top of this layer, we simply add the same fully connected layer as before to output three identity images at the beginning:</p><div class="informalexample"><pre class="programlisting">b = np.zeros((2, 3), <span class="strong"><strong class="calibre2">dtype</strong></span>=theano.config.floatX)
b[0, 0] = 1.0
b[1, 1] = 1.0

l_A_net = lasagne.layers.DenseLayer(
    l_shp,
    <span class="strong"><strong class="calibre2">num_units</strong></span>=6,
    <span class="strong"><strong class="calibre2">name</strong></span>='A_net',
    <span class="strong"><strong class="calibre2">b</strong></span>=b.flatten(),
    <span class="strong"><strong class="calibre2">W</strong></span>=lasagne.init.Constant(0.0),
    <span class="strong"><strong class="calibre2">nonlinearity</strong></span>=lasagne.nonlinearities.identity)

l_conv_to_transform <span class="strong"><strong class="calibre2">=</strong></span> lasagne.layers.ReshapeLayer(
    Repeat(l_dim, <span class="strong"><strong class="calibre2">n</strong></span>=num_steps), [-1] + list(l_dim.output_shape[-3:]))

l_transform = lasagne.layers.TransformerLayer(
    <span class="strong"><strong class="calibre2">incoming</strong></span>=l_conv_to_transform,
    <span class="strong"><strong class="calibre2">localization_network</strong></span>=l_A_net,
    <span class="strong"><strong class="calibre2">downsample_factor</strong></span>=args.downsample)

l_out = mnist_cnn.model(l_transform, <span class="strong"><strong class="calibre2">input_dim=</strong></span>dim, <span class="strong"><strong class="calibre2">p=</strong></span>sh_drp, <span class="strong"><strong class="calibre2">num_units=</strong></span>400)</pre></div><p class="calibre8">To test <a id="id274" class="calibre1"/>the classifier, let us create images of <span class="strong"><em class="calibre12">100x100</em></span> pixels with some distortions, and three digits this time:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">python</strong></span> create_mnist_sequence.py --nb_digits=3 --output_dim=100</pre></div><p class="calibre8">Plot the first three images (corresponding to sequences <span class="strong"><strong class="calibre2">296</strong></span>, <span class="strong"><strong class="calibre2">490</strong></span>, <span class="strong"><strong class="calibre2">125</strong></span>):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">python</strong></span> plot_data.py mnist_sequence3_sample_8distortions_9x9.npz</pre></div><div class="mediaobject"><img src="../images/00098.jpeg" alt="Recurrent neural net applied to images" class="calibre9"/></div><p class="calibre10"> </p><div class="mediaobject"><img src="../images/00099.jpeg" alt="Recurrent neural net applied to images" class="calibre9"/></div><p class="calibre10"> </p><div class="mediaobject"><img src="../images/00100.jpeg" alt="Recurrent neural net applied to images" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Let's run <a id="id275" class="calibre1"/>the command to train our recurrent model:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">python</strong></span> 3-recurrent-stn-mnist.py
<span class="strong"><em class="calibre12">Epoch 0 Acc Valid 0.268833333333, Acc Train = 0.268777777778, Acc Test = 0.272466666667</em></span>
<span class="strong"><em class="calibre12">Epoch 1 Acc Valid 0.621733333333, Acc Train = 0.611116666667, Acc Test = 0.6086</em></span>
<span class="strong"><em class="calibre12">Epoch 2 Acc Valid 0.764066666667, Acc Train = 0.75775, Acc Test = 0.764866666667</em></span>
<span class="strong"><em class="calibre12">Epoch 3 Acc Valid 0.860233333333, Acc Train = 0.852294444444, Acc Test = 0.859566666667</em></span>
<span class="strong"><em class="calibre12">Epoch 4 Acc Valid 0.895333333333, Acc Train = 0.892066666667, Acc Test = 0.8977</em></span>
<span class="strong"><em class="calibre12">Epoch 53 Acc Valid 0.980433333333, Acc Train = 0.984261111111, Acc Test = 0.97926666666</em></span>
</pre></div><p class="calibre8">The <a id="id276" class="calibre1"/>classification accuracy is 99.3%.</p><p class="calibre8">Plot the crops:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">python</strong></span> plot_crops.py res_test_3.npz</pre></div><div class="mediaobject"><img src="../images/00101.jpeg" alt="Recurrent neural net applied to images" class="calibre9"/></div><p class="calibre10"> </p></div></div>
<div class="book" title="Unsupervised learning with co-localization" id="23MNU1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec63" class="calibre1"/>Unsupervised learning with co-localization</h1></div></div></div><p class="calibre8">The <a id="id277" class="calibre1"/>first layers of the digit classifier trained in <a class="calibre1" title="Chapter 2. Classifying Handwritten Digits with a Feedforward Network" href="part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 2</a>, <span class="strong"><em class="calibre12">Classifying Handwritten Digits with a Feedforward Network</em></span> as an encoding function to represent the image in an embedding space, as for words:</p><div class="mediaobject"><img src="../images/00102.jpeg" alt="Unsupervised learning with co-localization" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">It is possible to train unsurprisingly the localization network of the spatial transformer network by minimizing the hinge loss objective function on random sets of two images supposed to contain the same digit:</p><div class="mediaobject"><img src="../images/00103.jpeg" alt="Unsupervised learning with co-localization" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Minimizing this sum leads to modifying the weights in the localization network, so that two localized digits become closer than two random crops.</p><p class="calibre8">Here <a id="id278" class="calibre1"/>are the results:</p><div class="mediaobject"><img src="../images/00104.jpeg" alt="Unsupervised learning with co-localization" class="calibre9"/><div class="caption"><p class="calibre29">(Spatial transformer networks paper, Jaderberg et al., 2015)</p></div></div><p class="calibre10"> </p></div>
<div class="book" title="Region-based localization networks" id="24L8G1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec64" class="calibre1"/>Region-based localization networks</h1></div></div></div><p class="calibre8">Historically, the basic <a id="id279" class="calibre1"/>approach in object localization was to use a classification network in a sliding window; it consists of sliding a window one pixel by one pixel in each direction and applying a classifier at each position and each scale in the image. The classifier learns to say if the object is present and centered. It requires a large amount of computations since the model has to be evaluated at every position and scale.</p><p class="calibre8">To accelerate <a id="id280" class="calibre1"/>such a process, the <span class="strong"><strong class="calibre2">Region Proposal Network</strong></span> (<span class="strong"><strong class="calibre2">RPN</strong></span>) in the Fast-R-CNN paper from the researcher Ross Girshick consists of transforming the fully connected layers of a neural net classifier such as MNIST CNN into convolutional layers as well; in fact, network dense on 28x28 image, there is no difference between a convolution and a linear layer when the convolution kernel has the same dimensions as the input. So, any fully connected layers can be rewritten as convolutional layers, with the same weights and the appropriate kernel dimensions, which enables the network to work on a wider image than 28x28, at any size, outputting a feature map with a classification score at each position. The only difference may come from the stride of the whole network, which can be set different to 1 and can be large (a few 10 pixels) with convolution kernels set to stride different to 1 in order to reduce the number of evaluation positions and thus the computations. Such a transformation is worth because the convolutions are very efficient:</p><div class="mediaobject"><img src="../images/00105.jpeg" alt="Region-based localization networks" class="calibre9"/><div class="caption"><p class="calibre29">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</p></div></div><p class="calibre10"> </p><p class="calibre8">An <a id="id281" class="calibre1"/>end-to-end network has been designed, taking ideas from deconvolution principles where an output feature map gives all bounding <a id="id282" class="calibre1"/>boxes at once: <span class="strong"><strong class="calibre2">You Only Look Once</strong></span> (<span class="strong"><strong class="calibre2">YOLO</strong></span>) architecture predicts B possible bounding boxes for each position in the feature map. Each bounding box is defined by its coordinates (x, y, w, h) in proportion <a id="id283" class="calibre1"/>to the whole image as a regression problem, and a confidence (probability) that corresponds to the <span class="strong"><strong class="calibre2">Intersection over Union</strong></span> (<span class="strong"><strong class="calibre2">IOU</strong></span>) between the box and the true box. Comparable approaches are proposed with SSD models.</p><p class="calibre8">Lastly, segmentation networks introduced in <a class="calibre1" title="Chapter 8. Translating and Explaining with Encoding – decoding Networks" href="part0083_split_000.html#2F4UM2-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 8, </a>
<span class="strong"><em class="calibre12">Translating and Explaining with Encoding – decoding Networks</em></span> can also be considered as neural net implementations towards localizing objects.</p></div>
<div class="book" title="Further reading" id="25JP21-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec65" class="calibre1"/>Further reading</h1></div></div></div><p class="calibre8">You can further refer to these sources for more information:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Spatial Transformer Networks, Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu, Jun 2015</li><li class="listitem">Recurrent Spatial Transformer Networks, Søren Kaae Sønderby, Casper Kaae Sønderby, Lars Maaløe, Ole Winther, Sept 2015</li><li class="listitem">Original code: <a class="calibre1" href="https://github.com/skaae/recurrent-spatial-transformer-code">https://github.com/skaae/recurrent-spatial-transformer-code</a></li><li class="listitem">Google Street View Character Recognition, Jiyue Wang, Peng Hui How</li><li class="listitem">Reading Text in the Wild with Convolutional Neural Networks, Max Jaderberg, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman, 2014</li><li class="listitem">Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks, Ian J. Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, Vinay Shet, 2013</li><li class="listitem">Recognizing Characters From Google Street View Images, Guan Wang, Jingrui Zhang</li><li class="listitem">Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition, Max Jaderberg, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman, 2014</li><li class="listitem">R-CNN minus R, Karel Lenc, Andrea Vedaldi, 2015</li><li class="listitem">Fast R-CNN, Ross Girshick, 2015</li><li class="listitem">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun, 2015</li><li class="listitem">You Only Look Once: Unified, Real-Time Object Detection, Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi, Jun 2015</li><li class="listitem">YOLO demo in real time <a class="calibre1" href="http://pjreddie.com/darknet/yolo/">http://pjreddie.com/darknet/yolo/</a></li><li class="listitem">YOLO9000: Better, Faster, Stronger, Joseph Redmon, Ali Farhadi, Dec 2016</li><li class="listitem">SSD: Single Shot MultiBox Detector, Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, Dec 2015</li><li class="listitem">Rich feature hierarchies for accurate object detection and semantic segmentation, Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, 2013</li><li class="listitem">Text Flow: A Unified Text Detection System in Natural Scene Images Shangxuan Tian, Yifeng Pan, Chang Huang, Shijian Lu, Kai Yu, Chew Lim Tan, 2016</li></ul></div></div>
<div class="book" title="Summary" id="26I9K1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec66" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">The spatial transformer layer is an original module to localize an area of the image, crop it and resize it to help the classifier focus on the relevant part in the image, and increase its accuracy. The layer is composed of differentiable affine transformation, for which the parameters are computed through another model, the localization network, and can be learned via backpropagation as usual.</p><p class="calibre8">An example of the application to reading multiple digits in an image can be inferred with the use of recurrent neural units. To simplify our work, the Lasagne library was introduced.</p><p class="calibre8">Spatial transformers are one solution among many others for localizations; region-based localizations, such as YOLO, SSD, or Faster RCNN, provide state-of-the-art results for bounding box prediction.</p><p class="calibre8">In the next chapter, we'll continue with image recognition to discover how to classify full size images that contain a lot more information than digits, such as natural images of indoor scenes and outdoor landscapes. In the meantime, we'll continue with Lasagne's prebuilt layer and optimization modules.</p></div></body></html>