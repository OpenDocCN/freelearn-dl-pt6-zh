<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building a Deep Convolutional Neural Network</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Inaccuracy of<span> </span>traditional<span> </span>neural network when images are translated</li>
<li>Building a CNN from scratch using Python</li>
<li>CNNs to improve accuracy in case of image translation</li>
<li>Gender classification using CNN</li>
<li>Data augmentation to improve network accuracy</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we looked at a traditional deep feedforward neural network. One of the limitations of a traditional <span>deep feedforw</span>ard neural network is that it is not translation-invariant, that is, a cat image in the upper-right corner of an image would be considered different from an image that has a cat in the center of the image. Additionally, traditional neural networks are affected by the scale of an object. If the object is big in the majority of the images and a new image has the same object in it but with a smaller scale (occupies a smaller portion of the image), traditional neural networks are likely to fail in classifying the image.</p>
<p><strong>Convolutional Neural Networks</strong> (<strong>CNNs</strong>) are used to deal with such issues. Given that a CNN is able to deal with translation in images and also the scale of images, it is considered a lot more useful in object classification/ detection.</p>
<p>In this chapter, you will learn about the following:</p>
<ul>
<li>Inaccuracy of traditional neural network when images are translated</li>
<li>Building a CNN from scratch using Python</li>
<li>Using CNNs to improve image classification on a MNIST dataset</li>
<li>Implementing data augmentation to improve network accuracy</li>
<li>Gender classification using CNNs</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Inaccuracy of traditional neural networks when images are translated</h1>
                </header>
            
            <article>
                
<p>To understand the need of CNNs further, we will first understand why a feed forward <strong>Neural Network</strong> (<strong>NN</strong>) does not work when an image is translated and then see how the CNN improves upon traditional feed forward NN.</p>
<p>Let's go through the following scenario:</p>
<ul>
<li>We will build a NN model to predict labels from the MNIST dataset</li>
<li>We will consider all images that have a label of 1 and take an average of all of them (generating an average of 1 image)</li>
<li>We will predict the label of the average 1 image that we have generated in the previous step using traditional NN</li>
<li>We will translate the average 1 image by 1 pixel to the left or right</li>
<li>We will make a prediction of the translated image using our traditional NN model</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The strategy defined above is coded as follows (please refer to <kbd>Issue_with_image translation.ipynb</kbd> file in GitHub while implementing the code)</p>
<ol>
<li>Download the dataset and extract the train and test MNIST datasets:</li>
</ol>
<pre style="padding-left: 60px">from keras.datasets import mnist<br/>from keras.layers import Flatten, Dense<br/>from keras.models import Sequential<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>(X_train, y_train), (X_test, y_test) = mnist.load_data()</pre>
<ol start="2">
<li>Fetch the training set corresponding to label <kbd>1</kbd> only:</li>
</ol>
<pre style="padding-left: 60px">X_train1 = X_train[y_train==1]</pre>
<ol start="3">
<li>Reshape and normalize the original training dataset:</li>
</ol>
<pre style="padding-left: 60px">num_pixels = X_train.shape[1] * X_train.shape[2]<br/>X_train = X_train.reshape(X_train.shape[0],num_pixels).astype('float32')<br/>X_test = X_test.reshape(X_test.shape[0],num_pixels).astype('float32')<br/>X_train = X_train / 255<br/>X_test = X_test / 255</pre>
<ol start="4">
<li>One-hot-encode the output labels:</li>
</ol>
<pre style="padding-left: 60px">y_train = np_utils.to_categorical(y_train)<br/>y_test = np_utils.to_categorical(y_test)<br/>num_classes = y_train.shape[1]</pre>
<ol start="5">
<li>Build a model and fit it:</li>
</ol>
<pre style="padding-left: 60px">model = Sequential()<br/>model.add(Dense(1000, input_dim=num_pixels, activation='relu'))<br/>model.add(Dense(num_classes, activation='softmax'))<br/>model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])<br/>model.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=5, batch_size=1024, verbose=1)</pre>
<ol start="6">
<li>Let's plot the average 1 image that we obtained in step 2:</li>
</ol>
<pre style="padding-left: 60px">pic=np.zeros((28,28))<br/>pic2=np.copy(pic)<br/>for i in range(X_train1.shape[0]):<br/>    pic2=X_train1[i,:,:]<br/>    pic=pic+pic2<br/>pic=(pic/X_train1.shape[0])<br/>plt.imshow(pic)</pre>
<p style="padding-left: 30px">In the preceding code, we initialized an empty picture that is 28 x 28 in dimension and took an average pixel value at the various pixel locations of images that have a label of 1 (the <kbd>X_train1</kbd> object) by looping through all the values in <span>the </span><kbd>X_train1</kbd><span> object</span>.</p>
<p style="padding-left: 30px">The plot of the average 1 image appears as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/11fffca9-ea5a-4ea8-8fc4-59bfc3402c22.png" width="268" height="258"/></p>
<p>It is to be noted that the more yellow (thick) the pixel is, the more often people have written on top of the pixel, and the less yellow (more blue/less thick) the pixel, the less often people have written on top of the pixel. Also, it is to be noted that the pixel in the middle is the yellowest/thickest (this is because most people would be writing over the middle pixels, irrespective of whether the whole digit is written in a vertical line or is slanted toward the left or right).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Problems with traditional NN</h1>
                </header>
            
            <article>
                
<p><strong>Scenario 1</strong>: Let's create a new image where the original image is translated by 1 pixel toward the left. In the following code, we are looping through the columns of the image and copying the pixel values of the next column to the current column:</p>
<pre>for i in range(pic.shape[0]):<br/>     if i&lt;20:<br/>         pic[:,i]=pic[:,i+1]<br/>     plt.imshow(pic)</pre>
<p>The left translated average 1 image looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/d18d3b21-d722-4c43-9860-79b25e008b2e.png" style="width:21.92em;height:21.92em;" width="256" height="257"/></p>
<p>Let’s go ahead and predict the label of the image using the built model:</p>
<pre>model.predict(pic.reshape(1,784)/255)</pre>
<p>The model's prediction on the translated image is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/a8682fa3-d0d9-4dec-9ba8-bf00aea80760.png" style="width:43.08em;height:4.58em;" width="526" height="56"/></p>
<p>We can see a prediction of 1, though with a lower probability than when pixels were not translated.</p>
<p><strong>Scenario 2</strong>: A new image is created in which the pixels of the original average 1 image are shifted by 2 pixels to the right:</p>
<pre>pic=np.zeros((28,28))<br/>pic2=np.copy(pic)<br/>for i in range(X_train1.shape[0]):<br/>    pic2=X_train1[i,:,:]<br/>    pic=pic+pic2<br/>pic=(pic/X_train1.shape[0])<br/>pic2=np.copy(pic)<br/>for i in range(pic.shape[0]):<br/>    if ((i&gt;6) and (i&lt;26)):<br/>    pic[:,i]=pic2[:,(i-1)]<br/>plt.imshow(pic)</pre>
<p>The right translated average 1 image looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/e2fed1a8-3e6d-4a03-ad3e-3ff32ce0d59c.png" style="width:21.25em;height:20.83em;" width="258" height="254"/></p>
<p>The prediction of this image is as follows:</p>
<pre>model.predict(pic.reshape(1,784)/255)</pre>
<p><span>The model's prediction on the translated image is as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/a2d7d8ca-e1ab-44d3-b039-0199a5ef6b43.png" style="width:43.75em;height:4.50em;" width="544" height="56"/></p>
<p>We can see that the prediction is incorrect with an output of 3. This is the problem that we will be addressing by using a CNN.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building a CNN from scratch using Python</h1>
                </header>
            
            <article>
                
<p>In this section, we will learn about how a CNN works by building a feedforward network from scratch using NumPy.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>A typical CNN has multiple components. In this section, we will go through the various components of a CNN before we understand how the CNN improves prediction accuracy when an image is translated.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Understanding convolution</h1>
                </header>
            
            <article>
                
<p>We are already aware of how a typical NN works. In this section, let's understand the working details of the convolution process in CNN.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Filter</h1>
                </header>
            
            <article>
                
<p>A convolution is a multiplication between two matrices—one matrix being big and the other being small. To understand convolution, consider the following example:</p>
<p>Matrix <em>A</em> is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1250 image-border" src="Images/2dd6021a-9a7e-45c5-9e81-94a543cb720d.png" style="width:13.17em;height:6.33em;" width="186" height="89"/></p>
<p>Matrix <em>B</em> is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1251 image-border" src="Images/e6ad679b-8864-42af-948f-745bea146353.png" style="width:6.50em;height:3.42em;" width="97" height="50"/></p>
<p>When performing convolutions, think of it as we are sliding the smaller matrix over the larger matrix, that is, we can potentially come up with nine such multiplications as the smaller matrix is slid over the entire area of the bigger matrix. Note that it is not matrix multiplication.</p>
<p>The various multiplications that happen between the bigger and smaller matrix are as follows:</p>
<ol>
<li><em>{1, 2, 5, 6}</em> of the bigger matrix is multiplied with <em>{1, 2, 3, 4}</em> of the smaller matrix:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><em>1*1 + 2*2 + 5*3 + 6*4 = 44</em></p>
<ol start="2">
<li><em>{2, 3, 6, 7}</em> of the bigger matrix is multiplied with <em>{1, 2, 3, 4}</em> of the smaller matrix:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><em>2*1 + 3*2 + 6*3 + 7*4 = 54</em></p>
<ol start="3">
<li><em>{3, 4, 7, 8}</em> of the bigger matrix is multiplied with <em>{1, 2, 3, 4}</em> of the smaller matrix:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><em>3*1 + 4*2 + 7*3 + 8*4 = 64</em></p>
<ol start="4">
<li><em>{5, 6, 9, 10}</em> of the bigger matrix is multiplied with <em>{1, 2, 3, 4}</em> of the smaller matrix:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><em>5*1 + 6*2 + 9*3 + 10*4 = 84</em></p>
<ol start="5">
<li><em>{6, 7, 10, 11}</em> of the bigger matrix is multiplied with <em>{1, 2, 3, 4}</em> of the smaller matrix:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><em>6*1 + 7*2 + 10*3 + 11*4 = 94</em></p>
<ol start="6">
<li><em>{7, 8, 11, 12}</em> of the bigger matrix is multiplied with <em>{1, 2, 3, 4}</em> of the smaller matrix:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><em>7*1 + 8*2 + 11*3 + 12*4 = 104</em></p>
<ol start="7">
<li><em>{9,10,13,14}</em> of the bigger matrix is multiplied with <em>{1, 2, 3, 4}</em> of the smaller matrix:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><em>9*1 + 10*2 + 13*3 + 14*4 = 124</em></p>
<ol start="8">
<li><em>{10, 11, 14, 15}</em> of the bigger matrix is multiplied with <em>{1, 2, 3 ,4}</em> of the smaller matrix:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><em>10*1 + 11*2 + 14*3 + 15*4 = 134</em></p>
<ol start="9">
<li><em>{11, 12, 15, 16}</em> of the bigger matrix is multiplied with <em>{1, 2, 3, 4}</em> of the smaller matrix:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><em>11*1 + 12*2 + 15*3 + 16*4 = 144</em></p>
<p style="padding-left: 60px">The result of the preceding steps would be the following matrix:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1252 image-border" src="Images/d341e2cc-5f69-4bb6-9d34-f6cb88e9ded8.png" style="width:10.75em;height:5.92em;" width="156" height="86"/></p>
<p>Conventionally, the smaller matrix is called a filter or kernel and the smaller matrix values are arrived at statistically through gradient descent. The values within the filter are the constituent weights.</p>
<p>Practically, when the image input shape is 224 x 224 x 3, where there are 3 channels, a filter that has a shape of 3 x 3 would also have 3 channels so that performing the matrix multiplication (sum product) is enabled.</p>
<div class="packt_tip packt_infobox">A filter will have as many channels as the number of channels in the matrix it multiplies with.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Strides</h1>
                </header>
            
            <article>
                
<p>In the preceding steps, given that the filter moved one step at a time both horizontally and vertically, the strides for the filter are (1, 1). The higher the number of strides, the higher the number of values that are skipped from the matrix multiplication.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Padding</h1>
                </header>
            
            <article>
                
<p>In the preceding steps, we missed out on multiplying the leftmost values of the filter with the rightmost values of the original matrix. If we were to perform such an operation, we would have ensure that there is zero padding around the edges (the edges of the image padded with zeros) of the original matrix. This form of padding is called <strong>valid</strong> padding. The matrix multiplication we performed in the <em>Filter</em> section of the <em>Understanding convolution</em> recipe was a result of the <strong>same</strong> padding.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">From convolution to activation</h1>
                </header>
            
            <article>
                
<p>In a traditional NN, a hidden layer not only multiplies the input values by the weights, but also applies a non-linearity to the data, that is, it passes the values through an activation function.</p>
<p>A similar activity happens in a typical CNN too, where the convolution is passed through an activation function. CNN supports the traditional activations functions we have seen so far: sigmoid, ReLU, tanh, and leaky ReLU.</p>
<p>For the preceding output, we can see that the output remains the same when passed through a ReLU activation function, as all the numbers are positive.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">From convolution activation to pooling</h1>
                </header>
            
            <article>
                
<p>In the previous section, we looked at how convolutions work. In this section, we will understand the typical next step after a convolution: pooling.</p>
<p>Let's say the output of the convolution step is as follows (we are not considering the preceding example, and this is a new example to only illustrate how pooling works):</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1253 image-border" src="Images/1262742f-f600-4d1e-96f8-8abe2641e16f.png" style="width:6.50em;height:3.33em;" width="98" height="51"/></p>
<p>In the preceding case, the output of a convolution step is a 2 x 2 matrix. Max pooling considers the 2 x 2 block and gives the maximum value as output. Similarly, imagine that the output of the convolution step is a bigger matrix, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1254 image-border" src="Images/9e1ef209-932a-43ac-beec-86f01d8aa9e5.png" style="width:12.83em;height:6.75em;" width="200" height="106"/></p>
<p>Max pooling divides the big matrix into non-overlapping blocks of 2 x 2 (when the stride value is 2), as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1255 image-border" src="Images/7e811be5-036f-47ac-a50c-2a6c992987dd.png" style="width:12.08em;height:5.58em;" width="186" height="88"/></p>
<p>From each block, only the element that has the highest value is chosen. So, the output of the max pooling operation on the preceding matrix would be the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1256 image-border" src="Images/b947e987-a657-4837-bde2-4ea0c4cdae85.png" style="width:8.17em;height:4.08em;" width="99" height="49"/></p>
<p>In practice, it is not necessary to have a 2 x 2 window in all cases, but it is used more often than not.</p>
<p>The other types of pooling involved are sum and average—again, in practice, we see a lot of max pooling when compared to other types of pooling.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How do convolution and pooling help?</h1>
                </header>
            
            <article>
                
<p>One of the drawbacks of traditional NN in the MNIST example is that each pixel is associated with a distinct weight.</p>
<p>Thus, if an adjacent pixel, other than the original pixel, were to be highlighted, instead of the original pixel, the output would not be very accurate (the example of <em>scenario 1</em>, where the average one was slightly to the left of the middle).</p>
<p>This scenario is now addressed, as the pixels share weights that are constituted within each filter.</p>
<p>All the pixels get multiplied by all the weights that constitute a filter. In the pooling layer, only the values post convolution that have a high value are chosen.</p>
<p>This way, irrespective of whether the highlighted pixel is at the center or is slightly away from the center, the output would more often than not be the expected value.</p>
<div class="packt_tip packt_infobox">However, the issue still remains the same when the highlighted pixels are very far away from the center.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>To gain a solid understanding, we'll build a CNN-based architecture using Keras and validate our understanding of how CNN works by matching the output obtained by building the feedforward propagation part of CNN from scratch with the output obtained from Keras.</p>
<p>Let's implement CNN with a toy example where the input and expected output data is defined (the code file is available as <kbd>CNN_working_details.ipynb</kbd> in GitHub):</p>
<ol>
<li>Create the input and output dataset:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>X_train=np.array([[[1,2,3,4],[2,3,4,5],[5,6,7,8],[1,3,4,5]],<br/>[[-1,2,3,-4],[2,-3,4,5],[-5,6,-7,8],[-1,-3,-4,-5]]])<br/>y_train=np.array([0,1])</pre>
<p style="padding-left: 60px">In the preceding code, we created data where positive input gives an output of <kbd>0</kbd> and negative input gives an output of <kbd>1</kbd>.</p>
<ol start="2">
<li>Scale the input dataset:</li>
</ol>
<pre style="padding-left: 60px">X_train = X_train / 8</pre>
<ol start="3">
<li>Reshape the input dataset so that each input image is represented in the format of width <kbd>x</kbd> height <kbd>x</kbd> number of channels:</li>
</ol>
<pre style="padding-left: 60px">X_train = X_train.reshape(X_train.shape[0],X_train.shape[1],X_train.shape[1],1 ).astype('float32')</pre>
<ol start="4">
<li>Build the model architecture:</li>
</ol>
<p style="padding-left: 60px">Instantiate the model after importing the relevant methods:</p>
<pre style="padding-left: 60px">from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense<br/>from keras.models import Sequential<br/>model = Sequential()</pre>
<p style="padding-left: 60px">In the next step, we are performing the convolution operation:</p>
<pre style="padding-left: 60px">model.add(Conv2D(1, (3,3), input_shape=(4,4,1),activation='relu'))</pre>
<p style="padding-left: 60px">In the preceding step, we are performing a 2D convolution (the matrix multiplication that we saw in the section on <em>Understanding convolution</em>) on input data where we have 1 filter of size 3 x 3.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">Additionally, given that this is the first layer since instantiating a model, we specify the input shape, which is (4 , 4, 1)</p>
<p style="padding-left: 60px">Finally, we perform ReLu activation on top of the output of the convolution.</p>
<p style="padding-left: 60px">The output of the convolution operation in this scenario is 2 x 2 x 1 in shape, as the matrix multiplication of <span><span>weights </span></span>with input would yield a 2 x 2 matrix (given that the default strides is 1 x 1).</p>
<p style="padding-left: 60px">Additionally, the size of output would shrink, as we have not padded the input (put zeros around the input image).</p>
<p style="padding-left: 60px">In the following step, we are adding a layer that performs a max pooling operation, as follows:</p>
<pre style="padding-left: 60px">model.add(MaxPooling2D(pool_size=(2, 2))) </pre>
<p style="padding-left: 60px">We are performing max pooling on top of the output obtained from the previous layer, where the pool size is 2 x 2. This means that the maximum value in a subset of the 2 x 2 portion of the image is calculated.</p>
<p style="padding-left: 60px">Note that a stride of 2 × 2 in the pooling layer would not affect the output in this case as the output of the previous step was 2 × 2. However, in general, a stride that is of a greater size than 1 × 1 would affect the output shape.</p>
<p style="padding-left: 60px"> Let's flatten the output from the pooling layer:</p>
<pre style="padding-left: 60px">model.add(Flatten())</pre>
<p style="padding-left: 60px">Once we perform flattening, the process becomes very similar to what we performed in standard feedforward neural networks where the input is connected to the hidden layer and then to the output layer (we can connect the input to more hidden layers, too!). </p>
<p style="padding-left: 60px"><span>We are directly connecting the output of the flatten layer to the output layer using the sigmoid activation:</span></p>
<pre style="padding-left: 60px">model.add(Dense(1, activation='sigmoid'))</pre>
<p style="padding-left: 60px">A summary of the model can be obtained and looks as follows:</p>
<pre style="padding-left: 60px">model.summary()</pre>
<p style="padding-left: 60px"> A summary of the output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1257 image-border" src="Images/6e708f38-84ee-4c40-9b05-299cf9c52816.png" style="width:38.17em;height:18.25em;" width="521" height="249"/></p>
<p style="padding-left: 60px">Note that there are 10 parameters in the convolution layer as the one 3 x 3 filter would have 9 weights and 1 bias term. The pooling layer and flatten layer do not have any parameters as they are either extracting maximum values in certain regioned (max pooling) or are flattening the output from the previous layer (flatten) and thus no operation where weights need to be modified in either of these layers.</p>
<p style="padding-left: 60px">The output layer has two parameters since the flatten layer has one output, which is connected to the output layer that has one value—hence we will have one weight and one bias term connecting the flatten layer and output layer.</p>
<ol start="5">
<li>Compile and fit the model:</li>
</ol>
<pre style="padding-left: 60px">model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])</pre>
<p style="padding-left: 60px">In the preceding code, we are specifying the loss as binary cross-entropy because the outcome is either a <kbd>1</kbd> or a <kbd>0</kbd>.</p>
<ol start="6">
<li>Fit the model:</li>
</ol>
<pre style="padding-left: 60px">model.fit(X_train, y_train, epochs = 500)</pre>
<p>We are fitting the model to have optimal weights that connect the input layer with the output layer.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Validating the CNN output</h1>
                </header>
            
            <article>
                
<p>Now that we have fit the model, let's validate the output we obtain from the model by implementing the feedforward portion of the CNN:</p>
<ol>
<li>Let's extract the order in which weights and biases are presented:</li>
</ol>
<pre style="padding-left: 60px">model.weights</pre>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1258 image-border" src="Images/bc1369d0-c303-4cbe-8080-4e302592a96a.png" style="width:54.33em;height:6.17em;" width="652" height="74"/></p>
<p style="padding-left: 60px">You can see that the weights of the convolution layer are presented first, then the bias, and finally the weight and bias in the output layer.</p>
<p style="padding-left: 60px">Also note that the shape of weights in the convolution layer is (3, 3, 1, 1) as the filter is 3 x 3 x 1 in shape (because the image is three-dimensional: 28 x 28 x 1 in shape) and the final 1 (the fourth value in shape) is for the number of filters that are specified in the convolution layer.</p>
<p style="padding-left: 60px">If we had specified 64 as the number of filters in the convolution, the shape of weights would have been 3 x 3 x 1 x 64.</p>
<p style="padding-left: 60px">Similarly, had the convolution operation been performed on an image with 3 channels, each filter's shape would have been 3 x 3 x 3.</p>
<ol start="2">
<li>Extract the weight values at various layers:</li>
</ol>
<pre style="padding-left: 60px">model.get_weights()</pre>
<ol start="3">
<li>Let's extract the output of the first input so that we can validate it with feedforward propagation:</li>
</ol>
<pre style="padding-left: 60px">model.predict(X_train[0].reshape(1,4,4,1))</pre>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1259 image-border" src="Images/cd6f5149-946f-4c97-8499-a4c6c7d6fe6c.png" style="width:20.42em;height:1.83em;" width="287" height="26"/></p>
<p style="padding-left: 60px">The output from the iteration we ran is 0.0428 (this could be different when you run the model, as the random initialization of weights could be different), which we will validate by performing matrix multiplication.</p>
<p class="mce-root"/>
<div class="packt_infobox">We are reshaping the input while passing it to the predict method as it expects the input to have a shape of (None, 4, 4, 1), where None specifies that the batch size could be any number.</div>
<ol start="4">
<li>Perform the convolution of the filter with the input image. Note that the input image is 4 x 4 in shape while the filter is 3 x 3 in shape. We will be performing the matrix multiplication (convolution) along the rows as well as columns in the code here:</li>
</ol>
<pre style="padding-left: 60px">sumprod = []<br/>for i in range(X_train[0].shape[0]-model.get_weights()[0].shape[0]+1):<br/>     for j in range(X_train[0].shape[0]-model.get_weights()[0].shape[0]+1):<br/>         img_subset = np.array(X_train[0,i:(i+3),j:(j+3),0])<br/>         filter = model.get_weights()[0].reshape(3,3)<br/>         val = np.sum(img_subset*filter) + model.get_weights()[1]<br/>         sumprod.append(val)</pre>
<p style="padding-left: 60px">In the preceding code, we are initializing an empty list named <kbd>sumprod</kbd> that stores the output of each matrix multiplication of the filter with the image's subset (the subset of the image is of the size of filter).</p>
<ol start="5">
<li>Reshape the output of <kbd>sumprod</kbd> so that it can then be passed to the pooling layer:</li>
</ol>
<pre style="padding-left: 60px">sumprod= np.array(sumprod).reshape(2,2,1)</pre>
<ol start="6">
<li>Perform activation on top of the convolution's output before it is passed to the pooling layer:</li>
</ol>
<pre style="padding-left: 60px">sumprod = np.where(sumprod&gt;0,sumprod,0)</pre>
<ol start="7">
<li>Pass the convolution output to the pooling layer. However, in the current case, given that the output of the convolution is 2 x 2, we will keep it simple and just take the maximum value in the output we obtained in <em>step 6</em>:</li>
</ol>
<pre style="padding-left: 60px">pooling_layer_output = np.max(sumprod)</pre>
<ol start="8">
<li>Connect the output of the pooling layer to the output layer:</li>
</ol>
<pre style="padding-left: 60px">intermediate_output_value = pooling_layer_output*model.get_weights()[2]+model.get_weights()[3]</pre>
<p style="padding-left: 60px">We multiplied by the pooling layer's output with the weight in the output layer and added the bias in the output layer.</p>
<ol start="9">
<li>Calculate the sigmoid output:</li>
</ol>
<pre style="padding-left: 60px">1/(1+np.exp(-intermediate_output_value))</pre>
<p style="padding-left: 60px">The output of preceding operation is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1260 image-border" src="Images/9f3d6105-094e-441a-9139-3c885798733d.png" style="width:19.58em;height:1.75em;" width="287" height="26"/></p>
<p>The output that you see here will be the same as the one that we obtained using the <kbd>model.predict</kbd> method, thus validating our understanding of how a CNN works.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">CNNs to improve accuracy in the case of image translation</h1>
                </header>
            
            <article>
                
<p>In the previous sections, we learned about the issue of translation in images and how a CNN works. In this section, we will leverage that knowledge to learn how a CNN works toward improving prediction accuracy when an image is translated.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The strategy that we will be adopting to build a CNN model is as follows:</p>
<ul>
<li>Given that the input shape is 28 x 28 x 1, the filters shall be 3 x 3 x 1 in size:
<ul>
<li>Note that the size of filter can change, however the number of channels cannot change</li>
</ul>
</li>
<li>Let's initialize 10 filters</li>
<li>We will perform pooling on top of the output obtained in the previous step of convolving 10 filters over the input image:
<ul>
<li>This would result in halving the image's dimension</li>
</ul>
</li>
<li>We will flatten the output obtained while pooling</li>
<li>The flattened layer will be connected to another hidden layer that has 1,000 units</li>
<li>Finally, we connect the hidden layer to the output layer where there are 10 possible classes (as there are 10 digits, from 0 to 9)</li>
</ul>
<p>Once we build the model, we will translate the average 1 image by 1 pixel and then test the CNN model's prediction on the translated image. Note that the feedforward NN architecture was not able to predict the right class in this scenario.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's understand using a CNN on MNIST data in code (The code file is available as <kbd>CNN_image_translation.ipynb</kbd> in GitHub):</p>
<ol>
<li>Load and preprocess the data:</li>
</ol>
<pre style="padding-left: 60px">(X_train, y_train), (X_test, y_test) = mnist.load_data()<br/>X_train = X_train.reshape(X_train.shape[0],X_train.shape[1],X_train.shape[1],1 ).astype('float32')<br/>X_test = X_test.reshape(X_test.shape[0],X_test.shape[1],X_test.shape[1],1).astype('float32')<br/><br/>X_train = X_train / 255<br/>X_test = X_test / 255<br/><br/>y_train = np_utils.to_categorical(y_train)<br/>y_test = np_utils.to_categorical(y_test)<br/><br/>num_classes = y_test.shape[1]</pre>
<p style="padding-left: 60px">Note that all the steps that we performed in this step are the same as what we performed in <a href="750fdf81-d758-47c9-a3b3-7cae6aae1576.xhtml" target="_blank">Chapter 2</a>, <em>Building a Deep Feedforward Neural Network</em>.</p>
<ol start="2">
<li>Build and compile the model:</li>
</ol>
<pre style="padding-left: 60px">from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense<br/>from keras.models import Sequential<br/>model = Sequential()<br/>model.add(Conv2D(10, (3,3), input_shape=(28, 28,1),activation='relu'))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/>model.add(Flatten())<br/>model.add(Dense(1000, activation='relu'))<br/>model.add(Dense(num_classes, activation='softmax'))</pre>
<p style="padding-left: 60px">A summary of the model that we initialized in the preceding code can be obtained and is as follows:</p>
<pre style="padding-left: 60px">model.summary()</pre>
<p style="padding-left: 60px">The summary of the model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1261 image-border" src="Images/3e4273c7-43a7-4785-9fcb-a5a0e92fc1bb.png" style="width:36.08em;height:19.92em;" width="512" height="283"/></p>
<p style="padding-left: 60px">We have a total of 100 parameters in the convolution layer as there are 10 of the 3 x 3 x 1 filters, resulting in a total of 90 weight parameters. Additionally, 10 bias terms (1 for each filter) add up to form a total of 100 parameters in the convolution layer.</p>
<p style="padding-left: 60px">Note that max pooling does not have any parameters, as it is about extracting the maximum value within a patch that is 2 x 2 in size.</p>
<ol start="3">
<li>Fit the model:</li>
</ol>
<pre style="padding-left: 60px">model.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=5, batch_size=1024, verbose=1)</pre>
<p style="padding-left: 60px">The preceding model gives an accuracy of 98% in 5 epochs:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1262 image-border" src="Images/2356a9a5-1d51-429c-b4ea-5ca798823335.png" style="width:31.92em;height:26.17em;" width="383" height="314"/></p>
<ol start="4">
<li>Let's identify the average 1 image and then translate it by <kbd>1</kbd> unit:</li>
</ol>
<pre style="padding-left: 60px">X_test1 = X_test[y_test[:,1]==1]</pre>
<p style="padding-left: 60px">In the preceding code, we filtered all the image inputs that have a label of <kbd>1</kbd>:</p>
<pre style="padding-left: 60px">import numpy as np<br/>pic=np.zeros((28,28))<br/>pic2=np.copy(pic)<br/>for i in range(X_test1.shape[0]):<br/>     pic2=X_test1[i,:,:,0]<br/>     pic=pic+pic2<br/>pic=(pic/X_test1.shape[0])</pre>
<p style="padding-left: 60px">In the preceding code, we took the average 1 image:</p>
<pre>for i in range(pic.shape[0]):<br/>     if i&lt;20:<br/>         pic[:,i]=pic[:,i+1]</pre>
<p style="padding-left: 60px">In the preceding code, we translated each pixel in the average 1 image by 1 unit to the left.</p>
<ol start="5">
<li>Predict on the translated 1 image:</li>
</ol>
<pre style="padding-left: 60px">model.predict(pic.reshape(1,28,28,1))</pre>
<p style="padding-left: 60px">The output of preceding step is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1263 image-border" src="Images/5d530d8c-c505-4b13-aafa-5d0c85335cdd.png" style="width:40.75em;height:5.83em;" width="542" height="77"/></p>
<p>Note that the prediction now (when we use a CNN) has more probability (0.9541) for 1 when compared to the scenario where the deep feed forward NN model, which is predicted as (0.6335) in the label of the translated image in the <em>Inaccuracy of<span> </span>traditional<span> </span>neural network when images are translated</em> section.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Gender classification using CNNs</h1>
                </header>
            
            <article>
                
<p>In the previous sections, we learned about how a CNN works and how CNNs solve the image-translation problem.</p>
<p>In this section, we will further our understanding of how a CNN works by building a model that works toward detecting the gender of person present in image.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this section, let's formulate our strategy of how we will solve this problem:</p>
<ul>
<li>We will collect a dataset of images and label each image based on the gender of person present in image</li>
<li>We'll work on only 2,000 images, as the data fetching process takes a considerably long time for our dataset (as we are manually downloading images from a website in this case study)</li>
<li>Additionally, we'll ensure that there is equal representation of male and female images in the dataset</li>
<li>Once the dataset is in place, we will reshape the images into the same size so that they can be fed into a CNN model</li>
<li>We will build the CNN model where the output layer has as many classes as the number of labels two</li>
<li>Given that this is a case of predicting one out of the two possible labels in the dataset, we will minimize the binary cross-entropy loss</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In this section, we will code the strategy that we defined prior (the code file is available as <kbd>Gender classification.ipynb</kbd> in GitHub):</p>
<ol>
<li>Download the dataset:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ wget https://d1p17r2m4rzlbo.cloudfront.net/wp-content/uploads/2017/04/a943287.csv</strong></pre>
<ol start="2">
<li>Load the dataset and inspect its content:</li>
</ol>
<pre style="padding-left: 60px">import pandas as pd, numpy as np<br/>from skimage import io<br/># Location of file is /content/a943287.csv<br/># be sure to change to location of downloaded file on your machine<br/>data = pd.read_csv('/content/a943287.csv')<br/>data.head() </pre>
<p style="padding-left: 60px">A sample of some of the key fields in the dataset is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1264 image-border" src="Images/6bbc8d52-1050-4284-9dc9-ac82eb5a039c.png" style="width:71.75em;height:15.08em;" width="861" height="181"/></p>
<ol start="3">
<li>Fetch 1,000 male images and 1,000 female images from the URL links provided in the dataset:</li>
</ol>
<pre style="padding-left: 60px">data_male = data[data['please_select_the_gender_of_the_person_in_the_picture']=="male"].reset_index(drop='index')<br/>data_female = data[data['please_select_the_gender_of_the_person_in_the_picture']=="female"].reset_index(drop='index')<br/>final_data = pd.concat([data_male[:1000],data_female[:1000]],axis=0).reset_index(drop='index')</pre>
<p style="padding-left: 60px">In the preceding code, <kbd>final_data</kbd> contains URL links for 1,000 male images and 1,000 female images. Read the URL links and fetch the images corresponding to the URL links. Ensure that all images are 300 × 300 × 3 in shape (as the majority of the images in his dataset have that shape) and also that we take care of any forbidden to access issues:</p>
<pre style="padding-left: 60px">x = []<br/>y = []<br/>for i in range(final_data.shape[0]):<br/>     try:<br/>         image = io.imread(final_data.loc[i]['image_url'])<br/>         if(image.shape==(300,300,3)):<br/>             x.append(image)<br/>             y.append(final_data.loc[i]['please_select_the_gender_of_the_person_in_the_picture'])<br/>     except:<br/>         continue</pre>
<p style="padding-left: 60px">A sample of the input and their corresponding emotion labels looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1265 image-border" src="Images/d08b8aaf-175f-470f-88f4-49b7f728ff26.png" style="width:28.42em;height:23.50em;" width="341" height="282"/></p>
<ol start="4">
<li>Create the input and output arrays:</li>
</ol>
<pre style="padding-left: 60px">x2 = []<br/>y2 = []<br/>for i in range(len(x)):<br/>      img = cv2.cvtColor(x[i], cv2.COLOR_BGR2GRAY)<br/>      img2 = cv2.resize(img,(50,50))<br/>      x2.append(img2)<br/>      img_label = np.where(y[i]=="male",1,0)<br/>      y2.append(img_label)</pre>
<p style="padding-left: 60px">In the preceding step, we have converted the color image into a grayscale image as the color of the image is likely to add additional information (we'll validate this hypothesis in <a href="c50d0373-e7d4-47d9-a514-df766f575a47.xhtml">C<span>hapter 5</span></a><span>, </span><em>Transfer Learning</em>).</p>
<p style="padding-left: 60px">Additionally, we have resized our images to a lower size (50 x 50 x 1) in shape. The result of this is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1266 image-border" src="Images/014738bb-62bc-4d19-86cf-1a5e419d57b6.png" style="width:25.58em;height:20.25em;" width="326" height="258"/></p>
<p style="padding-left: 60px">Finally, we converted the output into a one-hot-encoded version.</p>
<ol start="5">
<li>Create train and test datasets. First, we convert the input and output lists into arrays and then shape the input so that it is in a shape that can be be provided as input to the CNN:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">x2 = np.array(x2)<br/>x2 = x2.reshape(x2.shape[0],x2.shape[1],x2.shape[2],1)<br/>Y = np.array(y2)</pre>
<p style="padding-left: 60px"> The output of the first value of <kbd>x2</kbd> is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1269 image-border" src="Images/f75c95fd-9415-4496-9936-f1045c6aa1c3.png" style="width:18.92em;height:10.67em;" width="261" height="147"/></p>
<p style="padding-left: 60px"> Note that the input has values between <kbd>0</kbd> to <kbd>255</kbd> and thus we have to scale it:</p>
<pre style="padding-left: 60px">X = np.array(x2)/255<br/>Y = np.array(y2)</pre>
<p style="padding-left: 60px"> Finally, we split the input and output arrays into train and test datasets:</p>
<pre style="padding-left: 60px">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.1, random_state=42)<br/>print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)</pre>
<p style="padding-left: 60px">The shapes of the train and test input, output arrays are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1267 image-border" src="Images/dabfed13-ae88-4956-aa54-d93b53a6477a.png" style="width:27.67em;height:1.67em;" width="381" height="24"/></p>
<ol start="6">
<li>Build and compile the model:</li>
</ol>
<pre style="padding-left: 60px">from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense<br/>from keras.models import Sequential<br/>model = Sequential()<br/>model.add(Conv2D(64, kernel_size=(3, 3), activation='relu',input_shape=(50,50,1)))<br/>model.add(MaxPooling2D(pool_size=(5, 5)))<br/>model.add(Conv2D(128, kernel_size=(3, 3), activation='relu',padding='same'))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/>model.add(Conv2D(256, kernel_size=(3, 3), activation='relu',padding='same'))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/>model.add(Conv2D(512, kernel_size=(3, 3), activation='relu',padding='same'))<br/>model.add(Flatten())<br/>model.add(Dense(100, activation='relu'))<br/>model.add(Dense(1, activation='sigmoid'))<br/>model.summary()</pre>
<p style="padding-left: 60px">A summary of the model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1270 image-border" src="Images/e5f69cca-98f7-42f6-a04d-0f86e829cbe0.png" style="width:35.50em;height:31.50em;" width="510" height="454"/></p>
<p style="padding-left: 60px">Note that the number of channels in the output of the convolution layer would be equal to the number of filters specified in that layer. Additionally, we have performed a slightly more aggressive pooling on the first convolution layer's output.</p>
<p style="padding-left: 60px">Now, we'll compile the model to minimize binary cross entropy loss (as the output has only two classes) as follows:</p>
<pre style="padding-left: 60px">model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])</pre>
<ol start="7">
<li>Fit the model:</li>
</ol>
<pre style="padding-left: 60px">history = model.fit(X_train, y_train, batch_size=32, epochs=50,verbose=1,validation_data = (X_test, y_test))</pre>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1272 image-border" src="Images/8efa1177-0525-46b7-a427-c60dcefea2f0.png" style="width:33.08em;height:26.17em;" width="397" height="314"/></p>
<p style="padding-left: 60px">Once we fit the model, we can see that the preceding code results in an accuracy of ~80% in predicting the right gender in an image.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>The accuracy of classification can be further improved by:</p>
<ul>
<li>Working on more images</li>
<li>Working on bigger images (rather than 50 x 50 images) that are used to train a larger network</li>
<li>Leveraging transfer learning (which will be discussed in <a href="c50d0373-e7d4-47d9-a514-df766f575a47.xhtml">Chapter 5</a>, <em>Transfer Learning</em>)</li>
<li>Avoiding overfitting using regularization and dropout</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Data augmentation to improve network accuracy</h1>
                </header>
            
            <article>
                
<p>It is difficult to classify images accurately if they are translated from their original location. However, given an image, the label of the image will remain the same, even if we translate, rotate, or scale the image. Data augmentation is a way to create more images from the given set of images, that is, by rotating, translating, or scaling them and mapping them to the label of the original image.</p>
<p>An intuition for this is as follows: an image of a person will still be corresponding to the person, even if the image is rotated slightly or the person in the image is moved from the middle of the image to far right of the image.</p>
<p>Hence, we should be in a position to create more training data by rotating and translating the original images, where we already know the labels that correspond to each image.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will be working on the CIFAR-10 dataset, which contains images of objects of 10 different classes.</p>
<p>The strategy that we'll use is as follows:</p>
<ul>
<li>Download the CIFAR-10 dataset</li>
<li>Preprocess the dataset
<ul>
<li>Scale the input values</li>
<li>One-hot-encode the output classes</li>
</ul>
</li>
<li>Build a deep CNN with multiple convolution and pooling layers</li>
<li>Compile and fit the model to test its accuracy on the test dataset</li>
<li>Generate random translations of the original set of images in the training dataset</li>
<li>Fit the same model architecture that was built in the previous step on the total images (generated images, plus the original images)</li>
<li>Check the accuracy of the model on the test dataset</li>
</ul>
<p>We will be implementing data augmentation using the <kbd>ImageDataGenerator</kbd> method in the <kbd>keras.preprocessing.image</kbd> package.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>To understand the benefits of data augmentation, let's go through an example of calculating the accuracy on the CIFAR-10 dataset with data augmentation and without data augmentation (the code file is available as <kbd>Data_augmentation_to_improve_network_accuracy.ipynb</kbd> in GitHub).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Model accuracy without data augmentation</h1>
                </header>
            
            <article>
                
<p>Let's calculate the accuracy without data augmentation in the following steps:</p>
<ol>
<li>Import the packages and data:</li>
</ol>
<pre style="padding-left: 60px">from matplotlib import pyplot as plt<br/>%matplotlib inline<br/>import numpy as np<br/>from keras.utils import np_utils<br/>from keras.models import Sequential<br/>from keras.layers.core import Dense, Dropout, Activation, Flatten<br/>from keras.layers import Conv2D, MaxPooling2D<br/>from keras.layers.normalization import BatchNormalization<br/>from keras import regularizers<br/><br/>from keras.datasets import cifar10<br/>(X_train, y_train), (X_val, y_val) = cifar10.load_data()</pre>
<ol start="2">
<li>Preprocess the data:</li>
</ol>
<pre style="padding-left: 60px">X_train = X_train.astype('float32')/255.<br/>X_val = X_val.astype('float32')/255.<br/><br/>n_classes = 10<br/>y_train = np_utils.to_categorical(y_train, n_classes)<br/>y_val = np_utils.to_categorical(y_val, n_classes)</pre>
<p style="padding-left: 60px">A sample of images, along with their corresponding labels, is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1274 image-border" src="Images/dff754a8-553a-49ad-bd16-b9b339e04ec1.png" style="width:22.08em;height:18.17em;" width="300" height="248"/></p>
<ol start="3">
<li>Build and compile the model:</li>
</ol>
<pre style="padding-left: 60px">input_shape = X_train[0].shape<br/><br/>model = Sequential()<br/>model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=X_train.shape[1:]))<br/>model.add(Activation('relu'))<br/>model.add(BatchNormalization())<br/>model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))<br/>model.add(Activation('relu'))<br/>model.add(BatchNormalization())<br/>model.add(MaxPooling2D(pool_size=(2,2)))<br/>model.add(Dropout(0.2)) <br/>model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))<br/>model.add(Activation('relu'))<br/>model.add(BatchNormalization())<br/>model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))<br/>model.add(Activation('relu'))<br/>model.add(BatchNormalization())<br/>model.add(MaxPooling2D(pool_size=(2,2)))<br/>model.add(Dropout(0.3)) <br/>model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))<br/>model.add(Activation('relu'))<br/>model.add(BatchNormalization())<br/>model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))<br/>model.add(Activation('relu'))<br/>model.add(BatchNormalization())<br/>model.add(MaxPooling2D(pool_size=(2,2)))<br/>model.add(Dropout(0.4)) <br/>model.add(Flatten())<br/>model.add(Dense(10, activation='softmax'))<br/><br/>from keras.optimizers import Adam<br/>adam = Adam(lr = 0.01)<br/>model.compile(loss='categorical_crossentropy', optimizer=adam,metrics=['accuracy'])</pre>
<div class="packt_infobox">We have a higher learning rate only so that the model converges faster in fewer epochs. This enables a faster comparison of the non-data augmentation scenario with the data augmentation scenario. Ideally, we would let the model run for a greater number of epochs with a lesser learning rate.</div>
<ol start="4">
<li>Fit the model:</li>
</ol>
<pre style="padding-left: 60px">model.fit(X_train, y_train, batch_size=32,epochs=10, verbose=1, validation_data=(X_val, y_val))</pre>
<p style="padding-left: 60px">The accuracy of this network is ~66%:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1275 image-border" src="Images/92ccfaf3-5c22-4811-bab8-237bf334af4d.png" style="width:29.17em;height:23.08em;" width="399" height="315"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Model accuracy with data augmentation</h1>
                </header>
            
            <article>
                
<p>In the following code, we will implement data augmentation:</p>
<ol>
<li>Use the <kbd>ImageDataGenerator</kbd> method in the <kbd>keras.preprocessing.image</kbd> package:</li>
</ol>
<pre style="padding-left: 60px">from keras.preprocessing.image import ImageDataGenerator <br/>datagen = ImageDataGenerator(<br/>    rotation_range=20,<br/>    width_shift_range=0,<br/>    height_shift_range=0,<br/>    fill_mode = 'nearest')<br/><br/>datagen.fit(X_train)</pre>
<p style="padding-left: 60px">In the preceding code, we are generating new images where the images are randomly rotated between 0 to 20 degrees. A sample of images after being passed through the data generator is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1276 image-border" src="Images/a17f5da5-2d6d-4b24-b090-3399a1b49dbd.png" style="width:25.00em;height:20.42em;" width="300" height="245"/></p>
<p style="padding-left: 60px">Note that the images are tilted slightly when compared to the previous set of images.</p>
<ol start="2">
<li>Now, we will pass our total data through the data generator, as follows:</li>
</ol>
<pre style="padding-left: 60px">batch_size = 32<br/>model = Sequential()<br/>model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=X_train.shape[1:]))<br/>model.add(Activation('relu'))<br/>model.add(BatchNormalization())<br/>model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))<br/>model.add(Activation('relu'))<br/>model.add(BatchNormalization())<br/>model.add(MaxPooling2D(pool_size=(2,2)))<br/>model.add(Dropout(0.2)) <br/>model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))<br/>model.add(Activation('relu'))<br/>model.add(BatchNormalization())<br/>model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))<br/>model.add(Activation('relu'))<br/>model.add(BatchNormalization())<br/>model.add(MaxPooling2D(pool_size=(2,2)))<br/>model.add(Dropout(0.3)) <br/>model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))<br/>model.add(Activation('relu'))<br/>model.add(BatchNormalization())<br/>model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))<br/>model.add(Activation('relu'))<br/>model.add(BatchNormalization())<br/>model.add(MaxPooling2D(pool_size=(2,2)))<br/>model.add(Dropout(0.4)) <br/>model.add(Flatten())<br/>model.add(Dense(10, activation='softmax'))<br/>from keras.optimizers import Adam<br/>adam = Adam(lr = 0.01)<br/>model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])</pre>
<ol start="3">
<li>Note that we are rebuilding the model so that the weights are initialized one more time as we are comparing between a data augmentation and non-data augmentation scenario:</li>
</ol>
<pre style="padding-left: 60px">model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),steps_per_epoch=X_train.shape[0] // batch_size, epochs=10,validation_data=(X_val,y_val))</pre>
<p style="padding-left: 90px">Note that the <kbd>fit_generator</kbd> method fits the model while generating new images.</p>
<ol start="4">
<li>Additionally, <kbd>datagen.flow</kbd> specifies that new training data points need to be generated per the datagen strategy we initialized in step <em>1</em>. Along with this, we also specify the number of steps per epoch as the ratio of the total number of data points over the batch size:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1278 image-border" src="Images/8ce964fc-7aec-48ba-a6a6-485f4850a457.png" style="width:33.00em;height:26.17em;" width="396" height="314"/></p>
<p style="padding-left: 90px">The accuracy of this code is ~80%, which is better than the accuracy of 66% using just the given dataset (without data augmentation).</p>


            </article>

            
        </section>
    </div>



  </body></html>