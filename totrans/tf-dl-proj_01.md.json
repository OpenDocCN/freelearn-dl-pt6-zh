["```py\nN_CLASSES = 43\nRESIZED_IMAGE = (32, 32)\n```", "```py\nimport matplotlib.pyplot as plt\nimport glob\nfrom skimage.color import rgb2lab\nfrom skimage.transform import resize\nfrom collections import namedtuple\nimport numpy as np\nnp.random.seed(101)\n%matplotlib inline\nDataset = namedtuple('Dataset', ['X', 'y'])\ndef to_tf_format(imgs):\n   return np.stack([img[:, :, np.newaxis] for img in imgs], axis=0).astype(np.float32)\ndef read_dataset_ppm(rootpath, n_labels, resize_to):\nimages = []\nlabels = []\nfor c in range(n_labels):\n   full_path = rootpath + '/' + format(c, '05d') + '/'\n   for img_name in glob.glob(full_path + \"*.ppm\"):\n\n     img = plt.imread(img_name).astype(np.float32)\n     img = rgb2lab(img / 255.0)[:,:,0]\n     if resize_to:\n       img = resize(img, resize_to, mode='reflect')\n\n     label = np.zeros((n_labels, ), dtype=np.float32)\n     label[c] = 1.0\n    images.append(img.astype(np.float32))\n     labels.append(label)\nreturn Dataset(X = to_tf_format(images).astype(np.float32),\n                 y = np.matrix(labels).astype(np.float32))\ndataset = read_dataset_ppm('GTSRB/Final_Training/Images', N_CLASSES, RESIZED_IMAGE)\nprint(dataset.X.shape)\nprint(dataset.y.shape)\n```", "```py\n(39209, 32, 32, 1)\n(39209, 43)\n```", "```py\nplt.imshow(dataset.X[0, :, :, :].reshape(RESIZED_IMAGE)) #sample\nprint(dataset.y[0, :]) #label\n```", "```py\n[[1\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0.\n0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0.]]\n```", "```py\nplt.imshow(dataset.X[-1, :, :, :].reshape(RESIZED_IMAGE)) #sample\nprint(dataset.y[-1, :]) #label\n```", "```py\n[[0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0.\n0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0\\. 1.]]\n```", "```py\nfrom sklearn.model_selection import train_test_split\nidx_train, idx_test = train_test_split(range(dataset.X.shape[0]), test_size=0.25, random_state=101)\nX_train = dataset.X[idx_train, :, :, :]\nX_test = dataset.X[idx_test, :, :, :]\ny_train = dataset.y[idx_train, :]\ny_test = dataset.y[idx_test, :]\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)\n```", "```py\n(29406, 32, 32, 1)\n(29406, 43)\n(9803, 32, 32, 1)\n(9803, 43)\n```", "```py\ndef minibatcher(X, y, batch_size, shuffle):\nassert X.shape[0] == y.shape[0]\nn_samples = X.shape[0]\nif shuffle:\n   idx = np.random.permutation(n_samples)\nelse:\n   idx = list(range(n_samples))\nfor k in range(int(np.ceil(n_samples/batch_size))):\n   from_idx = k*batch_size\n   to_idx = (k+1)*batch_size\n   yield X[idx[from_idx:to_idx], :, :, :], y[idx[from_idx:to_idx], :]\n```", "```py\nfor mb in minibatcher(X_train, y_train, 10000, True):\nprint(mb[0].shape, mb[1].shape)\n```", "```py\n(10000, 32, 32, 1) (10000, 43)\n(10000, 32, 32, 1) (10000, 43)\n(9406, 32, 32, 1) (9406, 43)\n```", "```py\nimport tensorflow as tf\ndef fc_no_activation_layer(in_tensors, n_units):\nw = tf.get_variable('fc_W',\n   [in_tensors.get_shape()[1], n_units],\n   tf.float32,\n   tf.contrib.layers.xavier_initializer())\nb = tf.get_variable('fc_B',\n   [n_units, ],\n   tf.float32,\n   tf.constant_initializer(0.0))\nreturn tf.matmul(in_tensors, w) + b\n```", "```py\ndef fc_layer(in_tensors, n_units):\nreturn tf.nn.leaky_relu(fc_no_activation_layer(in_tensors, n_units))\n```", "```py\ndef conv_layer(in_tensors, kernel_size, n_units):\nw = tf.get_variable('conv_W',\n   [kernel_size, kernel_size, in_tensors.get_shape()[3], n_units],\n   tf.float32,\n   tf.contrib.layers.xavier_initializer())\nb = tf.get_variable('conv_B',\n   [n_units, ],\n   tf.float32,\n   tf.constant_initializer(0.0))\nreturn tf.nn.leaky_relu(tf.nn.conv2d(in_tensors, w, [1, 1, 1, 1], 'SAME') + b)\n```", "```py\ndef maxpool_layer(in_tensors, sampling):\nreturn tf.nn.max_pool(in_tensors, [1, sampling, sampling, 1], [1, sampling, sampling, 1], 'SAME')\n```", "```py\ndef dropout(in_tensors, keep_proba, is_training):\nreturn tf.cond(is_training, lambda: tf.nn.dropout(in_tensors, keep_proba), lambda: in_tensors)\n```", "```py\ndef model(in_tensors, is_training):\n# First layer: 5x5 2d-conv, 32 filters, 2x maxpool, 20% drouput\nwith tf.variable_scope('l1'):\n   l1 = maxpool_layer(conv_layer(in_tensors, 5, 32), 2)\n   l1_out = dropout(l1, 0.8, is_training)\n# Second layer: 5x5 2d-conv, 64 filters, 2x maxpool, 20% drouput\nwith tf.variable_scope('l2'):\n   l2 = maxpool_layer(conv_layer(l1_out, 5, 64), 2)\n   l2_out = dropout(l2, 0.8, is_training)\nwith tf.variable_scope('flatten'):\n   l2_out_flat = tf.layers.flatten(l2_out)\n# Fully collected layer, 1024 neurons, 40% dropout\nwith tf.variable_scope('l3'):\n   l3 = fc_layer(l2_out_flat, 1024)\n   l3_out = dropout(l3, 0.6, is_training)\n# Output\nwith tf.variable_scope('out'):\n   out_tensors = fc_no_activation_layer(l3_out, N_CLASSES)\nreturn out_tensors\n```", "```py\nfrom sklearn.metrics import classification_report, confusion_matrix\ndef train_model(X_train, y_train, X_test, y_test, learning_rate, max_epochs, batch_size):\nin_X_tensors_batch = tf.placeholder(tf.float32, shape = (None, RESIZED_IMAGE[0], RESIZED_IMAGE[1], 1))\nin_y_tensors_batch = tf.placeholder(tf.float32, shape = (None, N_CLASSES))\nis_training = tf.placeholder(tf.bool)\n```", "```py\nlogits = model(in_X_tensors_batch, is_training)\nout_y_pred = tf.nn.softmax(logits)\nloss_score = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=in_y_tensors_batch)\nloss = tf.reduce_mean(loss_score)\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n```", "```py\nwith tf.Session() as session:\n   session.run(tf.global_variables_initializer())\n   for epoch in range(max_epochs):\n    print(\"Epoch=\", epoch)\n     tf_score = []\n     for mb in minibatcher(X_train, y_train, batch_size, shuffle = True):\n       tf_output = session.run([optimizer, loss],\n                               feed_dict = {in_X_tensors_batch : mb[0],\n                                            in_y_tensors_batch : \nb[1],\n                                             is_training : True})\n       tf_score.append(tf_output[1])\n     print(\" train_loss_score=\", np.mean(tf_score))\n```", "```py\n   print(\"TEST SET PERFORMANCE\")\n   y_test_pred, test_loss = session.run([out_y_pred, loss],\n                                         feed_dict = {in_X_tensors_batch : X_test,                                                       in_y_tensors_batch : y_test,                                                       is_training : False})\n```", "```py\n   print(\" test_loss_score=\", test_loss)\n   y_test_pred_classified = np.argmax(y_test_pred, axis=1).astype(np.int32)\n   y_test_true_classified = np.argmax(y_test, axis=1).astype(np.int32)\n   print(classification_report(y_test_true_classified, y_test_pred_classified))\n   cm = confusion_matrix(y_test_true_classified, y_test_pred_classified)\n   plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n   plt.colorbar()\n   plt.tight_layout()\n   plt.show()\n   # And the log2 version, to enphasize the misclassifications\n   plt.imshow(np.log2(cm + 1), interpolation='nearest', cmap=plt.get_cmap(\"tab20\"))\n   plt.colorbar()\n   plt.tight_layout()\n   plt.show()\ntf.reset_default_graph()\n```", "```py\ntrain_model(X_train, y_train, X_test, y_test, 0.001, 10, 256)\n```", "```py\nEpoch= 0\ntrain_loss_score= 3.4909246\nEpoch= 1\ntrain_loss_score= 0.5096467\nEpoch= 2\ntrain_loss_score= 0.26641673\nEpoch= 3\ntrain_loss_score= 0.1706828\nEpoch= 4\ntrain_loss_score= 0.12737551\nEpoch= 5\ntrain_loss_score= 0.09745725\nEpoch= 6\ntrain_loss_score= 0.07730477\nEpoch= 7\ntrain_loss_score= 0.06734192\nEpoch= 8\ntrain_loss_score= 0.06815668\nEpoch= 9\ntrain_loss_score= 0.060291935\nTEST SET PERFORMANCE\ntest_loss_score= 0.04581982\n```", "```py\n precision   recall f1-score   support\n 0       1.00     0.96     0.98       67\n 1       0.99     0.99      0.99       539\n 2       0.99     1.00     0.99       558\n 3       0.99     0.98     0.98       364\n 4       0.99     0.99     0.99       487\n 5       0.98     0.98     0.98       479\n 6       1.00    0.99     1.00       105\n 7       1.00     0.98     0.99       364\n 8       0.99     0.99     0.99       340\n 9       0.99     0.99     0.99       384\n 10       0.99     1.00     1.00       513\n 11     0.99     0.98     0.99       334\n 12       0.99     1.00     1.00       545\n 13       1.00     1.00     1.00       537\n 14       1.00     1.00     1.00       213\n 15       0.98     0.99     0.98       164\n 16       1.00     0.99     0.99       98\n 17       0.99     0.99     0.99       281\n 18       1.00     0.98     0.99       286\n 19       1.00     1.00     1.00       56\n 20       0.99     0.97     0.98       78\n 21       0.97     1.00     0.98       95\n 22       1.00     1.00     1.00       97\n 23       1.00     0.97     0.98       123\n 24       1.00     0.96     0.98       77\n 25       0.99     1.00     0.99      401\n 26       0.98     0.96     0.97       135\n 27       0.94     0.98     0.96       60\n 28       1.00     0.97     0.98       123\n 29       1.00     0.97     0.99       69\n 30       0.88     0.99    0.93       115\n 31       1.00     1.00     1.00       178\n 32       0.98     0.96     0.97       55\n 33       0.99     1.00     1.00       177\n 34       0.99     0.99     0.99       103\n 35       1.00      1.00     1.00       277\n 36       0.99     1.00     0.99       78\n 37       0.98     1.00     0.99       63\n 38       1.00     1.00     1.00       540\n 39       1.00     1.00     1.00       60\n 40      1.00     0.98     0.99       85\n 41       1.00     1.00     1.00       47\n 42       0.98     1.00     0.99       53\navg / total       0.99     0.99     0.99     9803\n```"]