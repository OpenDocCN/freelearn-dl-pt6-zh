["```py\nRecordReader recordReader = new CSVRecordReader(1,',');\n```", "```py\nFile file = new File(\"Churn_Modelling.csv\");\n   recordReader.initialize(new FileSplit(file)); \n```", "```py\nTransformProcess transformProcess = new TransformProcess.Builder(schema)\n .removeColumns(\"RowNumber\",\"CustomerId\",\"Surname\")\n .build();    \n```", "```py\nDataQualityAnalysis analysis = AnalyzeLocal.analyzeQuality(schema,recordReader);\n System.out.println(analysis); \n```", "```py\nCondition condition = new NullWritableColumnCondition(\"columnName\");\n TransformProcess transformProcess = new TransformProcess.Builder(schema)\n   .conditionalReplaceValueTransform(\"columnName\",new IntWritable(0),condition)\n .build();\n```", "```py\nCondition condition = new NaNColumnCondition(\"columnName\");\n TransformProcess transformProcess = new TransformProcess.Builder(schema)\n   .conditionalReplaceValueTransform(\"columnName\",new IntWritable(0),condition)\n .build();\n```", "```py\nSchema.Builder schemaBuilder = new Schema.Builder();\n schemaBuilder.addColumnString(\"RowNumber\")\n schemaBuilder.addColumnInteger(\"CustomerId\")\n schemaBuilder.addColumnString(\"Surname\")\n schemaBuilder.addColumnInteger(\"CreditScore\");\n```", "```py\nschemaBuilder.addColumnCategorical(\"Geography\", Arrays.asList(\"France\",\"Germany\",\"Spain\"))\n schemaBuilder.addColumnCategorical(\"Gender\", Arrays.asList(\"Male\",\"Female\"));\n```", "```py\nSchema schema = schemaBuilder.build();\n TransformProcess.Builder transformProcessBuilder = new TransformProcess.Builder(schema);\n transformProcessBuilder.removeColumns(\"RowNumber\",\"CustomerId\",\"Surname\");\n```", "```py\ntransformProcessBuilder.categoricalToInteger(\"Gender\");\n```", "```py\ntransformProcessBuilder.categoricalToInteger(\"Gender\")\n transformProcessBuilder.categoricalToOneHot(\"Geography\");\n\n```", "```py\ntransformProcessBuilder.removeColumns(\"Geography[France]\")\n```", "```py\nTransformProcess transformProcess = transformProcessBuilder.build();\n TransformProcessRecordReader transformProcessRecordReader = new TransformProcessRecordReader(recordReader,transformProcess);\n```", "```py\nDataSetIterator dataSetIterator = new RecordReaderDataSetIterator.Builder(transformProcessRecordReader,batchSize) .classification(labelIndex,numClasses)\n .build();\n```", "```py\nDataNormalization dataNormalization = new NormalizerStandardize();\n dataNormalization.fit(dataSetIterator);\n dataSetIterator.setPreProcessor(dataNormalization);\n```", "```py\nDataSetIteratorSplitter dataSetIteratorSplitter = new DataSetIteratorSplitter(dataSetIterator,totalNoOfBatches,ratio);\n```", "```py\nDataSetIterator trainIterator = dataSetIteratorSplitter.getTrainIterator();\n DataSetIterator testIterator = dataSetIteratorSplitter.getTestIterator();\n\n```", "```py\nMultiLayerConfiguration.Builder builder = new NeuralNetConfiguration.Builder().weightInit(WeightInit.RELU_UNIFORM)\n .updater(new Adam(0.015D))\n .list();\n\n```", "```py\nbuilder.layer(new DenseLayer.Builder().nIn(incomingConnectionCount).nOut(outgoingConnectionCount).activation(Activation.RELU)\n.build())\n.build();\n```", "```py\nincoming neurons = outgoing neurons from preceding layer.\n outgoing neurons = incoming neurons for the next hidden layer.\n```", "```py\nbuilder.layer(new DenseLayer.Builder().nIn(incomingConnectionCount).nOut(outgoingConnectionCount).activation(Activation.RELU).build());\n\n```", "```py\nincoming neurons = outgoing neurons from preceding hidden layer.\n outgoing neurons = number of labels\n\n```", "```py\nbuilder.layer(new OutputLayer.Builder(new LossMCXENT(weightsArray)).nIn(incomingConnectionCount).nOut(labelCount).activation(Activation.SOFTMAX).build())\n```", "```py\nINDArray weightsArray = Nd4j.create(new double[]{0.35, 0.65});\n```", "```py\nnew OutputLayer.Builder(new LossMCXENT(weightsArray)).nIn(incomingConnectionCount).nOut(labelCount).activation(Activation.SOFTMAX))\n.build();\n```", "```py\nMultiLayerConfiguration configuration = builder.build();\n   MultiLayerNetwork multiLayerNetwork = new MultiLayerNetwork(configuration);\n multiLayerNetwork.init();\n multiLayerNetwork.setListeners(new ScoreIterationListener(iterationCount));\n```", "```py\n<dependency>\n <groupId>org.deeplearning4j</groupId>\n <artifactId>deeplearning4j-ui_2.10</artifactId>\n <version>1.0.0-beta3</version>\n </dependency>\n```", "```py\nUIServer uiServer = UIServer.getInstance();\n StatsStorage statsStorage = new InMemoryStatsStorage();\n```", "```py\nmultiLayerNetwork.setListeners(new ScoreIterationListener(100),\n new StatsListener(statsStorage));\n```", "```py\nuiServer.attach(statsStorage);\n```", "```py\nmultiLayerNetwork.fit(dataSetIteratorSplitter.getTrainIterator(),100);\n```", "```py\nEvaluation evaluation = multiLayerNetwork.evaluate(dataSetIteratorSplitter.getTestIterator(),Arrays.asList(\"0\",\"1\"));\n System.out.println(evaluation.stats()); //printing the evaluation metrics\n```", "```py\nFile file = new File(\"model.zip\");\n ModelSerializer.writeModel(multiLayerNetwork,file,true);\n ModelSerializer.addNormalizerToModel(file,dataNormalization);\n```", "```py\nprivate static Schema generateSchema(){\n Schema schema = new Schema.Builder()\n .addColumnString(\"RowNumber\")\n .addColumnInteger(\"CustomerId\")\n .addColumnString(\"Surname\")\n .addColumnInteger(\"CreditScore\")\n .addColumnCategorical(\"Geography\", Arrays.asList(\"France\",\"Germany\",\"Spain\"))\n .addColumnCategorical(\"Gender\", Arrays.asList(\"Male\",\"Female\"))\n .addColumnsInteger(\"Age\", \"Tenure\")\n .addColumnDouble(\"Balance\")\n .addColumnsInteger(\"NumOfProducts\",\"HasCrCard\",\"IsActiveMember\")\n .addColumnDouble(\"EstimatedSalary\")\n .build();\n return schema;\n }\n```", "```py\nprivate static RecordReader applyTransform(RecordReader recordReader, Schema schema){\n final TransformProcess transformProcess = new TransformProcess.Builder(schema)\n .removeColumns(\"RowNumber\",\"CustomerId\",\"Surname\")\n .categoricalToInteger(\"Gender\")\n .categoricalToOneHot(\"Geography\")\n .removeColumns(\"Geography[France]\")\n .build();\n final TransformProcessRecordReader transformProcessRecordReader = new TransformProcessRecordReader(recordReader,transformProcess);\n return transformProcessRecordReader;\n}\n```", "```py\nprivate static RecordReader generateReader(File file) throws IOException, InterruptedException {\n final RecordReader recordReader = new CSVRecordReader(1,',');\n recordReader.initialize(new FileSplit(file));\n final RecordReader transformProcessRecordReader=applyTransform(recordReader,generateSchema());\n```", "```py\nFile modelFile = new File(modelFilePath);\n MultiLayerNetwork network = ModelSerializer.restoreMultiLayerNetwork(modelFile);\n NormalizerStandardize normalizerStandardize = ModelSerializer.restoreNormalizerFromFile(modelFile);\n\n```", "```py\nDataSetIterator dataSetIterator = new RecordReaderDataSetIterator.Builder(recordReader,1).build();\n normalizerStandardize.fit(dataSetIterator);\n dataSetIterator.setPreProcessor(normalizerStandardize); \n```", "```py\npublic static INDArray generateOutput(File inputFile, String modelFilePath) throws IOException, InterruptedException {\n File modelFile = new File(modelFilePath);\n MultiLayerNetwork network = ModelSerializer.restoreMultiLayerNetwork(modelFile);\n   RecordReader recordReader = generateReader(inputFile);\n NormalizerStandardize normalizerStandardize = ModelSerializer.restoreNormalizerFromFile(modelFile);\n DataSetIterator dataSetIterator = new RecordReaderDataSetIterator.Builder(recordReader,1).build();\n normalizerStandardize.fit(dataSetIterator);\n dataSetIterator.setPreProcessor(normalizerStandardize);\n return network.output(dataSetIterator);\n }\n```", "```py\nmvn clean install\n```", "```py\n-DmodelFilePath={PATH-TO-MODEL-FILE}\n```", "```py\n<dependency>\n   <groupId>com.javadeeplearningcookbook.app</groupId>\n   <artifactId>cookbookapp</artifactId>\n   <version>1.0-SNAPSHOT</version>\n </dependency>\n```"]