<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Human Activity Recognition using Recurrent Neural Networks</h1>
                </header>
            
            <article>
                
<div class="chapter-content CDPAlignLeft CDPAlign">
<p>A <strong>recurrent neural network</strong> (<strong>RNN</strong>) is a class of artificial neural network where connections between units form a directed cycle. RNNs make use of information from the past. That way, they can make predictions for data with high temporal dependencies. This creates an internal state of the network that allows it to exhibit dynamic temporal behavior.</p>
<p>An RNN takes many input vectors to process them and output other vectors. Compared to a classical approach, using an RNN with <strong>Long Short-Term Memory</strong> cells (<strong>LSTMs</strong>) requires no, or very little, feature engineering. Data can be fed directly into the neural network, which acts like a black box, modeling the problem correctly. The approach here is rather simple in terms of how much data is preprocessed.</p>
<p>In this chapter, we will see how to develop a machine learning project using RNN implementation, called LSTM for <strong>human activity recognition</strong> (<strong>HAR</strong>), using the smartphones dataset. In short, our ML model will be able to classify the type of movement from six categories: walking, walking upstairs, walking downstairs, sitting, standing, and lying <span>down</span>.</p>
<p>In a nutshell, we will learn the following topics throughout this end-to-end project:</p>
<ul>
<li>Working with recurrent neural networks</li>
<li>Long term dependencies and drawbacks of RNN</li>
<li>Developing an LSTM model for human activity recognition</li>
<li>Tuning LSTM and RNN</li>
<li>Summary</li>
</ul>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with RNNs</h1>
                </header>
            
            <article>
                
<p>In this section, we will first provide some contextual information about RNNs. Then, we will highlight some potential drawbacks of classical RNNs. Finally, we will see an improved variation of RNNs called LSTM to address the drawbacks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Contextual information and the architecture of RNNs</h1>
                </header>
            
            <article>
                
<p>Human beings don't start thinking from scratch; the human mind has so-called <strong>persistence of memory</strong>, the ability to associate the past with recent information. Traditional neural networks, instead, ignore past events. For example, in a movie scenes classifier, it's not possible for a neural network to use a past scene to classify current ones. RNNs were developed to try to solve this problem:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="168" width="99" src="assets/19903995-1d34-4460-af08-79d4dab2a55f.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 1: RNNs have loops</div>
<p>In contrast to conventional neural networks, RNNs are networks with a loop that allows the information to be persistent (<em>Figure 1</em>). In a neural network say, <strong><span class="mi">A</span></strong>: at some time <strong>t</strong>, input <strong><span class="mi">x<sub>t</sub></span></strong> and outputs a value <strong><span class="mi">h<sub>t</sub></span></strong>. So from <em>Figure 1</em>, we can think of an RNN as multiple copies of the same network, each passing a message to a successor. Now, if we unroll the previous network, what will we receive? Well, the following figure gives you some insight:</p>
<div class="CDPAlignCenter CDPAlign"><img height="135" width="407" class="alignnone size-full wp-image-554 image-border" src="assets/ce3d30f1-828f-4530-932c-9af098e2e36e.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 2: An unrolled representation of the same RNN represented in Figure 1</div>
<p>However, the preceding unrolled figure does not provide detailed information about RNNs. Rather, an RNN is different from a traditional neural network because it introduces a transition weight <strong>W</strong> to transfer information between times. RNNs process a sequential input one at a time, updating a kind of vector state that contains information about all past elements of the sequence. The following figure shows a neural network that takes as input a value of <strong>X(t)</strong>, and then outputs a value <strong>Y(t)</strong>:</p>
<div class="CDPAlignCenter CDPAlign"><img height="160" width="290" class="alignnone size-full wp-image-555 image-border" src="assets/c664553f-02b8-4a9a-a128-625acf326972.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 3: An RNN architecture can use the previous states of the network to its advantage</div>
<p>As shown in <em>Figure 1</em>, the first half of the neural network is characterized by the function <em>Z (t) = X (t) * W<sub>in</sub></em>, and the second half of the neural network takes the form <em>Y(t)= Z(t) * W<sub>out</sub></em>. If you prefer, the whole neural network is just the function <em>Y (t) = (X (t) * W</em><sub>in</sub><em>) * W</em><sub>out</sub>.</p>
<p>At each time <em>t</em>, calls the learned model, this architecture does not take into account knowledge about the previous runs. It's like predicting stock market trends by only looking at data from the current day. A better idea would be to exploit overarching patterns from a week's worth or months worth of data:</p>
<div class="CDPAlignCenter CDPAlign"><img height="215" width="501" class="alignnone size-full wp-image-556 image-border" src="assets/eb70b26d-db1e-4a79-9670-e0d38d037b61.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 4: An RNN architecture where all the weights in all the layers have to be learned with time</div>
<p>A more explicit architecture can be found in <em>Figure 4</em>, where the temporally shared weights <strong>w2</strong> (for the hidden layer) must be learned in addition to <strong>w1</strong> (for the input layer) and <strong>w3</strong> (for the output layer).</p>
<p>Incredibly, over the last few years, RNNs have been used for a variety of problems, such as speech recognition, language modeling, translation, and image captioning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">RNN and the long-term dependency problem</h1>
                </header>
            
            <article>
                
<p>RNNs are very powerful and popular too. However, often, we only need to look at recent information to perform the present task rather than information that was stored a long time ago. This is frequent in NLP for language modeling. Let's see a common example:</p>
<div class="CDPAlignCenter CDPAlign"><img height="221" width="463" class="alignnone size-full wp-image-558 image-border" src="assets/907bff1c-57b0-4927-9a45-0a2922d87fea.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5: If the gap between the relevant information and the place that its needed is small, RNNs can learn to use past information</div>
<p>Suppose a language model is trying to predict the next word based on the previous words. As a human being, if we try to predict the last word in <em>the sky is blue</em>, without further context, it's most likely the next word that we will predict is <em>blue</em>. In such cases, the gap between the relevant information and the place is small. Thus, RNNs can learn to use past information easily.</p>
<p>But consider a longer sentence: <em>Asif grew up in Bangladesh... He studied in Korea... He speaks fluent Bengali</em> where we need more context. In this sentence, most recent information advises us that the next word will probably be the name of a language. However, if we want to narrow down which language, we need the context of <em>Bangladesh</em> from previous words:</p>
<div class="CDPAlignCenter CDPAlign"><img height="201" width="540" class="alignnone size-full wp-image-559 image-border" src="assets/44e0a9aa-4a4d-48df-87e7-a352f6f8b440.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 6: If the gap between the relevant information and the place that its needed is bigger, RNNs can't learn to use past information</div>
<p>Here, the gap is bigger so RNNs become unable to learn the information. This is a serious drawback of RNN. However, along comes LSTM to the rescue.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">LSTM networks</h1>
                </header>
            
            <article>
                
<p>One type of RNN model is an <strong>LSTM</strong>. The precise implementation details of LSTM are not within the scope of this book. An LSTM is a special RNN architecture, which was originally conceived by Hochreiter and Schmidhuber in 1997. This type of neural network has been recently rediscovered in the context of deep learning, because it is free from the problem of vanishing gradients, and offers excellent results and performance. LSTM-based networks are ideal for prediction and classification of temporal sequences, and are replacing many traditional approaches to deep learning.</p>
<p>It's a hilarious name, but it means exactly what it sounds. The name signifies that short-term patterns aren't forgotten in the long-term. An LSTM network is composed of cells (LSTM blocks) linked to each other. Each LSTM block contains three types of gate: input gate, output gate, and forget gate, respectively, that implement the functions of writing, reading, and resetting the cell memory. These gates are not binary, but analogical (generally managed by a sigmoidal activation function mapped in the range (0, 1), where 0 indicates total inhibition, and 1 shows total activation).</p>
<p>If you consider the LSTM cell as a black box, it can be used very much like a basic cell, except it will perform much better; training will converge faster, and it will detect long-term dependencies in the data. So how does an LSTM cell work? The architecture of a basic LSTM cell is shown in <em>Figure 7</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img height="258" width="514" src="assets/68fcf3fa-5376-443f-94b4-41670eda6887.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 7: Block diagram of an LSTM cell</div>
<p>Now, let's see the mathematical notation behind this architecture. If we don't look at what's inside the LSTM box, the LSTM cell itself looks exactly like a regular memory cell, except that its state is split into two vectors, <strong>h(t)</strong> and <strong>c(t)</strong>:</p>
<ul>
<li><strong>c</strong> is a cell</li>
<li><strong>h(t)</strong> is the short-term state</li>
<li><strong>c(t)</strong> is the long-term state</li>
</ul>
<p>Now let's open the box! The key idea is that the network can learn what to store in the long-term state, what to throw away, and what to read from it. As the long-term state <strong>c<sub>(t-1)</sub></strong> traverses the network from left to right, you can see that it first goes through a forget gate, dropping some memories, and then it adds some new memories via the addition operation (which adds the memories that were selected by an input gate). The resulting <strong>c(t)</strong> is sent straight out, without any further transformation.</p>
<p>So, at each timestamp, some memories are dropped and some memories are added. Moreover, after the addition operation, the long-term state is copied and passed through the <strong>tanh</strong> function, and then the result is filtered by the output gate. This produces the short-term state <strong>h(t)</strong> (which is equal to the cell's output for this time step <strong>y(t)</strong>). Now let's look at where new memories come from and how the gates work. First, the current input vector <strong>x(t)</strong> and the previous short-term state <strong>h(t-1)</strong> are fed to four different fully connected layers.</p>
<p>The presence of these gates allows LSTM cells to remember information for an indefinite time; if the input gate is below the activation threshold, the cell will retain the previous state, and if the current state is enabled, it will be combined with the input value. As the name suggests, the forget gate resets the current state of the cell (when its value is cleared to 0), and the output gate decides whether the value of the cell must be carried out or not. The following equations are used to do the LSTM computations of a cell's long-term state, its short-term state, and its output at each time step for a single instance:</p>
<div class="CDPAlignCenter CDPAlign"><img height="232" width="316" src="assets/72f5567e-1bd5-4ab9-abbd-452789a2b46b.png"/></div>
<p>In the preceding equation, <em>W<sub>xi</sub></em>, <em>W<sub>xf</sub></em>, <em>W<sub>xo</sub></em>, and <em>W<sub>xg</sub></em> are the weight matrices of each of the four layers for their connection to the input vector <em>x<sub>(t)</sub></em>. On the other hand, <em>W<sub>hi</sub></em>, <em>W<sub>hf</sub></em>, <em>W<sub>ho</sub></em>, and <em>W<sub>hg</sub></em> are the weight matrices of each of the four layers for their connection to the previous short-term state <em>h<sub>(t-1)</sub></em>. Finally, <em>b<sub>i</sub></em>, <em>b<sub>f</sub></em>, <em>b<sub>o</sub></em>, and <em>b<sub>g</sub></em> are the bias terms for each of the four layers.</p>
<p>Now that we know all that, how do both RNN and the LSTM network work? It's time to do some hands-on. We will start implementing an MXNet and Scala-based LSTM model for HAR.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Human activity recognition using the LSTM model</h1>
                </header>
            
            <article>
                
<p>The <strong>Human Activity Recognition</strong> (<strong>HAR</strong>) database was built from the recordings of 30 study participants performing <strong>activities of daily living</strong> (<strong>ADL</strong>) while carrying a waist-mounted smartphone with embedded inertial sensors. The objective is to classify activities into one of the six activities performed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dataset description</h1>
                </header>
            
            <article>
                
<p>The experiments have been carried out with a group of 30 volunteers within an age bracket of 19 - 48 years. Each person accomplished six activities, namely walking, walking upstairs, walking downstairs, sitting, standing, and laying by wearing a Samsung Galaxy S II smartphone on their waist. Using the accelerometer and gyroscope, the author captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50 Hz.</p>
<p>Only two sensors, that is, accelerometer and gyroscope, were used. The sensor signals were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap. This gives 128 readings/window. The gravitational and body motion components from the sensor acceleration signal were separated via a Butterworth low-pass filter into body acceleration and gravity.</p>
<div class="packt_tip">For more information, please refer to this paper: Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset for <em>Human Activity Recognition Using Smartphones</em>. <em>21st European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN</em> 2013. Bruges, Belgium 24-26 April 2013.</div>
<p>For simplicity, the gravitational force is assumed to have only a few but low-frequency components. Therefore, a filter of 0.3 Hz cut-off frequency was used. From each window, a feature vector was found by calculating variables from the time and frequency domain.</p>
<p>The experiments have been video recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers were selected for generating the training data and 30% the test data. Now, when I explore the dataset, both the training and test set have the following file structure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="397" width="159" src="assets/88720740-0311-4da6-b5f1-01ee9d3670a0.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 8: HAR dataset file structure</div>
<p>For each record in the dataset, the following is provided:</p>
<ul>
<li>Triaxial acceleration from the accelerometer and the estimated body acceleration</li>
<li>Triaxial angular velocity from the gyroscope sensor</li>
<li>A 561-feature vector with time and frequency domain variables</li>
<li>Its activity label</li>
<li>An identifier of the subject who carried out the experiment</li>
</ul>
<p>Now we know the problem that needs to be addressed, it's time to explore the technology and related challenges. Well, as I already stated, we will be using an MXNet-based LSTM implementation. One question you may ask is: why aren't we using H2O or DeepLearning4j? Well, the answer is that both of them either do not have LSTM-based implementation, or cannot be applied to solve this problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting and configuring MXNet for Scala</h1>
                </header>
            
            <article>
                
<p>Apache MXNet is a flexible and efficient library for deep learning. Building a high-performance deep learning library requires many system-level design decisions. In this design note, we share the rationale for the specific choices made when designing MXNet. We imagine that these insights may be useful to both deep learning practitioners and builders of other deep learning systems.</p>
<p>For this project, we will be needing different packages and libraries: Scala, Java, OpenBLAS, ATLAS, OpenCV, and overall, MXNet. Now let's start configuring these tools one by one. For Java and Scala, I am assuming that you already have Java and Scala configured. Now the next task is to install build tools and <kbd>git</kbd> since we will be using the MXNet from the GitHub repository. To do this, just execute the following commands on Ubuntu:</p>
<pre><strong>$ sudo apt-get update 
$ sudo apt-get install -y build-essential git</strong> </pre>
<p>Then we need to install OpenBLAS and ATLAS. These are required for linear algebra operations performed by MXNet. To install these, just execute the following command:</p>
<pre><strong>$ sudo apt-get install -y libopenblas-dev 
$ sudo apt-get install -y libatlas-base-dev</strong> </pre>
<p>We also need to install OpenCV for image processing. Let's install it by executing the following command:</p>
<pre><strong> $ sudo apt-get install -y libopencv-dev</strong> </pre>
<p>Finally, we need to generate the prebuilt MXNet binary. To do this, we need to clone and build MXNet for Scala:</p>
<pre><strong>$ git clone --recursive https://github.com/apache/incubator-mxnet.git mxnet --branch 0.12.0 
$ cd mxnet 
$ make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas 
$ make scalapkg 
$ make scalainsta</strong> </pre>
<p>Now, if the preceding steps went smoothly, a prebuilt-binary for MXNet will be generated in <kbd>/home/$user_name/mxnet/scala-package/assembly/linux-x86_64-cpu</kbd> (or <kbd>linux-x86_64-gpu</kbd> with GPU configured on Linux, and <kbd>osx-x86_64-cpu</kbd> on macOS). Take a look at the following screenshot of the CPU on Ubuntu:</p>
<div class="CDPAlignCenter CDPAlign"><img height="241" width="503" src="assets/037aaeb6-0dec-465d-8936-594577f7e5d9.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9: MXNet pre-built binary generated</div>
<p>Now, the next task before you start writing your Scala code on Eclipse (or IntelliJ) as a Maven (or SBT) project, is including this JAR in the build path. Additionally, we need some extra dependency for Scala plots and <kbd>args4j</kbd>:</p>
<pre>&lt;dependency&gt;<br/>    &lt;groupId&gt;org.sameersingh.scalaplot&lt;/groupId&gt;<br/>    &lt;artifactId&gt;scalaplot&lt;/artifactId&gt;<br/>    &lt;version&gt;0.0.4&lt;/version&gt;<br/>&lt;/dependency&gt;<br/>&lt;dependency&gt;<br/>    &lt;groupId&gt;args4j&lt;/groupId&gt;<br/>    &lt;artifactId&gt;args4j&lt;/artifactId&gt;<br/>    &lt;version&gt;2.0.29&lt;/version&gt;<br/>&lt;/dependency&gt;</pre>
<p>Well done! All set and we're ready to go! Let's start coding.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing an LSTM model for HAR</h1>
                </header>
            
            <article>
                
<p>The overall algorithm (<kbd>HumanAR.scala</kbd>) has the following workflow:</p>
<ul>
<li>Loading the data</li>
<li>Defining hyperparameters</li>
<li>Setting up the LSTM model using imperative programming and the hyperparameters</li>
<li>Applying batch wise training, that is, picking batch size data, feeding it to the model, then at some iterations evaluating the model and printing the batch loss and the accuracy</li>
<li>Output the chart for the training and test errors</li>
</ul>
<p>The preceding steps can be followed and constructed by way of a pipeline:</p>
<div class="CDPAlignCenter CDPAlign"><img height="235" width="468" src="assets/9db9b808-6ecd-4cbf-aa7e-6ee6a098d27d.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 10: MXNet pre-built binary generated</div>
<p>Now let's start the implementation step-by-step. Make sure that you understand each line of code then import the given project in Eclipse or SBT.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 1 - Importing necessary libraries and packages</h1>
                </header>
            
            <article>
                
<p>Let's start coding now. We start from the very beginning, that is, by importing libraries and packages:</p>
<pre><strong>package</strong> com.packt.ScalaML.HAR <br/><br/><strong>import</strong> ml.dmlc.mxnet.Context 
<strong>import</strong> LSTMNetworkConstructor.LSTMModel 
<strong>import</strong> scala.collection.mutable.ArrayBuffer 
<strong>import</strong> ml.dmlc.mxnet.optimizer.Adam 
<strong>import</strong> ml.dmlc.mxnet.NDArray 
<strong>import</strong> ml.dmlc.mxnet.optimizer.RMSProp 
<strong>import</strong> org.sameersingh.scalaplot.MemXYSeries 
<strong>import</strong> org.sameersingh.scalaplot.XYData 
<strong>import</strong> org.sameersingh.scalaplot.XYChart 
<strong>import</strong> org.sameersingh.scalaplot.Style._ 
<strong>import</strong> org.sameersingh.scalaplot.gnuplot.GnuplotPlotter 
<strong>import</strong> org.sameersingh.scalaplot.jfreegraph.JFGraphPlotter  </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 2 - Creating MXNet context</h1>
                </header>
            
            <article>
                
<p>Then we create an MXNet context for CPU-based computation. Since I am doing it by CPU, I instantiated for the CPU. Feel free to use the GPU if you have already configured it by providing the device ID:</p>
<pre>// Retrieves the name of this Context object <strong><br/>val</strong> ctx = Context.cpu() </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 3 - Loading and parsing the training and test set</h1>
                </header>
            
            <article>
                
<p>Now let's load the dataset. I am assuming that you copied your dataset to the <kbd>UCI_HAR_Dataset/</kbd> directory. Then, also place the other data files as described previously:</p>
<pre><strong>val</strong> datasetPath = "UCI_HAR_Dataset/" 
<strong>val</strong> trainDataPath = s"$datasetPath/train/Inertial Signals" 
<strong>val</strong> trainLabelPath = s"$datasetPath/train/y_train.txt" <br/><strong>val</strong> testDataPath = s"$datasetPath/test/Inertial Signals" 
<strong>val</strong> testLabelPath = s"$datasetPath/test/y_test.txt" </pre>
<p>Now it's time to load the training and test set separately. To do this I wrote two methods called <kbd>loadData()</kbd> and <kbd>loadLabels()</kbd> that are in the <kbd>Utils.scala</kbd> file. These two methods and their signatures will be provided soon:</p>
<pre><strong>val</strong> trainData = Utils.loadData(trainDataPath, "train") 
<strong>val</strong> trainLabels = Utils.loadLabels(trainLabelPath) 
<strong>val</strong> testData = Utils.loadData(testDataPath, "test") 
<strong>val</strong> testLabels = Utils.loadLabels(testLabelPath) </pre>
<p>The <kbd>loadData()</kbd> method loads and maps the data from each <kbd>.txt</kbd> file based on  the input signal type defined by the <kbd>INPUT_SIGNAL_TYPES</kbd> array in the <kbd>Array[Array[Array[Float]]]</kbd> format:</p>
<pre><strong>def</strong> loadData(dataPath: String, name: String): Array[Array[Array[Float]]] = { 
    <strong>val</strong> dataSignalsPaths = INPUT_SIGNAL_TYPES.map( signal =&gt; s"$dataPath/${signal}${name}.txt" ) 
    <strong>val</strong> signals = dataSignalsPaths.map { path =&gt;  
      <strong>Source</strong>.fromFile(path).mkString.split("n").map { line =&gt;  
        line.replaceAll("  ", " ").trim().split(" ").map(_.toFloat) } 
    } 
 
    <strong>val</strong> inputDim = signals.length 
    <strong>val</strong> numSamples = signals(0).length 
    <strong>val</strong> timeStep = signals(0)(0).length   
 
    (0 <strong>until</strong> numSamples).map { n =&gt;  
      (0 <strong>until</strong> timeStep).map { t =&gt; 
        (0 <strong>until</strong> inputDim).map( i =&gt; signals(i)(n)(t) ).toArray 
      }<br/>    .toArray 
    }<br/>    .toArray 
  } </pre>
<p>As stated earlier, the <kbd>INPUT_SIGNAL_TYPES</kbd> contains some useful constants: those are separate, normalized input features for the neural network:</p>
<pre><strong>private val</strong> INPUT_SIGNAL_TYPES = Array( 
    "body_acc_x_", 
    "body_acc_y_", 
    "body_acc_z_", 
    "body_gyro_x_", 
    "body_gyro_y_", 
    "body_gyro_z_", 
    "total_acc_x_", 
    "total_acc_y_", 
    "total_acc_z_") </pre>
<p>On the other hand, <kbd>loadLabels()</kbd> is also a user-defined method that is used to load only the labels in the training as well as the test set:</p>
<pre><strong>def</strong> loadLabels(labelPath: String): Array[Float] = {          <br/>       Source.fromFile(labelPath).mkString.split("n").map(_.toFloat - 1)<br/>            } </pre>
<p>The labels are defined in another array as shown in the following code:</p>
<pre>// Output classes: used to learn how to classify 
<strong>private val</strong> LABELS = Array( 
    "WALKING",  
    "WALKING_UPSTAIRS",  
    "WALKING_DOWNSTAIRS",  
    "SITTING",  
    "STANDING",  
    "LAYING") </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 4 - Exploratory analysis of the dataset</h1>
                </header>
            
            <article>
                
<p>Now let's see some statistics about the number of training series (as described earlier, this is with 50% overlap between each series), number of test series, number of timesteps per series, and number of input parameters per timestep:</p>
<pre><strong>val</strong> trainingDataCount = trainData.length // No. of training series  
<strong>val</strong> testDataCount = testData.length // No. of testing series 
<strong>val</strong> nSteps = trainData(0).length // No. of timesteps per series 
<strong>val</strong> nInput = trainData(0)(0).length // No. of input parameters per timestep <br/><br/>println("Number of training series: "+ trainingDataCount) 
println("Number of test series: "+ testDataCount) 
println("Number of timestep per series: "+ nSteps) 
println("Number of input parameters per timestep: "+ nInput) <br/>&gt;&gt;&gt;</pre>
<p>The output is:</p>
<pre>Number of training series: 7352<br/>Number of test series: 2947<br/>Number of timestep per series: 128<br/>Number of input parameters per timestep: 9</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 5 - Defining internal RNN structure and LSTM hyperparameters</h1>
                </header>
            
            <article>
                
<p>Now, let's define the internal neural network structure and hyperparameters for the LSTM network:</p>
<pre><strong>val</strong> nHidden = 128 // Number of features in a hidden layer  
<strong>val</strong> nClasses = 6 // Total classes to be predicted  
 
<strong>val</strong> learningRate = 0.001f 
<strong>val</strong> trainingIters = trainingDataCount * 100  // iterate 100 times on trainset: total 7352000 iterations 
<strong>val</strong> batchSize = 1500 
<strong>val</strong> displayIter = 15000  // To show test set accuracy during training 
<strong>val</strong> numLstmLayer = 3 </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 6 - LSTM network construction</h1>
                </header>
            
            <article>
                
<p>Now, let's set up an LSTM model with the preceding parameters and structure:</p>
<pre><strong>val</strong> model = LSTMNetworkConstructor.setupModel(nSteps, nInput, nHidden, nClasses, batchSize, ctx = ctx) </pre>
<p>In the preceding line, <kbd>setupModel()</kbd> is the method that does the trick. The <kbd>getSymbol()</kbd> method actually constructs the LSTM cell. We will see its signature, too, later on. It accepts sequence length, number of input, number of hidden layers, number of labels, batch size, number of LSTM layers, dropout MXNet context, and constructs an LSTM model of type using the case class <kbd>LSTMModel</kbd>:</p>
<pre><strong>case class</strong> LSTMModel(exec: Executor, symbol: Symbol, data: NDArray, label: NDArray, argsDict: Map[String,                     NDArray], gradDict: Map[String, NDArray]) </pre>
<p>Now here's the signature of the <kbd>setupModel()</kbd>:</p>
<pre><strong>def</strong> setupModel(seqLen: Int, nInput: Int, numHidden: Int, numLabel: Int, batchSize: Int, numLstmLayer: Int = 1, dropout: Float = 0f, ctx: Context = Context.cpu()): LSTMModel = { 
//get the symbolic model 
    <strong>val</strong> sym = LSTMNetworkConstructor.getSymbol(seqLen, numHidden, numLabel, numLstmLayer = numLstmLayer) 
    <strong>val</strong> argNames = sym.listArguments() 
    <strong>val</strong> auxNames = sym.listAuxiliaryStates() <br/>// defining the initial argument and binding them to the model 
    <strong>val</strong> initC = for (l &lt;- 0 until numLstmLayer) yield (s"l${l}_init_c", (batchSize, numHidden)) 
    <strong>val</strong> initH = for (l &lt;- 0 until numLstmLayer) yield (s"l${l}_init_h", (batchSize, numHidden)) 
    <strong>val</strong> initStates = (initC ++ initH).map(x =&gt; x._1 -&gt; Shape(x._2._1, x._2._2)).toMap 
    <strong>val</strong> dataShapes = Map("data" -&gt; Shape(batchSize, seqLen, nInput)) ++ initStates 
    <strong>val</strong> (argShapes, outShapes, auxShapes) = sym.inferShape(dataShapes) 
 
    <strong>val</strong> initializer = new Uniform(0.1f) 
    <strong>val</strong> argsDict = argNames.zip(argShapes).map { case (name, shape) =&gt; 
       <strong>val</strong> nda = NDArray.zeros(shape, ctx) 
       <strong>if</strong> (!dataShapes.contains(name) &amp;&amp; name != "softmax_label") { 
         initializer(name, nda) 
       } 
       name -&gt; nda 
    }.toMap 
 
    <strong>val</strong> argsGradDict = argNames.zip(argShapes) 
         .filter(x =&gt; x._1 != "softmax_label" &amp;&amp; x._1 != "data") 
         .map( x =&gt; x._1 -&gt; NDArray.zeros(x._2, ctx) ).toMap 
 
    <strong>val</strong> auxDict = auxNames.zip(auxShapes.map(NDArray.zeros(_, ctx))).toMap 
    <strong>val</strong> exec = sym.bind(ctx, argsDict, argsGradDict, "write", auxDict, null, null) 
    <strong>val</strong> data = argsDict("data") 
    <strong>val</strong> label = argsDict("softmax_label")  <br/>    LSTMModel(exec, sym, data, label, argsDict, argsGradDict)<br/>} </pre>
<p>In the preceding method, we obtained a symbolic model for the deep RNN using the <kbd>getSymbol()</kbd> method that can be seen as follows. I have provided detailed comments and believe that will be enough to understand the workflow of the code:</p>
<pre><strong>  private def</strong> getSymbol(seqLen: Int, numHidden: Int, numLabel: Int, numLstmLayer: Int = 1, <br/>                        dropout: Float = 0f): Symbol = {  
                //symbolic training and label variables 
               <strong> var</strong> inputX = Symbol.Variable("data") 
                <strong>val</strong> inputY = Symbol.Variable("softmax_label") 
 
                //the initial parameters and cells 
               <strong> var</strong> paramCells = Array[LSTMParam]() 
                <strong>var</strong> lastStates = Array[LSTMState]() <br/>                //numLstmLayer is 1  
                <strong>for</strong> (i &lt;- 0 until numLstmLayer) { 
                    paramCells = paramCells :+ LSTMParam(i2hWeight =<br/>                    Symbol.Variable(s"l${i}_i2h_weight"), 
                    i2hBias = Symbol.Variable(s"l${i}_i2h_bias"),                                                                                     <br/>                    h2hWeight = Symbol.Variable(s"l${i}_h2h_weight"),                                                                                                                                   <br/>                    h2hBias = Symbol.Variable(s"l${i}_h2h_bias")) 
                    lastStates = lastStates :+ LSTMState(c =<br/>                    Symbol.Variable(s"l${i}_init_c"),                                                                      <br/>                    h = Symbol.Variable(s"l${i}_init_h")) 
            } 
            <strong>assert</strong>(lastStates.length == numLstmLayer) <br/>            <strong>val</strong> lstmInputs = Symbol.SliceChannel()(inputX)(Map("axis" <br/>            &gt; 1, "num_outputs" -&gt; seqLen,       <br/>            "squeeze_axis" -&gt; 1)) 
 
            <strong>var</strong> hiddenAll = Array[Symbol]() 
           <strong> var</strong> dpRatio = 0f 
            <strong>var</strong> hidden: Symbol = null 
     
//for each one of the 128 inputs, create a LSTM Cell 
           <strong> for</strong> (seqIdx &lt;- 0 until seqLen) { 
                  hidden = lstmInputs.get(seqIdx) 
// stack LSTM, where numLstmLayer is 1 so the loop will be executed only one time 
                  <strong>for</strong> (i &lt;- 0 until numLstmLayer) { 
                        <strong>if</strong> (i == 0) dpRatio = 0f else dpRatio = dropout 
//for each one of the 128 inputs, create a LSTM Cell 
                        <strong>val</strong> nextState = lstmCell(numHidden, inData = hidden, 
                          prevState = lastStates(i), 
                          param = paramCells(i), 
                          seqIdx = seqIdx, layerIdx = i, dropout =<br/>                        dpRatio) 
                    hidden = nextState.h // has no effect 
                    lastStates(i) = nextState // has no effect 
              } 
// adding dropout before softmax has no effect- dropout is 0 due to numLstmLayer == 1 
          <strong>    if</strong> (dropout &gt; 0f) hidden = Symbol.Dropout()()(Map("data" -&gt; hidden, "p" -&gt; dropout)) 
// store the lstm cells output layers 
                  hiddenAll = hiddenAll :+ hidden<br/>    } </pre>
<p>In summary, the algorithm uses 128 LSTM cells in parallel, and I concatenated all 128 cells and fed them to the output activation layer. Let's concatenate the cells, outputs:</p>
<pre><strong>val</strong> finalOut = hiddenAll.reduce(_+_) </pre>
<p>Then we connect them to an output layer that corresponds to the 6 label:</p>
<pre><strong> val</strong> fc = Symbol.FullyConnected()()(Map("data" -&gt; finalOut, "num_hidden" -&gt; numLabel)) 
 //softmax activation against the label 
 Symbol.SoftmaxOutput()()(Map("data" -&gt; fc, "label" -&gt; inputY)) </pre>
<p class="mce-root">In the preceding code segment, <kbd>LSTMState</kbd> and <kbd>LSTMParam</kbd> are two case classes that used to define the state of each LSTM cell and the latter accepts the parameters needed to construct an LSTM cell. final case class <kbd>LSTMState(c: Symbol, h: Symbol)</kbd> final case class <kbd>LSTMParam(i2hWeight: Symbol, i2hBias: Symbol, h2hWeight: Symbol, h2hBias: Symbol)</kbd>.</p>
<p class="mce-root CDPAlignLeft CDPAlign">Now it's time to discuss the most important step, which is LSTM cell construction. We will use some diagrams and legends as shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img height="69" width="479" class="alignnone size-full wp-image-561 image-border" src="assets/fb82303a-6dd0-4ddc-b518-940435467dcb.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11: Legends used to describe LSTM cell in the following</div>
<p>The repeating module in an LSTM contains four interacting layers as shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="365" width="863" class="alignnone size-full wp-image-562 image-border" src="assets/47c687ee-3a27-4670-bc63-8f825d976de1.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 12: Inside an LSTM cell, that is the, repeating module in an LSTM contains four interacting layers</div>
<p>An LSTM cell is defined by its stats and parameters, as defined by the preceding two case classes:</p>
<ul>
<li><strong>LSTM state</strong>: <strong>c</strong> is the cell stat (its memory knowledge) to be used during the training and <strong>h</strong> is the output</li>
<li><strong>LSTM parameters</strong>: To be optimized by the training algorithm</li>
<li><strong>i2hWeight</strong>: Input to hidden weight</li>
<li><strong>i2hBias</strong>: Input to hidden bias</li>
<li><strong>h2hWeight</strong>: Hidden to hidden weight</li>
<li><strong>h2hBias</strong>: Hidden to hidden bias</li>
<li><strong>i2h</strong>: An NN for input data</li>
<li><strong>h2h</strong>: An NN from the previous <strong>h</strong></li>
</ul>
<p>In the code, the two fully connected layers have been created, concatenated, and transformed to four copies by the following code. Let's add a hidden layer of size <kbd>numHidden * 4</kbd> (<kbd>numHidden</kbd> set to 28) that takes as input the <kbd>inputdata</kbd>:</p>
<pre><strong>val</strong> i2h = Symbol.FullyConnected(s"t${seqIdx}_l${layerIdx}_i2h")()(Map("data" -&gt; inDataa, "weight" -&gt;                 param.i2hWeight, "bias" -&gt; param.i2hBias, "num_hidden" -&gt; numHidden * 4)) </pre>
<p>Then we add a hidden layer of size <kbd>numHidden * 4</kbd> (<kbd>numHidden</kbd> set to 28) that takes as input the previous output of the cell:</p>
<pre><strong>val</strong> h2h = Symbol.FullyConnected(s"t${seqIdx}_l${layerIdx}_h2h")()(Map("data" -&gt; prevState.h,"weight" -&gt;             param.h2hWeight,"bias" -&gt; param.h2hBias,"num_hidden" -&gt; numHidden * 4)) </pre>
<p>Now let's concatenate them together:</p>
<pre><strong>val</strong> gates = i2h + h2h </pre>
<p>Then let's make four copies of gates before we compute the gates:</p>
<pre><strong>val</strong> sliceGates = Symbol.SliceChannel(s"t${seqIdx}_l${layerIdx}_slice")(gates)(Map("num_outputs" -&gt; 4)) </pre>
<p class="mce-root">Then we compute the gates:</p>
<pre><strong>val</strong> sliceGates = Symbol.SliceChannel(s"t${seqIdx}_l${layerIdx}_slice")(gates)(Map("num_outputs" -&gt; 4)) </pre>
<p>Now the activation for the forget gate is represented by the following code:</p>
<pre><strong>val</strong> forgetGate = Symbol.Activation()()(Map("data" -&gt; sliceGates.get(2), "act_type" -&gt; "sigmoid")) </pre>
<p>We can see this in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="265" width="761" class="alignnone size-full wp-image-565 image-border" src="assets/11da362b-ef68-4a02-bd87-bf8c8ac8b0c0.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 13: Forget gate in an LSTM cell</div>
<p>Now, the activation for the in gate and in transform are represented by the following code:</p>
<pre><strong>val</strong> ingate = Symbol.Activation()()(Map("data" -&gt; sliceGates.get(0), "act_type" -&gt; "sigmoid"))   
<strong>val</strong> inTransform = Symbol.Activation()()(Map("data" -&gt; sliceGates.get(1), "act_type" -&gt; "tanh")) </pre>
<p><span>We can also see this in </span><em>Figure 14:</em></p>
<div class="CDPAlignCenter CDPAlign"><img height="722" width="2284" class="alignnone size-full wp-image-566 image-border" src="assets/358d8386-48d9-498a-92e8-e71974a4cd78.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 14: In gate and transform gate in an LSTM cell</div>
<p>The next state is defined by the following code:</p>
<pre><strong>val</strong> nextC = (forgetGate * prevState.c) + (ingate * inTransform) </pre>
<p>The preceding code can be represented by the following figure too:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ed5bd699-4ed1-40d4-9dab-01afb9246576.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 15: Next or transited gate in an LSTM cell</div>
<p>Finally, the output gate can be represented by the following code:</p>
<pre><strong>val</strong> nextH = outGate * Symbol.Activation()()(Map("data" -&gt; nextC, "act_type" -&gt; "tanh")) </pre>
<p>The preceding code can be represented by the following figure too:</p>
<div class="CDPAlignCenter CDPAlign"><img height="730" width="2112" class="alignnone size-full wp-image-567 image-border" src="assets/8bb371d3-d4e8-4768-a8c4-4aeecc5494b2.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 16: Output gate in an LSTM cell</div>
<p>Too much of a mouthful? No worries, here I have provided the full code for this method:</p>
<pre>  // LSTM Cell symbol 
  <strong>private def</strong> lstmCell( numHidden: Int, inData: Symbol, prevState: LSTMState, param: LSTMParam, 
                        seqIdx: Int, layerIdx: Int, dropout: Float = 0f): LSTMState = { 
        <strong>val</strong> inDataa = { 
              <strong>if</strong> (dropout &gt; 0f) Symbol.Dropout()()(Map("data" -&gt; inData, "p" -&gt; dropout)) 
              <strong>else</strong> inData 
                } 
        // add an hidden layer of size numHidden * 4 (numHidden set //to 28) that takes as input) 
        <strong>val</strong> i2h = Symbol.FullyConnected(s"t${seqIdx}_l${layerIdx}_i2h")()(Map("data" -&gt; inDataa,"weight"                             -&gt; param.i2hWeight,"bias" -&gt; param.i2hBias,"num_hidden" -&gt; numHidden * 4)) <br/>        // add an hidden layer of size numHidden * 4 (numHidden set to 28) that takes output of the cell  
       <strong> val</strong> h2h = Symbol.FullyConnected(s"t${seqIdx}_l${layerIdx}_h2h")()(Map("data" -&gt;                                    prevState.h,"weight" -&gt; param.h2hWeight,"bias" -&gt; param.h2hBias,"num_hidden" -&gt; numHidden * 4)) 
 
        //concatenate them                                        
        val gates = i2h + h2h  
    
        //make 4 copies of gates 
        <strong>val</strong> sliceGates=Symbol.SliceChannel(s"t${seqIdx}_l${layerIdx}_slice")(gates)(Map("num_outputs" <br/>       -&gt; 4)) <br/>        // compute the gates 
      <strong>  val</strong> ingate = Symbol.Activation()()(Map("data" -&gt; sliceGates.get(0), "act_type" -&gt; "sigmoid")) 
       <strong> val</strong> inTransform = Symbol.Activation()()(Map("data" -&gt; sliceGates.get(1), "act_type" -&gt; "tanh")) 
       <strong> val</strong> forgetGate = Symbol.Activation()()(Map("data" -&gt; sliceGates.get(2), "act_type" -&gt; "sigmoid")) 
       <strong> val</strong> outGate = Symbol.Activation()()(Map("data" -&gt; sliceGates.get(3), "act_type" -&gt; "sigmoid")) <br/>        // get the new cell state and the output 
       <strong> val</strong> nextC = (forgetGate * prevState.c) + (ingate * inTransform) 
      <strong>  val</strong> nextH = outGate * Symbol.Activation()()(Map("data" -&gt; nextC, "act_type" -&gt; "tanh")) 
        LSTMState(c = nextC, h = nextH) 
  } </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 7 - Setting up an optimizer</h1>
                </header>
            
            <article>
                
<p>As suggested by many researchers, the <kbd>RMSProp</kbd> optimizer helps an LSTM network to converge quickly. Therefore, I have decided to use it here too:</p>
<pre><strong>val</strong> opt = new RMSProp(learningRate = learningRate) </pre>
<p>Additionally, the model parameters to be optimized are its parameters, except the training data and the label (weights and biases):</p>
<pre><strong>val</strong> paramBlocks = model.symbol.listArguments() 
      .filter(x =&gt; x != "data" &amp;&amp; x != "softmax_label") 
      .zipWithIndex.map { case (name, idx) =&gt; 
        val state = opt.createState(idx, model.argsDict(name)) 
        (idx, model.argsDict(name), model.gradDict(name), state, name) 
      }<br/>    .toArray </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 8 - Training the LSTM network</h1>
                </header>
            
            <article>
                
<p>Now we will start training the LSTM network. However, before getting started, let's try to define some variables to keep track of the training's performance:</p>
<pre><strong>val</strong> testLosses = ArrayBuffer[Float]() 
<strong>val</strong> testAccuracies = ArrayBuffer[Float]() 
<strong>val</strong> trainLosses = ArrayBuffer[Float]() 
<strong>val</strong> trainAccuracies = ArrayBuffer[Float]()     </pre>
<p>Then, we start performing the training steps with <kbd>batch_size</kbd> iterations at each loop:</p>
<pre><strong>var</strong> step = 1 
<strong>while</strong> (step * batchSize &lt;= trainingIters) { 
   <strong> val</strong> (batchTrainData, batchTrainLabel) = { 
        <strong>val</strong> idx = ((step - 1) * batchSize) % trainingDataCount 
        <strong>if</strong> (idx + batchSize &lt;= trainingDataCount) { 
          <strong>val</strong> datas = trainData.drop(idx).take(batchSize) 
          <strong>val</strong> labels = trainLabels.drop(idx).take(batchSize) 
          (datas, labels) 
        } <strong>else</strong> { 
          <strong>val</strong> right = (idx + batchSize) - trainingDataCount 
          <strong>val</strong> left = trainingDataCount - idx 
          <strong>val</strong> datas = trainData.drop(idx).take(left) ++ trainData.take(right) 
          <strong>val</strong> labels = trainLabels.drop(idx).take(left) ++ trainLabels.take(right) 
          (datas, labels) 
    }  <br/>} </pre>
<p>Don't get derailed but take a quick look at <em>step 6</em> previously, where we have instantiated the LSTM model. Now it's time to feed the input and labels to the RNN:</p>
<pre>model.data.set(batchTrainData.flatten.flatten) 
model.label.set(batchTrainLabel) </pre>
<p>Then we do forward and backward passes:</p>
<pre>model.exec.forward(isTrain = true) 
model.exec.backward() </pre>
<p>Additionally, we need to update the parameters using the <kbd>RMSProp</kbd> optimizer that we defined in <em>step 7</em>:</p>
<pre>paramBlocks.foreach { <br/><strong>    case</strong> (idx, weight, grad, state, name) =&gt; opt.update(idx, weight, grad, state) 
    } </pre>
<p>It would also be great to get metrics such as training errors—that is, loss and accuracy over the training data:</p>
<pre><strong>val</strong> (acc, loss) = getAccAndLoss(model.exec.outputs(0), batchTrainLabel) 
      trainLosses += loss / batchSize 
      trainAccuracies += acc / batchSize </pre>
<p>In the preceding code segment, <kbd>getAccAndLoss()</kbd> is a method that computes the loss and accuracy and can be seen as follows:</p>
<pre><strong>def</strong> getAccAndLoss(pred: NDArray, label: Array[Float], dropNum: Int = 0): (Float, Float) = { 
    <strong>val</strong> shape = pred.shape 
   <strong> val</strong> maxIdx = NDArray.argmax_channel(pred).toArray 
    <strong>val</strong> acc = { 
      <strong>val</strong> sum = maxIdx.drop(dropNum).zip(label.drop(dropNum)).foldLeft(0f){ case (acc, elem) =&gt;  
        <strong>if</strong> (elem._1 == elem._2) acc + 1 else acc 
      } 
      sum 
    } 
    <strong>val</strong> loss = pred.toArray.grouped(shape(1)).drop(dropNum).zipWithIndex.map { case (array, idx) =&gt; 
        array(maxIdx(idx).toInt)   
      }.map(-Math.log(_)).sum.toFloat    
 (acc, loss)  
} </pre>
<p>Additionally, it would be exciting to evaluate only the network at some steps for faster training:</p>
<pre><strong>if</strong> ( (step * batchSize % displayIter == 0) || (step == 1) || (step * batchSize &gt; trainingIters) ) { 
        println(s"Iter ${step * batchSize}, Batch Loss = ${"%.6f".format(loss / batchSize)}, <br/>        Accuracy = ${acc / batchSize}") <br/>    }</pre>
<pre>Iter 1500, Batch Loss = 1.189168, Accuracy = 0.14266667<br/> Iter 15000, Batch Loss = 0.479527, Accuracy = 0.53866667<br/> Iter 30000, Batch Loss = 0.293270, Accuracy = 0.83933336<br/> Iter 45000, Batch Loss = 0.192152, Accuracy = 0.78933334<br/> Iter 60000, Batch Loss = 0.118560, Accuracy = 0.9173333<br/> Iter 75000, Batch Loss = 0.081408, Accuracy = 0.9486667<br/> Iter 90000, Batch Loss = 0.109803, Accuracy = 0.9266667<br/> Iter 105000, Batch Loss = 0.095064, Accuracy = 0.924<br/> Iter 120000, Batch Loss = 0.087000, Accuracy = 0.9533333<br/> Iter 135000, Batch Loss = 0.085708, Accuracy = 0.966<br/> Iter 150000, Batch Loss = 0.068692, Accuracy = 0.9573333<br/> Iter 165000, Batch Loss = 0.070618, Accuracy = 0.906<br/> Iter 180000, Batch Loss = 0.089659, Accuracy = 0.908<br/> Iter 195000, Batch Loss = 0.088301, Accuracy = 0.87333333<br/> Iter 210000, Batch Loss = 0.067824, Accuracy = 0.9026667<br/> Iter 225000, Batch Loss = 0.060650, Accuracy = 0.9033333<br/> Iter 240000, Batch Loss = 0.045368, Accuracy = 0.93733335<br/> Iter 255000, Batch Loss = 0.049854, Accuracy = 0.96<br/> Iter 270000, Batch Loss = 0.062839, Accuracy = 0.968<br/> Iter 285000, Batch Loss = 0.052522, Accuracy = 0.986<br/> Iter 300000, Batch Loss = 0.060304, Accuracy = 0.98733336<br/> Iter 315000, Batch Loss = 0.049382, Accuracy = 0.9993333<br/> Iter 330000, Batch Loss = 0.052441, Accuracy = 0.9766667<br/> Iter 345000, Batch Loss = 0.050224, Accuracy = 0.9546667<br/> Iter 360000, Batch Loss = 0.057141, Accuracy = 0.9306667<br/> Iter 375000, Batch Loss = 0.047664, Accuracy = 0.938<br/> Iter 390000, Batch Loss = 0.047909, Accuracy = 0.93333334<br/> Iter 405000, Batch Loss = 0.043014, Accuracy = 0.9533333<br/> Iter 420000, Batch Loss = 0.054124, Accuracy = 0.952<br/> Iter 435000, Batch Loss = 0.044272, Accuracy = 0.95133334<br/> Iter 450000, Batch Loss = 0.058916, Accuracy = 0.96066666<br/> Iter 465000, Batch Loss = 0.072512, Accuracy = 0.9486667<br/> Iter 480000, Batch Loss = 0.080431, Accuracy = 0.94733334<br/> Iter 495000, Batch Loss = 0.072193, Accuracy = 0.9726667<br/> Iter 510000, Batch Loss = 0.068242, Accuracy = 0.972<br/> Iter 525000, Batch Loss = 0.057797, Accuracy = 0.964<br/> Iter 540000, Batch Loss = 0.063531, Accuracy = 0.918<br/> Iter 555000, Batch Loss = 0.068177, Accuracy = 0.9126667<br/> Iter 570000, Batch Loss = 0.053257, Accuracy = 0.9206667<br/> Iter 585000, Batch Loss = 0.058263, Accuracy = 0.9113333<br/> Iter 600000, Batch Loss = 0.054180, Accuracy = 0.90466666<br/> Iter 615000, Batch Loss = 0.051008, Accuracy = 0.944<br/> Iter 630000, Batch Loss = 0.051554, Accuracy = 0.966<br/> Iter 645000, Batch Loss = 0.059238, Accuracy = 0.9686667<br/> Iter 660000, Batch Loss = 0.051297, Accuracy = 0.9713333<br/> Iter 675000, Batch Loss = 0.052069, Accuracy = 0.984<br/> Iter 690000, Batch Loss = 0.040501, Accuracy = 0.998<br/> Iter 705000, Batch Loss = 0.053661, Accuracy = 0.96066666<br/> ter 720000, Batch Loss = 0.037088, Accuracy = 0.958<br/> Iter 735000, Batch Loss = 0.039404, Accuracy = 0.9533333</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 9 - Evaluating the model</h1>
                </header>
            
            <article>
                
<p>Well done! We have finished the training. How about now evaluating the test set:</p>
<pre><strong>  val</strong> (testLoss, testAcc) = test(testDataCount, batchSize, testData, testLabels, model)         
  println(s"TEST SET DISPLAY STEP:  Batch Loss = ${"%.6f".format(testLoss)}, Accuracy = $testAcc") 
        testAccuracies += testAcc 
        testLosses += testLoss 
      } 
      step += 1 
    }     
  <strong>val</strong> (finalLoss, accuracy) = test(testDataCount, batchSize, testData, testLabels, model) 
  println(s"FINAL RESULT: Batch Loss= $finalLoss, Accuracy= $accuracy") </pre>
<pre class="mce-root">TEST SET DISPLAY STEP: Batch Loss = 0.065859, Accuracy = 0.9138107<br/> TEST SET DISPLAY STEP: Batch Loss = 0.077047, Accuracy = 0.912114<br/> TEST SET DISPLAY STEP: Batch Loss = 0.069186, Accuracy = 0.90566677<br/> TEST SET DISPLAY STEP: Batch Loss = 0.059815, Accuracy = 0.93043774<br/> TEST SET DISPLAY STEP: Batch Loss = 0.064162, Accuracy = 0.9192399<br/> TEST SET DISPLAY STEP: Batch Loss = 0.063574, Accuracy = 0.9307771<br/> TEST SET DISPLAY STEP: Batch Loss = 0.060209, Accuracy = 0.9229725<br/> TEST SET DISPLAY STEP: Batch Loss = 0.062598, Accuracy = 0.9290804<br/> TEST SET DISPLAY STEP: Batch Loss = 0.062686, Accuracy = 0.9311164<br/> TEST SET DISPLAY STEP: Batch Loss = 0.059543, Accuracy = 0.9250085<br/> TEST SET DISPLAY STEP: Batch Loss = 0.059646, Accuracy = 0.9263658<br/> TEST SET DISPLAY STEP: Batch Loss = 0.062546, Accuracy = 0.92941976<br/> TEST SET DISPLAY STEP: Batch Loss = 0.061765, Accuracy = 0.9263658<br/> TEST SET DISPLAY STEP: Batch Loss = 0.063814, Accuracy = 0.9307771<br/> TEST SET DISPLAY STEP: Batch Loss = 0.062560, Accuracy = 0.9324737<br/> TEST SET DISPLAY STEP: Batch Loss = 0.061307, Accuracy = 0.93518835<br/> TEST SET DISPLAY STEP: Batch Loss = 0.061102, Accuracy = 0.93281305<br/> TEST SET DISPLAY STEP: Batch Loss = 0.054946, Accuracy = 0.9375636<br/> TEST SET DISPLAY STEP: Batch Loss = 0.054461, Accuracy = 0.9365456<br/> TEST SET DISPLAY STEP: Batch Loss = 0.050856, Accuracy = 0.9290804<br/> TEST SET DISPLAY STEP: Batch Loss = 0.050600, Accuracy = 0.9334917<br/> TEST SET DISPLAY STEP: Batch Loss = 0.057579, Accuracy = 0.9277231<br/> TEST SET DISPLAY STEP: Batch Loss = 0.062409, Accuracy = 0.9324737<br/> TEST SET DISPLAY STEP: Batch Loss = 0.050926, Accuracy = 0.9409569<br/> TEST SET DISPLAY STEP: Batch Loss = 0.054567, Accuracy = 0.94027823<br/> FINAL RESULT: Batch Loss= 0.0545671,<br/> Accuracy= 0.94027823</pre>
<p>Yahoo! We have managed to achieve 94% accuracy, which is really outstanding. In the previous code, <kbd>test()</kbd> is the method used for evaluating the performance of the model. The signature of the model is given in the following code:</p>
<pre><strong>def</strong> test(testDataCount: Int, batchSize: Int, testDatas: Array[Array[Array[Float]]], 
      testLabels: Array[Float], model: LSTMModel): (Float, Float) = { 
   <strong> var</strong> testLoss, testAcc = 0f 
    <strong>for</strong> (begin &lt;- 0 until testDataCount by batchSize) { 
     <strong> val</strong> (testData, testLabel, dropNum) = { 
        <strong>if</strong> (begin + batchSize &lt;= testDataCount) { 
          <strong>val</strong> datas = testDatas.drop(begin).take(batchSize) 
          <strong>val</strong> labels = testLabels.drop(begin).take(batchSize) 
          (datas, labels, 0) 
        } <strong>else</strong> { 
          <strong>val</strong> right = (begin + batchSize) - testDataCount 
          <strong>val</strong> left = testDataCount - begin 
          <strong>val</strong> datas = testDatas.drop(begin).take(left) ++ testDatas.take(right) 
          <strong>val</strong> labels = testLabels.drop(begin).take(left) ++ testLabels.take(right) 
          (datas, labels, right) 
        } 
      } 
      //feed the test data to the deepNN 
     <strong> model</strong>.data.set(testData.flatten.flatten) 
      <strong>model</strong>.label.set(testLabel) 
     
      <strong>model</strong>.exec.forward(isTrain = false) 
      <strong>val</strong> (acc, loss) = getAccAndLoss(model.exec.outputs(0), testLabel) 
      testLoss += loss 
      testAcc += acc 
    } 
    (testLoss / testDataCount, testAcc / testDataCount) 
  } </pre>
<p>When done, it's good practice to destroy the model to release resources:</p>
<pre>model.exec.dispose() </pre>
<p>We saw earlier that we achieved up to 93% accuracy on the test set. How about seeing the previous accuracy and errors in a graph:</p>
<pre>    // visualize 
    <strong>val</strong> xTrain = (0 until trainLosses.length * batchSize by batchSize).toArray.map(_.toDouble) 
   <strong> val</strong> yTrainL = trainLosses.toArray.map(_.toDouble) 
    <strong>val</strong> yTrainA = trainAccuracies.toArray.map(_.toDouble) <br/>    
    <strong>val</strong> xTest = (0 until testLosses.length * displayIter by displayIter).toArray.map(_.toDouble) 
    <strong>val</strong> yTestL = testLosses.toArray.map(_.toDouble) 
    <strong>val</strong> yTestA = testAccuracies.toArray.map(_.toDouble) <br/>    <strong>var</strong> series = new MemXYSeries(xTrain, yTrainL, "Train losses") 
    <strong>val</strong> data = new XYData(series)       
    series = new MemXYSeries(xTrain, yTrainA, "Train accuracies") 
    data += series <br/>    series = new MemXYSeries(xTest, yTestL, "Test losses") 
    data += series     
    series = new MemXYSeries(xTest, yTestA, "Test accuracies") 
    data += series <br/>   <strong> val</strong> chart = new XYChart("Training session's progress over iterations!", data) 
    chart.showLegend = true 
    <strong>val</strong> plotter = new JFGraphPlotter(chart)<br/>    plotter.gui() <br/>&gt;&gt;&gt;</pre>
<div class="CDPAlignCenter CDPAlign"><img height="376" width="609" src="assets/60e98ce8-9c88-4c7d-9682-ec49100b8391.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 17: Training and test losses and accuracies per iteration</div>
<p>From the preceding graph, it is clear that with only a few iterations, our LSTM converged well and produced very good classification accuracy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tuning LSTM hyperparameters and GRU</h1>
                </header>
            
            <article>
                
<p>Nevertheless, I still believe it is possible to attain about 100% accuracy with more LSTM layers. The following are the hyperparameters that I would still try to tune to see the accuracy:</p>
<pre>// Hyper parameters for the LSTM training<br/><strong>val</strong> learningRate = 0.001f<br/><strong>val</strong> trainingIters = trainingDataCount * 1000 // Loop 1000 times on the dataset<br/><strong>val</strong> batchSize = 1500 // I would set it 5000 and see the performance<br/><strong>val</strong> displayIter = 15000 // To show test set accuracy during training<br/><strong>val</strong> numLstmLayer = 3 // 5, 7, 9 etc.</pre>
<p>There are many other variants of the LSTM cell. One particularly popular variant is the <strong>Gated Recurrent Unit</strong> (<strong>GRU</strong>) cell, which is a slightly dramatic variation on the LSTM. It also merges the cell state and hidden state and makes some other changes. The resulting model is simpler than standard LSTM models and has been growing increasingly popular. This cell was proposed by Kyunghyun Cho et al. in a 2014 paper that also introduced the encoder-decoder network we mentioned earlier.</p>
<div class="packt_tip">For this type of LSTM, interested readers should refer to the following publications:<br/>
<ul>
<li><em>Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</em>, K. Cho et al. (2014).</li>
<li>A 2015 paper by Klaus Greff et al., <em>LSTM: A Search Space Odyssey</em><span>, seems to show that all LSTM variants perform roughly the same.</span></li>
</ul>
</div>
<p>Technically, a GRU cell is a simplified version of an LSTM cell, where both the state vectors are merged into a single vector called <strong>h(t)</strong>. A single gate controller controls both the forget gate and the input gate. If the gate controller outputs a 1, the input gate is open and the forget gate is closed:</p>
<div class="CDPAlignCenter CDPAlign"><img height="257" width="545" src="assets/30895ce6-726f-4424-b456-f5c2ef15f40f.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 18: Internal structure of a GRU cell</div>
<p>On the other hand, if it outputs a 0, the opposite happens. Whenever a memory must be stored, the location where it will be stored is erased first, which is actually a frequent variant to the LSTM cell in and of itself. The second simplification is that since the full state vector is output at every time step, there is no output gate. However, there is a new gate controller introduced that controls which part of the previous state will be shown to the main layer. The following equations are used to do the GRU computations of a cell's long-term state, its short-term state, and its output at each time step for a single instance:</p>
<div class="CDPAlignCenter CDPAlign"><img height="138" width="294" src="assets/e585549b-3e79-453e-a74b-3e1c044ce67c.png"/></div>
<p>The LSTM and GRU cells are one of the main reasons for the success of RNNs in recent years, in particular for applications in NLP.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have seen how to develop an ML project using the RNN implementation, and called LSTM for HAR using the smartphones dataset. Our LSTM model has been able to classify the type of movement from six categories: walking, walking upstairs, walking downstairs, sitting, standing, and lying. In particular, we have achieved up to 94% accuracy. Later on, we discussed some possible ways to improve the accuracy further using GRU cell.</p>
<p>A <strong>convolutional neural network</strong> (<strong>CNN</strong>) is a type of feedforward neural network in which the connectivity pattern between its neurons is inspired by the animal visual cortex. Over the last few years, CNNs have demonstrated superhuman performance in complex visual tasks such as image search services, self-driving cars, automatic video classification, voice recognition, and <strong>natural language processing</strong> (<strong>NLP</strong>).</p>
<p>Considering these, in the next chapter we will see how to develop an end-to-end project for handling a multi-label (that is, each entity can belong to multiple classes) image classification problem using CNN based on the Scala and Deeplearning4j framework on real Yelp image datasets. We will also discuss some theoretical aspects of CNNs before getting started. Furthermore, we will discuss how to tune hyperparameters for better classification results.</p>


            </article>

            
        </section>
    </body></html>