- en: Chapter 7. Miscellaneous Deep Learning Operations using Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '|   | *"In pioneer days they used oxen for heavy pulling, and when one ox couldn''t
    budge a log, they didn''t try to grow a larger ox. We shouldn''t be trying for
    bigger computers, but for more systems of computers."* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   | --*Grace Hopper* |'
  prefs: []
  type: TYPE_TB
- en: So far in this book, we discussed various deep neural network models and their
    concepts, applications, and implementation of the models in distributed environments.
    We have also explained why it is difficult for a centralized computer to store
    and process vast amounts of data and extract information using these models. Hadoop
    has been used to overcome the limitations caused by large-scale data.
  prefs: []
  type: TYPE_NORMAL
- en: As we have now reached the final chapter of this book, we will mainly discuss
    the design of the three most commonly used machine learning applications. We will
    explain the general concept of large-scale video processing, large-scale image
    processing, and natural language processing using the Hadoop framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'The organization of this chapter is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Large-scale distributed video processing using Hadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large-scale image processing using Hadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Natural language processing using Hadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The large amount of videos available in the digital world are contributing to
    the lion's share of the big data generated in recent days. In [Chapter 2](ch02.html
    "Chapter 2.  Distributed Deep Learning for Large-Scale Data") , *Distributed Deep
    Learning for Large-Scale Data* we discussed how millions of videos are uploaded
    to various social media websites such as YouTube and Facebook. Apart from this,
    surveillance cameras installed for security purposes in various shopping malls,
    airports, or government organizations generate loads of videos on a daily basis.
    Most of these videos are typically stored as compressed video files due to their
    huge storage consumption. In most of these enterprises, the security cameras operate
    for the whole day and later store the important videos, to be investigated in
    future.
  prefs: []
  type: TYPE_NORMAL
- en: These videos contain hidden "hot data" or information, which needs to be processed
    and extracted quickly. As a consequence, the need to process and analyze these
    large-scale videos has become one of the priorities for data enthusiasts. Also,
    in many different fields of studies, such as bio-medical engineering, geology,
    and educational research, there is a need to process these large-scale videos
    and make them available at different locations for detailed analysis.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will look into the processing of large-scale video datasets
    using the Hadoop framework. The primary challenge of large-scale video processing
    is to transcode the videos from compressed to uncompressed format. For this reason,
    we need a distributed video transcoder that will write the video in the **Hadoop
    Distributed File System** (**HDFS**), decode the bit stream chunks in parallel,
    and generate a sequence file.
  prefs: []
  type: TYPE_NORMAL
- en: When a block of the input data is processed in the HDFS, each mapper process
    accesses the lines in each split separately. However, in case of a large-scale
    video dataset, when it is split into multiple blocks of predefined sizes, each
    mapper process is supposed to interpret the blocks of bit-stream separately. The
    mapper process will then provide access to the decoded video frames for subsequent
    analysis. In the following subsections, we will discuss how each block of the
    HDFS containing the video bit-stream can be transcoded into sets of images to
    be processed for the further analyses.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed video decoding in Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most of the popular video compression formats, such as MPEG-2 and MPEG-4, follow
    a hierarchical structure in the bit-stream. In this subsection, we will assume
    that the compression format used has a hierarchical structure for its bit-stream.
    For simplicity, we have divided the decoding task into two different Map-reduce
    jobs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extraction of video sequence level information**: From the outset, it can
    be easily predicted that the header information of all the video dataset can be
    found in the first block of the dataset. In this phase, the aim of the map-reduce
    job is to collect the sequence level information from the first block of the video
    dataset and output the result as a text file in the HDFS. The sequence header
    information is needed to set the format for the decoder object.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the video files, a new `FileInputFormat` should be implemented with its
    own record reader. Each record reader will then provide a `<key, value>` pair
    in this format to each map process: `<LongWritable, BytesWritable>`. The input
    key denotes the byte offset within the file; the value that corresponds to `BytesWritable`
    is a byte array containing the video bit-stream for the whole block of data.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For each map process, the key value is compared with `0` to identify if it is
    the first block of the video file. Once the first block is identified, the bit-stream
    is parsed to determine the sequence level information. This information is then
    dumped to a `.txt` file to be written to  HDFS. Let's denote the name of the `.txt`
    file as `input_filename_sequence_level_header_information.txt`. As only the map
    process can provide us the desired output, the reducer count for this method is
    set to `0`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Assume a text file with the following data: **Deep Learning** **with Hadoop**
    Now the offset for the first line is `0` and the input to the Hadoop job will
    be `<0,Deep Learning>` and for the second line the offset will be `<14,with Hadoop>`.
    Whenever we pass the text file to the Hadoop job, it internally calculates the
    byte offset.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Decode and convert the blocks of videos into sequence files**: The aim of
    this Map-reduce job is to decode  each block  of the video datasets and generate
    a corresponding sequence file. The sequence file will contain the decoded video
    frames of each block of data in JPEG format. The `InputFileFormat` file and record
    reader should be kept same as the first Map-reduce job. Therefore, the `<key,
    value>` pairs of the mapper input is `<LongWritable, BytesWritable>`.![Distributed
    video decoding in Hadoop](img/B05883_07_01-1.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 7.1: The overall representation of video decoding with Hadoop'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this second phase, the output of the first job is considered as the input
    to this second Map-reduce job. Therefore, each mapper of this job will read the
    sequence information file in the HDFS and pass this information along with the
    bit-stream buffer, which comes as the `BytesWritable` input.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The map process basically converts the decoded video frames to JPEG images and
    generates a `<key, value>` pair as the output of the map process. The key of this
    output of the map process encodes the input video filename and the block number
    as `video_filename_block_number`. The output value that corresponds to this key
    is `BytesWritable`, and it stores the JPEG bit-stream of the decoded video block.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The reducers will then take the blocks of data as input and simply write the
    decoded frames into a sequence file containing JPEG images as output format for
    further processing. A simple format and overview of the whole process is shown
    in *Figure 7.1*. We have taken an input video `sample.m2v` for illustration purposes.
    Further, in this chapter, we will discuss how to process the large-scale image
    files (from the sequence files) with the HDFS.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Input `<key,value>` for Mapper: `<LongWritable, BytesWritable>`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For example: `<17308965, BytesWritable>` Output `<key,value>` from Mapper:
    `<Text, BytesWritable>` For example: `<sample.m2v_3, BytesWritable>`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Large-scale image processing using Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have already mentioned in the earlier chapters how the size and volume of
    images are increasing day by day; the need to store and process these vast amount
    of images is difficult for centralized computers. Let''s consider an example to
    get a practical idea of such situations. Let''s take a large-scale image of size
    81025 pixels by 86273 pixels. Each pixel is composed of three values:red, green,
    and blue. Consider that, to store each of these values, a 32-bit precision floating
    point number is required. Therefore, the total memory consumption of that image
    can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*86273 * 81025 * 3 * 32 bits = 78.12 GB*'
  prefs: []
  type: TYPE_NORMAL
- en: Leave aside doing any post processing on this image, as it can be clearly concluded
    that it is impossible for a traditional computer to even store this amount of
    data in its main memory. Even though some advanced computers come with higher
    configurations, given the return on investment, most companies do not opt for
    these computers as they are much too expensive to be acquired and maintained.
    Therefore, the proper solution should be to run the images in commodity hardware
    so that the images can be stored in their memory. In this section, we will explain
    the use of Hadoop to process these vast amounts of images in a distributed manner.
  prefs: []
  type: TYPE_NORMAL
- en: Application of Map-Reduce jobs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will discuss how to process large image files using Map-reduce
    jobs with Hadoop. Before the job starts, all the input images to be processed
    are loaded to the HDFS. During the operation, the client sends a job request,
    which goes through NameNode. NameNode collects that request from the client, searches
    its metadata mapping, and then sends the data block information of the filesystem
    as well as location of the data block back to the client. Once the client gets
    the block's metadata, it automatically accesses the DataNodes, where the requested
    data block resides, then processes this data via the applicable commands.
  prefs: []
  type: TYPE_NORMAL
- en: The Map-reduce jobs used for large-scale image processing are primarily responsible
    for controlling the whole task. Basically, here we explain the concept of an executable
    shell script file, which is responsible for collecting the executable file's input
    data from the HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: The best way to use the Map-reduce programming model is to design our own Hadoop
    data types for processing large numbers of image files directly. The system will
    use Hadoop Streaming technology, which helps the users to create and run special
    kinds of Map-reduce jobs. These special kinds of jobs can be performed through
    an executable file mentioned earlier, which will act as a mapper or reducer. The
    mapper implementation of the program will use a shell script to perform the necessary
    operation. The shell script is responsible for calling the executable files of
    the image processing. The lists of image files are taken as the input to these
    executable files for further processing. The results of this processing or output
    are later written back to the HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: So, the input image files should be written to the HDFS first, and then a file
    list is generated in a particular directory of Hadoop Streaming's input. The directory
    will store a collection of file lists. Each line of the file list will contain
    the HDFS address of the images files to be processed. The input of the mapper
    will be `Inputsplit` class, which is a text file. The shell script manager reads
    the files line by line and retrieves the images from the metadata. It then calls
    the image processing executable file for further processing of the images, and
    then write the result back to the HDFS. Hence, the output of the mapper is the
    final desired result. The mapper thus does all the jobs, retrieving the image
    file from the HDFS, image processing, and then writing it back to the HDFS. The
    number of reducers in this process can be set to zero.
  prefs: []
  type: TYPE_NORMAL
- en: This is a simple design of how to process large numbers of images using Hadoop
    by the binary image processing method. Other complex image processing methods
    can also be deployed to process large-scale image datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Natural language processing using Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The exponential growth of information in the Web has increased the intensity
    of diffusion of large-scale unstructured natural language textual resources. Hence,
    in the last few years, the interest to extract, process, and share this information
    has increased substantially. Processing these sources of knowledge within a stipulated
    time frame has turned out to be a major challenge for various research and commercial
    industries. In this section, we will describe the process used to crawl the web
    documents, discover the information and run natural language processing in a distributed
    manner using Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: To design architecture for **natural language processing** (**NLP**), the first
    task to be performed is the extraction of annotated keywords and key phrases from
    the large-scale unstructured data. To perform the NLP on a distributed architecture,
    the Apache Hadoop framework can be chosen for its efficient and scalable solution,
    and also to improve the failure handling and data integrity. The large-scale web
    crawler can be set to extract all the unstructured data from the Web and write
    it in the Hadoop Distributed File System for further processing. To perform the
    particular NLP tasks, we can use the open source GATE application as shown in
    the paper [136]. An overview of the tentative design of a distributed natural
    language processing architecture is shown in *Figure 7.2*.
  prefs: []
  type: TYPE_NORMAL
- en: To distribute the working of the web crawler, map-reduce can be used and run
    across multiple nodes. The execution of the NLP tasks and also the writing of
    the final output is performed with Map-reduce. The whole architecture will depend
    on two input files i) the `seedurls` given for crawling a particular web page
    stored in `seed_urls.txt` and ii) the path location of the NLP application (such
    as where GATE is installed). The web crawler will take `seedurls` from the `.txt`
    file and run the crawler for those in parallel. Asynchronously, an extraction
    plugin searches the keywords and key phrases on the crawled web pages and executes
    independently along with the web pages crawled. At the last step, a dedicated
    program stores the extracted keywords and key phrases in an external SQL database
    or a NoSQL database such as `Elasticsearch`, as per the requirements. All these
    modules mentioned in the architecture are described in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Web crawler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To explain this phase, we won't go into a deep explanation, as it's almost out
    of the scope of this book. Web crawling has a few different phases. The first
    phase is the URL discovery stage, where the process takes each seed URL as the
    input of the `seed_urls.txt` file and navigates through the pagination URLs to
    discover relevant URLs. This phase defines the set of URLs that are going to be
    fetched in the next phase.
  prefs: []
  type: TYPE_NORMAL
- en: The next phase is fetching the page content of the URLs and saving in the disk.
    The operation is done segment-wise, where each segment will contain some predefined
    numbers of URLs. The operation will run in parallel on different `DataNodes`.
    The final outcome of the phases is stored in the Hadoop Distributed File System.
    The Keyword extractor will work on these saved page contents for the next phase.
  prefs: []
  type: TYPE_NORMAL
- en: '![Web crawler](img/B05883_07_02-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: The representation of  how natural language processing is performed
    in Hadoop that is going to be fetched in the next phase. The next phase is fetching
    the page content of the URLs and saving in the disk. The operation is done segment
    wise, where each segment will contain some pre-defined numbers of URLs. The operation
    will run in parallel on different DataNodes. The final outcome of the phases is stored
    in Hadoop Distributed File System. The keyword extractor will work on these saved
    page contents for the next phase.'
  prefs: []
  type: TYPE_NORMAL
- en: Extraction of keyword and module for natural language processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the page content of each URL, a **Document Object Model** (**DOM**) is created
    and stored back in the HDFS. In the *DOM*, documents have a logical structure
    like a tree. Using DOM, one can write the `xpath` to collect the required keywords
    and phrases in the natural language processing phase. In this module, we will
    define the Map-reduce job for executing the natural language processing application
    for the next phase. The map function defined as a `<key, value>` pair key is the
    URL, and values are a corresponding DOM of the URL. The *reduce* function will
    perform the configuration and execution of the natural language processing part.
    The subsequent estimation of the extracted keywords and phrases at the web domain
    level will be performed in the `reduce` method. For this purpose, we can write
    a custom plugin to generate the rule files to perform various string manipulations
    to filter out the noisy, undesired words from the extracted texts. The rule files
    can be a JSON file or any other easy to load and interpret file based on the use
    case. Preferably, the common nouns and adjectives are identified as common keywords
    from the texts.
  prefs: []
  type: TYPE_NORMAL
- en: Estimation of relevant keywords from a page
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The paper [136] has presented a very important formulation to find the relevant
    keywords and key phrases from a web document. They have provided the **Term Frequency
    - Inverse Document Frequency** (**TF-IDF**) metric to estimate the relevant information
    from the whole corpus, composed of all the documents and pages that belong to
    a single web domain. Computing the value of *TD-IDF* and assigning it a threshold
    value for discarding other keywords allows us to generate the most relevant words
    from the corpus. In other words, it discards the common articles and conjunctions
    that might have a high frequency of occurrence in the text, but generally do not
    possess any meaningful information. The *TF-IDF* metric is basically the product
    of two functions, *TF* and *IDF*.
  prefs: []
  type: TYPE_NORMAL
- en: '*TF* provides the frequency of each word in the corpus, that is, how many times
    a word is present in the corpus. Whereas *IDF* behaves as a balance term, showing
    higher values for terms having the lower frequency in the whole corpus.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the metric *TF-IDF* for a keyword or key phrase *i* in a document
    *d* contained in the document *D* is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(TF-IDF)[i] = TF[i] . IDF[i]*'
  prefs: []
  type: TYPE_NORMAL
- en: Here *TF[i] = f[i]/n[d]* and *IDF[i] = log N[d]/N[i]*
  prefs: []
  type: TYPE_NORMAL
- en: Here *f[i]* is the frequency of the candidate keyword or key phrase *i* in the
    document *d* and *n[d]* is the total number of terms in the document *d*. In *IDF*,
    *N[D] *denotes the total number of documents present in the corpus *D*, whereas
    *N[i]* denotes the number of documents in which the keyword or key phrase *i*
    is present.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the use cases, one should define a generic threshold frequency for
    *TF-IDF*. For a keyword or key phrase *i* if the value of *TF-IDF* becomes higher
    than the threshold value, that keyword or key phrase is accepted as final as written
    directly to the HDFS. On the other hand, if the corresponding value is less than
    the threshold value, that keyword is dropped from the final collection. In that
    way, finally, all the desired keywords will be written to the HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter discussed the most widely used applications of Machine learning
    and how they can be designed in the Hadoop framework. First, we started with a
    large video set and showed how the video can be decoded in the HDFS and later
    converted into a sequence file containing images for later processing. Large-scale
    image processing was discussed next in the chapter. The mapper used for this purpose
    has a shell script which performs all the tasks necessary. So, no reducer is necessary
    to perform this operation. Finally, we discussed how the natural language processing
    model can be deployed in Hadoop.
  prefs: []
  type: TYPE_NORMAL
