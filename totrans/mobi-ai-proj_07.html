<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">TensorFlow on Mobile with Speech-to-Text with the WaveNet Model</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we are going to learn how to convert audio to text using the WaveNet model. We will then build a model that will take audio and convert it into text using an Android application. </p>
<div class="mce-root packt_infobox">This chapter is based on the <em>WaveNet: A Generative Model for Raw Audio</em> paper, by Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. You can find this paper at <a href="https://arxiv.org/abs/1609.03499" target="_blank">https://arxiv.org/abs/1609.03499</a>.</div>
<p class="mce-root">In this chapter, we will cover the following topics:</p>
<ul>
<li>WaveNet and how it works</li>
<li>The WaveNet architecture</li>
<li>Building a model using WaveNet</li>
<li>Preprocessing datasets</li>
<li>Training the WaveNet network</li>
<li>Transforming a speech WAV file into English text</li>
<li>Building an Android application</li>
</ul>
<p class="mce-root">Let's dig deeper into what Wavenet actually is.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">WaveNet</h1>
                </header>
            
            <article>
                
<p>WaveNet is a deep generative network that is used to generate raw audio waveforms. Sounds waves are generated by WaveNet to mimic the human voice. This generated sound is more natural than any of the<span> currently existing text-to-speech systems, reducing the gap between system and human performance by 50%.</span></p>
<p>With a single WaveNet, we can differentiate between multiple speakers with equal fidelity. We can also switch between individual speakers based on their identity. This model is autoregressive and probabilistic, and it can be trained efficiently on thousands of audio samples per second. <span>A single WaveNet can capture the characteristics of many different </span><span>speakers with equal fidelity, and can switch between them by conditioning the </span><span>speaker identity. </span></p>
<p>As shown in the movie <em>Her</em>, the long-standing dream of human-computer interaction is to allow people to talk to machines. The computer's ability to understand voices has increased tremendously over the past few years as a result of deep neural networks (for example, Google Assistant, Siri, Alexa, and Cortana). On the other hand, to generate speech with computers, a process referred to as speech synthesis or text to speech is followed. In the <span>text-to-speech</span> method, a large database of short sound fragments are recorded by a single speaker and then combined to form the required utterances. This process is very difficult because we can't change the speaker.</p>
<p class="mce-root">This difficulty has led to a great need for other methods of generating speech, where all the information <span>that is needed for generating the data is stored in the parameters of the model. Additionally, </span><span>using the inputs that are given to the model,</span><span> we can control the contents and various attributes of speech. When speech is generated by adding sound fragments together, attribution graphs are generated.</span></p>
<p class="mce-root"><span>The following is the attribution graph of speech that is generated in <strong>1 second</strong>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-594 image-border" src="assets/6b3c68bc-06ff-4998-9a13-f5d1541ce051.png" style="width:25.33em;height:12.67em;"/></p>
<p><span>The following is the attribution graph of speech that is generated in </span><strong>100 milliseconds</strong>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-596 image-border" src="assets/aa119a92-08a6-4354-820c-5b7a2b351b3e.png" style="width:25.50em;height:12.50em;"/></p>
<p><span>The following is the attribution graph of speech that is generated in </span><strong>10 milliseconds</strong>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-597 image-border" src="assets/c40a3332-995c-4e3b-904f-0e5108ee9f50.png" style="width:25.17em;height:13.58em;"/></p>
<p><span>The following is the attribution graph of speech that is generated in <strong>1 millisecond</strong>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-598 image-border" src="assets/81b3eadd-6104-4a28-8e83-631eed845c03.png" style="width:25.58em;height:14.42em;"/></p>
<p>The <strong>Pixel Recurrent Neural Network</strong> (<strong>PixelRNN</strong>) and <strong>Pixel C</strong><strong>onvolutional Neural Network</strong> (<strong>PixelCNN</strong>) models from Google ensure that it's possible to generate images that include complex formations – not by generating one pixel at a time, but by an entire color channel altogether. At any one time, a color channel will need at least a thousand predictions per image. This way, we can alter a two-dimensional PixelNet into a one-dimensional WaveNet; this idea is shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-588 image-border" src="assets/d8f342fa-40b2-47a9-bb24-c3764be2bacd.png" style="width:36.25em;height:15.33em;"/></p>
<p>The preceding diagram displays the structure of a WaveNet model. WaveNet is a full CNN, in which the convolutional layers include a variety of dilation factors. These factors help the <span>receptive field of WaveNet to grow exponentially with depth, and it also helps to cover thousands of time steps.</span></p>
<p>During training, the human speaker records the input sequences to create waveforms. Once the training is complete, we generate synthetic utterances by sampling the network. A value is taken from the probability distribution which is computed by the network at each step of sampling. The value that's received is fed as the input for the next step, and then a new prediction is made. Building these samples at each step is expensive; however, it's necessary to generate complex and realistic-sounding audio.</p>
<div class="packt_quote packt_tip packt_infobox">More information about PixelRNN can be found at <a href="https://arxiv.org/pdf/1601.06759.pdf" target="_blank">https://arxiv.org/pdf/1601.06759.pdf</a>, while information about <em>Conditional Image Generation with PixelCNN Decoders</em> can be found at <a href="https://arxiv.org/pdf/1606.05328.pdf" target="_blank">https://arxiv.org/pdf/1606.05328.pdf</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Architecture</h1>
                </header>
            
            <article>
                
<p>The architecture of WaveNet neural networks shows amazing outputs by generating audio and text-to-speech translations, since it directly produces a raw audio waveform.</p>
<p class="mce-root">When the previous samples and additional parameters are given as the input, the network produces the next sample in the form of an audio waveform using conditional probability.</p>
<p class="mce-root">The waveform that is given as the input is quantized to a fixed range of integers. This happens after the audio is preprocessed. The tensors are produced by one-hot encoding these integer amplitudes. Hence, the dimensions of the channel are reduced by the convolutional layer that only accesses the current and previous inputs.</p>
<p class="mce-root">The following diagram displays the WaveNet architecture:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-599 image-border" src="assets/bf310496-903e-4995-8f65-fb92e47535cc.png" style="width:17.67em;height:41.58em;"/></p>
<p>A stack of causal dilated layers is used to build the network core. Each layer is a dilated convolution with holes, and it accesses only the past and current audio samples.</p>
<p class="mce-root">Then, the outputs that are received from all the layers are combined and, using an array of dense postprocessing layers, they are fed to the original channels. Later, the softmax function converts the output into a categorical distribution.</p>
<p class="mce-root">The loss function is calculated as the cross entropy between the output for each time step and the input at the next time step.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Network layers in WaveNet</h1>
                </header>
            
            <article>
                
<p>Here, we will focus on generating dilated causal convolution network layers with the filter size of two. Note that these ideas are relevant to larger filter sizes.</p>
<p>During this generation, the computational graph that's used to compute a single output value can be seen as a binary tree:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-589 image-border" src="assets/0bf29096-a2fc-4312-94bf-6ff9d81ebe45.png" style="width:37.33em;height:18.17em;"/></p>
<p><span>The <strong>Input</strong> nodes on the bottom layer of the diagram are the leaves of the tree, while the <strong>Output</strong> layer is the root. The intermediate computations are represented by the nodes above the <strong>Input</strong> layer. The edges of the graph correspond to multiple matrices. Since the computation is a binary tree, the overall computation time for the graph is <em>O(2^L)</em>. When <em>L</em> is large, the computation exponentially shoots up.</span></p>
<p>However, since this model is being applied repeatedly over time, there is a lot of redundant computation, which we can cache to increase the speed of generating a single sample.</p>
<p>The key insight is this – given certain nodes in the graph, we have all the information that we need to compute the current output. We call these nodes <strong>recurrent states</strong><span> by </span>using the analogy of RNNs. These nodes have already been computed, so all we need to do is cache them on the different layers, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-591 image-border" src="assets/b45b4536-8ba4-4a1d-a815-f6dfcca66d84.png" style="width:26.25em;height:16.75em;"/></p>
<p>Note that at the next time point, we will need a different subset of recurrent states. As a result, we will need to cache several recurrent states per layer. The number we need to keep is equal to the<span> </span>dilation<span> </span>of that layer, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-592 image-border" src="assets/86c05505-5b7f-42e1-9818-403ede26646a.png" style="width:29.25em;height:19.17em;"/></p>
<p>As shown in the preceding diagram with arrow marks, the number of recurrent states is the same as the dilation value in the layer.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The algorithm's components</h1>
                </header>
            
            <article>
                
<p>The algorithm behind building a speech detector has two components:</p>
<ul>
<li><strong>The generation model</strong></li>
<li><strong>The convolution queues</strong></li>
</ul>
<p>These two components are shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-593 image-border" src="assets/3654f9ba-3c2d-4ff7-8c5a-652e55f1b401.png" style="width:22.83em;height:21.50em;"/></p>
<p>The generation model can be viewed as one step of an RNN. It takes the current observation and several recurrent states as input, and then computes the output prediction and new recurrent states.</p>
<p>The convolution queues store the new recurrent states that have been computed by the layer underneath it.</p>
<p>Let's jump into building the model.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the model</h1>
                </header>
            
            <article>
                
<p>We will implement sentence-level English speech recognition using DeepMind's WaveNet. However, we need to consider a number of data points before building the model:</p>
<ul>
<li>First, while the paper on WaveNet (provided at the beginning of this chapter) used the TIMIT dataset for the speech recognition experiment, we will use the free VCTK dataset instead.</li>
<li class="mce-root">Second, the paper added a mean pooling layer after the dilated convolution layer for downsampling. We have extracted <strong>m<span>el-frequency cepstral coefficients</span></strong> (<strong>MFCC</strong>) from the <kbd>.wav</kbd> files and removed the final mean pooling layer because the original setting is impossible to run on our TitanX <strong>G<span>raphics Processing Unit</span></strong> <strong>GPU</strong>).</li>
<li class="mce-root">Third, since the TIMIT dataset has phoneme labels, the paper trained the model with two loss terms: <strong>phoneme classification</strong> and <strong>next phoneme prediction</strong>. Instead, we will use a single <strong>connectionist temporal classification</strong> (<strong>CTC</strong>) loss because VCTK provides sentence-level labels. As a result, we only use dilated Conv1D layers without any dilated Conv1D layers.</li>
<li class="mce-root">Finally, we won't do quantitative analyses, such as the <strong>bilingual evaluation understudy score</strong> (<strong>BLEU</strong>) score and postprocessing by combining a language model, due to time constraints.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dependencies </h1>
                </header>
            
            <article>
                
<p>Here is a list of all the dependency libraries that will need to be installed first: </p>
<ul>
<li><kbd>tensorflow</kbd><span> </span></li>
<li><kbd>sugartensor</kbd><span> </span></li>
<li><kbd>pandas</kbd><span> </span></li>
<li><kbd>librosa</kbd><span> </span></li>
<li><kbd>scikits.audiolab</kbd></li>
</ul>
<p>If you have problems with the <kbd>librosa</kbd> library, you can try installing <kbd>ffmpeg</kbd> using <kbd>pip</kbd>. </p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Datasets</h1>
                </header>
            
            <article>
                
<p class="mce-root">We used the<span> </span>VCTK,<span> </span>LibriSpeech,<span> </span>and<span> </span>TED-LIUM release 2<span> </span>datasets. The total number of sentences in the training set are composed of the previous three datasets, which equals 240,612 sentences. The validation and test sets are built using only LibriSpeech and the TED-LIUM corpus, because the VCTK corpus does not have validation and test sets. After downloading each corpus, extract them in the <kbd>asset/data/VCTK-Corpus</kbd>, <kbd>asset/data/LibriSpeech</kbd>, and <kbd>asset/data/TEDLIUM_release2</kbd> directories.</p>
<div class="mce-root packt_infobox">You can find the links to these datasets here:<span><em><br/>
CSTR VCTK Corpus:</em> <a href="http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html" target="_blank">http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html<br/></a></span><span><em>LibriSpeech ASR corpus:</em> <a href="http://www.openslr.org/12/" target="_blank">http://www.openslr.org/12</a></span><span><em><br/>
TED-LIUM:</em> <a href="https://lium.univ-lemans.fr/en/ted-lium2/">http://www-lium.univ-lemans.fr/en/content/ted-lium-corpus</a></span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preprocessing the dataset</h1>
                </header>
            
            <article>
                
<p>The TED-LIUM release 2 dataset provides audio data in the SPH format, so we should convert it into a format that the <kbd>librosa</kbd> library can handle. To do this, run the following command in the <kbd>asset/data</kbd> directory to convert the SPH format into the WAV format:</p>
<pre><strong>find -type f -name '*.sph' | awk '{printf "sox -t sph %s -b 16 -t wav %s\n", $0, $0".wav" }' | bash</strong></pre>
<p>If you don't have <kbd>sox</kbd><span> installed,<em> </em></span>please install it first.</p>
<p>We found that the main bottleneck is the disk read time when training because of the size of the audio files. It is better to have smaller audio files before processing for faster execution. So, we have decided to preprocess the whole audio data into the MFCC feature files, which are much smaller. Additionally, we highly recommend using a <strong>solid-state drive</strong> (<strong>SSD</strong>) instead of a hard drive.</p>
<p>Run the following command in the console to preprocess the whole dataset:</p>
<pre><strong><span>python preprocess.py</span></strong></pre>
<p>With the processed audio files, we can now train the network.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the network</h1>
                </header>
            
            <article>
                
<p>We will start training the network by executing the following command:</p>
<pre><strong>python train.py ( &lt;== Use all available GPUs )</strong></pre>
<p>If you are using a machine with CUDA enabled, use the following command:</p>
<pre><strong>CUDA_VISIBLE_DEVICES=0,1 python train.py ( &lt;== Use only GPU 0, 1 )</strong></pre>
<p>You can see the resulting <kbd>.ckpt</kbd> files and log files in the <kbd>asset/train</kbd> directory. Launch <kbd>tensorboard--logdir asset/train/log</kbd> to monitor the training process.</p>
<p>We've trained this model on a 3 Nvidia 1080 Pascal GPU for 40 hours until 50 epochs were reached, and then we picked the epoch when the validation loss is at a minimum. In our case, it is epoch 40. If you can see the out-of-memory error, reduce <kbd>batch_size</kbd> in the <kbd>train.py</kbd> file from 16 to 4.</p>
<p>The CTC losses at each epoch are as follows:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td><strong>Epoch</strong></td>
<td><strong>Train set</strong></td>
<td><strong>Valid set</strong></td>
<td><strong>Test set</strong></td>
</tr>
<tr>
<td><span>20</span></td>
<td>79.541500</td>
<td>73.645237</td>
<td>83.607269</td>
</tr>
<tr>
<td>30</td>
<td>72.884180</td>
<td>69.738348</td>
<td>80.145867</td>
</tr>
<tr>
<td>40</td>
<td>69.948266</td>
<td>66.834316</td>
<td>77.316114</td>
</tr>
<tr>
<td>50</td>
<td>69.127240</td>
<td>67.639895</td>
<td>77.866674</td>
</tr>
</tbody>
</table>
<p><br/>
Here, you can see the difference between the values from the training dataset and the testing dataset. The difference is largely due to the bigger volume of data in the training dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing the network</h1>
                </header>
            
            <article>
                
<p>After training the network, you can check the validation or test set CTC loss by using the following command:</p>
<pre>python test.py --set train|valid|test --frac 1.0(0.01~1.0)</pre>
<p>The <kbd>frac</kbd> option will be useful if you want to test only a fraction of the dataset for fast evaluation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transforming a speech WAV file into English text</h1>
                </header>
            
            <article>
                
<p>Next, you can convert the speech WAV file into English text by executing the following command:</p>
<pre>python recognize.py --file </pre>
<p>This will transform a speech WAV file into an English sentence.</p>
<p>The result will be printed on the console; try the following command as an example:</p>
<pre><br/>python recognize.py --file asset/data/LibriSpeech/test-clean/1089/134686/1089-134686-0000.flac<br/>python recognize.py --file asset/data/LibriSpeech/test-clean/1089/134686/1089-134686-0001.flac<br/>python recognize.py --file asset/data/LibriSpeech/test-clean/1089/134686/1089-134686-0002.flac<br/>python recognize.py --file asset/data/LibriSpeech/test-clean/1089/134686/1089-134686-0003.flac<br/>python recognize.py --file asset/data/LibriSpeech/test-clean/1089/134686/1089-134686-0004.flac</pre>
<p>The result will be as follows:</p>
<p style="padding-left: 30px"><em>he hoped there would be stoo for dinner turnips and charrats and bruzed patatos and fat mutton pieces to be ladled out in th thick peppered flower fatan sauce stuffid into you his belly counsiled him after early night fall the yetl lampse woich light hop here and there on the squalled quarter of the browfles o berty and he god in your mind numbrt tan fresh nalli is waiting on nou cold nit husband</em></p>
<p>The ground truth is as follows:</p>
<p style="padding-left: 30px"><em>HE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOUR FATTENED SAUCE STUFF IT INTO YOU HIS BELLY COUNSELLED HIM AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS HELLO BERTIE ANY GOOD IN YOUR MIND NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND</em></p>
<p>As we mentioned earlier, there is no language model, so there are some cases where capital letters and punctuation are misused, or words are misspelled.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting the model</h1>
                </header>
            
            <article>
                
<p>Unlike image problems, it's not easy to find a pretrained deep learning model for speech-to-text that gives out checkpoints. Luckily, I found the following WaveNet speech-to-text implementation. To export the model for compression, I ran the Docker image, loaded the checkpoint, and wrote it into a protocol buffers file. To run this, use the following command:</p>
<pre class="mce-root">python export_wave_pb.py</pre>
<p class="mce-root">We will build the graph for inference, load the checkpoint, and write it into a protocol buffer file, as follows: </p>
<pre>batch_size = 1 # batch size<br/>voca_size = data.voca_size<br/>x = tf.placeholder(dtype=tf.sg_floatx, shape=(batch_size, None, 20))<br/># sequence length except zero-padding<br/>seq_len = tf.not_equal(x.sg_sum(axis=2), 0.).sg_int().sg_sum(axis=1)<br/># encode audio feature<br/>logit = get_logit(x, voca_size)<br/># ctc decoding<br/>decoded, _ = tf.nn.ctc_beam_search_decoder(logit.sg_transpose(perm=[1, 0, 2]), seq_len, merge_repeated=False)<br/># to dense tensor<br/>y = tf.add(tf.sparse_to_dense(decoded[0].indices, decoded[0].dense_shape, decoded[0].values), 1, name="output")<br/><br/>with tf.Session() as sess:<br/> tf.sg_init(sess)<br/> saver = tf.train.Saver()<br/> saver.restore(sess, tf.train.latest_checkpoint('asset/train'))<br/><br/>graph = tf.get_default_graph()<br/>input_graph_def = graph.as_graph_def()<br/><br/>with tf.Session() as sess:<br/> tf.sg_init(sess)<br/> saver = tf.train.Saver()<br/> saver.restore(sess, tf.train.latest_checkpoint('asset/train'))<br/> # Output model's graph details for reference.<br/> tf.train.write_graph(sess.graph_def, '/root/speech-to-text-wavenet/asset/train', 'graph.txt', as_text=True)<br/> # Freeze the output graph.<br/> output_graph_def = graph_util.convert_variables_to_constants(sess,input_graph_def,"output".split(","))<br/> # Write it into .pb file.<br/> with tfw.gfile.GFile("/root/speech-to-text-wavenet/asset/train/wavenet_model.pb", "wb") as f:<br/> f.write(output_graph_def.SerializeToString())</pre>
<p class="mce-root">Next, we need to build a TensorFlow model and quantize the model so that it can be consumed in the mobile application. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bazel build TensorFlow and quantizing the model</h1>
                </header>
            
            <article>
                
<p class="mce-root">To quantize the model with TensorFlow, you need to have Bazel installed and the cloned Tensorflow repository. I recommend creating a new virtual environment to install and build TensorFlow there. Once you're done, you can run the following command:</p>
<pre class="mce-root">bazel build tensorflow/tools/graph_transforms:transform_graph<br/> bazel-bin/tensorflow/tools/graph_transforms/transform_graph \<br/> --in_graph=/your/.pb/file \<br/> --outputs="output_node_name" \<br/> --out_graph=/the/quantized/.pb/file \<br/> --transforms='quantize_weights'</pre>
<p class="mce-root">You can check out the official quantization tutorial on the TensorFlow website for other options in transforms. After quantization, the model was<span> down</span>sized by 75%, from 15.5 MB to 4 MB due to the 8-bit conversion. Due to the time limit, I haven't calculated the letter error rate with a test set to quantify the accuracy drop before and after quantization. </p>
<div class="packt_infobox">For a detailed discussion on neural network quantization, there is a great post by Pete Warden, called <em>Neural network quantization with TensorFlow</em> (<a href="https://petewarden.com/2016/05/03/how-to-quantize-neural-networks-with-tensorflow/">https://petewarden.com/2016/05/03/how-to-quantize-neural-networks-with-tensorflow/</a>).</div>
<p class="mce-root">Note that you can also do a full 8-bit calculation graph transformation by following the instructions in this section.</p>
<p class="mce-root">The model's size is down to 5.9 MB after this conversion, and the inference time is doubled. This could be due to the fact that the 8-bit calculation is not optimized for the Intel i5 processor on the macOS platform, which was used to write the application.</p>
<p class="mce-root">So, now that we have a compressed pretrained model, let's see what else we need to deploy the model on Android.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow ops registration</h1>
                </header>
            
            <article>
                
<p class="mce-root">Here, we will build the TensorFlow model using Bazel to create a <kbd>.so</kbd> file that can be called by the <strong>Java Native Interface</strong> (<strong>JNI</strong>), and includes all the operation libraries that we need for the pretrained WaveNet model inference. We will use built model in the Android application. </p>
<div class="packt_infobox">To find out more about Bazel, you can refer the following link: <a href="https://docs.bazel.build/versions/master/bazel-overview.html">https://docs.bazel.build/versions/master/bazel-overview.html</a>.</div>
<p class="mce-root">Let's begin by editing the WORKSPACE file in the cloned TensorFlow repository by uncommenting and updating the paths to <span><strong>Software</strong> <strong>Development</strong> <strong>Kit</strong> (</span><strong>SDK</strong>) and <span><strong>Native Development Kit</strong> (</span><strong>NDK</strong>).</p>
<p class="mce-root">Next, we need to find out what ops were used in the pretrained model and generate a <kbd>.so</kbd> file with that piece of information. </p>
<p class="mce-root">First, run the following command:</p>
<pre class="mce-root">bazel build tensorflow/python/tools:print_selective_registration_header &amp;&amp; \<br/> bazel-bin/tensorflow/python/tools/print_selective_registration_header \<br/> --graphs=path/to/graph.pb &gt; ops_to_register.h</pre>
<p class="mce-root">All the ops in the <kbd>.pb</kbd> file will be listed in <kbd>ops_to_register.h</kbd>.</p>
<p class="mce-root">Next, move <kbd>op_to_register.h</kbd> to <kbd>/tensorflow/tensorflow/core/framework/</kbd> and run the following command:</p>
<pre class="mce-root">bazel build -c opt --copt="-DSELECTIVE_REGISTRATION" \<br/> --copt="-DSUPPORT_SELECTIVE_REGISTRATION" \<br/> //tensorflow/contrib/android:libtensorflow_inference.so \<br/> --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \<br/> --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a</pre>
<p class="mce-root">Unfortunately, while I didn't get any error message, the <kbd>.so</kbd> file still didn't include all the ops listed in the header file:</p>
<pre class="mce-root">Modify BUILD in /tensorflow/tensorflow/core/kernels/</pre>
<p class="mce-root">If you haven't tried the first option and have got the list of ops in the model, you can get the ops by using the <kbd>tf.train.write_graph</kbd> command and typing the following i<span>nto your Terminal</span>:</p>
<pre class="mce-root">grep "op: " PATH/TO/mygraph.txt | sort | uniq | sed -E 's/^.+"(.+)".?$/\1/g'</pre>
<p class="mce-root">Next, edit the BUILD file by adding the missing ops into <kbd>android_extended_ops_group1</kbd> or <kbd>android_extended_ops_group2</kbd> in the Android libraries section. You can also make the <kbd>.so</kbd> file smaller by removing any unnecessary ops. Now, run the following command:</p>
<pre class="mce-root">bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so \<br/> --crosstool_top=//external:android/crosstool \<br/> --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \<br/> --cpu=armeabi-v7a</pre>
<p class="mce-root">You'll find the <kbd>libtensorflow_inference.so</kbd> file, as follows:</p>
<pre class="mce-root">bazel-bin/tensorflow/contrib/android/libtensorflow_inference.so</pre>
<p class="mce-root">Note that while running this command on Android, we ran into an error with the <kbd>sparse_to_dense</kbd> op. If you'd like to repeat this work, add <kbd>REGISTER_KERNELS_ALL(int64);</kbd> to <kbd>sparse_to_dense_op.cc</kbd> on line 153, and compile again.</p>
<p class="mce-root">In addition to the <kbd>.so</kbd> file, we also need a JAR file. You can simply add this in the <kbd>build.gradle</kbd> file, as follows:</p>
<pre class="mce-root">allprojects {<br/> repositories {<br/>     jcenter()<br/>     }<br/> }<br/><br/>dependencies {<br/> compile 'org.tensorflow:tensorflow-android:+'<br/> }<br/> </pre>
<p>Or, you can run the following command:</p>
<pre class="mce-root">bazel build //tensorflow/contrib/android:android_tensorflow_inference_java</pre>
<p class="mce-root"/>
<p class="mce-root">You'll find the file, as shown in the following code block:</p>
<pre class="mce-root">bazel-bin/tensorflow/contrib/android/libandroid_tensorflow_inference_java.jar<br/> </pre>
<p class="mce-root">Now, move both files into your Android project.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building an Android application</h1>
                </header>
            
            <article>
                
<p>In this section, we are going to build an Android application that will convert the user's voice input into text. Essentially, we are going to build a speech-to-text converter. We have modified the TensorFlow speech example in the TensorFlow Android demo repository for this exercise.</p>
<div class="packt_tip">You can find the TensorFlow Android demo application at <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android</a>.</div>
<p>The <kbd>build.gradle</kbd> file in the demo actually helps you build the <kbd>.so</kbd> and JAR files. So, if you'd like to start the demo examples with your own model, you can simply get the list of your ops, modify the BUILD file, and let the <kbd>build.gradle</kbd> file take care of the rest. We will get into the details of setting up the Android application in the following sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Requirements</h1>
                </header>
            
            <article>
                
<p>The requirements you will need to build the Android application are as follows:</p>
<ul>
<li class="mce-root">TensorFlow 1.13</li>
<li class="mce-root">Python 3.7</li>
<li class="mce-root">NumPy 1.15</li>
<li class="mce-root">python-speech-features</li>
</ul>
<div class="packt_infobox">TensorFlow link: <a href="https://github.com/tensorflow/tensorflow/releases">https://github.com/tensorflow/tensorflow/releases</a> <br/>
Python link: <a href="https://pip.pypa.io/en/stable/installing/">https://pip.pypa.io/en/stable/installing/</a> <br/>
Numpy link: <a href="https://docs.scipy.org/doc/numpy-1.13.0/user/install.html">https://docs.scipy.org/doc/numpy-1.13.0/user/install.html<br/></a>Python-speech-features link: <a href="https://github.com/jameslyons/python_speech_features">https://github.com/jameslyons/python_speech_features</a></div>
<p>Now, let's start building the Android application from scratch. In this application, we will record audio and then convert it into text. </p>
<div class="packt_tip">Set up Android Studio based on your operating system by going to the following link: <a href="https://developer.android.com/studio/install" target="_blank">https://developer.android.com/studio/install</a>.<span>The code repository that's used in this project has been modified from the TensorFlow example provided here</span><span>: </span><a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android" target="_blank">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android.</a><a href="https://developer.android.com/studio/install" target="_blank"/></div>
<p>We will use the TensorFlow sample application and edit it according to our needs. </p>
<p><span>Add <span class="packt_screen">Application name</span> and the <span class="packt_screen">Company domain</span> name, as shown in the following screenshot:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-600 image-border" src="assets/5683c7fa-8f1d-4492-87c0-0cd7db2e0d83.png" style="width:52.00em;height:33.67em;"/></p>
<p>In the next step, select the <span class="packt_screen">Target Android Devices</span> version. We will select the minimum version as <span class="packt_screen">API 15</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-601 image-border" src="assets/c5033299-5f41-41d1-8d47-856fa7f342a4.png" style="width:52.00em;height:34.25em;"/></p>
<p class="CDPAlignLeft CDPAlign">After this, we will add either <span class="packt_screen">Empty Activity</span> or <span class="packt_screen">No Activity</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-602 image-border" src="assets/43d69a73-8f73-4ac9-9c39-dbc1de2ae0d9.png" style="width:52.00em;height:37.08em;"/></p>
<p class="CDPAlignLeft CDPAlign">Now, let's start adding the activity and use the generated TensorFlow model to get the result. We need to enable two permissions so that we can use them in our application, as shown in the following code block:</p>
<pre><span>&lt;manifest </span><span>xmlns:</span><span>android</span><span>=</span><span>"http://schemas.android.com/apk/res/android"<br/></span><span>    </span><span>package=</span><span>"com.mlmobileapps.speech"</span><span>&gt;<br/></span><span><br/></span><span>    &lt;uses-permission </span><span>android</span><span>:name=</span><span>"android.permission.WRITE_EXTERNAL_STORAGE"</span><span>/&gt;<br/></span><span>    &lt;uses-permission </span><span>android</span><span>:name=</span><span>"android.permission.RECORD_AUDIO" </span><span>/&gt;<br/></span><span><br/></span><span>    &lt;uses-sdk<br/></span><span>        </span><span>android</span><span>:minSdkVersion=</span><span>"25"<br/></span><span>        </span><span>android</span><span>:targetSdkVersion=</span><span>"25" </span><span>/&gt;<br/></span><span><br/></span><span>    &lt;application </span><span>android</span><span>:allowBackup=</span><span>"true"<br/></span><span>        </span><span>android</span><span>:debuggable=</span><span>"true"<br/></span><span>        </span><span>android</span><span>:label=</span><span>"@string/app_name"<br/></span><span>        </span><span>android</span><span>:icon=</span><span>"@drawable/ic_launcher"<br/></span><span>        </span><span>android</span><span>:theme=</span><span>"@style/MaterialTheme"</span><span>&gt;<br/></span><span><br/></span><span>        &lt;activity </span><span>android</span><span>:name=</span><span>"org.tensorflow.demo.SpeechActivity"<br/></span><span>            </span><span>android</span><span>:screenOrientation=</span><span>"portrait"<br/></span><span>            </span><span>android</span><span>:label=</span><span>"@string/activity_name_speech"</span><span>&gt;<br/></span><span>            &lt;intent-filter&gt;<br/></span><span>               &lt;action </span><span>android</span><span>:name=</span><span>"android.intent.action.MAIN" </span><span>/&gt;<br/></span><span>               &lt;category </span><span>android</span><span>:name=</span><span>"android.intent.category.LAUNCHER" </span><span>/&gt;<br/></span><span>            &lt;/intent-filter&gt;<br/></span><span>        &lt;/activity&gt;<br/></span><span>    &lt;/application&gt;</span><span><br/></span><span>&lt;/manifest&gt;</span></pre>
<p>We will have a minimal UI for the application, with a couple of <kbd>TextView</kbd> components and a <kbd>Button</kbd>: </p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-603 image-border" src="assets/922ef678-9ff8-4124-ae40-3d2095533b4f.png" style="width:52.00em;height:33.58em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The following XML layout mimics the UI in the preceding screenshot:</p>
<pre><span>&lt;FrameLayout<br/></span><span>    </span><span>xmlns:</span><span>android</span><span>=</span><span>"http://schemas.android.com/apk/res/android"<br/></span><span>    </span><span>xmlns:</span><span>app</span><span>=</span><span>"http://schemas.android.com/apk/res-auto"<br/></span><span>    </span><span>xmlns:</span><span>tools</span><span>=</span><span>"http://schemas.android.com/tools"<br/></span><span>    </span><span>android</span><span>:layout_width=</span><span>"match_parent"<br/></span><span>    </span><span>android</span><span>:layout_height=</span><span>"match_parent"<br/></span><span>    </span><span>android</span><span>:background=</span><span>"#6200EE"<br/></span><span>    </span><span>tools</span><span>:context=</span><span>"org.tensorflow.demo.SpeechActivity"</span><span>&gt;<br/></span><span><br/></span><span>    &lt;LinearLayout<br/></span><span>        </span><span>android</span><span>:layout_width=</span><span>"match_parent"<br/></span><span>        </span><span>android</span><span>:orientation=</span><span>"vertical"<br/></span><span>        </span><span>android</span><span>:layout_height=</span><span>"wrap_content"</span><span>&gt;<br/></span><span>        &lt;TextView<br/></span><span>            </span><span>android</span><span>:id=</span><span>"@+id/textView"<br/></span><span>            </span><span>android</span><span>:layout_width=</span><span>"wrap_content"<br/></span><span>            </span><span>android</span><span>:layout_height=</span><span>"wrap_content"<br/></span><span><br/></span><span>            </span><span>android</span><span>:layout_gravity=</span><span>"top"<br/></span><span>            </span><span>android</span><span>:textColor=</span><span>"#fff"<br/></span><span>            </span><span>android</span><span>:layout_marginLeft=</span><span>"10dp"<br/></span><span>            </span><span>android</span><span>:layout_marginTop=</span><span>"30dp"<br/></span><span>            </span><span>android</span><span>:text=</span><span>"Talk within 5 seconds"<br/></span><span>            </span><span>android</span><span>:textAlignment=</span><span>"center"<br/></span><span>            </span><span>android</span><span>:textSize=</span><span>"24dp" </span><span>/&gt;<br/></span><span><br/></span><span>        &lt;TextView<br/></span><span>            </span><span>android</span><span>:id=</span><span>"@+id/output_text"<br/></span><span>            </span><span>android</span><span>:layout_width=</span><span>"wrap_content"<br/></span><span>            </span><span>android</span><span>:layout_height=</span><span>"wrap_content"<br/></span><span>            </span><span>android</span><span>:textColor=</span><span>"#fff"<br/></span><span>            </span><span>android</span><span>:layout_gravity=</span><span>"top"<br/></span><span>            </span><span>android</span><span>:layout_marginLeft=</span><span>"10dp"<br/></span><span>            </span><span>android</span><span>:layout_marginTop=</span><span>"10dp"<br/></span><span>            </span><span>android</span><span>:textAlignment=</span><span>"center"<br/></span><span>            </span><span>android</span><span>:textSize=</span><span>"24dp" </span><span>/&gt;<br/></span><span>    &lt;/LinearLayout&gt;<br/></span><span><br/></span><span>    &lt;Button<br/></span><span>        </span><span>android</span><span>:id=</span><span>"@+id/start"<br/></span><span>        </span><span>android</span><span>:background=</span><span>"#ff0266"<br/></span><span>        </span><span>android</span><span>:textColor=</span><span>"#fff"<br/></span><span>        </span><span>android</span><span>:layout_width=</span><span>"wrap_content"<br/></span><span>        </span><span>android</span><span>:padding=</span><span>"20dp"<br/></span><span>        </span><span>android</span><span>:layout_height=</span><span>"wrap_content"<br/></span><span>        </span><span>android</span><span>:layout_gravity=</span><span>"bottom|center_horizontal"<br/></span><span>        </span><span>android</span><span>:layout_marginBottom=</span><span>"50dp"<br/></span><span>        </span><span>android</span><span>:text=</span><span>"Record Voice" </span><span>/&gt;<br/></span><span><br/></span><span>&lt;/FrameLayout&gt;<br/></span></pre>
<p class="CDPAlignLeft CDPAlign">Let's add the steps for the speech recognizer activity, as follows:</p>
<pre><span>@Override<br/></span><span>protected void </span><span>onCreate</span>(Bundle savedInstanceState) {<br/>  <span>// Set up the UI.<br/></span><span>  </span><span>super</span>.onCreate(savedInstanceState)<span>;<br/></span><span>  </span>setContentView(R.layout.<span>activity_speech</span>)<span>;<br/></span><span>  </span><span>startButton </span>= (Button) findViewById(R.id.<span>start</span>)<span>;<br/></span><span>  </span><span>startButton</span>.setOnClickListener(<br/>      <span>new </span>View.OnClickListener() {<br/>        <span>@Override<br/></span><span>        </span><span>public void </span><span>onClick</span>(View view) {<br/>            startRecording()<span>;<br/></span><span>        </span>}<br/>      })<span>;<br/></span><span>  </span><span>outputText </span>= (TextView) findViewById(R.id.<span>output_text</span>)<span>;</span><span><br/></span><span>  </span><span>// Load the Pretrained WaveNet model.<br/></span><span>  </span><span>inferenceInterface </span>= <span>new </span>TensorFlowInferenceInterface(getAssets()<span>, </span><span>MODEL_FILENAME</span>)<span>;</span><span><br/></span><span>  </span>requestMicrophonePermission()<span>;<br/></span>}</pre>
<div class="CDPAlignLeft CDPAlign packt_infobox"><span>Note that we are not going to discuss the basics of Android here.</span></div>
<p class="CDPAlignLeft CDPAlign">Next, we will launch the recorder, as follows:</p>
<pre><span>public synchronized void </span><span>startRecording</span>() {<br/>  <span>if </span>(<span>recordingThread </span>!= <span>null</span>) {<br/>    <span>return;<br/></span><span>  </span>}<br/>  <span>shouldContinue </span>= <span>true;<br/></span><span>  </span><span>recordingThread </span>=<br/>      <span>new </span>Thread(<br/>          <span>new </span>Runnable() {<br/>            <span>@Override<br/></span><span>            </span><span>public void </span><span>run</span>() {<br/>              record()<span>;<br/></span><span>            </span>}<br/>          })<span>;<br/></span><span>  </span><span>recordingThread</span>.start()<span>;<br/></span>}</pre>
<p>The following code shows the implementation of the <kbd>record()</kbd> method:</p>
<pre><span>private void </span><span>record</span>() {<br/>  android.os.Process.<span>setThreadPriority</span>(android.os.Process.<span>THREAD_PRIORITY_AUDIO</span>)<span>;<br/></span><span><br/></span><span>  </span><span>// Estimate the buffer size we'll need for this device.<br/></span><span>  </span><span>int </span>bufferSize =<br/>      AudioRecord.<span>getMinBufferSize</span>(<br/>              <span>SAMPLE_RATE</span><span>, </span>AudioFormat.<span>CHANNEL_IN_MONO</span><span>, </span>AudioFormat.<span>ENCODING_PCM_16BIT</span>)<span>;<br/></span><span>  if </span>(bufferSize == AudioRecord.<span>ERROR </span>|| bufferSize == AudioRecord.<span>ERROR_BAD_VALUE</span>) {<br/>    bufferSize = <span>SAMPLE_RATE </span>* <span>2</span><span>;<br/></span><span>  </span>}<br/>  <span>short</span>[] audioBuffer = <span>new short</span>[bufferSize / <span>2</span>]<span>;<br/></span><span><br/></span><span>  </span>AudioRecord record =<br/>      <span>new </span>AudioRecord(<br/>          MediaRecorder.AudioSource.<span>DEFAULT</span><span>,<br/></span><span>          </span><span>SAMPLE_RATE</span><span>,<br/></span><span>          </span>AudioFormat.<span>CHANNEL_IN_MONO</span><span>,<br/></span><span>          </span>AudioFormat.<span>ENCODING_PCM_16BIT</span><span>,<br/></span><span>          </span>bufferSize)<span>;<br/></span><span><br/></span><span>  if </span>(record.getState() != AudioRecord.<span>STATE_INITIALIZED</span>) {<br/>    Log.<span>e</span>(<span>LOG_TAG</span><span>, </span><span>"Audio Record can't initialize!"</span>)<span>;<br/></span><span>    return;<br/></span><span>  </span>}<br/><br/>  record.startRecording()<span>;<br/></span><span><br/></span><span>  </span>Log.<span>v</span>(<span>LOG_TAG</span><span>, </span><span>"Start recording"</span>)<span>;<br/></span><span><br/></span><span>  while </span>(<span>shouldContinue</span>) {<br/>    <span>int </span>numberRead = record.read(audioBuffer<span>, </span><span>0</span><span>, </span>audioBuffer.<span>length</span>)<span>;<br/></span><span>      </span>Log.<span>v</span>(<span>LOG_TAG</span><span>, </span><span>"read: " </span>+ numberRead)<span>;<br/></span><span>    int </span>maxLength = <span>recordingBuffer</span>.<span>length</span><span>;<br/></span><span>    </span><span>recordingBufferLock</span>.lock()<span>;<br/></span><span>    try </span>{<br/>        <span>if </span>(<span>recordingOffset </span>+ numberRead &lt; maxLength) {<br/>            System.<span>arraycopy</span>(audioBuffer<span>, </span><span>0</span><span>, </span><span>recordingBuffer</span><span>, </span><span>recordingOffset</span><span>, </span>numberRead)<span>;<br/></span><span>        </span>} <span>else </span>{<br/>            <span>shouldContinue </span>= <span>false;<br/></span><span>        </span>}<br/>      <span>recordingOffset </span>+= numberRead<span>;<br/></span><span>    </span>} <span>finally </span>{<br/>      <span>recordingBufferLock</span>.unlock()<span>;<br/></span><span>    </span>}<br/>  }<br/>  record.stop()<span>;<br/></span><span>  </span>record.release()<span>;<br/></span><span>  </span>startRecognition()<span>;<br/></span>}</pre>
<p class="mce-root">The following code shows the implementation of the audio recognizing method:</p>
<pre><span>public synchronized void </span><span>startRecognition</span>() {<br/>  <span>if </span>(<span>recognitionThread </span>!= <span>null</span>) {<br/>    <span>return;<br/></span><span>  </span>}<br/>  <span>shouldContinueRecognition </span>= <span>true;<br/></span><span>  </span><span>recognitionThread </span>=<br/>      <span>new </span>Thread(<br/>          <span>new </span>Runnable() {<br/>            <span>@Override<br/></span><span>            </span><span>public void </span><span>run</span>() {<br/>              recognize()<span>;<br/></span><span>            </span>}<br/>          })<span>;<br/></span><span>  </span><span>recognitionThread</span>.start()<span>;<br/></span>}<br/><br/><span>private void </span><span>recognize</span>() {<br/>  Log.<span>v</span>(<span>LOG_TAG</span><span>, </span><span>"Start recognition"</span>)<span>;<br/></span><span><br/></span><span>  short</span>[] inputBuffer = <span>new short</span>[<span>RECORDING_LENGTH</span>]<span>;<br/></span><span>  double</span>[] doubleInputBuffer = <span>new double</span>[<span>RECORDING_LENGTH</span>]<span>;<br/></span><span>  long</span>[] outputScores = <span>new long</span>[<span>157</span>]<span>;<br/></span><span>  </span>String[] outputScoresNames = <span>new </span>String[]{<span>OUTPUT_SCORES_NAME</span>}<span>;<br/></span><span><br/></span><span><br/></span><span>    </span><span>recordingBufferLock</span>.lock()<span>;<br/></span><span>    try </span>{<br/>      <span>int </span>maxLength = <span>recordingBuffer</span>.<span>length</span><span>;<br/></span><span>        </span>System.<span>arraycopy</span>(<span>recordingBuffer</span><span>, </span><span>0</span><span>, </span>inputBuffer<span>, </span><span>0</span><span>, </span>maxLength)<span>;<br/></span><span>    </span>} <span>finally </span>{<br/>      <span>recordingBufferLock</span>.unlock()<span>;<br/></span><span>    </span>}<br/><br/>    <span>// We need to feed in float values between -1.0 and 1.0, so divide the<br/></span><span>    // signed 16-bit inputs.<br/></span><span>    </span><span>for </span>(<span>int </span>i = <span>0</span><span>; </span>i &lt; <span>RECORDING_LENGTH</span><span>; </span>++i) {<br/>      doubleInputBuffer[i] = inputBuffer[i] / <span>32767.0</span><span>;<br/></span><span>    </span>}<br/><br/>    <span>//MFCC java library.<br/></span><span>    </span>MFCC mfccConvert = <span>new </span>MFCC()<span>;<br/></span><span>    float</span>[] mfccInput = mfccConvert.process(doubleInputBuffer)<span>;<br/></span><span>    </span>Log.<span>v</span>(<span>LOG_TAG</span><span>, </span><span>"MFCC Input======&gt; " </span>+ Arrays.<span>toString</span>(mfccInput))<span>;<br/></span><span><br/></span><span>    </span><span>// Run the model.<br/></span><span>    </span><span>inferenceInterface</span>.feed(<span>INPUT_DATA_NAME</span><span>, </span>mfccInput<span>, </span><span>1</span><span>, </span><span>157</span><span>, </span><span>20</span>)<span>;<br/></span><span>    </span><span>inferenceInterface</span>.run(outputScoresNames)<span>;<br/></span><span>    </span><span>inferenceInterface</span>.fetch(<span>OUTPUT_SCORES_NAME</span><span>, </span>outputScores)<span>;<br/></span><span>    </span>Log.<span>v</span>(<span>LOG_TAG</span><span>, </span><span>"OUTPUT======&gt; " </span>+ Arrays.<span>toString</span>(outputScores))<span>;<br/></span><span><br/></span><span><br/></span><span>    </span><span>//Output the result.<br/></span><span>    </span>String result = <span>""</span><span>;<br/></span><span>    for </span>(<span>int </span>i = <span>0</span><span>;</span>i&lt;outputScores.<span>length</span><span>;</span>i++) {<br/>        <span>if </span>(outputScores[i] == <span>0</span>)<br/>            <span>break;<br/></span><span>        </span>result += <span>map</span>[(<span>int</span>) outputScores[i]]<span>;<br/></span><span>    </span>}<br/>    <span>final </span>String r = result<span>;<br/></span><span>    this</span>.runOnUiThread(<span>new </span>Runnable() {<br/>        <span>@Override<br/></span><span>        </span><span>public void </span><span>run</span>() {<br/>            <span>outputText</span>.setText(<span>r</span>)<span>;<br/></span><span>        </span>}<br/>    })<span>;</span><span><br/></span><span>    </span>Log.<span>v</span>(<span>LOG_TAG</span><span>, </span><span>"End recognition: " </span>+result)<span>;<br/></span><span>  </span>}</pre>
<p>The model is run through the <kbd>TensorFlowInferenceInterface</kbd><em> </em>class, as shown in the preceding code.</p>
<p>Once we have the completed code running, <span>run the application</span>.</p>
<p>On the first run, you will need to allow the application to use the phone's internal microphone, <span>as demonstrated in the following screenshot</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-604 image-border" src="assets/8c35468a-ce89-472c-98d3-94da70ed4595.jpg" style="width:20.42em;height:36.33em;"/></p>
<p>Once we give permission to use the microphone, click on <span class="packt_screen">RECORD VOICE</span> and give your voice input within <span class="packt_screen">5 seconds</span>. There are two attempts shown in the following screenshots for the <kbd>how are you</kbd> <span>input keyword </span>with an Indian accent. It works better with US and UK accents. </p>
<p>The first attempt is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-847 image-border" src="assets/6434a237-92fa-4d77-9063-99252d3510ab.png" style="width:20.25em;height:42.17em;"/></p>
<p>The second attempt is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-606 image-border" src="assets/84aa3d2f-5f7f-4a8b-b273-5d3b7ad89d80.jpg" style="width:22.50em;height:39.92em;"/></p>
<p>You should try this with your own accent to get the correct output. This is a very simple way to start building your own speech detector that you can improve on even further. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you learned how to build a complete speech detector on your own. We discussed how the WaveNet model works in detail. With this application, we can make a simple speech-to-text converter work; however, a lot of improvements and updates need to be done to get perfect results. You can build the same application on the iOS platform as well by converting the model into CoreML.</p>
<p>In the next chapter, we will move on and build a handwritten digit classifier using the <strong>Modified National Institute of Standards and Technology</strong> (<strong>MNIST</strong>) model.</p>


            </article>

            
        </section>
    </body></html>