["```py\nalpha = .9\narms = [['bronze' , 1],['gold', 3], ['silver' , 2], ['bronze' , 1]]\nv = [0,0,0,0]\n\nfor i in range(10):\n    for a in range(len(arms)):\n        print('pulling arm '+ arms[a][0])\n        v[a] = v[a] + alpha * (arms[a][1]-v[a])\n\nprint(v)\n```", "```py\nimport random\n\nalpha = .9\nbandits = [[['bronze' , 1],['gold', 3], ['silver' , 2], ['bronze' , 1]],\n           [['bronze' , 1],['gold', 3], ['silver' , 2], ['bronze' , 1]],\n           [['bronze' , 1],['gold', 3], ['silver' , 2], ['bronze' , 1]],\n           [['bronze' , 1],['gold', 3], ['silver' , 2], ['bronze' , 1]]]\nq = [[0,0,0,0],\n     [0,0,0,0],\n     [0,0,0,0],\n     [0,0,0,0]]\n\nfor i in range(10): \n    for b in range(len(bandits)):\n        arm = random.randint(0,3)\n        print('pulling arm {0} on bandit {1}'.format(arm,b))\n        q[b][arm] = q[b][arm] + alpha * (bandits[b][arm][1]-q[b][arm])\n\nprint(q)\n```", "```py\nconda create -n gym\nconda activate gym\nconda install python=3.5  # reverts Python, for use with TensorFlow later\npip install tensorflow\npip install keras pip install gym\n```", "```py\nfrom collections import deque\nimport numpy as np\nimport os\nclear = lambda: os.system('cls') #linux/mac use 'clear'\nimport time\nimport gym\nfrom gym import wrappers, logger\n```", "```py\nenvironment = 'FrozenLake-v0'\nenv = gym.make(environment)\n```", "```py\noutdir = os.path.join('monitor','q-learning-{0}'.format(environment))\nenv = wrappers.Monitor(env, directory=outdir, force=True)\nenv.seed(0)\nenv.is_slippery = False\nq_table = np.zeros([env.observation_space.n, env.action_space.n])\n\n#parameters\nwins = 0\nepisodes = 40000\ndelay = 1\n\nepsilon = .8\nepsilon_min = .1\nepsilon_decay = .001\ngamma = .9\nlearning_rate = .1\n```", "```py\nfor episode in range(episodes): \n    state = env.reset()\n    done = False\n    while not done:\n        action = act(env.action_space,state)\n        next_state, reward, done, _ = env.step(action)\n        clear()\n        env.render()\n        learn(state, action, reward, next_state)\n        if done:\n            if reward > 0:\n                wins += 1\n            time.sleep(3*delay)\n        else:\n            time.sleep(delay)\n\nprint(\"Goals/Holes: %d/%d\" % (wins, episodes - wins))\nenv.close() \n```", "```py\ndef is_explore():\n    global epsilon, epsilon_decay, epsilon_min\n    epsilon = max(epsilon-epsilon_decay,epsilon_min)\n    if np.random.rand() < epsilon:\n        return True\n    else:\n        return False\n\ndef act(action_space, state):\n    # 0 - left, 1 - Down, 2 - Right, 3 - Up\n    global q_table\n    if is_explore():\n        return action_space.sample()\n    else:\n        return np.argmax(q_table[state])\n```", "```py\ndef learn(state, action, reward, next_state):\n    # Q(s, a) += alpha * (reward + gamma * max_a' Q(s', a') - Q(s, a))\n    global q_table\n    q_value = gamma * np.amax(q_table[next_state])\n    q_value += reward\n    q_value -= q_table[state, action]\n    q_value *= learning_rate\n    q_value += q_table[state, action]\n    q_table[state, action] = q_value\n```", "```py\nimport random\nimport gym\nimport numpy as np\nfrom collections import deque\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\n\nEPISODES = 1000\n```", "```py\nclass DQNAgent:\n    def __init__(self, state_size, action_size):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.memory = deque(maxlen=2000)\n        self.gamma = 0.95 # discount rate\n        self.epsilon = 1.0 # exploration rate\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        self.learning_rate = 0.001\n        self.model = self._build_model()\n```", "```py\ndef _build_model(self):\n    # Neural Net for Deep-Q learning Model\n    model = Sequential()\n    model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n    model.add(Dense(24, activation='relu'))\n    model.add(Dense(self.action_size, activation='linear'))\n    model.compile(loss='mse',\n                      optimizer=Adam(lr=self.learning_rate))\n    return model\n```", "```py\nif __name__ == \"__main__\":\n    env = gym.make('CartPole-v1')\n    state_size = env.observation_space.shape[0]\n    action_size = env.action_space.n\n    agent = DQNAgent(state_size, action_size)\n    # agent.load(\"./save/cartpole-dqn.h5\")\n    done = False\n    batch_size = 32\n\n    for e in range(EPISODES):\n        state = env.reset()\n        state = np.reshape(state, [1, state_size]) \n        for time in range(500):\n            # env.render()\n            action = agent.act(state)\n            env.render()\n            next_state, reward, done, _ = env.step(action)\n            reward = reward if not done else -10\n            next_state = np.reshape(next_state, [1, state_size])\n agent.remember(state, action, reward, next_state, done)\n            state = next_state\n            if done:\n                print(\"episode: {}/{}, score: {}, e: {:.2}\"\n                      .format(e, EPISODES, time, agent.epsilon))\n                break\n            if len(agent.memory) > batch_size:\n                agent.replay(batch_size)\n```", "```py\ndef remember(self, state, action, reward, next_state, done):\n    self.memory.append((state, action, reward, next_state, done))\n```", "```py\ndef replay(self, batch_size):\n    minibatch = random.sample(self.memory, batch_size)\n    for state, action, reward, next_state, done in minibatch:\n        target = reward\n        if not done:\n            target = (reward+self.gamma*\n                      np.amax(self.model.predict(next_state)[0]))\n            target_f = self.model.predict(state)\n            target_f[0][action] = target\n            self.model.fit(state, target_f, epochs=1, verbose=0)\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n```", "```py\nif __name__ == \"__main__\":\n    env = gym.make('MountainCar-v0')\n```", "```py\npip install Pillow\npip install keras-rl\n\npip install gym[atari] # on Linux or Mac\npip install --no-index -f https://github.com/Kojoley/atari-py/releases atari_py  # on Windows thanks to Nikita Kniazev\n```", "```py\nmodel = Sequential()\nif K.image_dim_ordering() == 'tf':\n    # (width, height, channels)\n    model.add(Permute((2, 3, 1), input_shape=input_shape))\nelif K.image_dim_ordering() == 'th':\n    # (channels, width, height)\n    model.add(Permute((1, 2, 3), input_shape=input_shape))\nelse:\n    raise RuntimeError('Unknown image_dim_ordering.')\nmodel.add(Convolution2D(32, (8, 8), strides=(4, 4)))\nmodel.add(Activation('relu'))\nmodel.add(Convolution2D(64, (4, 4), strides=(2, 2)))\nmodel.add(Activation('relu'))\nmodel.add(Convolution2D(64, (3, 3), strides=(1, 1)))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dense(nb_actions))\nmodel.add(Activation('linear'))\nprint(model.summary())\n```"]