<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;1.&#xA0;Theano Basics" id="F8901-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01" class="calibre1"/>Chapter 1. Theano Basics</h1></div></div></div><p class="calibre8">This chapter presents Theano as a compute engine and the basics for symbolic computing with Theano. Symbolic computing consists of building graphs of operations that will be optimized later on for a specific architecture, using the computation libraries available for this architecture.</p><p class="calibre8">Although this chapter might appear to be a long way from practical applications, it is essential to have an understanding of the technology for the following chapters; what is it capable of and what value does it bring? All the following chapters address the applications of Theano when building all possible deep learning architectures.</p><p class="calibre8">Theano <a id="id0" class="calibre1"/>may be defined as a library for scientific computing; it has been available since 2007 and is particularly suited to deep learning. Two important features are at the core of any deep learning library: tensor operations, and the capability to run the code on CPU or <span class="strong"><strong class="calibre2">Graphical Computation Unit</strong></span> (<span class="strong"><strong class="calibre2">GPU</strong></span>). These two features enable us to work with a massive amount of multi-dimensional data. Moreover, Theano proposes automatic differentiation, a very useful feature that can solve a wider range of numeric optimizations than deep learning problems.</p><p class="calibre8">The chapter covers the following topics:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Theano installation and loading</li><li class="listitem">Tensors and algebra</li><li class="listitem">Symbolic programming</li><li class="listitem">Graphs</li><li class="listitem">Automatic differentiation</li><li class="listitem">GPU programming</li><li class="listitem">Profiling</li><li class="listitem">Configuration</li></ul></div></div>

<div class="book" title="Chapter&#xA0;1.&#xA0;Theano Basics" id="F8901-ccdadb29edc54339afcb9bdf9350ba6b">
<div class="book" title="The need for tensors"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch01lvl1sec08" class="calibre1"/>The need for tensors</h1></div></div></div><p class="calibre8">Usually, input <a id="id1" class="calibre1"/>data is represented with multi-dimensional arrays:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">Images have three dimensions</strong></span>: The number of channels, the width, and the height of the image</li><li class="listitem"><span class="strong"><strong class="calibre2">Sounds and times series have one dimension</strong></span>: The duration</li><li class="listitem"><span class="strong"><strong class="calibre2">Natural language sequences can be represented by two-dimensional arrays</strong></span>: The duration and the alphabet length or the vocabulary length</li></ul></div><p class="calibre8">We'll see more examples of input data arrays in the future chapters.</p><p class="calibre8">In Theano, multi-dimensional arrays are implemented with an abstraction class, named <span class="strong"><strong class="calibre2">tensor</strong></span>, with many more transformations available than traditional arrays in a computer language such as Python.</p><p class="calibre8">At each stage of a neural net, computations such as matrix multiplications involve multiple operations on these multi-dimensional arrays.</p><p class="calibre8">Classical arrays in programming languages do not have enough built-in functionalities to quickly and adequately address multi-dimensional computations and manipulations.</p><p class="calibre8">Computations on multi-dimensional arrays have a long history of optimizations, with tons of libraries and hardware. One of the most important gains in speed has been permitted by the massive parallel architecture of the GPU, with computation ability on a large number of cores, from a few hundred to a few thousand.</p><p class="calibre8">Compared <a id="id2" class="calibre1"/>to the traditional CPU, for example, a quadricore, 12-core, or 32-core engine, the gains with GPU can range from 5x to 100x, even if part of the code is still being executed on the CPU (data loading, GPU piloting, and result outputting). The main bottleneck with the use of GPU is usually the transfer of data between the memory of the CPU and the memory of the GPU, but still, when well programmed, the use of GPU helps bring a significant increase in speed of an order of magnitude. Getting results in days rather than months, or hours rather than days, is an undeniable benefit for experimentation.</p><p class="calibre8">The Theano engine has been designed to address the challenges of multi-dimensional arrays and architecture abstraction from the beginning.</p><p class="calibre8">There is another undeniable benefit of Theano for scientific computation: the automatic differentiation of functions of multi-dimensional arrays, a well-suited feature for model parameter inference via objective function minimization. Such a feature facilitates experimentation by releasing the pain to compute derivatives, which might not be very complicated, but are prone to many errors.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Installing and loading Theano" id="G6PI1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec09" class="calibre1"/>Installing and loading Theano</h1></div></div></div><p class="calibre8">In this <a id="id3" class="calibre1"/>section, we'll install Theano, run it on the CPU and GPU devices, and <a id="id4" class="calibre1"/>save the configuration.</p></div>

<div class="book" title="Installing and loading Theano" id="G6PI1-ccdadb29edc54339afcb9bdf9350ba6b">
<div class="book" title="Conda package and environment manager"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch01lvl2sec07" class="calibre1"/>Conda package and environment manager</h2></div></div></div><p class="calibre8">The easiest <a id="id5" class="calibre1"/>way to install Theano is to use <code class="email">conda</code>, a cross-platform package and environment manager.</p><p class="calibre8">If <code class="email">conda</code> is <a id="id6" class="calibre1"/>not already installed on your operating system, the fastest way to install <code class="email">conda</code> is to download the <code class="email">miniconda</code> installer from <a class="calibre1" href="https://conda.io/miniconda.html">https://conda.io/miniconda.html</a>. For example, for <code class="email">conda under Linux 64 bit and Python 2.7</code>, use this command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">wget</strong></span> https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh
<span class="strong"><strong class="calibre2">chmod</strong></span> +x Miniconda2-latest-Linux-x86_64.sh
<span class="strong"><strong class="calibre2">bash</strong></span> ./Miniconda2-latest-Linux-x86_64.sh</pre></div><p class="calibre8">Conda enables us to create new environments in which versions of Python (2 or 3) and the installed packages may differ. The <code class="email">conda</code> root environment uses the same version of Python as the version installed on the system on which you installed <code class="email">conda</code>.</p></div></div>

<div class="book" title="Installing and loading Theano" id="G6PI1-ccdadb29edc54339afcb9bdf9350ba6b">
<div class="book" title="Installing and running Theano on CPU"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch01lvl2sec08" class="calibre1"/>Installing and running Theano on CPU</h2></div></div></div><p class="calibre8">Let's <a id="id7" class="calibre1"/>install Theano:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">conda</strong></span> install theano</pre></div><p class="calibre8">Run a <a id="id8" class="calibre1"/>Python session and try the following commands to check your configuration:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from theano import theano

&gt;&gt;&gt; theano.config.device
'cpu'

&gt;&gt;&gt; theano.config.floatX
'float64'

&gt;&gt;&gt; print(theano.config)</pre></div><p class="calibre8">The last command prints all the configuration of Theano. The <code class="email">theano.config</code> object contains keys to <a id="id9" class="calibre1"/>many configuration options.</p><p class="calibre8">To <a id="id10" class="calibre1"/>infer the configuration options, Theano looks first at the <code class="email">~/.theanorc </code>file, then at any environment variables that are available, which override the former options, and lastly at the variable set in the code that are first in order of precedence:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; theano.config.floatX='float32'</pre></div><p class="calibre8">Some of the properties might be read-only and cannot be changed in the code, but <code class="email">floatX</code>, which sets the default floating point precision for floats, is among the properties that can be changed directly in the code.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note02" class="calibre1"/>Note</h3><p class="calibre8">It is advised to use <code class="email">float32</code> since GPU has a long history without <code class="email">float64</code>.<code class="email"> float64</code> execution speed on GPU is slower, sometimes much slower (2x to 32x on latest generation Pascal hardware), and <code class="email">float32</code> precision is enough in practice.</p></div></div></div>

<div class="book" title="Installing and loading Theano" id="G6PI1-ccdadb29edc54339afcb9bdf9350ba6b">
<div class="book" title="GPU drivers and libraries"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch01lvl2sec09" class="calibre1"/>GPU drivers and libraries</h2></div></div></div><p class="calibre8">Theano <a id="id11" class="calibre1"/>enables the use of GPU, units that are usually used to compute the graphics to display on the computer screen.</p><p class="calibre8">To have <a id="id12" class="calibre1"/>Theano work on the GPU as well, a GPU backend library is required on your system.</p><p class="calibre8">The CUDA library (for NVIDIA GPU cards only) is the main choice for GPU computations. There is also the OpenCL standard, which is open source but far less developed, and much more experimental and rudimentary on Theano.</p><p class="calibre8">Most <a id="id13" class="calibre1"/>scientific computations still occur on NVIDIA cards at the moment. If you have an NVIDIA GPU card, download CUDA from the NVIDIA website, <a class="calibre1" href="https://developer.nvidia.com/cuda-downloads">https://developer.nvidia.com/cuda-downloads</a>, and install it. The installer will install the latest version of the GPU drivers first, if they are not already installed. It will install the CUDA library in the <code class="email">/usr/local/cuda </code>directory.</p><p class="calibre8">Install the cuDNN library, a library by NVIDIA, that offers faster implementations of some operations for the GPU. To install it, I usually copy the <code class="email">/usr/local/cuda </code>directory to a new directory, <code class="email">/usr/local/cuda-{CUDA_VERSION}-cudnn-{CUDNN_VERSION}</code>, so that I can choose the version of CUDA and cuDNN, depending on the deep learning technology I use and its compatibility.</p><p class="calibre8">In your .<code class="email">bashrc</code> profile, add the following line to set the <code class="email">$PATH</code> and <code class="email">$LD_LIBRARY_PATH</code> variables:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">export</strong></span> PATH=/usr/local/cuda-8.0-cudnn-5.1/bin:$PATH
<span class="strong"><strong class="calibre2">export</strong></span> LD_LIBRARY_PATH=/usr/local/cuda-8.0-cudnn-5.1/lib64:/usr/local/cuda-8.0-cudnn-5.1/lib:$LD_LIBRARY_PATH</pre></div></div></div>

<div class="book" title="Installing and loading Theano" id="G6PI1-ccdadb29edc54339afcb9bdf9350ba6b">
<div class="book" title="Installing and running Theano on GPU"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch01lvl2sec10" class="calibre1"/>Installing and running Theano on GPU</h2></div></div></div><p class="calibre8">N-dimensional <a id="id14" class="calibre1"/>GPU arrays have been implemented in Python in six <a id="id15" class="calibre1"/>different GPU libraries (<code class="email">Theano/CudaNdarray,PyCUDA</code>/ <code class="email">GPUArray,CUDAMAT</code>/ <code class="email">CUDAMatrix</code>, <code class="email">PYOPENCL</code>/<code class="email">GPUArray</code>, <code class="email">Clyther</code>, <code class="email">Copperhead</code>), are a  subset of <code class="email">NumPy.ndarray</code>. <code class="email">Libgpuarray</code> is a backend library to have them in a common interface with the same property.</p><p class="calibre8">To install <code class="email">libgpuarray</code> with <code class="email">conda</code>, use this command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">conda</strong></span> install pygpu</pre></div><p class="calibre8">To <a id="id16" class="calibre1"/>run Theano in GPU mode, you need to configure the <code class="email">config.device</code> variable before execution since it is a read-only variable once the code is run. Run this <a id="id17" class="calibre1"/>command with the <code class="email">THEANO_FLAGS</code> environment variable:</p><div class="informalexample"><pre class="programlisting">THEANO_FLAGS="device=cuda,floatX=float32" <span class="strong"><strong class="calibre2">python</strong></span>
&gt;&gt;&gt; <span class="strong"><strong class="calibre2">import</strong></span> theano
Using cuDNN version 5110 on context None
Mapped name None to device cuda: Tesla K80 (0000:83:00.0)

&gt;&gt;&gt; theano.config.device
'gpu'

&gt;&gt;&gt; theano.config.floatX
'float32'</pre></div><p class="calibre8">The first return shows that GPU device has been correctly detected, and specifies which GPU it uses.</p><p class="calibre8">By default, Theano activates CNMeM, a faster CUDA memory allocator. An initial pre-allocation can be specified with the <code class="email">gpuarra.preallocate</code> option. At the end, my launch command will be as follows:</p><div class="informalexample"><pre class="programlisting">THEANO_FLAGS="device=cuda,floatX=float32,gpuarray.preallocate=0.8" <span class="strong"><strong class="calibre2">python</strong></span>
&gt;&gt;&gt; <span class="strong"><strong class="calibre2">from</strong></span> theano import <span class="strong"><strong class="calibre2">theano</strong></span>
Using cuDNN version 5110 on context None
Preallocating 9151/11439 Mb (0.800000) on cuda
Mapped name None to device cuda: Tesla K80 (0000:83:00.0)</pre></div><p class="calibre8">The first line confirms that cuDNN is active, the second confirms memory pre-allocation. The third line gives the default <span class="strong"><strong class="calibre2">context name</strong></span> (that is, <code class="email">None</code> when <code class="email">flag device=cuda</code> is set) and the model of GPU used, while the default context name for the CPU will always be <code class="email">cpu</code>.</p><p class="calibre8">It is possible to specify a different GPU than the first one, setting the device to <code class="email">cuda0</code>, <code class="email">cuda1</code>,... for multi-GPU computers. It is also possible to run a program on multiple GPU in parallel or in sequence (when the memory of one GPU is not sufficient), in particular when training very deep neural nets, as for classification of full images as described in <a class="calibre1" title="Chapter 7. Classifying Images with Residual Networks" href="part0075_split_000.html#27GQ61-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 7</a>, <span class="strong"><em class="calibre12">Classifying Images with Residual Networks</em></span>. In this case, the <code class="email">contexts=dev0-&gt;cuda0;dev1-&gt;cuda1;dev2-&gt;cuda2;dev3-&gt;cuda3</code> flag activates multiple GPUs instead of one, and designates the context name to each GPU device to be used in the code. Here is an example on a 4-GPU instance:</p><div class="informalexample"><pre class="programlisting">THEANO_FLAGS="contexts=dev0-&gt;cuda0;dev1-&gt;cuda1;dev2-&gt;cuda2;dev3-&gt;cuda3,floatX=float32,gpuarray.preallocate=0.8" <span class="strong"><strong class="calibre2">python</strong></span>
&gt;&gt;&gt; <span class="strong"><strong class="calibre2">import</strong></span> theano
Using cuDNN version 5110 on context None
Preallocating 9177/11471 Mb (0.800000) on cuda0
Mapped name dev0 to device cuda0: Tesla K80 (0000:83:00.0)
Using cuDNN version 5110 on context dev1
Preallocating 9177/11471 Mb (0.800000) on cuda1
Mapped name dev1 to device cuda1: Tesla K80 (0000:84:00.0)
Using cuDNN version 5110 on context dev2
Preallocating 9177/11471 Mb (0.800000) on cuda2
Mapped name dev2 to device cuda2: Tesla K80 (0000:87:00.0)
Using cuDNN version 5110 on context dev3
Preallocating 9177/11471 Mb (0.800000) on cuda3
Mapped name dev3 to device cuda3: Tesla K80 (0000:88:00.0)</pre></div><p class="calibre8">To assign computations to a specific GPU in this multi-GPU setting, the names we choose, <code class="email">dev0</code>, <code class="email">dev1</code>, <code class="email">dev2</code>, and <code class="email">dev3</code>, have been mapped to each device (<code class="email">cuda0</code>, <code class="email">cuda1</code>, <code class="email">cuda2</code>, <code class="email">cuda3</code>).</p><p class="calibre8">This <a id="id18" class="calibre1"/>name mapping enables to write codes that are independent of the underlying GPU assignments and libraries (CUDA or others).</p><p class="calibre8">To <a id="id19" class="calibre1"/>keep the current configuration flags active at every Python session or execution without using environment variables, save your configuration in the <code class="email">~/.theanorc</code> file as follows:</p><div class="informalexample"><pre class="programlisting"> [global]
 floatX = float32
 device = cuda0
 [gpuarray]
 preallocate = 1</pre></div><p class="calibre8">Now you can simply run <code class="email">python</code> command. You are now all set.</p></div></div>
<div class="book" title="Tensors" id="H5A41-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec10" class="calibre1"/>Tensors</h1></div></div></div><p class="calibre8">In Python, some scientific libraries such as NumPy provide multi-dimensional arrays. Theano doesn't replace <a id="id20" class="calibre1"/>Numpy, but it works in concert with it. NumPy is used for the initialization of tensors.</p><p class="calibre8">To perform the same computation on CPU and GPU, variables are symbolic and represented by the tensor class, an abstraction, and writing numerical expressions consists of building a computation graph of variable nodes and apply nodes. Depending on the platform on which the computation graph will be compiled, tensors are replaced by either of the following:</p><div class="book"><ul class="itemizedlist"><li class="listitem">A <code class="email">TensorType</code> variable, which has to be on CPU</li><li class="listitem">A <code class="email">GpuArrayType</code> variable, which has to be on GPU</li></ul></div><p class="calibre8">That way, the code can be written indifferently of the platform where it will be executed.</p><p class="calibre8">Here are a few tensor objects:</p><div class="informalexample"><table border="1" class="calibre14"><colgroup class="calibre15"><col class="calibre16"/><col class="calibre16"/><col class="calibre16"/></colgroup><thead class="calibre17"><tr class="calibre18"><th valign="bottom" class="calibre19">
<p class="calibre20">Object class</p>
</th><th valign="bottom" class="calibre19">
<p class="calibre20">Number of dimensions</p>
</th><th valign="bottom" class="calibre19">
<p class="calibre20">Example</p>
</th></tr></thead><tbody class="calibre21"><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">theano.tensor.scalar</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">0-dimensional array</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">1, 2.5</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">theano.tensor.vector</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">1-dimensional array</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">[0,3,20]</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">theano.tensor.matrix</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">2-dimensional array</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">[[2,3][1,5]]</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">theano.tensor.tensor3</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">3-dimensional array</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">[[[2,3][1,5]],[[1,2],[3,4]]]</p>
</td></tr></tbody></table></div><p class="calibre8">Playing with these Theano objects in the Python shell gives us a better idea:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; <span class="strong"><strong class="calibre2">import</strong></span> theano.tensor <span class="strong"><strong class="calibre2">as</strong></span> T

&gt;&gt;&gt; T.scalar()
&lt;TensorType(float32, scalar)&gt;

&gt;&gt;&gt; T.iscalar()
&lt;TensorType(int32, scalar)&gt;

&gt;&gt;&gt; T.fscalar()
&lt;TensorType(float32, scalar)&gt;

&gt;&gt;&gt; T.dscalar()
&lt;TensorType(float64, scalar)&gt;</pre></div><p class="calibre8">With <code class="email">i</code>, <code class="email">l</code>, <code class="email">f</code>, or <code class="email">d</code> in front of the object name, you initiate a tensor of a given type, <code class="email">integer32</code>, <code class="email">integer64</code>, <code class="email">float32</code>, or <code class="email">float64</code>. For real-valued (floating point) data, it is advised to use the direct form <code class="email">T.scalar()</code> instead of the <code class="email">f</code> or <code class="email">d</code> variants since the direct form will use your current configuration for floats:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; theano.config.floatX <span class="strong"><strong class="calibre2">= </strong></span>'float64'

&gt;&gt;&gt; T.scalar()
&lt;TensorType(float64, scalar)&gt;

&gt;&gt;&gt; T.fscalar()
&lt;TensorType(float32, scalar)&gt;

&gt;&gt;&gt; theano.config.floatX = 'float32'

&gt;&gt;&gt; T.scalar()
&lt;TensorType(float32, scalar)&gt;</pre></div><p class="calibre8">Symbolic <a id="id21" class="calibre1"/>variables do either of the following:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Play the role of placeholders, as a starting point to build your graph of numerical operations (such as addition, multiplication): they receive the flow of the incoming data during the evaluation once the graph has been compiled</li><li class="listitem">Represent intermediate or output results</li></ul></div><p class="calibre8">Symbolic variables and operations are both part of a computation graph that will be compiled either on CPU or GPU for fast execution. Let's write our first computation graph consisting of a simple addition:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; x = T.matrix('x')

&gt;&gt;&gt; y = T.matrix('y')

&gt;&gt;&gt; z = x + y

&gt;&gt;&gt; theano.pp(z)
'(x + y)'

&gt;&gt;&gt; z.eval({x: [[1, 2], [1, 3]], y: [[1, 0], [3, 4]]})
array([[ 2.,  2.],
       [ 4.,  7.]], dtype=float32)</pre></div><p class="calibre8">First, two symbolic variables, or <span class="strong"><em class="calibre12">variable nodes</em></span>, are created, with the names <code class="email">x</code> and <code class="email">y</code>, and an addition operation, an <span class="strong"><em class="calibre12">apply node</em></span>, is applied between both of them to create a new symbolic variable, <code class="email">z</code>, in the computation graph.</p><p class="calibre8">The pretty print function, <code class="email">pp</code>, prints the expression represented by Theano symbolic variables. <code class="email">Eval</code> evaluates the value of the output variable, <code class="email">z</code>, when the first two variables, <code class="email">x</code> and <code class="email">y</code>, are initialized with two numerical 2-dimensional arrays.</p><p class="calibre8">The following example shows the difference between the variables <code class="email">x</code> and <code class="email">y</code>, and their names <code class="email">x</code> and <code class="email">y</code>:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; a = T.matrix()

&gt;&gt;&gt; b = T.matrix()

&gt;&gt;&gt; theano.pp(a + b)
'(&lt;TensorType(float32, matrix)&gt; + &lt;TensorType(float32, matrix)&gt;)'<span class="strong"><em class="calibre12">.</em></span>
</pre></div><p class="calibre8">Without names, it is more complicated to trace the nodes in a large graph. When printing the computation graph, names significantly help diagnose problems, while variables are only used to handle the objects in the graph:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; x = T.matrix('x')

&gt;&gt;&gt; x = x + x

&gt;&gt;&gt; theano.pp(x)
<span class="strong"><em class="calibre12">'(x + x)'</em></span>
</pre></div><p class="calibre8">Here, the original symbolic variable, named <code class="email">x</code>, does not change and stays part of the computation graph. <code class="email">x + x</code> creates a new symbolic variable we assign to the Python variable <code class="email">x</code>.</p><p class="calibre8">Note <a id="id22" class="calibre1"/>also that with the names, the plural form initializes multiple tensors at the same time:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; x, y, z = T.matrices('x', 'y', 'z')</pre></div><p class="calibre8">Now, let's have a look at the different functions to display the graph.</p></div>
<div class="book" title="Graphs and symbolic computing"><div class="book" id="I3QM2-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec11" class="calibre1"/>Graphs and symbolic computing</h1></div></div></div><p class="calibre8">Let's <a id="id23" class="calibre1"/>take back the simple addition example and present different ways to display the same information:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; x = T.matrix('x')

&gt;&gt;&gt; y = T.matrix('y')

&gt;&gt;&gt; z = x + y

&gt;&gt;&gt; z

Elemwise{add,no_inplace}.0

&gt;&gt;&gt; theano.pp(z)

<span class="strong"><em class="calibre12">'(x + y)</em></span>

&gt;&gt;&gt; theano.printing.pprint(z)

<span class="strong"><em class="calibre12">'(x + y)'</em></span>

&gt;&gt;&gt; theano.printing.debugprint(z)
Elemwise{add,no_inplace} [id A] ''   
 |x [id B]
 |y [id C]</pre></div><p class="calibre8">Here, the <code class="email">debugprint</code> function prints the pre-compilation graph, the unoptimized graph. In this case, it is <a id="id24" class="calibre1"/>composed of two variable nodes, <code class="email">x</code> and <code class="email">y</code>, and an apply node, the elementwise addition, with the <code class="email">no_inplace</code> option. The <code class="email">inplace</code> option will be used in the optimized graph to save memory and re-use the memory of the input to store the result of the operation.</p><p class="calibre8">If the <code class="email">graphviz</code> and <code class="email">pydot</code> libraries have been installed, the <code class="email">pydotprint</code> command outputs a PNG image of the graph:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; theano.printing.pydotprint(z)
The output file is available at ~/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-2.7.12-64/theano.pydotprint.gpu.png.
</pre></div><div class="mediaobject"><img src="../images/00002.jpeg" alt="Graphs and symbolic computing" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">You might have noticed that the <code class="email">z.eval</code> command takes while to execute the first time. The reason for this <a id="id25" class="calibre1"/>delay is the time required to optimize the mathematical expression and compile the code for the CPU or GPU before being evaluated.</p><p class="calibre8">The <a id="id26" class="calibre1"/>compiled expression can be obtained explicitly and used as a function that behaves as a traditional Python function:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; addition = theano.function([x, y], [z])

&gt;&gt;&gt; addition([[1, 2], [1, 3]], [[1, 0], [3, 4]])
[array([[ 2.,  2.],
       [ 4.,  7.]], dtype=float32)]</pre></div><p class="calibre8">The first argument in the function creation is a list of variables representing the input nodes of the graph. The second argument is the array of output variables. To print the post compilation graph, use this command:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; theano.printing.debugprint(addition)
HostFromGpu(gpuarray) [id A] ''   3
 |GpuElemwise{Add}[(0, 0)]&lt;gpuarray&gt; [id B] ''   2
   |GpuFromHost&lt;None&gt; [id C] ''   1
   | |x [id D]
   |GpuFromHost&lt;None&gt; [id E] ''   0
     |y [id F]

&gt;&gt;&gt; theano.printing.pydotprint(addition)

The output file is available at ~/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-2.7.12-64/theano.pydotprint.gpu.png:</pre></div><div class="mediaobject"><img src="../images/00003.jpeg" alt="Graphs and symbolic computing" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">This <a id="id27" class="calibre1"/>case has been printed while using the GPU. During compilation, each operation has <a id="id28" class="calibre1"/>chosen the available GPU implementation. The main program still runs on CPU, where the data resides, but a <code class="email">GpuFromHost</code> instruction performs a data transfer from the CPU to the GPU for input, while the opposite operation, <code class="email">HostFromGpu</code>, fetches the result for the main program to display it:</p><div class="mediaobject"><img src="../images/00004.jpeg" alt="Graphs and symbolic computing" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Theano performs some mathematical optimizations, such as grouping elementwise operations, adding a new value to the previous addition:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; z= z * x

&gt;&gt;&gt; theano.printing.debugprint(theano.function([x,y],z))
HostFromGpu(gpuarray) [id A] ''   3
 |GpuElemwise{Composite{((i0 + i1) * i0)}}[(0, 0)]&lt;gpuarray&gt; [id B] ''   2
   |GpuFromHost&lt;None&gt; [id C] ''   1
   | |x [id D]
   |GpuFromHost&lt;None&gt; [id E] ''   0
     |y [id F]</pre></div><p class="calibre8">The number of nodes in the graph has not increased: two additions have been merged into one node. Such <a id="id29" class="calibre1"/>optimizations make it more tricky to debug, so we'll show you at the end of this chapter <a id="id30" class="calibre1"/>how to disable optimizations for debugging.</p><p class="calibre8">Lastly, let's see a bit more about setting the initial value with NumPy:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; theano.config.floatX
'float32'

&gt;&gt;&gt; x = T.matrix()

&gt;&gt;&gt; x
&lt;TensorType(float32, matrix)&gt;

&gt;&gt;&gt; y = T.matrix()

&gt;&gt;&gt; addition = theano.function([x, y], [x+y])

&gt;&gt;&gt; addition(numpy.ones((2,2)),numpy.zeros((2,2)))
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "/usr/local/lib/python2.7/site-packages/theano/compile/function_module.py", line 786, in __call__
    allow_downcast=s.allow_downcast)

  File "/usr/local/lib/python2.7/site-packages/theano/tensor/type.py", line 139, in filter
    raise TypeError(err_msg, data)
TypeError: ('Bad input argument to theano function with name "&lt;stdin&gt;:1"  at index 0(0-based)', 'TensorType(float32, matrix) cannot store a value of dtype float64 without risking loss of precision. If you do not mind this loss, you can: 1) explicitly cast your data to float32, or 2) set "allow_input_downcast=True" when calling "function".', array([[ 1.,  1.],
       [ 1.,  1.]]))</pre></div><p class="calibre8">Executing the function on the NumPy arrays throws an error related to loss of precision, since the NumPy arrays here have <code class="email">float64</code> and <code class="email">int64</code> <code class="email">dtypes</code>, but <code class="email">x</code> and <code class="email">y</code> are <code class="email">float32</code>. There are multiple solutions to this; the first is to create the NumPy arrays with the right <code class="email">dtype</code>:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; <span class="strong"><strong class="calibre2">import</strong></span> numpy

&gt;&gt;&gt; addition(numpy.ones((2,2), dtype=theano.config.floatX),numpy.zeros((2,2), dtype=theano.config.floatX))
[array([[ 1.,  1.],
        [ 1.,  1.]], dtype=float32)]</pre></div><p class="calibre8">Alternatively, cast the NumPy arrays (in particular for <code class="email">numpy.diag</code>, which does not allow us to choose the <code class="email">dtype</code> directly):</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; addition(numpy.ones((2,2)).astype(theano.config.floatX),numpy.diag((2,3)).astype(theano.config.floatX))
[array([[ 3.,  1.],
        [ 1.,  4.]], dtype=float32)]</pre></div><p class="calibre8">Or we could allow downcasting:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; addition = theano.function([x, y], [x+y],allow_input_downcast=True)

&gt;&gt;&gt; addition(numpy.ones((2,2)),numpy.zeros((2,2)))
[array([[ 1.,  1.],
        [ 1.,  1.]], dtype=float32)]</pre></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Operations on tensors"><div class="book" id="J2B82-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec12" class="calibre1"/>Operations on tensors</h1></div></div></div><p class="calibre8">We have <a id="id31" class="calibre1"/>seen how to create a computation graph composed of symbolic variables and operations, and compile the resulting expression for an evaluation or as a function, either on GPU or on CPU.</p><p class="calibre8">As tensors are very important to deep learning, Theano provides lots of operators to work with tensors. Most operators that exist in scientific computing libraries such as NumPy for numerical arrays have their equivalent in Theano and have a similar name, in order to be more familiar to NumPy's users. But contrary to NumPy, expressions written with Theano can be compiled either on CPU or GPU.</p><p class="calibre8">This, for example, is the case for tensor creation:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">T.zeros()</code>, <code class="email">T.ones()</code>, <code class="email">T.eye()</code> operators take a shape tuple as input</li><li class="listitem"><code class="email">T.zeros_like()</code>, <code class="email">T.one_like()</code>, <code class="email">T.identity_like()</code> use the shape of the tensor argument</li><li class="listitem"><code class="email">T.arange()</code>, <code class="email">T.mgrid()</code>, <code class="email">T.ogrid()</code> are used for range and mesh grid arrays</li></ul></div><p class="calibre8">Let's have a look in the Python shell:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; a = T.zeros((2,3))

&gt;&gt;&gt; a.eval()
array([[ 0.,  0.,  0.],
       [ 0.,  0.,  0.]])

&gt;&gt;&gt; b = T.identity_like(a)

&gt;&gt;&gt; b.eval()
array([[ 1.,  0.,  0.],
        [ 0.,  1.,  0.]])

&gt;&gt;&gt; c = T.arange(10)

&gt;&gt;&gt; c.eval()
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</pre></div><p class="calibre8">Information such as the number of dimensions, <code class="email">ndim</code>, and the type, <code class="email">dtype</code>, are defined at tensor creation and cannot be modified later:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; c.ndim
<span class="strong"><em class="calibre12">1</em></span>

&gt;&gt;&gt; c.dtype
'int64'

&gt;&gt;&gt; c.type
TensorType(int64, vector)</pre></div><p class="calibre8">Some other information, such as shape, is evaluated by the computation graph:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; a = T.matrix()

&gt;&gt;&gt; a.shape
Shape.0

&gt;&gt;&gt; a.shape.eval({a: [[1, 2], [1, 3]]})
array([2, 2])

&gt;&gt;&gt; shape_fct = theano.function([a],a.shape)

&gt;&gt;&gt; shape_fct([[1, 2], [1, 3]])
array([2, 2])

&gt;&gt;&gt; n = T.iscalar()

&gt;&gt;&gt; c = T.arange(n)

&gt;&gt;&gt; c.shape.eval({n:10})
array([10])</pre></div></div>

<div class="book" title="Operations on tensors">
<div class="book" title="Dimension manipulation operators"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch01lvl2sec11" class="calibre1"/>Dimension manipulation operators</h2></div></div></div><p class="calibre8">The first <a id="id32" class="calibre1"/>type of operator on tensor is for <span class="strong"><strong class="calibre2">dimension manipulation</strong></span>. This <a id="id33" class="calibre1"/>type of operator takes a tensor as input and returns a new tensor:</p><div class="informalexample"><table border="1" class="calibre14"><colgroup class="calibre15"><col class="calibre16"/><col class="calibre16"/></colgroup><thead class="calibre17"><tr class="calibre18"><th valign="bottom" class="calibre19">
<p class="calibre20">Operator</p>
</th><th valign="bottom" class="calibre19">
<p class="calibre20">Description</p>
</th></tr></thead><tbody class="calibre21"><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.reshape</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">Reshape the dimension of the tensor</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.fill</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">Fill the array with the same value</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.flatten</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">Return all elements in a 1-dimensional tensor (vector)</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.dimshuffle</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">Change the order of the dimension, more or less like NumPy's transpose method – the main difference is that it can be used to add or remove broadcastable dimensions (of length 1).</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.squeeze</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">Reshape by removing dimensions equal to 1</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.transpose</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">Transpose</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.swapaxes</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">Swap dimensions</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.sort, T.argsort</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">Sort tensor, or indices of the order</p>
</td></tr></tbody></table></div><p class="calibre8">For example, the reshape operation's output represents a new tensor, containing the same elements in the same order but in a different shape:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; a = T.arange(10)

&gt;&gt;&gt; b = T.reshape( a, (5,2) )

&gt;&gt;&gt; b.eval()
array([[0, 1],
       [2, 3], 
       [4, 5],
       [6, 7],
       [8, 9]])</pre></div><p class="calibre8">The operators can be chained:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; T.arange(10).reshape((5,2))[::-1].T.eval()
array([[8, 6, 4, 2, 0],
       [9, 7, 5, 3, 1]])</pre></div><p class="calibre8">Notice the use of traditional <code class="email">[::-1]</code> array access by indices in Python and the <code class="email">.T</code> for <code class="email">T.transpose</code>.</p></div></div>

<div class="book" title="Operations on tensors">
<div class="book" title="Elementwise operators"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch01lvl2sec12" class="calibre1"/>Elementwise operators</h2></div></div></div><p class="calibre8">The <a id="id34" class="calibre1"/>second <a id="id35" class="calibre1"/>type of operations on multi-dimensional arrays is elementwise operators.</p><p class="calibre8">The  first category of elementwise operations takes two input tensors of the same dimensions and applies a function, <code class="email">f</code>, elementwise, which means on all pairs of elements with the same coordinates in the respective tensors <code class="email">f([a,b],[c,d]) = [ f(a,c), f(b,d)]</code>. For example, here's multiplication:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; a, b = T.matrices('a', 'b')

&gt;&gt;&gt; z = a * b

&gt;&gt;&gt; z.eval({a:numpy.ones((2,2)).astype(theano.config.floatX), b:numpy.diag((3,3)).astype(theano.config.floatX)})
array([[ 3.,  0.],
       [ 0.,  3.]])</pre></div><p class="calibre8">The <a id="id36" class="calibre1"/>same multiplication can be written as follows:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; z = T.mul(a, b)</pre></div><p class="calibre8">
<code class="email">T.add</code> and <code class="email">T.mul</code> accept an arbitrary number of inputs:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; z = T.mul(a, b, a, b)</pre></div><p class="calibre8">Some <a id="id37" class="calibre1"/>elementwise operators accept only one input tensor <code class="email">f([a,b]) = [f(a),f(b)])</code>:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; a = T.matrix()

&gt;&gt;&gt; z = a ** 2 

&gt;&gt;&gt; z.eval({a:numpy.diag((3,3)).astype(theano.config.floatX)})
array([[ 9.,  0.], 
       [ 0.,  9.]])</pre></div><p class="calibre8">Lastly, I would like to introduce the mechanism of <span class="strong"><strong class="calibre2">broadcasting</strong></span>. When the input tensors do not have the <a id="id38" class="calibre1"/>same number of dimensions, the missing dimension will be broadcasted, meaning the tensor will be repeated along that dimension to match the dimension of the other tensor. For example, taking one multi-dimensional tensor and a scalar (0-dimensional) tensor, the scalar will be repeated in an array of the same shape as the multi-dimensional tensor so that the final shapes will match and the elementwise operation will be applied, <code class="email">f([a,b], c) = [ f(a,c), f(b,c) ]</code>:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; a = T.matrix()

&gt;&gt;&gt; b = T.scalar()

&gt;&gt;&gt; z = a * b

&gt;&gt;&gt; z.eval({a:numpy.diag((3,3)).astype(theano.config.floatX),b:3})
array([[ 6.,  0.],
       [ 0.,  6.]])</pre></div><p class="calibre8">Here is a list of elementwise operations:</p><div class="informalexample"><table border="1" class="calibre14"><colgroup class="calibre15"><col class="calibre16"/><col class="calibre16"/><col class="calibre16"/></colgroup><thead class="calibre17"><tr class="calibre18"><th valign="bottom" class="calibre19">
<p class="calibre20">Operator</p>
</th><th valign="bottom" class="calibre19">
<p class="calibre20">Other form</p>
</th><th valign="bottom" class="calibre19">
<p class="calibre20">Description</p>
</th></tr></thead><tbody class="calibre21"><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.add, T.sub, T.mul, T.truediv</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">+, -, *, /</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">Add, subtract, multiply, divide</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.pow, T.sqrt</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">**, T.sqrt</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">Power, square root</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.exp, T.log</code>
</p>
</td><td valign="top" class="calibre22"> </td><td valign="top" class="calibre22">
<p class="calibre20">Exponential, logarithm</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.cos, T.sin, T.tan</code>
</p>
</td><td valign="top" class="calibre22"> </td><td valign="top" class="calibre22">
<p class="calibre20">Cosine, sine, tangent</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.cosh, T.sinh, T.tanh</code>
</p>
</td><td valign="top" class="calibre22"> </td><td valign="top" class="calibre22">
<p class="calibre20">Hyperbolic trigonometric functions</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.intdiv, T.mod</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">//, %</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">Int div, modulus</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.floor, T.ceil, T.round</code>
</p>
</td><td valign="top" class="calibre22"> </td><td valign="top" class="calibre22">
<p class="calibre20">Rounding operators</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.sgn</code>
</p>
</td><td valign="top" class="calibre22"> </td><td valign="top" class="calibre22">
<p class="calibre20">Sign</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.and_, T.xor, T.or_, T.invert</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">&amp;,^,|,~</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">Bitwise operators</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.gt, T.lt, T.ge, T.le</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">&gt;, &lt;, &gt;=, &lt;=</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">Comparison operators</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.eq, T.neq, T.isclose</code>
</p>
</td><td valign="top" class="calibre22"> </td><td valign="top" class="calibre22">
<p class="calibre20">Equality, inequality, or close with tolerance</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.isnan</code>
</p>
</td><td valign="top" class="calibre22"> </td><td valign="top" class="calibre22">
<p class="calibre20">Comparison with NaN (not a number)</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.abs_</code>
</p>
</td><td valign="top" class="calibre22"> </td><td valign="top" class="calibre22">
<p class="calibre20">Absolute value</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.minimum, T.maximum</code>
</p>
</td><td valign="top" class="calibre22"> </td><td valign="top" class="calibre22">
<p class="calibre20">Minimum and maximum elementwise</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.clip</code>
</p>
</td><td valign="top" class="calibre22"> </td><td valign="top" class="calibre22">
<p class="calibre20">Clip the values between a maximum and a minimum</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.switch </code>
</p>
</td><td valign="top" class="calibre22"> </td><td valign="top" class="calibre22">
<p class="calibre20">Switch</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.cast</code>
</p>
</td><td valign="top" class="calibre22"> </td><td valign="top" class="calibre22">
<p class="calibre20">Tensor type casting</p>
</td></tr></tbody></table></div><p class="calibre8">The elementwise operators always return an array with the same size as the input array. <code class="email">T.switch</code> and <code class="email">T.clip</code> accept three inputs.</p><p class="calibre8">In particular, <code class="email">T.switch</code> will perform the traditional <code class="email">switch </code>operator elementwise:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; cond = T.vector('cond')

&gt;&gt;&gt; x,y = T.vectors('x','y')

&gt;&gt;&gt; z = T.switch(cond, x, y)

&gt;&gt;&gt; z.eval({ cond:[1,0], x:[10,10], y:[3,2] })
array([ 10.,   2.], dtype=float32)</pre></div><p class="calibre8">At the <a id="id39" class="calibre1"/>same position where <code class="email">cond</code> tensor is true, the result has the <code class="email">x</code> value; otherwise, if it is false, it has the <code class="email">y</code> value.</p><p class="calibre8">For the <code class="email">T.switch</code> operator, there is a specific equivalent, <code class="email">ifelse</code>, that takes a scalar condition instead of a <a id="id40" class="calibre1"/>tensor condition. It is not an elementwise operation though, and supports lazy evaluation (not all elements are computed if the answer is known before it finishes):</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; <span class="strong"><strong class="calibre2">from</strong></span> theano.ifelse <span class="strong"><strong class="calibre2">import</strong></span> ifelse

&gt;&gt;&gt; z=ifelse(1, 5, 4)

&gt;&gt;&gt; z.eval()
array(5, dtype=int8)</pre></div></div></div>

<div class="book" title="Operations on tensors">
<div class="book" title="Reduction operators"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch01lvl2sec13" class="calibre1"/>Reduction operators</h2></div></div></div><p class="calibre8">Another <a id="id41" class="calibre1"/>type of operation on tensors is reductions, reducing all elements to a <a id="id42" class="calibre1"/>scalar value in most cases, and for that purpose, it is required to scan all the elements of the tensor to compute the output:</p><div class="informalexample"><table border="1" class="calibre14"><colgroup class="calibre15"><col class="calibre16"/><col class="calibre16"/></colgroup><thead class="calibre17"><tr class="calibre18"><th valign="bottom" class="calibre19">
<p class="calibre20">Operator</p>
</th><th valign="bottom" class="calibre19">
<p class="calibre20">Description</p>
</th></tr></thead><tbody class="calibre21"><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.max, T.argmax, T.max_and_argmax</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">Maximum, index of the maximum</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.min, T.argmin</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">Minimum, index of the minimum</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.sum, T.prod</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">Sum or product of elements</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.mean, T.var, T.std</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">Mean, variance, and standard deviation</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.all, T.any</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">AND and OR operations with all elements</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.ptp</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">Range of elements (minimum, maximum)</p>
</td></tr></tbody></table></div><p class="calibre8">These <a id="id43" class="calibre1"/>operations are also available row-wise or column-wise by specifying <a id="id44" class="calibre1"/>an axis and the dimension along which the reduction is performed:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; a = T.matrix('a')

&gt;&gt;&gt; T.max(a).eval({a:[[1,2],[3,4]]})
array(4.0, dtype=float32)

&gt;&gt;&gt; T.max(a,axis=0).eval({a:[[1,2],[3,4]]})
array([ 3.,  4.], dtype=float32)

&gt;&gt;&gt; T.max(a,axis=1).eval({a:[[1,2],[3,4]]})
array([ 2.,  4.], dtype=float32)</pre></div></div></div>

<div class="book" title="Operations on tensors">
<div class="book" title="Linear algebra operators"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch01lvl2sec14" class="calibre1"/>Linear algebra operators</h2></div></div></div><p class="calibre8">A third <a id="id45" class="calibre1"/>category of operations are the linear algebra operators, such as matrix multiplication:</p><div class="mediaobject"><img src="../images/00005.jpeg" alt="Linear algebra operators" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Also <a id="id46" class="calibre1"/>called inner product for vectors:</p><div class="mediaobject"><img src="../images/00006.jpeg" alt="Linear algebra operators" class="calibre9"/></div><p class="calibre10"> </p><div class="informalexample"><table border="1" class="calibre14"><colgroup class="calibre15"><col class="calibre16"/><col class="calibre16"/></colgroup><thead class="calibre17"><tr class="calibre18"><th valign="bottom" class="calibre19">
<p class="calibre20">Operator</p>
</th><th valign="bottom" class="calibre19">
<p class="calibre20">Description</p>
</th></tr></thead><tbody class="calibre21"><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.dot</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">Matrix multiplication/inner product</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">T.outer</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">Outer product</p>
</td></tr></tbody></table></div><p class="calibre8">There are some generalized (<code class="email">T.tensordot</code> to specify the axis), or batched (<code class="email">batched_dot, batched_tensordot</code>) versions of the operators.</p><p class="calibre8">Lastly, a few operators remain and can be very useful, but they do not belong to any of the previous categories: <code class="email">T.concatenate</code> concatenates the tensors along the specified dimension, <code class="email">T.stack</code> creates a new dimension to stack the input tensors, and <code class="email">T.stacklist</code> creates new patterns to stack tensors together:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; a = T.arange(10).reshape((5,2))

&gt;&gt;&gt; b = a[::-1]

&gt;&gt;&gt; b.eval()
array([[8, 9],
       [6, 7],
       [4, 5],
       [2, 3],
       [0, 1]])
&gt;&gt;&gt; a.eval()
array([[0, 1],
       [2, 3],
       [4, 5],
       [6, 7],
       [8, 9]])
&gt;&gt;&gt; T.concatenate([a,b]).eval()
array([[0, 1],
       [2, 3],
       [4, 5],
       [6, 7],
       [8, 9],
       [8, 9],
       [6, 7],
       [4, 5],
       [2, 3],
       [0, 1]])
&gt;&gt;&gt; T.concatenate([a,b],axis=1).eval()
array([[0, 1, 8, 9],
       [2, 3, 6, 7],
       [4, 5, 4, 5],
       [6, 7, 2, 3],
       [8, 9, 0, 1]])

&gt;&gt;&gt; T.stack([a,b]).eval()
array([[[0, 1],
        [2, 3],
        [4, 5],
        [6, 7],
        [8, 9]],
       [[8, 9],
        [6, 7],
        [4, 5],
        [2, 3],
        [0, 1]]])</pre></div><p class="calibre8">An <a id="id47" class="calibre1"/>equivalent <a id="id48" class="calibre1"/>of the NumPy expressions <code class="email">a[5:] = 5</code> and <code class="email">a[5:] += 5</code> exists as two functions:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; a.eval()
array([[0, 1],
       [2, 3],
       [4, 5],
       [6, 7],
       [8, 9]])

&gt;&gt;&gt; T.set_subtensor(a[3:], [-1,-1]).eval()

array([[ 0,  1],
       [ 2,  3],
       [ 4,  5],
       [-1, -1],
       [-1, -1]])

&gt;&gt;&gt; T.inc_subtensor(a[3:], [-1,-1]).eval()
array([[0, 1],
       [2, 3],
       [4, 5],
       [5, 6],
       [7, 8]])</pre></div><p class="calibre8">Unlike NumPy's syntax, the original tensor is not modified; instead, a new variable is created that represents <a id="id49" class="calibre1"/>the result of that modification. Therefore, the original <a id="id50" class="calibre1"/>variable <code class="email">a</code> still refers to the original value, and the returned variable (here unassigned) represents the updated one, and the user should use that new variable in the rest of their computation.</p></div></div>
<div class="book" title="Memory and variables" id="K0RQ1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec13" class="calibre1"/>Memory and variables</h1></div></div></div><p class="calibre8">It is <a id="id51" class="calibre1"/>good practice to always cast float arrays to the <code class="email">theano.config.floatX</code> type:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Either at the array creation with <code class="email">numpy.array(array, dtype=theano.config.floatX)</code></li><li class="listitem">Or by casting the array as <code class="email">array.as_type(theano.config.floatX)</code> so that when compiling on the GPU, the correct type is used</li></ul></div><p class="calibre8">For <a id="id52" class="calibre1"/>example, let's transfer the data manually to the GPU (for which the default context is None), and for that purpose, we need to use <code class="email">float32</code> values:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; theano.config.floatX = 'float32'

&gt;&gt;&gt; a = T.matrix()

&gt;&gt;&gt; b = a.transfer(None)

&gt;&gt;&gt; b.eval({a:numpy.ones((2,2)).astype(theano.config.floatX)})
gpuarray.array([[ 1.  1.]
 [ 1.  1.]], dtype=float32)

 &gt;&gt;&gt; theano.printing.debugprint(b)
GpuFromHost&lt;None&gt; [id A] ''   
 |&lt;TensorType(float32, matrix)&gt; [id B]</pre></div><p class="calibre8">The <code class="email">transfer(device)</code> functions, such as <code class="email">transfer('cpu')</code>, enable us to move the data from one device to another one. It is particularly useful when parts of the graph have to be executed on different devices. Otherwise, Theano adds the transfer functions automatically to the GPU in the optimization phase:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; a = T.matrix('a')

&gt;&gt;&gt; b = a ** 2

&gt;&gt;&gt; sq = theano.function([a],b)

&gt;&gt;&gt; theano.printing.debugprint(sq)
HostFromGpu(gpuarray) [id A] ''   2
 |GpuElemwise{Sqr}[(0, 0)]&lt;gpuarray&gt; [id B] ''   1
   |GpuFromHost&lt;None&gt; [id C] ''   0
     |a [id D]</pre></div><p class="calibre8">Using the transfer function explicitly, Theano removes the transfer back to CPU. Leaving the output tensor on the GPU saves a costly transfer:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; b = b.transfer(None)

&gt;&gt;&gt; sq = theano.function([a],b)

&gt;&gt;&gt; theano.printing.debugprint(sq)
GpuElemwise{Sqr}[(0, 0)]&lt;gpuarray&gt; [id A] ''   1
 |GpuFromHost&lt;None&gt; [id B] ''   0
   |a [id C]</pre></div><p class="calibre8">The <a id="id53" class="calibre1"/>default context for the CPU is <code class="email">cpu</code>:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; b = a.transfer('cpu')

&gt;&gt;&gt; theano.printing.debugprint(b)
&lt;TensorType(float32, matrix)&gt; [id A]</pre></div><p class="calibre8">A <a id="id54" class="calibre1"/>hybrid concept between numerical values and symbolic variables is the shared variables. They can also lead to better performance on the GPU by avoiding transfers. Initializing a shared variable with the scalar zero:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; state = shared(0)

&gt;&gt;&gt; state

&lt;TensorType(int64, scalar)&gt;

&gt;&gt;&gt; state.get_value()
array(0)

&gt;&gt;&gt; state.set_value(1)

&gt;&gt;&gt; state.get_value()
array(1)</pre></div><p class="calibre8">Shared values are designed to be shared between functions. They can also be seen as an internal state. They can be used indifferently from the GPU or the CPU compile code. By default, shared variables are created on the default device (here, <code class="email">cuda</code>), except for scalar integer values (as is the case in the previous example).</p><p class="calibre8">It is possible to specify another context, such as <code class="email">cpu</code>. In the case of multiple GPU instances, you'll define your contexts in the Python command line, and decide on which context to create the shared variables:</p><div class="informalexample"><pre class="programlisting">PATH=/usr/local/cuda-8.0-cudnn-5.1/bin:$PATH THEANO_FLAGS="contexts=dev0-&gt;cuda0;dev1-&gt;cuda1,floatX=float32,gpuarray.preallocate=0.8" <span class="strong"><strong class="calibre2">python</strong></span>
</pre></div><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from theano import theano
Using cuDNN version 5110 on context dev0
Preallocating 9151/11439 Mb (0.800000) on cuda0
Mapped name dev0 to device cuda0: Tesla K80 (0000:83:00.0)
Using cuDNN version 5110 on context dev1
Preallocating 9151/11439 Mb (0.800000) on cuda1
Mapped name dev1 to device cuda1: Tesla K80 (0000:84:00.0)

&gt;&gt;&gt; import theano.tensor as T

&gt;&gt;&gt; import numpy

&gt;&gt;&gt; theano.shared(numpy.random.random((1024, 1024)).astype('float32'),target='dev1')
&lt;GpuArrayType&lt;dev1&gt;(float32, (False, False))&gt;</pre></div></div>
<div class="book" title="Functions and automatic differentiation" id="KVCC1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec14" class="calibre1"/>Functions and automatic differentiation</h1></div></div></div><p class="calibre8">The <a id="id55" class="calibre1"/>previous section introduced the <code class="email">function</code> instruction to compile the expression. In this section, we develop some of the following arguments in its signature:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">def</strong></span> theano.function(inputs, 
	<span class="strong"><strong class="calibre2">outputs</strong></span>=None, <span class="strong"><strong class="calibre2">updates</strong></span>=None, <span class="strong"><strong class="calibre2">givens</strong></span>=None,
 <span class="strong"><strong class="calibre2">allow_input_downcast</strong></span>=None, <span class="strong"><strong class="calibre2">mode</strong></span>=None, <span class="strong"><strong class="calibre2">profile</strong></span>=None,
  	)</pre></div><p class="calibre8">We've <a id="id56" class="calibre1"/>already used the <code class="email">allow_input_downcast</code> feature to convert data from <code class="email">float64</code> to <code class="email">float32</code>, <code class="email">int64</code> to <code class="email">int32</code> and so on. The <code class="email">mode</code>
<span class="strong"><strong class="calibre2"> </strong></span>and <code class="email">profile</code>
<span class="strong"><strong class="calibre2"> </strong></span>features are also displayed because they'll be presented in the optimization and debugging section.</p><p class="calibre8">Input variables of a Theano function should be contained in a list, even when there is a single input.</p><p class="calibre8">For outputs, it is possible to use a list in the case of multiple outputs to be computed in parallel:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; a = T.matrix()

&gt;&gt;&gt; ex = theano.function([a],[T.exp(a),T.log(a),a**2])

&gt;&gt;&gt; ex(numpy.random.randn(3,3).astype(theano.config.floatX))
[array([[ 2.33447003,  0.30287042,  0.63557744],
       [ 0.18511547,  1.34327984,  0.42203984],
       [ 0.87083125,  5.01169062,  6.88732481]], dtype=float32),
array([[-0.16512829,         nan,         nan],
       [        nan, -1.2203927 ,         nan],
       [        nan,  0.47733498,  0.65735561]], dtype=float32),
array([[ 0.71873927,  1.42671108,  0.20540957],
       [ 2.84521151,  0.08709242,  0.74417454],
       [ 0.01912885,  2.59781313,  3.72367549]], dtype=float32)]</pre></div><p class="calibre8">The second useful attribute is the <code class="email">updates</code> attribute, used to set new values to shared variables once the expression has been evaluated:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; w = shared(1.0)

&gt;&gt;&gt; x = T.scalar('x')

&gt;&gt;&gt; mul = theano.function([x],updates=[(w,w*x)])

&gt;&gt;&gt; mul(4)
[]

&gt;&gt;&gt; w.get_value()
array(4.0)</pre></div><p class="calibre8">Such a mechanism can be used as an internal state. The shared variable <code class="email">w</code> has been defined outside the function.</p><p class="calibre8">With the <code class="email">givens</code> parameter, it is possible to change the value of any symbolic variable in the graph, without changing the graph. The new value will then be used by all the other expressions that were pointing to it.</p><p class="calibre8">The last and most important feature in Theano is the automatic differentiation, which means that Theano computes the derivatives of all previous tensor operators. Such a differentiation is performed via the <code class="email">theano.grad</code> operator:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; a = T.scalar()

&gt;&gt;&gt; pow = a ** 2

&gt;&gt;&gt; g = theano.grad(pow,a)

&gt;&gt;&gt; theano.printing.pydotprint(g)

&gt;&gt;&gt; theano.printing.pydotprint(theano.function([a],g))</pre></div><div class="mediaobject"><img src="../images/00007.jpeg" alt="Functions and automatic differentiation" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">In <a id="id57" class="calibre1"/>the optimization graph, <code class="email">theano.grad</code> has computed the gradient of <span class="strong"><img src="../images/00008.jpeg" alt="Functions and automatic differentiation" class="calibre23"/></span> with respect to <code class="email">a</code>, which is a symbolic expression equivalent to <span class="strong"><em class="calibre12">2 * a</em></span>.</p><p class="calibre8">Note <a id="id58" class="calibre1"/>that it is only possible to take the gradient of a scalar, but the <span class="strong"><em class="calibre12">wrt</em></span> variables can be arbitrary tensors.</p></div>
<div class="book" title="Loops in symbolic computing"><div class="book" id="LTSU2-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec15" class="calibre1"/>Loops in symbolic computing</h1></div></div></div><p class="calibre8">The <a id="id59" class="calibre1"/>Python <code class="email">for</code> loop can be used outside the symbolic graph, as in a normal Python program. But outside the graph, a traditional Python <code class="email">for</code> loop isn't compiled, so it will <a id="id60" class="calibre1"/>not be optimized with parallel and algebra libraries, cannot be automatically differentiated, and introduces costly data transfers if the computation subgraph has been optimized for GPU.</p><p class="calibre8">That's why a symbolic operator, <code class="email">T.scan</code>, is designed to create a <code class="email">for</code> loop as an operator inside the graph. Theano will unroll the loop into the graph structure and the whole unrolled loop is going to be compiled on the target architecture as the rest of the computation graph. Its signature is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">def</strong></span> scan(fn,
         <span class="strong"><strong class="calibre2">sequences</strong></span>=None,
         <span class="strong"><strong class="calibre2">outputs_info</strong></span>=None,
         <span class="strong"><strong class="calibre2">non_sequences</strong></span>=None,
         <span class="strong"><strong class="calibre2">n_steps</strong></span>=None,
         <span class="strong"><strong class="calibre2">truncate_gradient</strong></span>=-1,
         <span class="strong"><strong class="calibre2">go_backwards</strong></span>=False,
         <span class="strong"><strong class="calibre2">mode</strong></span>=None,
         <span class="strong"><strong class="calibre2">name</strong></span>=None,
         <span class="strong"><strong class="calibre2">profile</strong></span>=False,
         <span class="strong"><strong class="calibre2">allow_gc</strong></span>=None,
         <span class="strong"><strong class="calibre2">strict</strong></span>=False)</pre></div><p class="calibre8">The <code class="email">scan</code> operator is very useful to implement array loops, reductions, maps, multi-dimensional derivatives such as Jacobian or Hessian, and recurrences.</p><p class="calibre8">The <code class="email">scan</code> operator is running the <code class="email">fn</code> function repeatedly for <code class="email">n_steps</code>. If <code class="email">n_steps</code> is <code class="email">None</code>, the operator will find out by the length of the sequences:</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note03" class="calibre1"/>Note</h3><p class="calibre8">The step <code class="email">fn</code> function is a function that builds a symbolic graph, and that function will only get called once. However, that graph will then be compiled into another Theano function that will be called repeatedly. Some users try to pass a compile Theano function as <code class="email">fn</code>, which is not possible.</p></div><p class="calibre8">Sequences <a id="id61" class="calibre1"/>are the lists of input variables to loop over. The number <a id="id62" class="calibre1"/>of steps will correspond to the shortest sequence in the list. Let's have a look:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; a = T.matrix()

&gt;&gt;&gt; b = T.matrix()

&gt;&gt;&gt; def fn(x): return x + 1

&gt;&gt;&gt; results, updates = theano.scan(fn, sequences=a)

&gt;&gt;&gt; f = theano.function([a], results, updates=updates)

&gt;&gt;&gt; f(numpy.ones((2,3)).astype(theano.config.floatX))

array([[ 2.,  2.,  2.],
       [ 2.,  2.,  2.]], dtype=float32)</pre></div><p class="calibre8">The <code class="email">scan</code> operator has been running the function against all elements in the input tensor, <code class="email">a</code>, and kept the same shape as the input tensor, <code class="email">(2,3)</code>.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note04" class="calibre1"/>Note</h3><p class="calibre8">It is a good practice to add the updates returned by <code class="email">theano.scan</code> in the <code class="email">theano.function</code>, even if these updates are empty.</p></div><p class="calibre8">The arguments given to the <code class="email">fn</code> function can be much more complicated. <code class="email">T.scan</code> will call the <code class="email">fn</code> function at each step with the following argument list, in the following order:</p><div class="informalexample"><pre class="programlisting">fn( <span class="strong"><strong class="calibre2">sequences</strong></span> (if any), <span class="strong"><strong class="calibre2">prior results</strong></span> (if needed), <span class="strong"><strong class="calibre2">non-sequences</strong></span> (if any) )</pre></div><p class="calibre8">As shown in the following figure, three arrows are directed towards the <code class="email">fn</code> step function and represent the three types of possible input at each time step in the loop:</p><div class="mediaobject"><img src="../images/00009.jpeg" alt="Loops in symbolic computing" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">If specified, the <code class="email">outputs_info</code> parameter is the initial state to use to start recurrence from. The parameter <a id="id63" class="calibre1"/>name does not sound very good, but the initial state also <a id="id64" class="calibre1"/>gives the shape information of the last state, as well as all other states. The initial state can be seen as the first output. The final output will be an array of states.</p><p class="calibre8">For example, to compute the cumulative sum in a vector, with an initial state of the sum at <code class="email">0</code>, use this code:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; a = T.vector()

&gt;&gt;&gt; s0 = T.scalar("s0")

&gt;&gt;&gt; def fn( current_element, prior ):
...   return prior + current_element

&gt;&gt;&gt; results, updates = theano.scan(fn=fn,outputs_info=s0,sequences=a)

&gt;&gt;&gt; f = theano.function([a,s0], results, updates=updates)

&gt;&gt;&gt; f([0,3,5],0)
<span class="strong"><em class="calibre12">array([ 0.,  3.,  8.], dtype=float32)</em></span>
</pre></div><p class="calibre8">When <code class="email">outputs_info</code> is set, the first dimension of the <code class="email">outputs_info</code> and sequence variables is the time step. The second dimension is the dimensionality of data at each time step.</p><p class="calibre8">In particular, <code class="email">outputs_info</code> has the number of previous time-steps required to compute the first step.</p><p class="calibre8">Here is the same example, but with a vector at each time step instead of a scalar for the input data:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; a = T.matrix()

&gt;&gt;&gt; s0 = T.scalar("s0")

&gt;&gt;&gt; def fn( current_element, prior ):
...   return prior + current_element.sum()

&gt;&gt;&gt; results, updates = theano.scan(fn=fn,outputs_info=s0,sequences=a)

&gt;&gt;&gt; f = theano.function([a,s0], results, updates=updates)

&gt;&gt;&gt; f(numpy.ones((20,5)).astype(theano.config.floatX),0)

array([   5.,   10.,   15.,   20.,   25.,   30.,   35.,   40.,   45.,
         50.,   55.,   60.,   65.,   70.,   75.,   80.,   85.,   90.,
         95.,  100.], dtype=float32)</pre></div><p class="calibre8">Twenty <a id="id65" class="calibre1"/>steps along the rows (times) have accumulated the sum of all elements. Note that initial state (here <code class="email">0</code>) given by the <code class="email">outputs_info</code> argument is not part of the output <a id="id66" class="calibre1"/>sequence.</p><p class="calibre8">The recurrent function, <code class="email">fn</code>, may be provided with some fixed data, independent of the step in the loop, thanks to the <code class="email">non_sequences</code> scan parameter:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; a = T.vector()

&gt;&gt;&gt; s0 = T.scalar("s0")

&gt;&gt;&gt; def fn( current_element, prior, non_seq ):
...   return non_seq * prior + current_element

&gt;&gt;&gt; results, updates = theano.scan(fn=fn,n_steps=10,sequences=a,outputs_info=T.constant(0.0),non_sequences=s0)

&gt;&gt;&gt; f = theano.function([a,s0], results, updates=updates)

&gt;&gt;&gt; f(numpy.ones((20)).astype(theano.),5)
array([  1.00000000e+00,   6.00000000e+00,   3.10000000e+01,
         1.56000000e+02,   7.81000000e+02,   3.90600000e+03,
         1.95310000e+04,   9.76560000e+04,   4.88281000e+05,
         2.44140600e+06], dtype=float32)</pre></div><p class="calibre8">It is multiplying the prior value by <code class="email">5</code> and adding the new element.</p><p class="calibre8">Note that <code class="email">T.scan</code> in the optimized graph on GPU does not execute different iterations of the loop in parallel, even in the absence of recurrence.</p></div>
<div class="book" title="Configuration, profiling and debugging" id="MSDG1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec16" class="calibre1"/>Configuration, profiling and debugging</h1></div></div></div><p class="calibre8">For <a id="id67" class="calibre1"/>debugging <a id="id68" class="calibre1"/>purpose, Theano can print more verbose <a id="id69" class="calibre1"/>information and offers different optimization modes:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; theano.config.exception_verbosity='high'

&gt;&gt;&gt; theano.config.mode
'Mode'

&gt;&gt;&gt; theano.config.optimizer='fast_compile'</pre></div><p class="calibre8">In order for Theano to use the <code class="email">config.optimizer</code> value, the mode has to be set to <code class="email">Mode</code>, otherwise the value in <code class="email">config.mode</code> will be used:</p><div class="informalexample"><table border="1" class="calibre14"><colgroup class="calibre15"><col class="calibre16"/><col class="calibre16"/><col class="calibre16"/></colgroup><thead class="calibre17"><tr class="calibre18"><th valign="bottom" class="calibre19">
<p class="calibre20">config.mode / function mode</p>
</th><th valign="bottom" class="calibre19">
<p class="calibre20">config.optimizer (*)</p>
</th><th valign="bottom" class="calibre19">
<p class="calibre20">Description</p>
</th></tr></thead><tbody class="calibre21"><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">FAST_RUN</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">fast_run</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">Default; best run performance, slow compilation</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">FAST_RUN</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">None</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">Disable optimizations</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">FAST_COMPILE</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">fast_compile</code>
</p>
</td><td valign="top" class="calibre22">
<p class="calibre20">Reduce the number of optimizations, compiles faster</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">None</code>
</p>
</td><td valign="top" class="calibre22"> </td><td valign="top" class="calibre22">
<p class="calibre20">Use the default mode, equivalent to <code class="literal">FAST_RUN</code>; <code class="literal">optimizer=None</code>
</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">NanGuardMode</code>
</p>
</td><td valign="top" class="calibre22"> </td><td valign="top" class="calibre22">
<p class="calibre20">NaNs, Infs, and abnormally big value will raise errors</p>
</td></tr><tr class="calibre18"><td valign="top" class="calibre22">
<p class="calibre20">
<code class="literal">DebugMode</code>
</p>
</td><td valign="top" class="calibre22"> </td><td valign="top" class="calibre22">
<p class="calibre20">Self-checks and assertions during compilation</p>
</td></tr></tbody></table></div><p class="calibre8">The same parameter as in <code class="email">config.mode</code> can be used in the <code class="email">Mode</code> parameter in the function compile:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; f = theano.function([a,s0], results, updates=updates, mode='FAST_COMPILE')</pre></div><p class="calibre8">Disabling <a id="id70" class="calibre1"/>optimization and choosing high verbosity will help finding errors <a id="id71" class="calibre1"/>in the computation graph.</p><p class="calibre8">For <a id="id72" class="calibre1"/>debugging on the GPU, you need to set a synchronous execution with the environment variable <code class="email">CUDA_LAUNCH_BLOCKING</code>, since GPU execution is by default, fully asynchronous:</p><div class="informalexample"><pre class="programlisting">  CUDA_LAUNCH_BLOCKING=1 <span class="strong"><strong class="calibre2">python</strong></span>
</pre></div><p class="calibre8">To find out the origin of the latencies in your computation graph, Theano provides a profiling mode.</p><p class="calibre8">Activate profiling:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; theano.config.profile=True </pre></div><p class="calibre8">Activate memory profiling:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; theano.config.profile_memory=True</pre></div><p class="calibre8">Activate profiling of optimization phase:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; theano.config.profile_optimizer=True </pre></div><p class="calibre8">Or directly during compilation:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; f = theano.function([a,s0], results, profile=True)

&gt;&gt;&gt; f.profile.summary()
Function profiling
==================
  Message: &lt;stdin&gt;:1
  Time in 1 calls to Function.__call__: 1.490116e-03s
  Time in Function.fn.__call__: 1.251936e-03s (84.016%)
  Time in thunks: 1.203537e-03s (80.768%)
  Total compile time: 1.720619e-01s
    Number of Apply nodes: 14
    Theano Optimizer time: 1.382768e-01s
       Theano validate time: 1.308680e-03s
    Theano Linker time (includes C, CUDA code generation/compiling): 2.405691e-02s
       Import time 1.272917e-03s
       Node make_thunk time 2.329803e-02s

Time in all call to theano.grad() 0.000000e+00s
Time since theano import 520.661s
Class
---
&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;type&gt; &lt;#call&gt; &lt;#apply&gt; &lt;Class name&gt;
  58.2%    58.2%       0.001s       7.00e-04s     Py       1       1   theano.scan_module.scan_op.Scan
  27.3%    85.4%       0.000s       1.64e-04s     Py       2       2   theano.sandbox.cuda.basic_ops.GpuFromHost
   6.1%    91.5%       0.000s       7.30e-05s     Py       1       1   theano.sandbox.cuda.basic_ops.HostFromGpu
   5.5%    97.0%       0.000s       6.60e-05s     C        1       1   theano.sandbox.cuda.basic_ops.GpuIncSubtensor
   1.1%    98.0%       0.000s       3.22e-06s     C        4       4   theano.tensor.elemwise.Elemwise
   0.7%    98.8%       0.000s       8.82e-06s     C        1       1   theano.sandbox.cuda.basic_ops.GpuSubtensor
   0.7%    99.4%       0.000s       7.87e-06s     C        1       1   theano.sandbox.cuda.basic_ops.GpuAllocEmpty
   0.3%    99.7%       0.000s       3.81e-06s     C        1       1   theano.compile.ops.Shape_i
   0.3%   100.0%       0.000s       1.55e-06s     C        2       2   theano.tensor.basic.ScalarFromTensor
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;type&gt; &lt;#call&gt; &lt;#apply&gt; &lt;Op name&gt;
  58.2%    58.2%       0.001s       7.00e-04s     Py       1        1   forall_inplace,gpu,scan_fn}
  27.3%    85.4%       0.000s       1.64e-04s     Py       2        2   GpuFromHost
   6.1%    91.5%       0.000s       7.30e-05s     Py       1        1   HostFromGpu
   5.5%    97.0%       0.000s       6.60e-05s     C        1        1   GpuIncSubtensor{InplaceSet;:int64:}
   0.7%    97.7%       0.000s       8.82e-06s     C        1        1   GpuSubtensor{int64:int64:int16}
   0.7%    98.4%       0.000s       7.87e-06s     C        1        1   GpuAllocEmpty
   0.3%    98.7%       0.000s       4.05e-06s     C        1        1   Elemwise{switch,no_inplace}
   0.3%    99.0%       0.000s       4.05e-06s     C        1        1   Elemwise{le,no_inplace}
   0.3%    99.3%       0.000s       3.81e-06s     C        1        1   Shape_i{0}
   0.3%    99.6%       0.000s       1.55e-06s     C        2        2   ScalarFromTensor
   0.2%    99.8%       0.000s       2.86e-06s     C        1        1   Elemwise{Composite{Switch(LT(i0, i1), i0, i1)}}
   0.2%   100.0%       0.000s       1.91e-06s     C        1        1   Elemwise{Composite{Switch(i0, i1, minimum(i2, i3))}}[(0, 2)]
   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)

Apply
------
&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;#call&gt; &lt;id&gt; &lt;Apply name&gt;
  58.2%    58.2%       0.001s       7.00e-04s      1    12   forall_inplace,gpu,scan_fn}(TensorConstant{10}, GpuSubtensor{int64:int64:int16}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, GpuFromHost.0)
  21.9%    80.1%       0.000s       2.64e-04s      1     3   GpuFromHost(&lt;TensorType(float32, vector)&gt;)
   6.1%    86.2%       0.000s       7.30e-05s      1    13   HostFromGpu(forall_inplace,gpu,scan_fn}.0)
   5.5%    91.6%       0.000s       6.60e-05s      1     4   GpuIncSubtensor{InplaceSet;:int64:}(GpuAllocEmpty.0, CudaNdarrayConstant{[ 0.]}, Constant{1})
   5.3%    97.0%       0.000s       6.41e-05s      1     0   GpuFromHost(s0)
   0.7%    97.7%       0.000s       8.82e-06s      1    11   GpuSubtensor{int64:int64:int16}(GpuFromHost.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1})
   0.7%    98.4%       0.000s       7.87e-06s      1     1   GpuAllocEmpty(TensorConstant{10})
   0.3%    98.7%       0.000s       4.05e-06s      1     8   Elemwise{switch,no_inplace}(Elemwise{le,no_inplace}.0, TensorConstant{0}, TensorConstant{0})
   0.3%    99.0%       0.000s       4.05e-06s      1     6   Elemwise{le,no_inplace}(Elemwise{Composite{Switch(LT(i0, i1), i0, i1)}}.0, TensorConstant{0})
   0.3%    99.3%       0.000s       3.81e-06s      1     2   Shape_i{0}(&lt;TensorType(float32, vector)&gt;)
   0.3%    99.6%       0.000s       3.10e-06s      1    10   ScalarFromTensor(Elemwise{switch,no_inplace}.0)
   0.2%    99.8%       0.000s       2.86e-06s      1     5   Elemwise{Composite{Switch(LT(i0, i1), i0, i1)}}(TensorConstant{10}, Shape_i{0}.0)
   0.2%   100.0%       0.000s       1.91e-06s      1     7   Elemwise{Composite{Switch(i0, i1, minimum(i2, i3))}}[(0, 2)](Elemwise{le,no_inplace}.0, TensorConstant{0}, Elemwise{Composite{Switch(LT(i0, i1), i0, i1)}}.0, Shape_i{0}.0)
   0.0%   100.0%       0.000s       0.00e+00s      1     9   ScalarFromTensor(Elemwise{Composite{Switch(i0, i1, minimum(i2, i3))}}[(0, 2)].0)
   ... (remaining 0 Apply instances account for 0.00%(0.00s) of the runtime)</pre></div></div>
<div class="book" title="Summary" id="NQU21-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch01lvl1sec17" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">The first concept is symbolic computing, which consists in building graph, that can be compiled and then executed wherever we decide in the Python code. A compiled graph is acting as a function that can be called anywhere in the code. The purpose of symbolic computing is to have an abstraction of the architecture on which the graph will be executed, and which libraries to compile it with. As presented, symbolic variables are typed for the target architecture during compilation.</p><p class="calibre8">The second concept is the tensor, and the operators provided to manipulate tensors. Most of these were already available in CPU-based computation libraries, such as NumPy or SciPy. They have simply been ported to symbolic computing, requiring their equivalents on GPU. They use underlying acceleration libraries, such as BLAS, Nvidia Cuda, and cuDNN.</p><p class="calibre8">The last concept introduced by Theano is automatic differentiation—a very useful feature in deep learning to backpropagate errors and adjust the weights following the gradients, a process known as <span class="strong"><em class="calibre12">gradient descent</em></span>. Also, the <code class="email">scan</code> operator enables us to program loops (<code class="email">while...</code>, <code class="email">for...</code>,) on the GPU, and, as other operators, available through backpropagation as well, simplifying the training of models a lot.</p><p class="calibre8">We are now ready to apply this to deep learning in the next few chapters and have a look at this knowledge in practice.</p></div></body></html>