["```py\nimport org.apache.spark.sql.SparkSession \nval spark: SparkSession = SparkSession \n    .builder() \n    .appName(\"MovieSimilarityApp\") \n    .master(\"local[*]\") \n    .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\") \n    .getOrCreate() \n```", "```py\nval TRAIN_FILENAME = \"data/ua.base\" \nval TEST_FIELNAME = \"data/ua.test\" \nval MOVIES_FILENAME = \"data/u.item\" \n\n  // get movie names keyed on id \nval movies = spark.sparkContext.textFile(MOVIES_FILENAME) \n    .map(line => { \n      val fields = line.split(\"\\|\") \n      (fields(0).toInt, fields(1)) \n    }) \nval movieNames = movies.collectAsMap() \n  // extract (userid, movieid, rating) from ratings data \nval ratings = spark.sparkContext.textFile(TRAIN_FILENAME) \n    .map(line => { \n      val fields = line.split(\"t\") \n      (fields(0).toInt, fields(1).toInt, fields(2).toInt) \n    }) \n```", "```py\n// get num raters per movie, keyed on movie id \nval numRatersPerMovie = ratings \n    .groupBy(tup => tup._2) \n    .map(grouped => (grouped._1, grouped._2.size)) \n\n// join ratings with num raters on movie id \nval ratingsWithSize = ratings \n    .groupBy(tup => tup._2) \n    .join(numRatersPerMovie) \n    .flatMap(joined => { \n      joined._2._1.map(f => (f._1, f._2, f._3, joined._2._2)) \n    }) \n```", "```py\nval ratings2 = ratingsWithSize.keyBy(tup => tup._1) \nval ratingPairs = \n    ratingsWithSize \n      .keyBy(tup => tup._1) \n      .join(ratings2) \n      .filter(f => f._2._1._2 < f._2._2._2) \n```", "```py\nval vectorCalcs = ratingPairs \n      .map(data => { \n        val key = (data._2._1._2, data._2._2._2) \n        val stats = \n          (data._2._1._3 * data._2._2._3, // rating 1 * rating 2 \n            data._2._1._3, // rating movie 1 \n            data._2._2._3, // rating movie 2 \n            math.pow(data._2._1._3, 2), // square of rating movie 1 \n            math.pow(data._2._2._3, 2), // square of rating movie 2 \n            data._2._1._4, // number of raters movie 1 \n            data._2._2._4) // number of raters movie 2 \n        (key, stats) \n      }) \n.groupByKey() \n.map(data => { \n    val key = data._1 \n    val vals = data._2 \n    val size = vals.size \n    val dotProduct = vals.map(f => f._1).sum \n    val ratingSum = vals.map(f => f._2).sum \n    val rating2Sum = vals.map(f => f._3).sum \n    val ratingSq = vals.map(f => f._4).sum \n    val rating2Sq = vals.map(f => f._5).sum \n    val numRaters = vals.map(f => f._6).max \n    val numRaters2 = vals.map(f => f._7).max \n        (key, (size, dotProduct, ratingSum, rating2Sum, ratingSq, rating2Sq, numRaters, numRaters2))}) \n```", "```py\n  val similarities = \n    vectorCalcs \n      .map(fields => { \n        val key = fields._1 \n        val (size, dotProduct, ratingSum, rating2Sum, ratingNormSq, rating2NormSq, numRaters, numRaters2) = fields._2 \n        val corr = correlation(size, dotProduct, ratingSum, rating2Sum, ratingNormSq, rating2NormSq) \n        val regCorr = regularizedCorrelation(size, dotProduct, ratingSum, rating2Sum,ratingNormSq, rating2NormSq, PRIOR_COUNT, PRIOR_CORRELATION) \n        val cosSim = cosineSimilarity(dotProduct, scala.math.sqrt(ratingNormSq), scala.math.sqrt(rating2NormSq)) \n        val jaccard = jaccardSimilarity(size, numRaters, numRaters2) \n        (key, (corr, regCorr, cosSim, jaccard))}) \n```", "```py\ndef correlation(size: Double, dotProduct: Double, ratingSum: Double, \n    rating2Sum: Double, ratingNormSq: Double, rating2NormSq: Double) = { \n    val numerator = size * dotProduct - ratingSum * rating2Sum \n    val denominator = scala.math.sqrt(size * ratingNormSq - ratingSum * ratingSum)  \n                        scala.math.sqrt(size * rating2NormSq - rating2Sum * rating2Sum) \n    numerator / denominator} \n```", "```py\ndef regularizedCorrelation(size: Double, dotProduct: Double, ratingSum: Double, \n    rating2Sum: Double, ratingNormSq: Double, rating2NormSq: Double, \n    virtualCount: Double, priorCorrelation: Double) = { \n    val unregularizedCorrelation = correlation(size, dotProduct, ratingSum, rating2Sum, ratingNormSq, rating2NormSq) \n    val w = size / (size + virtualCount) \n    w * unregularizedCorrelation + (1 - w) * priorCorrelation \n  } \n```", "```py\ndef cosineSimilarity(dotProduct: Double, ratingNorm: Double, rating2Norm: Double) = { \n    dotProduct / (ratingNorm * rating2Norm) \n  } \n```", "```py\ndef jaccardSimilarity(usersInCommon: Double, totalUsers1: Double, totalUsers2: Double) = { \n    val union = totalUsers1 + totalUsers2 - usersInCommon \n    usersInCommon / union \n    } \n```", "```py\nevaluateModel(\"Die Hard (1988)\") \n>>>\n```", "```py\nevaluateModel(\"Postino, Il (1994)\") \n>>>\n```", "```py\nevaluateModel(\"Star Wars (1977)\") \n>>>\n```", "```py\ndef evaluateModel(movieName: String): Unit = { \n    val sample = similarities.filter(m => { \n    val movies = m._1\n    (movieNames(movies._1).contains(movieName)) \n    }) \n// collect results, excluding NaNs if applicable \nval result = sample.map(v => { \nval m1 = v._1._1 \nval m2 = v._1._2 \nval corr = v._2._1 \nval rcorr = v._2._2 \nval cos = v._2._3 \nval j = v._2._4 \n(movieNames(m1), movieNames(m2), corr, rcorr, cos, j) \n}).collect().filter(e => !(e._4 equals Double.NaN)) // test for NaNs must use equals rather than == \n      .sortBy(elem => elem._4).take(10) \n    // print the top 10 out \nresult.foreach(r => println(r._1 + \" | \" + r._2 + \" | \" + r._3.formatted(\"%2.4f\") + \" | \" + r._4.formatted(\"%2.4f\") \n      + \" | \" + r._5.formatted(\"%2.4f\") + \" | \" + r._6.formatted(\"%2.4f\"))) } \n```", "```py\npackage com.packt.ScalaML.MovieRecommendation \nimport org.apache.spark.sql.SparkSession \nimport org.apache.spark.mllib.recommendation.ALS \nimport org.apache.spark.mllib.recommendation.MatrixFactorizationModel \nimport org.apache.spark.mllib.recommendation.Rating \nimport scala.Tuple2 \nimport org.apache.spark.rdd.RDD \n```", "```py\nval ratigsFile = \"data/ratings.csv\"\nval df1 = spark.read.format(\"com.databricks.spark.csv\").option(\"header\", true).load(ratigsFile)    \nval ratingsDF = df1.select(df1.col(\"userId\"), df1.col(\"movieId\"), df1.col(\"rating\"), df1.col(\"timestamp\"))\nratingsDF.show(false)\n```", "```py\nval moviesFile = \"data/movies.csv\"\nval df2 = spark.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(moviesFile)\nval moviesDF = df2.select(df2.col(\"movieId\"), df2.col(\"title\"), df2.col(\"genres\"))\n```", "```py\nratingsDF.createOrReplaceTempView(\"ratings\")\nmoviesDF.createOrReplaceTempView(\"movies\")\n```", "```py\nval numRatings = ratingsDF.count()\nval numUsers = ratingsDF.select(ratingsDF.col(\"userId\")).distinct().count()\nval numMovies = ratingsDF.select(ratingsDF.col(\"movieId\")).distinct().count() \nprintln(\"Got \" + numRatings + \" ratings from \" + numUsers + \" users on \" + numMovies + \" movies.\") \n>>>\nGot 105339 ratings from 668 users on 10325 movies.\n```", "```py\n// Get the max, min ratings along with the count of users who have rated a movie.\nval results = spark.sql(\"select movies.title, movierates.maxr, movierates.minr, movierates.cntu \"\n       + \"from(SELECT ratings.movieId,max(ratings.rating) as maxr,\"\n       + \"min(ratings.rating) as minr,count(distinct userId) as cntu \"\n       + \"FROM ratings group by ratings.movieId) movierates \"\n       + \"join movies on movierates.movieId=movies.movieId \" + \"order by movierates.cntu desc\") \nresults.show(false) \n```", "```py\nval mostActiveUsersSchemaRDD = spark.sql(\"SELECT ratings.userId, count(*) as ct from ratings \"+ \"group by ratings.userId order by ct desc limit 10\")\nmostActiveUsersSchemaRDD.show(false) \n>>> \n```", "```py\nval results2 = spark.sql( \n              \"SELECT ratings.userId, ratings.movieId,\"  \n              + \"ratings.rating, movies.title FROM ratings JOIN movies\" \n              + \"ON movies.movieId=ratings.movieId\"  \n              + \"where ratings.userId=668 and ratings.rating > 4\") \nresults2.show(false) \n>>>\n```", "```py\n// Split ratings RDD into training RDD (75%) & test RDD (25%) \nval splits = ratingsDF.randomSplit(Array(0.75, 0.25), seed = 12345L) \nval (trainingData, testData) = (splits(0), splits(1)) \nval numTraining = trainingData.count() \nval numTest = testData.count() \nprintln(\"Training: \" + numTraining + \" test: \" + numTest)\n```", "```py\nval ratingsRDD = trainingData.rdd.map(row => { \n                    val userId = row.getString(0) \n                    val movieId = row.getString(1) \n                    val ratings = row.getString(2) \n                    Rating(userId.toInt, movieId.toInt, ratings.toDouble)\n})\n```", "```py\nval testRDD = testData.rdd.map(row => { \n    val userId = row.getString(0) \n    val movieId = row.getString(1) \n    val ratings = row.getString(2) \n    Rating(userId.toInt, movieId.toInt, ratings.toDouble)\n})\n```", "```py\nval rank = 20 \nval numIterations = 15 \nval lambda = 0.10 \nval alpha = 1.00 val block = -1 \nval seed = 12345L \nval implicitPrefs = false \n\nval model = new ALS().setIterations(numIterations)\n        .setBlocks(block).setAlpha(alpha)\n        .setLambda(lambda)\n        .setRank(rank) .setSeed(seed)\n        .setImplicitPrefs(implicitPrefs)\n        .run(ratingsRDD)\n```", "```py\n// Making Predictions. Get the top 6 movie predictions for user 668 \nprintln(\"Rating:(UserID, MovieID, Rating)\") println(\"----------------------------------\") \nval topRecsForUser = model.recommendProducts(668, 6) for (rating <- topRecsForUser) { println(rating.toString()) } println(\"----------------------------------\")\n>>>\n```", "```py\nval rmseTest = computeRmse(model, testRDD, true) \nprintln(\"Test RMSE: = \" + rmseTest) //Less is better\n```", "```py\nTest RMSE: = 0.9019872589764073\n```", "```py\ndef computeRmse(model: MatrixFactorizationModel, data: RDD[Rating], implicitPrefs: Boolean): Double = {         val predictions: RDD[Rating] = model.predict(data.map(x => (x.user, x.product))) \n    val predictionsAndRatings = predictions.map { x => ((x.user, x.product), x.rating) }\n        .join(data.map(x => ((x.user, x.product), x.rating))).values \n    if (implicitPrefs) { println(\"(Prediction, Rating)\")                 \n        println(predictionsAndRatings.take(5).mkString(\"n\")) } \n        math.sqrt(predictionsAndRatings.map(x => (x._1 - x._2) * (x._1 - x._2)).mean()) \n    }\n>>>\n```", "```py\nprintln(\"Recommendations: (MovieId => Rating)\") \nprintln(\"----------------------------------\") \nval recommendationsUser = model.recommendProducts(668, 6) \nrecommendationsUser.map(rating => (rating.product, rating.rating)).foreach(println) println(\"----------------------------------\")\n>>>\n```", "```py\n//Saving the model for future use \nval savedALSModel = model.save(spark.sparkContext, \"model/MovieRecomModel\")\n```", "```py\nval same_model = MatrixFactorizationModel.load(spark.sparkContext, \"model/MovieRecomModel/\")\n```", "```py\npackage com.packt.ScalaML.MovieRecommendation \n\nimport org.apache.spark.sql.SparkSession \nimport org.apache.spark.mllib.recommendation.ALS \nimport org.apache.spark.mllib.recommendation.MatrixFactorizationModel \nimport org.apache.spark.mllib.recommendation.Rating \nimport scala.Tuple2 \nimport org.apache.spark.rdd.RDD \n\nobject RecommendationModelReuse { \n def main(args: Array[String]): Unit = { \n val spark: SparkSession = SparkSession.builder()\n                                  .appName(\"JavaLDAExample\")\n                                  .master(\"local[*]\")\n                                  .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\")\n                                  .getOrCreate() \n\n val ratigsFile = \"data/ratings.csv\" \n val ratingDF =  spark.read\n                        .format(\"com.databricks.spark.csv\")\n                        .option(\"header\", true)\n                        .load(ratigsFile) \n\n val selectedRatingsDF = ratingDF.select(ratingDF.col(\"userId\"), ratingDF.col(\"movieId\"),                                                     ratingDF.col(\"rating\"), ratingDF.col(\"timestamp\")) \n\n        // Randomly split ratings RDD into training data RDD (75%) and test data RDD (25%) \n        val splits = selectedRatingsDF.randomSplit(Array(0.75, 0.25), seed = 12345L) \n        val testData = splits(1) \n        val testRDD = testData.rdd.map(row => { \n        val userId = row.getString(0) \n        val movieId = row.getString(1) \n        val ratings = row.getString(2) \n        Rating(userId.toInt, movieId.toInt, ratings.toDouble) }) \n\n        //Load the workflow back \n        val same_model = MatrixFactorizationModel.load(spark.sparkContext, \"model/MovieRecomModel/\") \n\n        // Making Predictions. Get the top 6 movie predictions for user 668 \n        println(\"Rating:(UserID, MovieID, Rating)\") \n        println(\"----------------------------------\") \n        val topRecsForUser = same_model.recommendProducts(458, 10) \n\n        for (rating <- topRecsForUser) { \n            println(rating.toString()) } \n\n        println(\"----------------------------------\") \n        val rmseTest = MovieRecommendation.computeRmse(same_model, testRDD, true) \n        println(\"Test RMSE: = \" + rmseTest) //Less is better \n\n        //Movie recommendation for a specific user. Get the top 6 movie predictions for user 668 \n        println(\"Recommendations: (MovieId => Rating)\") \n        println(\"----------------------------------\") \n        val recommendationsUser = same_model.recommendProducts(458, 10) \n\n        recommendationsUser.map(rating => \n        (rating.product, rating.rating)).foreach(println) \n        println(\"----------------------------------\") \n        spark.stop() \n    } \n}\n```"]