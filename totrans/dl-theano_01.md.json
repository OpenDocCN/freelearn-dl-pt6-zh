["```py\nwget https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh\nchmod +x Miniconda2-latest-Linux-x86_64.sh\nbash ./Miniconda2-latest-Linux-x86_64.sh\n```", "```py\nconda install theano\n```", "```py\n>>> from theano import theano\n\n>>> theano.config.device\n'cpu'\n\n>>> theano.config.floatX\n'float64'\n\n>>> print(theano.config)\n```", "```py\n>>> theano.config.floatX='float32'\n```", "```py\nexport PATH=/usr/local/cuda-8.0-cudnn-5.1/bin:$PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-8.0-cudnn-5.1/lib64:/usr/local/cuda-8.0-cudnn-5.1/lib:$LD_LIBRARY_PATH\n```", "```py\nconda install pygpu\n```", "```py\nTHEANO_FLAGS=\"device=cuda,floatX=float32\" python\n>>> import theano\nUsing cuDNN version 5110 on context None\nMapped name None to device cuda: Tesla K80 (0000:83:00.0)\n\n>>> theano.config.device\n'gpu'\n\n>>> theano.config.floatX\n'float32'\n```", "```py\nTHEANO_FLAGS=\"device=cuda,floatX=float32,gpuarray.preallocate=0.8\" python\n>>> from theano import theano\nUsing cuDNN version 5110 on context None\nPreallocating 9151/11439 Mb (0.800000) on cuda\nMapped name None to device cuda: Tesla K80 (0000:83:00.0)\n```", "```py\nTHEANO_FLAGS=\"contexts=dev0->cuda0;dev1->cuda1;dev2->cuda2;dev3->cuda3,floatX=float32,gpuarray.preallocate=0.8\" python\n>>> import theano\nUsing cuDNN version 5110 on context None\nPreallocating 9177/11471 Mb (0.800000) on cuda0\nMapped name dev0 to device cuda0: Tesla K80 (0000:83:00.0)\nUsing cuDNN version 5110 on context dev1\nPreallocating 9177/11471 Mb (0.800000) on cuda1\nMapped name dev1 to device cuda1: Tesla K80 (0000:84:00.0)\nUsing cuDNN version 5110 on context dev2\nPreallocating 9177/11471 Mb (0.800000) on cuda2\nMapped name dev2 to device cuda2: Tesla K80 (0000:87:00.0)\nUsing cuDNN version 5110 on context dev3\nPreallocating 9177/11471 Mb (0.800000) on cuda3\nMapped name dev3 to device cuda3: Tesla K80 (0000:88:00.0)\n```", "```py\n [global]\n floatX = float32\n device = cuda0\n [gpuarray]\n preallocate = 1\n```", "```py\n>>> import theano.tensor as T\n\n>>> T.scalar()\n<TensorType(float32, scalar)>\n\n>>> T.iscalar()\n<TensorType(int32, scalar)>\n\n>>> T.fscalar()\n<TensorType(float32, scalar)>\n\n>>> T.dscalar()\n<TensorType(float64, scalar)>\n```", "```py\n>>> theano.config.floatX = 'float64'\n\n>>> T.scalar()\n<TensorType(float64, scalar)>\n\n>>> T.fscalar()\n<TensorType(float32, scalar)>\n\n>>> theano.config.floatX = 'float32'\n\n>>> T.scalar()\n<TensorType(float32, scalar)>\n```", "```py\n>>> x = T.matrix('x')\n\n>>> y = T.matrix('y')\n\n>>> z = x + y\n\n>>> theano.pp(z)\n'(x + y)'\n\n>>> z.eval({x: [[1, 2], [1, 3]], y: [[1, 0], [3, 4]]})\narray([[ 2.,  2.],\n       [ 4.,  7.]], dtype=float32)\n```", "```py\n>>> a = T.matrix()\n\n>>> b = T.matrix()\n\n>>> theano.pp(a + b)\n'(<TensorType(float32, matrix)> + <TensorType(float32, matrix)>)'*.*\n\n```", "```py\n>>> x = T.matrix('x')\n\n>>> x = x + x\n\n>>> theano.pp(x)\n*'(x + x)'*\n\n```", "```py\n>>> x, y, z = T.matrices('x', 'y', 'z')\n```", "```py\n>>> x = T.matrix('x')\n\n>>> y = T.matrix('y')\n\n>>> z = x + y\n\n>>> z\n\nElemwise{add,no_inplace}.0\n\n>>> theano.pp(z)\n\n*'(x + y)*\n\n>>> theano.printing.pprint(z)\n\n*'(x + y)'*\n\n>>> theano.printing.debugprint(z)\nElemwise{add,no_inplace} [id A] ''   \n |x [id B]\n |y [id C]\n```", "```py\n>>> theano.printing.pydotprint(z)\nThe output file is available at ~/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-2.7.12-64/theano.pydotprint.gpu.png.\n\n```", "```py\n>>> addition = theano.function([x, y], [z])\n\n>>> addition([[1, 2], [1, 3]], [[1, 0], [3, 4]])\n[array([[ 2.,  2.],\n       [ 4.,  7.]], dtype=float32)]\n```", "```py\n>>> theano.printing.debugprint(addition)\nHostFromGpu(gpuarray) [id A] ''   3\n |GpuElemwise{Add}[(0, 0)]<gpuarray> [id B] ''   2\n   |GpuFromHost<None> [id C] ''   1\n   | |x [id D]\n   |GpuFromHost<None> [id E] ''   0\n     |y [id F]\n\n>>> theano.printing.pydotprint(addition)\n\nThe output file is available at ~/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-2.7.12-64/theano.pydotprint.gpu.png:\n```", "```py\n>>> z= z * x\n\n>>> theano.printing.debugprint(theano.function([x,y],z))\nHostFromGpu(gpuarray) [id A] ''   3\n |GpuElemwise{Composite{((i0 + i1) * i0)}}[(0, 0)]<gpuarray> [id B] ''   2\n   |GpuFromHost<None> [id C] ''   1\n   | |x [id D]\n   |GpuFromHost<None> [id E] ''   0\n     |y [id F]\n```", "```py\n>>> theano.config.floatX\n'float32'\n\n>>> x = T.matrix()\n\n>>> x\n<TensorType(float32, matrix)>\n\n>>> y = T.matrix()\n\n>>> addition = theano.function([x, y], [x+y])\n\n>>> addition(numpy.ones((2,2)),numpy.zeros((2,2)))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python2.7/site-packages/theano/compile/function_module.py\", line 786, in __call__\n    allow_downcast=s.allow_downcast)\n\n  File \"/usr/local/lib/python2.7/site-packages/theano/tensor/type.py\", line 139, in filter\n    raise TypeError(err_msg, data)\nTypeError: ('Bad input argument to theano function with name \"<stdin>:1\"  at index 0(0-based)', 'TensorType(float32, matrix) cannot store a value of dtype float64 without risking loss of precision. If you do not mind this loss, you can: 1) explicitly cast your data to float32, or 2) set \"allow_input_downcast=True\" when calling \"function\".', array([[ 1.,  1.],\n       [ 1.,  1.]]))\n```", "```py\n>>> import numpy\n\n>>> addition(numpy.ones((2,2), dtype=theano.config.floatX),numpy.zeros((2,2), dtype=theano.config.floatX))\n[array([[ 1.,  1.],\n        [ 1.,  1.]], dtype=float32)]\n```", "```py\n>>> addition(numpy.ones((2,2)).astype(theano.config.floatX),numpy.diag((2,3)).astype(theano.config.floatX))\n[array([[ 3.,  1.],\n        [ 1.,  4.]], dtype=float32)]\n```", "```py\n>>> addition = theano.function([x, y], [x+y],allow_input_downcast=True)\n\n>>> addition(numpy.ones((2,2)),numpy.zeros((2,2)))\n[array([[ 1.,  1.],\n        [ 1.,  1.]], dtype=float32)]\n```", "```py\n>>> a = T.zeros((2,3))\n\n>>> a.eval()\narray([[ 0.,  0.,  0.],\n       [ 0.,  0.,  0.]])\n\n>>> b = T.identity_like(a)\n\n>>> b.eval()\narray([[ 1.,  0.,  0.],\n        [ 0.,  1.,  0.]])\n\n>>> c = T.arange(10)\n\n>>> c.eval()\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n```", "```py\n>>> c.ndim\n*1*\n\n>>> c.dtype\n'int64'\n\n>>> c.type\nTensorType(int64, vector)\n```", "```py\n>>> a = T.matrix()\n\n>>> a.shape\nShape.0\n\n>>> a.shape.eval({a: [[1, 2], [1, 3]]})\narray([2, 2])\n\n>>> shape_fct = theano.function([a],a.shape)\n\n>>> shape_fct([[1, 2], [1, 3]])\narray([2, 2])\n\n>>> n = T.iscalar()\n\n>>> c = T.arange(n)\n\n>>> c.shape.eval({n:10})\narray([10])\n```", "```py\n>>> a = T.arange(10)\n\n>>> b = T.reshape( a, (5,2) )\n\n>>> b.eval()\narray([[0, 1],\n       [2, 3], \n       [4, 5],\n       [6, 7],\n       [8, 9]])\n```", "```py\n>>> T.arange(10).reshape((5,2))[::-1].T.eval()\narray([[8, 6, 4, 2, 0],\n       [9, 7, 5, 3, 1]])\n```", "```py\n>>> a, b = T.matrices('a', 'b')\n\n>>> z = a * b\n\n>>> z.eval({a:numpy.ones((2,2)).astype(theano.config.floatX), b:numpy.diag((3,3)).astype(theano.config.floatX)})\narray([[ 3.,  0.],\n       [ 0.,  3.]])\n```", "```py\n>>> z = T.mul(a, b)\n```", "```py\n>>> z = T.mul(a, b, a, b)\n```", "```py\n>>> a = T.matrix()\n\n>>> z = a ** 2 \n\n>>> z.eval({a:numpy.diag((3,3)).astype(theano.config.floatX)})\narray([[ 9.,  0.], \n       [ 0.,  9.]])\n```", "```py\n>>> a = T.matrix()\n\n>>> b = T.scalar()\n\n>>> z = a * b\n\n>>> z.eval({a:numpy.diag((3,3)).astype(theano.config.floatX),b:3})\narray([[ 6.,  0.],\n       [ 0.,  6.]])\n```", "```py\n>>> cond = T.vector('cond')\n\n>>> x,y = T.vectors('x','y')\n\n>>> z = T.switch(cond, x, y)\n\n>>> z.eval({ cond:[1,0], x:[10,10], y:[3,2] })\narray([ 10.,   2.], dtype=float32)\n```", "```py\n>>> from theano.ifelse import ifelse\n\n>>> z=ifelse(1, 5, 4)\n\n>>> z.eval()\narray(5, dtype=int8)\n```", "```py\n>>> a = T.matrix('a')\n\n>>> T.max(a).eval({a:[[1,2],[3,4]]})\narray(4.0, dtype=float32)\n\n>>> T.max(a,axis=0).eval({a:[[1,2],[3,4]]})\narray([ 3.,  4.], dtype=float32)\n\n>>> T.max(a,axis=1).eval({a:[[1,2],[3,4]]})\narray([ 2.,  4.], dtype=float32)\n```", "```py\n>>> a = T.arange(10).reshape((5,2))\n\n>>> b = a[::-1]\n\n>>> b.eval()\narray([[8, 9],\n       [6, 7],\n       [4, 5],\n       [2, 3],\n       [0, 1]])\n>>> a.eval()\narray([[0, 1],\n       [2, 3],\n       [4, 5],\n       [6, 7],\n       [8, 9]])\n>>> T.concatenate([a,b]).eval()\narray([[0, 1],\n       [2, 3],\n       [4, 5],\n       [6, 7],\n       [8, 9],\n       [8, 9],\n       [6, 7],\n       [4, 5],\n       [2, 3],\n       [0, 1]])\n>>> T.concatenate([a,b],axis=1).eval()\narray([[0, 1, 8, 9],\n       [2, 3, 6, 7],\n       [4, 5, 4, 5],\n       [6, 7, 2, 3],\n       [8, 9, 0, 1]])\n\n>>> T.stack([a,b]).eval()\narray([[[0, 1],\n        [2, 3],\n        [4, 5],\n        [6, 7],\n        [8, 9]],\n       [[8, 9],\n        [6, 7],\n        [4, 5],\n        [2, 3],\n        [0, 1]]])\n```", "```py\n>>> a.eval()\narray([[0, 1],\n       [2, 3],\n       [4, 5],\n       [6, 7],\n       [8, 9]])\n\n>>> T.set_subtensor(a[3:], [-1,-1]).eval()\n\narray([[ 0,  1],\n       [ 2,  3],\n       [ 4,  5],\n       [-1, -1],\n       [-1, -1]])\n\n>>> T.inc_subtensor(a[3:], [-1,-1]).eval()\narray([[0, 1],\n       [2, 3],\n       [4, 5],\n       [5, 6],\n       [7, 8]])\n```", "```py\n>>> theano.config.floatX = 'float32'\n\n>>> a = T.matrix()\n\n>>> b = a.transfer(None)\n\n>>> b.eval({a:numpy.ones((2,2)).astype(theano.config.floatX)})\ngpuarray.array([[ 1\\.  1.]\n [ 1\\.  1.]], dtype=float32)\n\n >>> theano.printing.debugprint(b)\nGpuFromHost<None> [id A] ''   \n |<TensorType(float32, matrix)> [id B]\n```", "```py\n>>> a = T.matrix('a')\n\n>>> b = a ** 2\n\n>>> sq = theano.function([a],b)\n\n>>> theano.printing.debugprint(sq)\nHostFromGpu(gpuarray) [id A] ''   2\n |GpuElemwise{Sqr}[(0, 0)]<gpuarray> [id B] ''   1\n   |GpuFromHost<None> [id C] ''   0\n     |a [id D]\n```", "```py\n>>> b = b.transfer(None)\n\n>>> sq = theano.function([a],b)\n\n>>> theano.printing.debugprint(sq)\nGpuElemwise{Sqr}[(0, 0)]<gpuarray> [id A] ''   1\n |GpuFromHost<None> [id B] ''   0\n   |a [id C]\n```", "```py\n>>> b = a.transfer('cpu')\n\n>>> theano.printing.debugprint(b)\n<TensorType(float32, matrix)> [id A]\n```", "```py\n>>> state = shared(0)\n\n>>> state\n\n<TensorType(int64, scalar)>\n\n>>> state.get_value()\narray(0)\n\n>>> state.set_value(1)\n\n>>> state.get_value()\narray(1)\n```", "```py\nPATH=/usr/local/cuda-8.0-cudnn-5.1/bin:$PATH THEANO_FLAGS=\"contexts=dev0->cuda0;dev1->cuda1,floatX=float32,gpuarray.preallocate=0.8\" python\n\n```", "```py\n>>> from theano import theano\nUsing cuDNN version 5110 on context dev0\nPreallocating 9151/11439 Mb (0.800000) on cuda0\nMapped name dev0 to device cuda0: Tesla K80 (0000:83:00.0)\nUsing cuDNN version 5110 on context dev1\nPreallocating 9151/11439 Mb (0.800000) on cuda1\nMapped name dev1 to device cuda1: Tesla K80 (0000:84:00.0)\n\n>>> import theano.tensor as T\n\n>>> import numpy\n\n>>> theano.shared(numpy.random.random((1024, 1024)).astype('float32'),target='dev1')\n<GpuArrayType<dev1>(float32, (False, False))>\n```", "```py\ndef theano.function(inputs, \n\toutputs=None, updates=None, givens=None,\n allow_input_downcast=None, mode=None, profile=None,\n  \t)\n```", "```py\n>>> a = T.matrix()\n\n>>> ex = theano.function([a],[T.exp(a),T.log(a),a**2])\n\n>>> ex(numpy.random.randn(3,3).astype(theano.config.floatX))\n[array([[ 2.33447003,  0.30287042,  0.63557744],\n       [ 0.18511547,  1.34327984,  0.42203984],\n       [ 0.87083125,  5.01169062,  6.88732481]], dtype=float32),\narray([[-0.16512829,         nan,         nan],\n       [        nan, -1.2203927 ,         nan],\n       [        nan,  0.47733498,  0.65735561]], dtype=float32),\narray([[ 0.71873927,  1.42671108,  0.20540957],\n       [ 2.84521151,  0.08709242,  0.74417454],\n       [ 0.01912885,  2.59781313,  3.72367549]], dtype=float32)]\n```", "```py\n>>> w = shared(1.0)\n\n>>> x = T.scalar('x')\n\n>>> mul = theano.function([x],updates=[(w,w*x)])\n\n>>> mul(4)\n[]\n\n>>> w.get_value()\narray(4.0)\n```", "```py\n>>> a = T.scalar()\n\n>>> pow = a ** 2\n\n>>> g = theano.grad(pow,a)\n\n>>> theano.printing.pydotprint(g)\n\n>>> theano.printing.pydotprint(theano.function([a],g))\n```", "```py\ndef scan(fn,\n         sequences=None,\n         outputs_info=None,\n         non_sequences=None,\n         n_steps=None,\n         truncate_gradient=-1,\n         go_backwards=False,\n         mode=None,\n         name=None,\n         profile=False,\n         allow_gc=None,\n         strict=False)\n```", "```py\n>>> a = T.matrix()\n\n>>> b = T.matrix()\n\n>>> def fn(x): return x + 1\n\n>>> results, updates = theano.scan(fn, sequences=a)\n\n>>> f = theano.function([a], results, updates=updates)\n\n>>> f(numpy.ones((2,3)).astype(theano.config.floatX))\n\narray([[ 2.,  2.,  2.],\n       [ 2.,  2.,  2.]], dtype=float32)\n```", "```py\nfn( sequences (if any), prior results (if needed), non-sequences (if any) )\n```", "```py\n>>> a = T.vector()\n\n>>> s0 = T.scalar(\"s0\")\n\n>>> def fn( current_element, prior ):\n...   return prior + current_element\n\n>>> results, updates = theano.scan(fn=fn,outputs_info=s0,sequences=a)\n\n>>> f = theano.function([a,s0], results, updates=updates)\n\n>>> f([0,3,5],0)\n*array([ 0.,  3.,  8.], dtype=float32)*\n\n```", "```py\n>>> a = T.matrix()\n\n>>> s0 = T.scalar(\"s0\")\n\n>>> def fn( current_element, prior ):\n...   return prior + current_element.sum()\n\n>>> results, updates = theano.scan(fn=fn,outputs_info=s0,sequences=a)\n\n>>> f = theano.function([a,s0], results, updates=updates)\n\n>>> f(numpy.ones((20,5)).astype(theano.config.floatX),0)\n\narray([   5.,   10.,   15.,   20.,   25.,   30.,   35.,   40.,   45.,\n         50.,   55.,   60.,   65.,   70.,   75.,   80.,   85.,   90.,\n         95.,  100.], dtype=float32)\n```", "```py\n>>> a = T.vector()\n\n>>> s0 = T.scalar(\"s0\")\n\n>>> def fn( current_element, prior, non_seq ):\n...   return non_seq * prior + current_element\n\n>>> results, updates = theano.scan(fn=fn,n_steps=10,sequences=a,outputs_info=T.constant(0.0),non_sequences=s0)\n\n>>> f = theano.function([a,s0], results, updates=updates)\n\n>>> f(numpy.ones((20)).astype(theano.),5)\narray([  1.00000000e+00,   6.00000000e+00,   3.10000000e+01,\n         1.56000000e+02,   7.81000000e+02,   3.90600000e+03,\n         1.95310000e+04,   9.76560000e+04,   4.88281000e+05,\n         2.44140600e+06], dtype=float32)\n```", "```py\n>>> theano.config.exception_verbosity='high'\n\n>>> theano.config.mode\n'Mode'\n\n>>> theano.config.optimizer='fast_compile'\n```", "```py\n>>> f = theano.function([a,s0], results, updates=updates, mode='FAST_COMPILE')\n```", "```py\n  CUDA_LAUNCH_BLOCKING=1 python\n\n```", "```py\n>>> theano.config.profile=True \n```", "```py\n>>> theano.config.profile_memory=True\n```", "```py\n>>> theano.config.profile_optimizer=True \n```", "```py\n>>> f = theano.function([a,s0], results, profile=True)\n\n>>> f.profile.summary()\nFunction profiling\n==================\n  Message: <stdin>:1\n  Time in 1 calls to Function.__call__: 1.490116e-03s\n  Time in Function.fn.__call__: 1.251936e-03s (84.016%)\n  Time in thunks: 1.203537e-03s (80.768%)\n  Total compile time: 1.720619e-01s\n    Number of Apply nodes: 14\n    Theano Optimizer time: 1.382768e-01s\n       Theano validate time: 1.308680e-03s\n    Theano Linker time (includes C, CUDA code generation/compiling): 2.405691e-02s\n       Import time 1.272917e-03s\n       Node make_thunk time 2.329803e-02s\n\nTime in all call to theano.grad() 0.000000e+00s\nTime since theano import 520.661s\nClass\n---\n<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>\n  58.2%    58.2%       0.001s       7.00e-04s     Py       1       1   theano.scan_module.scan_op.Scan\n  27.3%    85.4%       0.000s       1.64e-04s     Py       2       2   theano.sandbox.cuda.basic_ops.GpuFromHost\n   6.1%    91.5%       0.000s       7.30e-05s     Py       1       1   theano.sandbox.cuda.basic_ops.HostFromGpu\n   5.5%    97.0%       0.000s       6.60e-05s     C        1       1   theano.sandbox.cuda.basic_ops.GpuIncSubtensor\n   1.1%    98.0%       0.000s       3.22e-06s     C        4       4   theano.tensor.elemwise.Elemwise\n   0.7%    98.8%       0.000s       8.82e-06s     C        1       1   theano.sandbox.cuda.basic_ops.GpuSubtensor\n   0.7%    99.4%       0.000s       7.87e-06s     C        1       1   theano.sandbox.cuda.basic_ops.GpuAllocEmpty\n   0.3%    99.7%       0.000s       3.81e-06s     C        1       1   theano.compile.ops.Shape_i\n   0.3%   100.0%       0.000s       1.55e-06s     C        2       2   theano.tensor.basic.ScalarFromTensor\n   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)\n\nOps\n---\n<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>\n  58.2%    58.2%       0.001s       7.00e-04s     Py       1        1   forall_inplace,gpu,scan_fn}\n  27.3%    85.4%       0.000s       1.64e-04s     Py       2        2   GpuFromHost\n   6.1%    91.5%       0.000s       7.30e-05s     Py       1        1   HostFromGpu\n   5.5%    97.0%       0.000s       6.60e-05s     C        1        1   GpuIncSubtensor{InplaceSet;:int64:}\n   0.7%    97.7%       0.000s       8.82e-06s     C        1        1   GpuSubtensor{int64:int64:int16}\n   0.7%    98.4%       0.000s       7.87e-06s     C        1        1   GpuAllocEmpty\n   0.3%    98.7%       0.000s       4.05e-06s     C        1        1   Elemwise{switch,no_inplace}\n   0.3%    99.0%       0.000s       4.05e-06s     C        1        1   Elemwise{le,no_inplace}\n   0.3%    99.3%       0.000s       3.81e-06s     C        1        1   Shape_i{0}\n   0.3%    99.6%       0.000s       1.55e-06s     C        2        2   ScalarFromTensor\n   0.2%    99.8%       0.000s       2.86e-06s     C        1        1   Elemwise{Composite{Switch(LT(i0, i1), i0, i1)}}\n   0.2%   100.0%       0.000s       1.91e-06s     C        1        1   Elemwise{Composite{Switch(i0, i1, minimum(i2, i3))}}[(0, 2)]\n   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)\n\nApply\n------\n<% time> <sum %> <apply time> <time per call> <#call> <id> <Apply name>\n  58.2%    58.2%       0.001s       7.00e-04s      1    12   forall_inplace,gpu,scan_fn}(TensorConstant{10}, GpuSubtensor{int64:int64:int16}.0, GpuIncSubtensor{InplaceSet;:int64:}.0, GpuFromHost.0)\n  21.9%    80.1%       0.000s       2.64e-04s      1     3   GpuFromHost(<TensorType(float32, vector)>)\n   6.1%    86.2%       0.000s       7.30e-05s      1    13   HostFromGpu(forall_inplace,gpu,scan_fn}.0)\n   5.5%    91.6%       0.000s       6.60e-05s      1     4   GpuIncSubtensor{InplaceSet;:int64:}(GpuAllocEmpty.0, CudaNdarrayConstant{[ 0.]}, Constant{1})\n   5.3%    97.0%       0.000s       6.41e-05s      1     0   GpuFromHost(s0)\n   0.7%    97.7%       0.000s       8.82e-06s      1    11   GpuSubtensor{int64:int64:int16}(GpuFromHost.0, ScalarFromTensor.0, ScalarFromTensor.0, Constant{1})\n   0.7%    98.4%       0.000s       7.87e-06s      1     1   GpuAllocEmpty(TensorConstant{10})\n   0.3%    98.7%       0.000s       4.05e-06s      1     8   Elemwise{switch,no_inplace}(Elemwise{le,no_inplace}.0, TensorConstant{0}, TensorConstant{0})\n   0.3%    99.0%       0.000s       4.05e-06s      1     6   Elemwise{le,no_inplace}(Elemwise{Composite{Switch(LT(i0, i1), i0, i1)}}.0, TensorConstant{0})\n   0.3%    99.3%       0.000s       3.81e-06s      1     2   Shape_i{0}(<TensorType(float32, vector)>)\n   0.3%    99.6%       0.000s       3.10e-06s      1    10   ScalarFromTensor(Elemwise{switch,no_inplace}.0)\n   0.2%    99.8%       0.000s       2.86e-06s      1     5   Elemwise{Composite{Switch(LT(i0, i1), i0, i1)}}(TensorConstant{10}, Shape_i{0}.0)\n   0.2%   100.0%       0.000s       1.91e-06s      1     7   Elemwise{Composite{Switch(i0, i1, minimum(i2, i3))}}[(0, 2)](Elemwise{le,no_inplace}.0, TensorConstant{0}, Elemwise{Composite{Switch(LT(i0, i1), i0, i1)}}.0, Shape_i{0}.0)\n   0.0%   100.0%       0.000s       0.00e+00s      1     9   ScalarFromTensor(Elemwise{Composite{Switch(i0, i1, minimum(i2, i3))}}[(0, 2)].0)\n   ... (remaining 0 Apply instances account for 0.00%(0.00s) of the runtime)\n```"]