["```py\n    from nltk.corpus import stopwords\n    from nltk.tokenize import word_tokenize\n    from nltk.tokenize import sent_tokenize \n    import nltk\n    nltk.download('punkt')\n    ```", "```py\n    example_sentence = \"This course is great. I'm going to learn deep learning; Artificial Intelligence is amazing and I love robotics...\"\n    sent_tokenize(example_sentence) # Divide the text into sentences\n    ```", "```py\n    word_tokenize(example_sentence)\n    ```", "```py\n    nltk.download('stopwords')\n    ```", "```py\n    stop_words = set(stopwords.words(\"english\")) \n    print(stop_words)\n    ```", "```py\n    print(word_tokenize(example_sentence))\n    print([w for w in word_tokenize(example_sentence.lower()) if w not in stop_words]) \n    ```", "```py\n    stop_words = stop_words - set(('this', 'i', 'and')) \n    print([w for w in word_tokenize(example_sentence.lower()) if w not in stop_words]) \n    ```", "```py\n    from nltk.stem.porter import *    # importing a stemmer\n    stemmer = PorterStemmer()    # importing a stemmer     \n    print([stemmer.stem(w) for w in  word_tokenize(example_sentence)])\n    ```", "```py\n    nltk.download('averaged_perceptron_tagger')\n    t = nltk.pos_tag(word_tokenize(example_sentence)) #words with each tag\n    t\n    ```", "```py\n    import spacy\n    ```", "```py\n    import en_core_web_sm\n    nlp = spacy.load('en')\n    ```", "```py\n    example_sentence = \"This course is great. I'm going to learn deep learning; Artificial Intelligence is amazing and I love robotics...\"\n    doc1 = nlp(example_sentence)\n    ```", "```py\n    print(\"Doc structure: {}\".format(doc1))\n    print(\"Type of doc1:{}\".format(type(doc1)))\n    print(\"5th and 10th Token of the Doc: {}, {}\".format(doc1[5], doc1[11]))\n    print(\"Span between the 5th token and the 10th: {}\".format(doc1[5:11]))\n    ```", "```py\n    for s in doc1.sents:\n        print(s)\n    ```", "```py\n    for i in doc1:\n        print(i)\n    ```", "```py\n    from spacy.lang.en.stop_words import STOP_WORDS\n    print(\"Some stopwords of spaCy: {}\".format(list(STOP_WORDS)[:10]))\n    type(STOP_WORDS)\n    ```", "```py\n    for i in doc1[0:5]:\n        print(\"Token: {} | Stop word: {}\".format(i, i.is_stop)\n    ```", "```py\n    nlp.vocab[\"This\"].is_stop = True doc1[0].is_stop\n    ```", "```py\n    for i in doc1[0:5]:\n        print(\"Token: {} | Tag: {}\".format(i.text, i.pos_))\n    ```", "```py\n    doc2 = nlp(\"I live in Madrid and I am working in Google from 10th of December.\")\n    for i in doc2.ents:\n        print(\"Word: {} | Entity: {}\".format(i.text, i.label_))\n    ```", "```py\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn.decomposition import TruncatedSVD\n    ```", "```py\n    corpus = [\n         'My cat is white',\n         'I am the major of this city',\n         'I love eating toasted cheese',\n         'The lazy cat is sleeping',\n    ]\n    ```", "```py\n    vectorizer = TfidfVectorizer()\n    X = vectorizer.fit_transform(corpus)\n    ```", "```py\n    vectorizer.get_feature_names()\n    ```", "```py\n    X.todense()\n    ```", "```py\n    lsa = TruncatedSVD(n_components=10,algorithm='randomized',n_iter=10,random_state=0)\n    lsa.fit_transform(X)\n    ```", "```py\n    lsa.components_\n    ```", "```py\n    import numpy as np\n    corpus = [\n         'My cat is white',\n         'I am the major of this city',\n         'I love eating toasted cheese',\n         'The lazy cat is sleeping',\n    ]\n    ```", "```py\n    import spacy\n    import en_core_web_sm\n    from spacy.lang.en.stop_words import STOP_WORDS\n    nlp = en_core_web_sm.load()\n    ```", "```py\n    tokens = []\n    tokens_doc = []\n    distinc_tokens = []\n    ```", "```py\n    for c in corpus:\n        doc = nlp(c)\n        tokens_aux = []\n    ```", "```py\n        for t in doc:\n            tokens_aux.append(t.text)\n            if t.text not in tokens:\n                distinc_tokens.append(t.text) # without duplicates \n            tokens.append(t.text)\n        tokens_doc.append(tokens_aux)\n        tokens_aux = []\n        print(tokens)\n        print(distinc_tokens)\n        print(tokens_doc)\n    ```", "```py\n    def unigram_model(word):\n        return tokens.count(word)/len(tokens)\n    unigram_model(\"cat\")\n    ```", "```py\n    def unigram_model_smoothing(word):\n        return (tokens.count(word) + 1)/(len(tokens) + len(distinc_tokens))\n    unigram_model_smoothing(\"cat\")\n    ```", "```py\n    def bigram_model(word1, word2):\n        hit = 0\n    ```", "```py\n        for d in tokens_doc:\n            for t,i in zip(d, range(len(d))): # i is the length of d  \n                if i <= len(d)-2:\n                    if word1 == d[i] and word2 == d[i+1]:\n                        hit += 1\n        print(\"Hits: \",hit)\n        return hit/tokens.count(word1)\n    bigram_model(\"I\",\"am\")\n    ```", "```py\n    def bigram_model_smoothing(word1, word2):\n        hit = 0\n        for d in tokens_doc:\n            for t,i in zip(d, range(len(d))):\n                if i <= len(d)-2:\n                    if word1 == d[i] and word2 == d[i+1]:\n                        hit += 1\n        return (hit+1)/(tokens.count(word1)+len(distinc_tokens))\n    bigram_model(\"I\",\"am\")\n    ```"]