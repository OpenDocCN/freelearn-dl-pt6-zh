- en: Build Your First CNN and Performance Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **convolutional neural network** (**CNN**) is a type of **feed-forward neural
    network** (**FNN**) in which the connectivity pattern between its neurons is inspired
    by an animal's visual cortex. In the last few years, CNNs have demonstrated superhuman
    performance in image search services, self-driving cars, automatic video classification,
    voice recognition, and **natural language processing **(**NLP**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering these motivations, in this chapter, we will construct a simple
    CNN model for image classification from scratch, followed by some theoretical
    aspects, such as convolutional and pooling operations. Then we will discuss how
    to tune hyperparameters and optimize the training time of CNNs for improved classification
    accuracy. Finally, we will build the second CNN model by considering some best
    practices. In a nutshell, the following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: CNN architectures and drawbacks of DNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The convolution operations and pooling layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and training a CNN for image classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model performance optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an improved CNN for optimized performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNN architectures and drawbacks of DNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](00f0eb08-6d6c-48b7-8ffe-db69c7f90a73.xhtml), *Introduction to
    Convolutional Neural Networks*, we discussed that a regular multilayer perceptron
    works fine for small images (for example, MNIST or CIFAR-10). However, it breaks
    down for larger images because of the huge number of parameters it requires. For
    example, a 100 × 100 image has 10,000 pixels, and if the first layer has just
    1,000 neurons (which already severely restricts the amount of information transmitted
    to the next layer), this means 10 million connections; and that is just for the
    first layer.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs solve this problem using partially connected layers. Because consecutive
    layers are only partially connected and because it heavily reuses its weights,
    a CNN has far fewer parameters than a fully connected DNN, which makes it much
    faster to train, reduces the risk of overfitting, and requires much less training
    data. Moreover, when a CNN has learned a kernel that can detect a particular feature,
    it can detect that feature anywhere on the image. In contrast, when a DNN learns
    a feature in one location, it can detect it only in that particular location.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since images typically have very repetitive features, CNNs are able to generalize
    much better than DNNs for image processing tasks such as classification, using
    fewer training examples. Importantly, a DNN has no prior knowledge of how pixels
    are organized; it does not know that nearby pixels are close. A CNN''s architecture
    embeds this prior knowledge. Lower layers typically identify features in small
    areas of the images, while higher layers combine the lower-level features into
    larger features. This works well with most natural images, giving CNNs a decisive
    head start compared to DNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/685a8fc6-999c-4f76-92ef-0377bfa260f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Regular DNN versus CNN, where each layer has neurons arranged in
    3D'
  prefs: []
  type: TYPE_NORMAL
- en: For example, in *Figure 1*, on the left, you can see a regular three-layer neural
    network. On the right, a ConvNet arranges its neurons in three dimensions (width,
    height, and depth) as visualized in one of the layers. Every layer of a ConvNet
    transforms the 3D input volume to a 3D output volume of neuron activations. The
    red input layer holds the image, so its width and height would be the dimensions
    of the image, and the depth would be three (red, green, and blue channels). Therefore,
    all the multilayer neural networks we looked at had layers composed of a long
    line of neurons, and we had to flatten input images or data to 1D before feeding
    them to the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: However, what happens once you try to feed them a 2D image directly? The answer
    is that in CNNs, each layer is represented in 2D, which makes it easier to match
    neurons with their corresponding inputs. We will see examples of this in upcoming
    sections. Another important fact is that all the neurons in a feature map share
    the same parameters, so it dramatically reduces the number of parameters in the
    model; but more importantly, it means that once the CNN has learned to recognize
    a pattern in one location, it can recognize it in any other location.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, once a regular DNN has learned to recognize a pattern in one location,
    it can recognize it only in that particular location. In multilayer networks such
    as MLP or DBN, the outputs of all neurons of the input layer are connected to
    each neuron in the hidden layer, and then the output will again act as the input
    to the fully connected layer. In CNN networks, the connection scheme that defines
    the convolutional layer is significantly different. The convolutional layer is
    the main type of layer in a CNN, where each neuron is connected to a certain region
    of the input area called the **receptive field**.
  prefs: []
  type: TYPE_NORMAL
- en: In a typical CNN architecture, a few convolutional layers are connected in a
    cascade style. Each layer is followed by a **Rectified Linear Unit** (**ReLU**)
    layer, then a pooling layer, then one or more convolutional layers (+ReLU), then
    another pooling layer, and finally one or more fully connected layers. Pretty
    much depending on problem type, the network might be deep though. The output from
    each convolution layer is a set of objects called **feature maps**, generated
    by a single kernel filter. Then the feature maps can be used to define a new input
    to the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each neuron in a CNN network produces an output, followed by an activation
    threshold, which is proportional to the input and not bound:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03186daf-dff9-499d-ad13-731d480942fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A conceptual architecture of a CNN'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in *Figure 2*, the pooling layers are usually placed after the
    convolutional layers (for example, between two convolutional layers). A pooling
    layer into subregions then divides the convolutional region. Then, a single representative
    value is selected, using either a max-pooling or an average pooling technique,
    to reduce the computational time of subsequent layers. This way, a CNN can be
    thought of as a feature extractor. To understand this more clearly, refer to the
    following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6a1addd-d986-4e6a-b27b-388aa2bfd8f3.png)'
  prefs: []
  type: TYPE_IMG
- en: In this way, the robustness of the feature with respect to its spatial position
    is increased too. To be more specific, when feature maps are used as image properties
    and pass through the grayscale image, it gets smaller and smaller as it progresses
    through the network; but it also typically gets deeper and deeper, as more feature
    maps will be added.
  prefs: []
  type: TYPE_NORMAL
- en: We've already discussed the limitations of such FFNN - that is, a very high
    number of neurons would be necessary, even in a shallow architecture, due to the
    very large input sizes associated with images, where each pixel is a relevant
    variable. The convolution operation brings a solution to this problem as it reduces
    the number of free parameters, allowing the network to be deeper with fewer parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A convolution is a mathematical operation that slides one function over another
    and measures the integral of their pointwise multiplication. It has deep connections
    with the Fourier transformation and the Laplace transformation and is heavily
    used in signal processing. Convolutional layers actually use cross-correlations,
    which are very similar to convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In mathematics, convolution is a mathematical operation on two functions that
    produces a third function—that is, the modified (convoluted) version of one of
    the original functions. The resulting function gives in integral of the pointwise
    multiplication of the two functions as a function of the amount that one of the
    original functions is translated. Interested readers can refer to this URL for
    more information: [https://en.wikipedia.org/wiki/Convolution](https://en.wikipedia.org/wiki/Convolution).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the most important building block of a CNN is the convolutional layer.
    Neurons in the first convolutional layer are not connected to every single pixel
    in the input image (that is, like FNNs—for example, MLP and DBN) but only to pixels
    in their receptive fields. See *Figure 3*. In turn, each neuron in the second
    convolutional layer is connected only to neurons located within a small rectangle
    in the first layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7c30e9b-9df7-4948-9dc4-617c8ef86b52.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Each convolutional neuron processes data only for its receptive field'
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 2](00f0eb08-6d6c-48b7-8ffe-db69c7f90a73.xhtml), *Introduction to
    Convolutional Neural Networks*, we have seen that all multilayer neural networks
    (for example, MLP) have layers composed of so many neurons, and we have to flatten
    input images to 1D before feeding them to the neural network. Instead, in a CNN,
    each layer is represented in 2D, which makes it easier to match neurons with their
    corresponding inputs.
  prefs: []
  type: TYPE_NORMAL
- en: The receptive fields concept is used by CNNs to exploit spatial locality by
    enforcing a local connectivity pattern between neurons of adjacent layers.
  prefs: []
  type: TYPE_NORMAL
- en: This architecture allows the network to concentrate on low-level features in
    the first hidden layer, and then assemble them into higher-level features in the
    next hidden layer, and so on. This hierarchical structure is common in real-world
    images, which is one of the reasons why CNNs work so well for image recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it not only requires a low number of neurons but also reduces the number
    of trainable parameters significantly. For example, regardless of image size,
    building regions of size 5 x 5, each with the same-shared weights, requires only
    25 learnable parameters. In this way, it resolves the vanishing or exploding gradients
    problem in training traditional multilayer neural networks with many layers by
    using backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling, stride, and padding operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once you''ve understood how convolutional layers work, the pooling layers are
    quite easy to grasp. A pooling layer typically works on every input channel independently,
    so the output depth is the same as the input depth. You may alternatively pool
    over the depth dimension, as we will see next, in which case the image''s spatial
    dimensions (for example, height and width) remain unchanged but the number of
    channels is reduced. Let''s see a formal definition of pooling layers from the
    well-known TensorFlow website:'
  prefs: []
  type: TYPE_NORMAL
- en: '"The pooling ops sweep a rectangular window over the input tensor, computing
    a reduction operation for each window (average, max, or max with argmax). Each
    pooling op uses rectangular windows of size called ksize separated by offset strides.
    For example, if strides are all ones, every window is used, if strides are all
    twos, every other window is used in each dimension, and so on."'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, in summary, just like convolutional layers, each neuron in a pooling
    layer is connected to the outputs of a limited number of neurons in the previous
    layer, located within a small rectangular receptive field. However, we must define
    its size, the stride, and the padding type. So in summary, the output can be computed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, the indices also take the padding values into consideration.
  prefs: []
  type: TYPE_NORMAL
- en: A pooling neuron has no weights. Therefore, all it does is aggregate the inputs
    using an aggregation function such as max or mean.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the goal of using pooling is to subsample the input image in
    order to reduce the computational load, memory usage, and number of parameters.
    This helps to avoid overfitting in the training stage. Reducing the input image
    size also makes the neural network tolerate a little bit of image shift. The spatial
    semantics of the convolution ops depend on the padding scheme chosen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Padding is an operation to increase the size of the input data. In the case
    of one-dimensional data, you just append/prepend the array with a constant; in
    two-dimensional data, you surround the matrix with these constants. In n-dimensional,
    you surround your n-dimensional hypercube with the constant. In most of the cases,
    this constant is zero and it is called **zero padding**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**VALID padding**: Only drops the rightmost columns (or bottommost rows)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SAME padding**: Tries to pad evenly left and right, but if the number of
    columns to be added is odd, it will add the extra column to the right, as is the
    case in this example'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's explain the preceding definition graphically, in the following figure.
    If we want a layer to have the same height and width as the previous layer, it
    is common to add zeros around the inputs, as shown in the diagram. This is called
    **SAME** or **zero** **padding**.
  prefs: []
  type: TYPE_NORMAL
- en: The term **SAME** means that the output feature map has the same spatial dimensions
    as the input feature map.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, zero padding is introduced to make the shapes match as needed,
    equally on every side of the input map. **VALID** means no padding and only drops
    the rightmost columns (or bottommost rows):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fdd19f4b-8552-4d31-8035-6101490b48c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: SAME versus VALID padding with CNN'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example (*Figure 5*), we use a 2 × 2 pooling kernel and a
    stride of 2 with no padding. Only the **max** input value in each kernel makes
    it to the next layer since the other inputs are dropped (we will see this later
    on):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10a39cda-7b65-48aa-8ef5-1f412325d5c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: An example using max pooling, that is, subsampling'
  prefs: []
  type: TYPE_NORMAL
- en: Fully connected layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the top of the stack, a regular fully connected layer (also known as **FNN**
    or **dense layer**) is added; it acts similar to an MLP, which might be composed
    of a few fully connected layers (+ReLUs). The final layer outputs (for example,
    softmax) the prediction. An example is a softmax layer that outputs estimated
    class probabilities for a multiclass classification.
  prefs: []
  type: TYPE_NORMAL
- en: Fully connected layers connect every neuron in one layer to every neuron in
    another layer. Although fully connected FNNs can be used to learn features as
    well as classify data, it is not practical to apply this architecture to images.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution and pooling operations in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have seen how convolutional and pooling operations are performed
    theoretically, let's see how we can perform these operation hands-on using TensorFlow.
    So let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: Applying pooling operations in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using TensorFlow, a subsampling layer can normally be represented by a `max_pool`
    operation by maintaining the initial parameters of the layer. For `max_pool`,
    it has the following signature in TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s learn how to create a function that utilizes the preceding signature
    and returns a tensor with type `tf.float32`, that is, the max pooled output tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code segment, the parameters can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`value`: This is a 4D tensor of `float32` elements and shape (batch length,
    height, width, and channels)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ksize`: A list of integers representing the window size on each dimension'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strides`: The step of the moving windows on each dimension'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_format`: `NHWC`, `NCHW`, and `NCHW_VECT_C` are supported'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ordering`: `NHWC` or `NCHW`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding`: `VALID` or `SAME`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, depending upon the layering structures in a CNN, there are other pooling
    operations supported by TensorFlow, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.nn.avg_pool`: This returns a reduced tensor with the average of each window'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.nn.max_pool_with_argmax`: This returns the `max_pool` tensor and a tensor
    with the flattened index of `max_value`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.nn.avg_pool3d`: This performs an `avg_pool` operation with a cubic-like'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: window; the input has an added depth
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.nn.max_pool3d`: This performs the same function as (...) but applies the
    max operation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now let''s see a concrete example of how the padding thing works in TensorFlow.
    Suppose we have an input image `x` with shape `[2, 3]` and one channel. Now we
    want to see the effect of both `VALID` and `SAME` paddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '`valid_pad`: Max pool with 2 x 2 kernel, stride 2, and `VALID` padding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`same_pad`: Max pool with 2 x 2 kernel, stride 2, and `SAME` padding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s see how we can attain this in Python and TensorFlow. Suppose we have
    an input image of shape `[2, 4]`, which is one channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s give it a shape accepted by `tf.nn.max_pool`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to apply the `VALID` padding with the max pool with a 2 x 2 kernel,
    stride 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, using the max pool with a 2 x 2 kernel, stride 2 and `SAME`
    padding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'For `VALID` padding, since there is no padding, the output shape is `[1, 1]`.
    However, for the `SAME` padding, since we pad the image to the shape `[2, 4]`
    (with - `inf`) and then apply the max pool, the output shape is `[1, 2]`. Let''s
    validate them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Convolution operations in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TensorFlow provides a variety of methods for convolution. The canonical form
    is applied by the `conv2d` operation. Let''s have a look at the usage of this
    operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters we use are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input`: The operation will be applied to this original tensor. It has a definite
    format of four dimensions, and the default dimension order is shown next.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter`: This is a tensor representing a kernel or filter. It has a very generic
    method: (`filter_height`, `filter_width`, `in_channels`, and `out_channels`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strides`: This is a list of four `int` tensor datatypes, which indicate the
    sliding windows for each dimension.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding`: This can be `SAME` or `VALID`. `SAME` will try to conserve the initial
    tensor dimension, but `VALID` will allow it to grow if the output size and padding
    are computed. We will see later how to perform padding along with the pooling
    layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_cudnn_on_gpu`: This indicates whether to use the `CUDA GPU CNN` library
    to accelerate calculations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data_format`: This specifies the order in which data is organized (`NHWC`
    or `NCWH`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dilations`: This signifies an optional list of `ints`. It defaults to (1,
    1, 1, 1). 1D tensor of length 4\. The dilation factor for each dimension of input.
    If it is set to k > 1, there will be k-1 skipped cells between each filter element
    on that dimension. The dimension order is determined by the value of `data_format`;
    see the preceding code example for details. Dilations in the batch and depth dimensions
    must be 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: A name for the operation (optional).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is an example of a convolutional layer. It concatenates a convolution,
    adds a bias parameter sum, and finally returns the activation function we have
    chosen for the whole layer (in this case, the ReLU operation, which is a frequently
    used one):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, x is the 4D tensor input (batch size, height, width, and channel). TensorFlow
    also offers a few other kinds of convolutional layers. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.layers.conv1d()` creates a convolutional layer for 1D inputs. This is useful,
    for example, in NLP, where a sentence may be represented as a 1D array of words,
    and the receptive field covers a few neighboring words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.layers.conv3d()` creates a convolutional layer for 3D inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.nn.atrous_conv2d()` creates an a trous convolutional layer (*a* tro*us*
    is French for with holes). This is equivalent to using a regular convolutional
    layer with a filter dilated by inserting rows and columns of zeros. For example,
    a 1 × 3 filter equal to (1, 2, 3) may be dilated with a dilation rate of 4, resulting
    in a dilated filter (1, 0, 0, 0, 2, 0, 0, 0, 3). This allows the convolutional
    layer to have a larger receptive field at no computational price and using no
    extra parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.layers.conv2d_transpose ()` creates a transpose convolutional layer, sometimes
    called a **deconvolutional layer,** which up-samples an image. It does so by inserting
    zeros between the inputs, so you can think of this as a regular convolutional
    layer using a fractional stride.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.nn.depthwise_conv2d()` creates a depth-wise convolutional layer that applies
    every filter to every individual input channel independently. Thus, if there are
    *f[n]* filters and *f[n]*[′] input channels, then this will output *f[n ]*× *f[n]*[′]
    feature maps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.layers.separable_conv2d()` creates a separable convolutional layer that
    first acts like a depth-wise convolutional layer and then applies a 1 × 1 convolutional
    layer to the resulting feature maps. This makes it possible to apply filters to
    arbitrary sets of inputs channels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we have seen how to construct a CNN and apply different
    operations on its different layers. Now when it comes to training a CNN, it is
    much trickier as it needs a lot of considerations to control those operations
    such as applying appropriate activation function, weight and bias initialization,
    and of course, using optimizers intelligently.
  prefs: []
  type: TYPE_NORMAL
- en: There are also some advanced considerations such as hyperparameter tuning for
    optimized too. However, that will be discussed in the next section. We first start
    our discussion with weight and bias initialization.
  prefs: []
  type: TYPE_NORMAL
- en: Weight and bias initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most common initialization techniques in training a DNN is random
    initialization. The idea of using random initialization is just sampling each
    weight from a normal distribution of the input dataset with low deviation. Well,
    a low deviation allows you to bias the network towards the simple 0 solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'But what does it mean? The thing is that, the initialization can be completed
    without the bad repercussions of actually initializing the weights to 0\. Secondly,
    Xavier initialization is often used to train CNNs. It is similar to random initialization
    but often turns out to work much better. Now let me explain the reason for this:'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that you initialize the network weights randomly but they turn out to
    start too small. Then the signal shrinks as it passes through each layer until
    it is too tiny to be useful.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, if the weights in a network start too large, then the signal
    grows as it passes through each layer until it is too massive to be useful.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The good thing is that using Xavier initialization makes sure the weights are
    just right, keeping the signal in a reasonable range of values through many layers.
    In summary, it can automatically determine the scale of initialization based on
    the number of input and output neurons.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interested readers should refer to this publication for detailed information:
    Xavier Glorot and Yoshua Bengio, *Understanding the difficulty of training deep
    FNNs*, Proceedings of the 13th International Conference on **Artificial Intelligence
    and Statistics** (**AISTATS**) 2010, Chia Laguna Resort, Sardinia, Italy. Volume
    9 of JMLR: W&CP.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, you may ask an intelligent question, *Can''t I get rid of the random
    initialization while training a regular DNN (for example, MLP or DBN)*? Well,
    recently, some researchers have been talking about random orthogonal matrix initializations
    that perform better than just any random initialization for training DNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**When it comes to initializing the biases**, it is possible and common to
    initialize the biases to be zero since the asymmetry breaking is provided by the
    small random numbers in the weights. Setting the biases to a small constant value
    such as 0.01 for all biases ensures that all ReLU units can propagate some gradient.
    However, it neither performs well nor does consistent improvement. Therefore,
    sticking with zero is recommended.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several ways of controlling training of CNNs to prevent overfitting
    in the training phase. For example, L2/L1 regularization, max norm constraints,
    and drop out:'
  prefs: []
  type: TYPE_NORMAL
- en: '**L2 regularization**: This is perhaps the most common form of regularization.
    It can be implemented by penalizing the squared magnitude of all parameters directly
    in the objective. For example, using the gradient descent parameter update, L2
    regularization ultimately means that every weight is decayed linearly: *W += -*lambda
    * *W* towards zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**L1 regularization**: This is another relatively common form of regularization,
    where for each weight *w* we add the term *λ∣w∣* to the objective. However, it
    is also possible to possible to combine the L1 regularization with the L2 regularization:
    *λ1∣w∣+λ2w2*, which is commonly known as **Elastic-net regularization**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Max-norm constraints**: Another form of regularization is to enforce an absolute
    upper bound on the magnitude of the weight vector for every neuron and use projected
    gradient descent to enforce the constraint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, dropout is an advanced variant of regularization, which will be discussed
    later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The activation ops provide different types of nonlinearities for use in neural
    networks. These include smooth nonlinearities, such as `sigmoid`, `tanh`, `elu`,
    `softplus`, and `softsign`. On the other hand, some continuous but not-everywhere-differentiable
    functions that can be used are `relu`, `relu6`, `crelu`, and `relu_x`. All activation
    ops apply component-wise and produce a tensor of the same shape as the input tensor.
    Now let us see how to use a few commonly used activation functions in TensorFlow
    syntax.
  prefs: []
  type: TYPE_NORMAL
- en: Using sigmoid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In TensorFlow, the signature `tf.sigmoid(x, name=None)` computes sigmoid of
    `x` element-wise using *y = 1 / (1 + exp(-x))* and returns a tensor with the same
    type `x`. Here is the parameter description:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x`: A tensor. This must be one of the following types: `float32`, `float64`,
    `int32`, `complex64`, `int64`, or `qint32`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: A name for the operation (optional).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using tanh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In TensorFlow, the signature `tf.tanh(x, name=None)` computes a hyperbolic
    tangent of `x` element-wise and returns a tensor with the same type `x`. Here
    is the parameter description:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x`: A tensor or sparse. This is a tensor with type `float`, `double`, `int32`,
    `complex64`, `int64`, or `qint32`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: A name for the operation (optional).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using ReLU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In TensorFlow, the signature `tf.nn.relu(features, name=None)` computes a rectified
    linear using `max(features, 0)` and returns a tensor having the same type as features.
    Here is the parameter description:'
  prefs: []
  type: TYPE_NORMAL
- en: '`features`: A tensor. This must be one of the following types: `float32`, `float64`,
    `int32`, `int64`, `uint8`, `int16`, `int8`, `uint16`, and `half`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: A name for the operation (optional).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more on how to use other activation functions, please refer to the TensorFlow
    website. Up to this point, we have the minimal theoretical knowledge to build
    our first CNN network for making a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Building, training, and evaluating our first CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the next section, we will look at how to classify and distinguish between
    dogs from cats based on their raw images. We will also look at how to implement
    our first CNN model to deal with the raw and color image having three channels.
    This network design and implementation are not straightforward; TensorFlow low-level
    APIs will be used for this. However, do not worry; later in this chapter, we will
    see another example of implementing a CNN using TensorFlow's high-level contrib
    API. Before we formally start, a short description of the dataset is a mandate.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset description
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this example, we will use the dog versus cat dataset from Kaggle that was
    provided for the infamous Dogs versus Cats classification problem as a playground
    competition with kernels enabled. The dataset can be downloaded from [https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data).
  prefs: []
  type: TYPE_NORMAL
- en: The train folder contains 25,000 images of dogs and cats. Each image in this
    folder has the label as part of the filename. The test folder contains 12,500
    images, named according to a numeric ID. For each image in the test set, you should
    predict a probability that the image is a dog (1 = dog, 0 = cat); that is, a binary
    classification problem. For this example, there are three Python scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – Loading the required packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here we import the required packages and libraries. Note that depending upon
    the platform, your imports might be different:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Step 2 – Loading the training/test images to generate train/test set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We set the number of color channels as 3 for the images. In the previous section,
    we have seen that it should be 1 for grayscale images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'For the simplicity, we assume the image dimensions should be squares only.
    Let''s set the size to be `128`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the image size (that is, `128`) and the number of the channel
    (that is, 3), the size of the image when flattened to a single dimension would
    be the multiplication of the image dimension and the number of channels, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that, at a later stage, we might need to reshape the image for the max
    pooling and convolutional layers, so we need to reshape the image. For our case,
    it would be the tuple with height and width of images used to reshape arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We should have explicitly defined the labels (that is, classes) since we only
    have the raw color image, and so the images do not have the labels like other
    numeric machine learning dataset, have. Let''s explicitly define the class info
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to define the batch size that needs to be trained on our CNN model
    later on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we also can define what portion of the training set will be used
    as the validation split. Let''s assume that 16% will be used, for simplicity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'One important thing to set is how long to wait after the validation loss stops
    improving before terminating the training. We should use none if we do not want
    to implement early stopping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, download the dataset and you have to do one thing manually: separate the
    images of dogs and cats and place them in two separate folders. To be more specific,
    suppose you put your training set under the path `/home/DoG_CaT/data/train/`.
    In the train folder, create two separate folders `dogs` and `cats` but only show
    the path to `DoG_CaT/data/train/`. We also assume that our test set is in the
    `/home/DoG_CaT/data/test/` directory. In addition, you can define the checkpoint
    directory where the logs and model checkpoint files will be written:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Then we start reading the training set and prepare it for the CNN model. For
    processing the test and train set, we have another script `Preprocessor.py`. Nonetheless,
    it would be better to prepare the test set as well**:**
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line of code reads the raw images of cats and dogs and creates
    the training set. The `read_train_sets()` function goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code segment, we have used the method `load_train()` to load
    the images which is an instance of a class called `DataSet`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The `DataSet` class, which is used to generate the batches of the training
    set, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, similarly, we prepare the test set from the test images that are mixed
    (dogs and cats):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We have the `read_test_set()` function for ease, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, similar to the training set, we have a dedicated function called `load_test
    ()` for loading the test set, which goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Well done! We can now see some randomly selected images. For this, we have
    the helper function called `plot_images()`; it creates a figure with 3 x 3 sub-plots.
    So, all together, nine images will be plotted, along with their true label. It
    goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s get some random images and their labels from the train set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we plot the images and labels using our helper-function in the preceding
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line of code generates the true labels of the images that are
    randomly selected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9839726-b1c1-4883-87a7-19b3883db8a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The true labels of the images that are randomly selected'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can print the dataset statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Step 3- Defining CNN hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have the training and test set, it''s time to define the hyperparameters
    for the CNN model before we start constructing. In the first and the second convolutional
    layers, we define the width and height of each filter, that is, `3`, where the
    number of filters is `32`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The third convolutional layer has equal dimensions but twice the filters; that
    is, `64` filters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The last two layers are fully connected layers, specifying the number of neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s make the training slower for more intensive training by setting
    a lower value of the learning rate, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Step 4 – Constructing the CNN layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we have defined the CNN hyperparameters, the next task is to implement
    the CNN network. As you can guess, for our task, we will construct a CNN network
    having three convolutional layers, a flattened layer and two fully connected layers
    (refer to `LayersConstructor.py`). Moreover, we need to define the weight and
    the bias as well. Furthermore, we will have implicit max-pooling layers too. At
    first, let''s define the weight. In the following, we have the `new_weights()`
    method that asks for the image shape and returns the truncated normal shapes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we define the biases using the `new_biases()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s define a method, `new_conv_layer()`, for constructing a convolutional
    layer. The method takes the input batch, number of input channels, filter size,
    and number of filters and it also uses the max pooling (if true, we use a 2 x
    2 max pooling) to construct a new convolutional layer. The workflow of the method
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the shape of the filter weights for the convolution, which is determined
    by the TensorFlow API.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the new weights (that is, filters) with the given shape and new biases,
    one for each filter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the TensorFlow operation for the convolution where the strides are set
    to 1 in all dimensions. The first and last stride must always be 1, because the
    first is for the image-number and the last is for the input channel. For example,
    strides= (1, 2, 2, 1) would mean that the filter is moved two pixels across the
    *x* axis and *y* axis of the image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the biases to the results of the convolution. Then a bias-value is added
    to each filter-channel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It then uses the pooling to downsample the image resolution. This is 2 x 2 max
    pooling, which means that we consider 2 x 2 windows and select the largest value
    in each window. Then we move two pixels to the next window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ReLU is then used to calculate the *max(x, 0)* for each input pixel *x*. As
    stated earlier, a ReLU is normally executed before the pooling, but since `relu(max_pool(x))
    == max_pool(relu(x))` we can save 75% of the relu-operations by max-pooling first.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, it returns both the resulting layer and the filter-weights because
    we will plot the weights later.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now we define a function to construct the convolutional layer to be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The next task is to define the flattened layer:'
  prefs: []
  type: TYPE_NORMAL
- en: Get the shape of the input layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The number of features is `img_height * img_width * num_channels`. The `get_shape()`
    function TensorFlow is used to calculate this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It will then reshape the layer to (`num_images` and `num_features`). We just
    set the size of the second dimension to `num_features` and the size of the first
    dimension to -1, which means the size in that dimension is calculated so the total
    size of the tensor is unchanged from the reshaping.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, it returns both the flattened layer and the number of features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code does exactly the same as described before `defflatten_layer(layer)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need to construct the fully connected layers. The following function,
    `new_fc_layer()`, takes the input batches, number of batches, and number of outputs
    (that is, predicted classes) and it uses the ReLU. It then creates the weights
    and biases based on the methods we define earlier in this step. Finally, it calculates
    the layer as the matrix multiplication of the input and weights, and then adds
    the bias values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Step 5 – Preparing the TensorFlow graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We now create the placeholders for the TensorFlow graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Step 6 – Creating a CNN model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we have the input; that is, `x_image` is ready to feed to the convolutional
    layer. We formally create a convolutional layer, followed by the max pooling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We must have the second convolutional layer, where the input is the first convolutional
    layer, `layer_conv1`, followed by the max pooling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have the third convolutional layer where the input is the output of
    the second convolutional layer, that is, `layer_conv2` followed by the max pooling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the third convolutional layer is instantiated, we then instantiate the
    flattened layer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have flattened the images, they are ready to be fed to the first fully
    connected layer. We use the ReLU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we have to have the second and the final fully connected layer where
    the input is the output of the first fully connected layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Step 7 – Running the TensorFlow graph to train the CNN model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following steps are used to perform the training. The codes are self-explanatory,
    like the ones that we have already used in our previous examples. We use softmax
    to predict the classes by comparing them with true classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the `cost` function and then the optimizer (Adam optimizer in this
    case). Then we compute the accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we initialize all the ops using the `global_variables_initializer()` function
    from TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we create and run the TensorFlow session to carry the training across
    the tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We then feed out training data so that the batch size to 32 (see *Step 2*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We maintain two lists to track the training and validation accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We then count the total number of iterations performed so far and create an
    empty list to keep track of all the iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We formally start the training by invoking the `optimize()` function, which
    takes a number of iterations. It needs two:'
  prefs: []
  type: TYPE_NORMAL
- en: The `x_batch` of training examples that holds a batch of images and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y_true_batch`, the true labels for those images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It then converts the shape of each image from (`num` examples, rows, columns,
    depth) to (`num` examples, flattened image shape). After that, we put the batch
    into a `dict` for placeholder variables in the TensorFlow graph. Later on, we
    run the optimizer on the batch of training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, TensorFlow assigns the variables in `feed_dict_train` to the placeholder
    variables. Optimizer is then executed to print the status at end of each epoch.
    Finally, it updates the total number of iterations that we performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: We will show how our training went along in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Step 8 – Model evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have managed to finish the training. It is time to evaluate the model. Before,
    we start evaluating the model, let's implement some auxiliary functions for plotting
    the example errors and printing the validation accuracy. The `plot_example_errors()`
    takes two parameters. The first is `cls_pred`, which is an array of the predicted
    class-number for all images in the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second parameter, `correct`, is a `boolean` array to predict whether the
    predicted class is equal to `true` class for each image in the test set. At first,
    it gets the images from the test set that have been incorrectly classified. Then
    it gets the predicted and the true classes for those images, and finally it plots
    the first nine images with their classes (that is, predicted versus true labels):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The second auxiliary function is called `print_validation_accuracy()`; it prints
    the validation accuracy. It allocates an array for the predicted classes, which
    will be calculated in batches and filled into this array, and then it calculates
    the predicted classes for the batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have our auxiliary functions, we can start the optimization. At
    the first place, let''s iterate the fine-tuning 10,000 times and see the performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'After 10,000 iterations, we observe the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'This means the accuracy on the test set is about 79%. Also, let''s see how
    well our classifier performs on a sample image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0972b7f4-b784-4307-b74c-87f81c5a50d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Random prediction on the test set (after 10,000 iterations)'
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, we further iterate the optimization up to 100,000 times and observe
    better accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d5393dde-dbf3-406f-b795-50d28d431e2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Random prediction on the test set (after 100,000 iterations)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'So it did not improve that much but was a 2% increase on the overall accuracy.
    Now is the time to evaluate our model for a single image. For simplicity, we will
    take two random images of a dog and a cat and see the prediction power of our
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/45e9ed02-1b51-41d2-8f8a-e50e99679671.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Example image for the cat and dog to be classified'
  prefs: []
  type: TYPE_NORMAL
- en: 'At first, we load these two images and prepare the test set accordingly, as
    we have seen in an earlier step in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we have the following function for making the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, when we''re done, we close the TensorFlow session by invoking the
    `close()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Model performance optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since CNNs are different from the layering architecture's perspective, they
    have different requirements as well as tuning criteria. How do you know what combination
    of hyperparameters is the best for your task? Of course, you can use a grid search
    with cross-validation to find the right hyperparameters for linear machine learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: However, for CNNs, there are many hyperparameters to tune, and since training
    a neural network on a large dataset takes a lot of time, you will only be able
    to explore a tiny part of the hyperparameter space in a reasonable amount of time.
    Here are some insights that can be followed.
  prefs: []
  type: TYPE_NORMAL
- en: Number of hidden layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For many problems, you can just begin with a single hidden layer and you will
    get reasonable results. It has actually been shown that an MLP with just one hidden
    layer can model even the most complex functions provided it has enough neurons.
    For a long time, these facts convinced researchers that there was no need to investigate
    any deeper neural networks. However, they overlooked the fact that deep networks
    have a much higher parameter efficiency than shallow ones; they can model complex
    functions using exponentially fewer neurons than shallow nets, making them much
    faster to train.
  prefs: []
  type: TYPE_NORMAL
- en: It is to be noted that this might not be always the case. However, in summary,
    for many problems, you can start with just one or two hidden layers. It will work
    just fine using two hidden layers with the same total amount of neurons, in roughly
    the same amount of training time. For a more complex problem, you can gradually
    ramp up the number of hidden layers, until you start overfitting the training
    set. Very complex tasks, such as large image classification or speech recognition,
    typically require networks with dozens of layers and a huge amount of training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Number of neurons per hidden layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Obviously, the number of neurons in the input and output layers is determined
    by the type of input and output your task requires. For example, if your dataset
    has the shape of 28 x 28 it should expect to have input neurons with size 784
    and the output neurons should be equal to the number of classes to be predicted.
    As for the hidden layers, a common practice is to size them to form a funnel,
    with fewer and fewer neurons at each layer, the rationale being that many low-level
    features can coalesce into far fewer high-level features. However, this practice
    is not as common now, and you may simply use the same size for all hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'If there are four convolutional layers with 256 neurons, that''s just one hyperparameter
    to tune instead of one per layer. Just like the number of layers, you can try
    increasing the number of neurons gradually until the network starts overfitting.
    Another important question is: when would you want to add a max pooling layer
    rather than a convolutional layer with the same stride? The thing is that a max-pooling
    layer has no parameters at all, whereas a convolutional layer has quite a few.'
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, adding a local response normalization layer that makes the neurons
    that most strongly activate inhibit neurons at the same location but in neighboring
    feature maps, encourages different feature maps to specialize and pushes them
    apart, forcing them to explore a wider range of features. It is typically used
    in the lower layers to have a larger pool of low-level features that the upper
    layers can build upon.
  prefs: []
  type: TYPE_NORMAL
- en: Batch normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Batch normalization** (**BN**) is a method to reduce internal covariate shift
    while training regular DNNs. This can apply to CNNs too. Due to the normalization,
    BN further prevents smaller changes to the parameters to amplify and thereby allows
    higher learning rates, making the network even faster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/844dd566-706b-49c2-81d2-3d64964ff092.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The idea is placing an additional step between the layers, in which the output
    of the layer before is normalized. To be more specific, in the case of non-linear
    operations (for example, ReLU), BN transformation has to be applied to the non-linear
    operation. Typically, the overall process has the following workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: Transforming the network into a BN network (see *Figure 1*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then training the new network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming the batch statistic into a population statistic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This way, BN can fully partake in the process of backpropagation. As shown in
    *Figure 1*, BN is performed before the other processes of the network in this
    layer are applied. However, any kind of gradient descent (for example, **stochastic
    gradient descent** (**SGD**) and its variants) can be applied to train the BN
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interested readers can refer to the original paper to get to more information:
    Ioffe, Sergey, and Christian Szegedy. *Batch normalization: Accelerating deep
    network training by reducing internal covariate shift*. arXiv preprint arXiv:1502.03167
    (2015).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now a valid question would be: where to place the BN layer? Well, to know the
    answer, a quick evaluation of BatchNorm layer performance on ImageNet-2012 ([https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md](https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md))
    shows the following benchmark:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ea9e9c0-8cf4-42a0-a436-c50a531f1857.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding table, it can be seen that placing BN after non-linearity
    would be the right way. The second question would be: what activation function
    should be used in a BN layer? Well, from the same benchmark, we can see the following
    result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/71798011-3bf7-421a-a5dd-05a4786e3b4e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding table, we can assume that using ReLU or its variants would
    be a better idea. Now, another question would be how to use these using deep learning
    libraries. Well, in TensorFlow, it is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'A general warning: set this to `True` for training and `False` for testing.
    However, the preceding addition introduces extra ops to be performed on the graph,
    which is updating its mean and variance variables in such a way that they will
    not be dependencies of your training op. To do it, we can just run the ops separately,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Advanced regularization and avoiding overfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in the previous chapter, one of the main disadvantages observed
    during the training of large neural networks is overfitting, that is, generating
    very good approximations for the training data but emitting noise for the zones
    between single points. There are a couple of ways to reduce or even prevent this
    issue, such as dropout, early stop, and limiting the number of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of overfitting, the model is specifically adjusted to the training
    dataset, so it will not be used for generalization. Therefore, although it performs
    well on the training set, its performance on the test dataset and subsequent tests
    is poor because it lacks the generalization property:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a06dfa25-f5a6-4e97-bd16-c73552f1582a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Dropout versus without dropout'
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of this method is that it avoids holding all the neurons
    in a layer to optimize their weights synchronously. This adaptation made in random
    groups prevents all the neurons from converging to the same goals, thus de-correlating
    the adapted weights. A second property found in the dropout application is that
    the activation of the hidden units becomes sparse, which is also a desirable characteristic.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, we have a representation of an original fully connected
    multilayer neural network and the associated network with the dropout linked.
    As a result, approximately half of the input was zeroed (this example was chosen
    to show that probabilities will not always give the expected four zeroes). One
    factor that could have surprised you is the scale factor applied to the non-dropped
    elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'This technique is used to maintain the same network, and restore it to the
    original architecture when training, using `dropout_keep_prob` as 1\. A major
    drawback of using dropout is that it does not have the same benefits for convolutional
    layers, where the neurons are not fully connected. To address this issue, there
    are a few techniques can be applied, such as DropConnect and stochastic pooling:'
  prefs: []
  type: TYPE_NORMAL
- en: DropConnect is similar to dropout as it introduces dynamic sparsity within the
    model, but it differs in that the sparsity is on the weights, rather than the
    output vectors of a layer. The thing is that a fully connected layer with DropConnect
    becomes a sparsely connected layer in which the connections are chosen at random
    during the training stage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In stochastic pooling, the conventional deterministic pooling operations are
    replaced with a stochastic procedure, where the activation within each pooling
    region is picked randomly according to a multinomial distribution, given by the
    activities within the pooling region. The approach is hyperparameter free and
    can be combined with other regularization approaches, such as dropout and data
    augmentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stochastic pooling versus standard max pooling:** Stochastic pooling is equivalent
    to standard max pooling but with many copies of an input image, each having small
    local deformations.'
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, one of the simplest methods to prevent overfitting of a network is
    to simply stop the training before overfitting gets a chance to occur. This comes
    with the disadvantage that the learning process is halted. Thirdly, limiting the
    number of parameters is sometimes helpful and helps avoid overfitting. When it
    comes to CNN training, the filter size also affects the number of parameters.
    Thus, limiting this type of parameter restricts the predictive power of the network
    directly, reducing the complexity of the function that it can perform on the data,
    and that limits the amount of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Applying dropout operations with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we apply the dropout operation to a sample vector, it will work on transmitting
    the dropout to all the architecture-dependent units. In order to apply the dropout
    operation, TensorFlow implements the `tf.nn.dropout` method, which works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Where `x` is the original tensor. The `keep_prob` means the probability of
    keeping a neuron and the factor by which the remaining nodes are multiplied. The
    `noise_shape` signifies a four-element list that determines whether a dimension
    will apply zeroing independently or not. Let''s have a look at this code segment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, you can see the results of applying dropout to the
    *x* variable, with a 0.5 probability of zero; in the cases in which it didn't
    occur, the values were doubled (multiplied by 1/1.5, the dropout probability).
  prefs: []
  type: TYPE_NORMAL
- en: Which optimizer to use?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When using a CNN, since one of the objective functions is to minimize the evaluated
    cost, we must define an optimizer. Using the most common optimizer , such as SGD,
    the learning rates must scale with *1/T* to get convergence, where *T* is the
    number of iterations. Adam or RMSProp try to overcome this limitation automatically
    by adjusting the step size so that the step is on the same scale as the gradients.
    In addition, in the previous example, we have used Adam optimizer, which performs
    well in most cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, if you are training a neural network but computing the gradients
    is mandatory, using the `RMSPropOptimizer` function (which implements the `RMSProp`
    algorithm) is a better idea since it would be the faster way of learning in a
    mini-batch setting. Researchers also recommend using the momentum optimizer, while
    training a deep CNN or DNN. Technically, `RMSPropOptimizer` is an advanced form
    of gradient descent that divides the learning rate by an exponentially decaying
    average of squared gradients. The suggested setting value of the decay parameter
    is 0.9, while a good default value for the learning rate is 0.001\. For example,
    in TensorFlow, `tf.train.RMSPropOptimizer()` helps us to use this with ease:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Memory tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we try to provide some insights. We start with an issue and
    its solution; convolutional layers require a huge amount of RAM, especially during
    training, because the reverse pass of backpropagation requires all the intermediate
    values computed during the forward pass. During inference (that is, when making
    a prediction for a new instance), the RAM occupied by one layer can be released
    as soon as the next layer has been computed, so you only need as much RAM as required
    by two consecutive layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, during training, everything computed during the forward pass
    needs to be preserved for the reverse pass, so the amount of RAM needed is (at
    least) the total amount of RAM required by all layers. If your GPU runs out of
    memory while training a CNN, here are five things you can try to solve the problem
    (other than purchasing a GPU with more RAM):'
  prefs: []
  type: TYPE_NORMAL
- en: Reduce the mini-batch size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce dimensionality using a larger stride in one or more layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove one or more layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use 16-bit floats instead of 32-bit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distribute the CNN across multiple devices (see more at [https://www.tensorflow.org/deploy/distributed](https://www.tensorflow.org/deploy/distributed))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appropriate layer placement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another important question would be: when do you want to add a max pooling
    layer rather than a convolutional layer with the same stride? The thing is that
    a max-pooling layer has no parameters at all, whereas a convolutional layer has
    quite a few.'
  prefs: []
  type: TYPE_NORMAL
- en: Even adding a local response normalization layer sometimes makes the neurons
    that most strongly activate inhibit neurons at the same location but in neighboring
    feature maps, which encourages different feature maps to specialize and pushes
    them apart, forcing them to explore a wider range of features. It is typically
    used in the lower layers to have a larger pool of low-level features that the
    upper layers can build upon.
  prefs: []
  type: TYPE_NORMAL
- en: Building the second CNN by putting everything together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we know how to optimize the layering structure in a CNN by adding dropout,
    BN, and biases initializers, such as Xavier. Let's try to apply these to a less
    complex CNN. Throughout this example, we will see how to solve a real-life classification
    problem. To be more specific, our CNN model will be able to classify the traffic
    sign from a bunch of images.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset description and preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this we will be using the Belgian traffic dataset (BelgiumTS for Classification
    (cropped images)). This dataset can be download from [http://btsd.ethz.ch/shareddata/](http://btsd.ethz.ch/shareddata/).
    Here are a quick glimpse about the traffic signs convention in Belgium:'
  prefs: []
  type: TYPE_NORMAL
- en: Belgian traffic signs are usually in Dutch and French. This is good to know,
    but for the dataset that you'll be working with, it's not too important!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are six categories of traffic signs in Belgium: warning signs, priority
    signs, prohibitory signs, mandatory signs, signs related to parking and standing
    still on the road and, lastly, designatory signs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once we download the aforementioned dataset, we will see the following directory
    structure (training left, test right):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce5f8479-60f0-4703-8e95-c991d7d6b829.png)'
  prefs: []
  type: TYPE_IMG
- en: The images are in `.ppm` format; otherwise we could've used TensorFlow built-in
    image loader (example, `tf.image.decode_png`). However, we can use the `skimage`
    Python package.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python 3, execute `$ sudo pip3 install scikit-image` for `skimage` to install
    and use this package. So let''s get started by showing the directory path as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Then let''s write a function using the `skimage` library to read the images
    and returns two lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '`images`: A list of Numpy arrays, each representing an image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels`: A list of numbers that represent the images labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code block is straightforward and contains inline comments. How
    about showing related statistics about images? However, before that, let''s invoke
    the preceding function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Then let''s see some statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'So we have 62 classes to be predicted (that is, a multiclass image classification
    problem) and we have many images too that should be sufficient to satisfy a smaller
    CNN.Now let''s see the class distribution visually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/03894908-617b-4b51-9011-b3d5f4a084e3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, from the preceding figure, we can see that classes are very imbalanced.
    However, to make it simpler, we won''t take care of this but next, it would be
    great to visually inspect some files, say displaying the first image of each label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/05a6bed9-b026-42a6-ac02-e2dc40663a60.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now you can see from the preceding figure that the images come in different
    sizes and shapes. Moreover, we can see it using Python code, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, we need to apply some pre-processing such as resizing, reshaping,
    and so on to each image. Let''s say each image will have size of 32 x 32:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, all of our images have same size. The next task would be to convert labels
    and image features as a `numpy` array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: Fantastic! The next task would be creating our second CNN, but this time we
    will be using TensorFlow `contrib` package, which is a high-level API that supports
    layering ops.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the CNN model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are going to construct a complex network. However, it has a straightforward
    architecture. At the beginning, we use Xavier as the network initializer. Once
    we initialize the network bias using the Xavier initializer. The input layer is
    followed by a convolutional layer (convolutional layer 1), which is again followed
    by a BN layer (that is, BN layer 1). Then there is a pooling layer with strides
    of two and a kernel size of two. Then another BN layer follows the second convolutional
    layer. Next, there is the second pooling layer with strides of two and kernel
    size of two. Well, then the max polling layer is followed by a flattening layer
    that flattens the input from (None, height, width, channels) to (None, height
    * width * channels) == (None, 3072).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the flattening is completed, the input is fed into the first fully connected
    layer 1\. Then third BN is applied as a normalizer function. Then we will have
    a dropout layer before we feed the lighter network into the fully connected layer
    2 that generates logits of size (None, 62). Too much of a mouthful? Don''t worry;
    we will see it step by step. Let''s start the coding by creating the computational
    graph, creating both features, and labeling placeholders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Up to this point, we have managed to generate the logits of size (`None, 62`).
    Then we need to convert the logits to label indexes (`int`) with the shape (`None`),
    which is a 1D vector of `length == batch_size:predicted_labels = tf.argmax(logits,
    axis=1)`. Then we define cross-entropy as the `loss` function, which is a good
    choice for classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Now one of the most important parts is updating the ops and creating an optimizer
    (Adam in our case):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we initialize all the ops:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: Training and evaluating the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We start by create a session to run the graph we created. Note that for faster
    training, we should use a GPU. However, if you do not have a GPU, just set `log_device_placement=False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the training is completed, let us pick 10 random images and see the predictive
    power of our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Then let''s run the `predicted_labels op`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'So we can see that some images were correctly classified and some wrongly.
    However, visual inspection would be more helpful. So let''s display the predictions
    and the ground truth:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/bc955eb3-eac0-47a3-b447-c0b6722c80cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we can evaluate our model using the test set. To see the predictive
    power, we compute the accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: Not that bad in terms of accuracy. In addition to this, we can also compute
    other performance metrics such as precision, recall, f1 measure and also visualize
    the result in a confusion matrix to show the predicted versus actual labels count.
    Nevertheless, we can still improve the accuracy by tuning the network and hyperparameters.
    But I leave these up to the readers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we are done, so let''s close the TensorFlow session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how to use CNNs, which are a type of feed-forward
    artificial neural network in which the connectivity pattern between neurons is
    inspired by the organization of an animal's visual cortex. We saw how to cascade
    a set of layers to construct a CNN and perform different operations in each layer.
    Then we saw how to train a CNN. Later on, we discussed how to optimize the CNN
    hyperparameters and optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we built another CNN, where we utilized all the optimization techniques.
    Our CNN models did not achieve outstanding accuracy since we iterated both of
    the CNNs a few times and did not even apply any grid searching techniques; that
    means we did not hunt for the best combinations of the hyperparameters. Therefore,
    the takeaway would be to apply more robust feature engineering in the raw images,
    iterate the training for more epochs with the best hyperparameters, and observe
    the performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how to use some deeper and popular CNN architectures,
    such as ImageNet, AlexNet, VGG, GoogLeNet, and ResNet. We will see how to utilize
    these trained models for transfer learning.
  prefs: []
  type: TYPE_NORMAL
