- en: Text Classification Using Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recurrent neural networks are useful for solving problems where data involves
    sequences. Some examples of applications involving sequences are seen in text
    classification, time series prediction, the sequence of frames in videos, DNA
    sequences, and speech recognition.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will develop a sentiment (positive or negative) classification
    model using a recurrent neural network. We will begin by preparing the data for
    developing the text classification model, followed by developing a sequential
    model, compiling the model, fitting the model, evaluating the model, prediction,
    and model performance assessment using a confusion matrix. We will also review
    some tips for sentiment classification performance optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, in this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing data for model building
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing a recurrent neural network model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fitting the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model evaluation and prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance optimization tips and best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing data for model building
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll be using the **Internet Movie Database** (**IMDb**) movie
    reviews text data that's available in the Keras package. Note that there is no
    need to download this data from anywhere as it can be easily accessed from the
    Keras library using code that we will discuss soon. In addition, this dataset
    is preprocessed so that text data is converted into a sequence of integers. We
    cannot use text data directly for model building, and such preprocessing of text
    data into a sequence of integers is necessary before the data can be used as input
    for developing deep learning networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by loading the `imdb` data using the `dataset_imdb` function,
    where we will also specify the number of most frequent words as 500 using `num_words`.
    Then, we''ll split the `imdb` data into `train` and `test` datasets. Let''s take
    a look at the following code to understand this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`train_x` and `test_x` contain integers representing reviews in the train and
    test data, respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, `train_y` and `test_y` contain `0` and `1` labels, representing negative
    and positive sentiments, respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the `length` function, we can see that both `train_x` and `test_x` are
    based on 25,000 movie reviews each.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tables for `train_y` and `test_y` show that there is an equal number of
    positive (12,500) and negative (12,500) reviews in the train and test data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having such a balanced dataset is useful in avoiding any bias due to class imbalance
    issues.
  prefs: []
  type: TYPE_NORMAL
- en: The words in the movie review are represented by unique integers and each integer
    that is assigned to a word is based on its overall frequency in the dataset. For
    example, integer 1 represents the most frequent word, while integer 2 represents
    the second most frequent word, and so on. In addition, integer 0 is not used for
    any specific word but it indicates an unknown word.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the third and sixth sequences in the `train_x` data using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code and output, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: From the output of the third movie review-related sequence of integers, we can
    observe that the third review contains 141 integers between 1 (1st integer) and
    369 (16th integer).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since we restricted the use of the most frequent words to 500, for the third
    review, there is no integer larger than 500.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, from the output of the sixth review's related sequence of integers,
    we can observe that the sixth review contains 43 integers between 1 (1st integer)
    and 226 (35th integer).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at the length of the first six sequences in the `train_x` data, we can
    observe that the length of the movie review varies between 43 (6th review in train
    data) and 550 (4th review in train data). Such variation in the length of the
    movie reviews is normal and is as expected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we can develop a movie review sentiment classification model, we need
    to find a way to make the length of a sequence of integers the same for all the
    movie reviews. We can achieve this by padding sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Padding sequences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Padding the text sequences is carried out to ensure that all the sequences
    have the same length. Let''s take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We can achieve equal length for all the sequences of integers with the help
    of the `pad_sequences` function and by specifying a value for `maxlen`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this example, we have restricted the length of each movie review sequence
    in the train and test data to 100\. Note that before padding of sequences, the
    structure of `train_x` and `test_x` is a list of 25,000 reviews.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, after padding the sequences, the structure for both changes to a matrix
    that's 25,000 x 100\. This can be easily verified by running `str(train_x)` before
    and after padding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To observe the impact of padding on a sequence of integers, let''s take a look
    at the following code, along with its output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the third sequence of integers after padding of the `train_x` can
    be seen in the preceding code. Here, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The third sequence now has a length of 100\. The third sequence originally had
    141 integers and we can observe that 41 integers that were located at the beginning
    of the sequence have been truncated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, the output of the sixth sequence shows a different pattern.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sixth sequence originally had a length of 43, but now 57 zeros have been
    added to the beginning of the sequence to artificially extended the length to
    100.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All 25,000 sequences of integers related to movie reviews in each of the train
    and test data are impacted in a similar way.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will develop an architecture for a recurrent neural
    network that will be used for developing a movie review sentiment classification
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Developing a recurrent neural network model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will develop the architecture for the recurrent neural
    network and compile it. Let''s look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We start by initializing the model using the `keras_model_sequential` function.
    Then, we add embedding and simple **recurrent neural network** (**RNN**) layers.
    For the embedding layer, we specify `input_dim` to be 500, which is the same as
    the number of most frequent words that we had specified earlier. The next layer
    is a simple RNN layer, with the number of hidden units specified as 8.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the default activation function for the `layer_simple_rnn` layer is
    a hyperbolic tangent (tanh), which is an S-shaped curve where the output ranges
    from -1 to +1.
  prefs: []
  type: TYPE_NORMAL
- en: The last dense layer has one unit to capture movie review sentiment (positive
    or negative) with the activation function sigmoid. When an output lies between
    0 and 1, as in this case, it is convenient for interpretation as it can be thought
    of as a probability.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the sigmoid activation function is an S-shaped curve where the output
    ranges between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at the model summary and understand how we can calculate on
    the number of parameters that are required.
  prefs: []
  type: TYPE_NORMAL
- en: Calculation of parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The summary of the RNN model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The number of parameters for the embedding layer is arrived at by multiplying
    500 (number of most frequent words) and 32 (output dimension) to obtain 16,000\.
    To arrive at the number of parameters for the simple RNN layer, we use *(h(h+i)
    + h)*, where *h* represents the number of hidden units and *i* represents the
    input dimension for this layer. In this case, this is 32.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we have (8(8 + 32)+8) = 328 parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Note that if we consider a fully connected dense layer here, we would have obtained
    (8 x 32 + 8) = 264\. However, the additional 64 parameters are due to the fact
    that we use recurrent layers to capture sequences in the text data.
  prefs: []
  type: TYPE_NORMAL
- en: In recurrent layers, information from the previous input is also used, which
    leads to these extra parameters that we can see here. This is the reason why RNNs
    are better suited for handling sequence data compared to a regular densely connected
    neural network layer. For the last layer, which is a dense layer, we have (1 x
    8 + 1) = 9 parameters. Overall, this architecture has 16,337 parameters.
  prefs: []
  type: TYPE_NORMAL
- en: In recurrent layers, the use of information from the previous input helps to
    provide a better representation of a sequence that is present in text or similar
    data that contains some kind of sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code for compiling the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We compile the model with the `rmsprop` optimizer, which is recommended for
    recurrent neural networks. We make use of `binary_crossentropy` as the loss function
    due to a binary type of response since movie reviews are either positive or negative.
    Finally, for metrics, we have specified accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will use this architecture to develop a movie review
    sentiment classification model that uses recurrent neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code for fitting the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: For fitting the model,  we will make use of a 20% validation split, which uses
    20,000 movie review data from training data for building the model. The remaining
    5,000 movie review training data is used for assessing validation in the form
    of loss and accuracy. We run 10 epochs with a batch size of 128.
  prefs: []
  type: TYPE_NORMAL
- en: When using a validation split, it is important to note that, with 20%, it uses
    the first 80% of the training data for training and the last 20% of the training
    data for validation. Thus, if the first 50% of the review data was negative and
    the last 50% was positive, the 20% validation split will cause model validation
    to be based only on positive reviews. Therefore, before using a validation split,
    we must verify that this is not the case; otherwise, it will introduce significant
    bias.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy and loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The accuracy and loss values after 10 epochs for training and validation data
    using `plot(model_one)` can be seen in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/296b64bb-74b6-4079-b07d-d50070d75845.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding graph, the following observations can be made:'
  prefs: []
  type: TYPE_NORMAL
- en: The training loss continues to decrease from epoch 1 to 10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validation loss reduces initially, but it starts to look flat after 3 epochs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A similar pattern is also observed for accuracy in the opposite direction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will evaluate the classification model and assess model
    prediction performance with the help of train and test data.
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation and prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we will evaluate the model based on the train data for loss and accuracy.
    We will also obtain a confusion matrix based on the train data. The same process
    shall be repeated with the test data.
  prefs: []
  type: TYPE_NORMAL
- en: Training the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the `evaluate` function to obtain the loss and accuracy values,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As seen from the preceding output, the loss and accuracy values based on the
    training data are 0.406 and 0.821, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Predictions using training data are used for developing a confusion matrix,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following observations can be made by looking at the preceding confusion
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: There are 9,778 movie reviews that are correctly classified as negative and
    there are 10,738 movie reviews that are correctly classified as positive. We can
    observe that the model does a decent job of classifying the reviews as positive
    or negative.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at the misclassifications, we can also observe that, on 2,722 occasions,
    negative movie reviews are misclassified as positive movie reviews. This is relatively
    higher compared to the misclassification of positive reviews as negative (1,762
    times) by the classification model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let's do a similar assessment based on test data.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code to obtain the loss and accuracy values is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that the loss and accuracy based on the test data are 0.467
    and 0.778, respectively. These results are slightly inferior to what we observed
    for the train data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we''ll predict the classes for the test data and use the results to obtain
    a confusion matrix, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Apart from the overall results being slightly inferior to the ones that we obtained
    from the train data, we can't see any major differences between the train and
    test data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore a few strategies to improve model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Performance optimization tips and best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When developing a recurrent neural network model, we come across situations
    where we need to make several decisions related to the network. These decisions
    could include trying a different activation function rather than the default one
    that we had used. Let's make such changes and see what impact they have on the
    movie review sentiment classification performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will experiment with the following four factors:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of units in the simple RNN layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using different activation functions in the simple RNN layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding more recurrent layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changes in the maximum length for padding sequences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of units in the simple RNN layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code for incorporating this change and then compiling/fitting the model
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, we change the architecture by increasing the number of units in the simple
    RNN layer from 8 to 32\. Everything else is kept the same. Then, we compile and
    fit the model, as shown in the preceding code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The accuracy and loss values after 10 epochs can be seen in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a5f3db7e-27c4-4c12-a305-528783ec3d78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding plot indicates the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A significantly bigger gap between training and validation data on epoch 3 onward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This clearly suggests an increased level of overfitting compared to the preceding
    plot, where the number of units in the simple RNN was 8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is also reflected in the higher loss value of 0.585 and the lower accuracy
    value of 0.757 that we obtained for the test data based on this new model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's experiment with a different activation function in the simple RNN
    layer and see whether this overfitting issue can be resolved.
  prefs: []
  type: TYPE_NORMAL
- en: Using different activation functions in the simple RNN layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This change can be seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are changing the default activation function in the
    simple RNN layer to a ReLU activation function. We keep everything else the same
    as what we had in the previous experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The accuracy and loss values after 10 epochs can be seen in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/381e8b5e-2d09-42d2-bdce-ff8d0b3feb7c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding plot, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The loss and accuracy values look much better now.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both the loss and accuracy curves based on training and validation are now closer
    to each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We used the model to find the loss and accuracy values based on the test data
    that we obtained, that is, 0.423 and 0.803, respectively. This shows better results
    compared to the results we've obtained so far.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will experiment further by adding more recurrent layers. This will
    help us build a deeper recurrent neural network model.
  prefs: []
  type: TYPE_NORMAL
- en: Adding more recurrent layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will experiment by adding two additional recurrent layers to the current
    network. The code that''s incorporating this change is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'When we add these additional recurrent layers, we also set `return_sequences`
    to `TRUE`. We keep everything else the same and compile/fit the model. The plot
    for the loss and accuracy values based on the training and validation data is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82787f84-dbc6-4bba-a200-7fe4294bfbf3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding plot, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: After 10 epochs, the loss and accuracy values for training and validation show
    a reasonable level of closeness, indicating the absence of overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss and accuracy based on the test data we calculated show a decent improvement
    in the results with 0.403 and 0.816, respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This shows that deeper recurrent layers did help capture sequences of words
    in the movie reviews in a much better way. This, in turn, enabled improved classification
    of the sentiment in movie reviews as positive or negative.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum length for padding sequences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have used a maximum length of 100 for padding sequences of movie
    reviews in the train and test data. Let''s look at the summary of the length of
    movie reviews in the `train` and `test` data using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: From the summary of the length of movie reviews in the train data, we can see
    that the minimum length is 11, the maximum length is 2,494, and that the median
    length is 178.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, the test data has a minimum review length of 7, a maximum length
    of 2,315, and a median length of 174.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when the maximum padding length is below the median (which is the
    case with a maximum length of 100), we tend to truncate more movie reviews by
    removing words beyond 100\. At the same time, when we choose a maximum length
    for padding to be significantly above the median, we will have a situation where
    a higher number of movie reviews will need to contain zeros and fewer number of
    reviews will be truncated.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we are going to explore the impact of keeping the maximum
    length of the sequence of words in the movie reviews near the median value. The
    code for incorporating this change is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding code, we can see that we run the model after specifying `maxlen`
    as 200\. We keep everything else the same as what we had for `model_four`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The plot for the loss and accuracy for the training and validation data is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43039c1f-594a-45eb-861a-3800e19aab7a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding plot, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: There's the absence of an overfitting issue since the training and validation
    data points are very close to each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss and accuracy based on the test data were calculated as 0.383 and 0.830,
    respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss and accuracy values are at their best level at this stage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The confusion matrix based on the test data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'From the confusion matrix, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: This classification model seems to performs slightly better when correctly classifying
    the movie review as positive (10,681) compared to when classifying a negative
    (10,066) review correctly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As far as reviews that are classified incorrectly are concerned, the trend that
    we had observed earlier, where negative movie reviews were mistakenly classified
    by the model as positive being on the higher side, exists in this case too.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we experimented with a number of units, activation functions,
    the number of recurrent layers in the network, and the amount of padding in order
    to improve the movie review sentiment classification model. Some other factors
    that you could explore further include the number of most frequent words to include
    and changing the maximum length at the time of padding sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we illustrated the use of the recurrent neural network model
    for text sentiment classification using IMDb movie review data. Compared to a
    regular densely connected network, recurrent neural networks are better suited
    to deal with data that has sequences in it. Text data is one such example that
    we worked with in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In general, deep networks involve many factors or variables, and this calls
    for some amount of experimentation involving making changes to the levels for
    such factors before arriving at a useful model. In this chapter, we also developed
    five different movie review sentiment classification models.
  prefs: []
  type: TYPE_NORMAL
- en: A variant of recurrent neural networks that has become popular is **Long Short-Term
    Memory** (**LSTM**) networks. LSTM networks are capable of learning long-term
    dependencies and help recurrent networks remember inputs for a longer time.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will go over an application example of using an LSTM
    network, where we will continue to use IMDb movie review data and explore further
    improvements that can be made to the sentiment classification model's performance.
  prefs: []
  type: TYPE_NORMAL
