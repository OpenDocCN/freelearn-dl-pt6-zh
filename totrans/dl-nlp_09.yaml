- en: '*Chapter 9*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Practical NLP Project Workflow in an Organization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify the requirements of a natural language processing project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand how different teams in an organization might be involved
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Google Colab notebooks to leverage a GPU to train Deep Learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy a model on AWS to be used as Software as a Service (SaaS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get acquainted with a simple tech stack for deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will be looking at a real-time NLP project and its flow
    in an organization,right till the final stage through the entire chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Up to this point in the book, we have studied several deep learning techniques
    that can be applied to solve specific problems in the NLP domain. Having knowledge
    of these techniques has empowered us to build good models and deliver high-quality
    performance. However, when it comes to delivering a working machine learning product
    in an organization, several other aspects need to be considered.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will go through a practical project workflow when delivering
    a working deep learning system in an organization. Specifically, you will be introduced
    to the possible roles of various teams within your organization, building a deep
    learning pipeline and, finally, delivering your product in the form of SaaS.
  prefs: []
  type: TYPE_NORMAL
- en: General Workflow for the Development of a Machine Learning Product
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Today, there are several ways of working with data science in an organization.
    Most organizations have a workflow that is specific to their environment. Some
    example workflows are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1: General workflow for the development of a machine learning product](img/C13783_9_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: General workflow for the development of a machine learning product'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The Presentation Workflow:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](img/C13783_9_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: General presentation workflow'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The presentation workflow can be elaborated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The data science team receives a request to solve a problem using machine learning.
    The requester could be some other team within the organization or some other company
    that has hired you as consultants.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You obtain the relevant data and apply specific machine learning techniques.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You showcase the results and insights in the form of a report/presentation to
    the stakeholders. This could also be a potential way to approach the *Proof of
    Concept* (*PoC*) phase of a project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The Research Workflow:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](img/C13783_9_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3: Research workflow'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The main focus of this approach is to conduct research to solve a particular
    problem that caters to a use case. The solution can be leveraged both by the organization
    as well as the community in general. Other factors that distinguish this workflow
    from the presentation workflow are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The timelines for such projects are typically longer than those imposed on presentation
    workflows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The deliverable is in the form of research papers and/or toolboxes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The workflow can be broken down as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Your organization has a research wing that wishes to enhance the existing machine
    learning state in the community, while also allowing your company to leverage
    the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Your team goes through the existing research that caters to the problem you
    are being asked to solve. This involves reading research papers in detail and
    implementing them to establish the baseline performance on some datasets suggested
    in the research papers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You then either try to tailor the existing research to solve your problem or
    come up with novel ways to solve it yourself.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The end product could be research papers and/or toolboxes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Production-Oriented Workflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Figure 9.4: Production-oriented workflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_9_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.4: Production-oriented workflow'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The workflow can be elaborated on as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The data science team receives a request to solve a problem using machine learning.
    The requester could be some other team within the organization or another company
    that has hired you as consultants. It could also be that the data science team
    wishes to build a product that they think will bring value to the organization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You obtain the data, do the necessary research, and build the machine learning
    model. The data could be obtained either from within the organization or, if the
    problem is general enough (for example: language translation), it could also be
    an open source dataset. The model built could, hence, qualify as *PoC* to be shown
    to the stakeholders.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You define a Minimum Viable Product (MVP): for example, a machine learning
    model in the form of SaaS.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once MVP is achieved, you iteratively add other aspects, such as *Data Acquisition
    Pipelines*, *Continuous Integration*, *Monitoring* and so on.
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that even the sample workflows share components. In this chapter,
    our focus will be on part of *The Production Workflow*. We will build a Minimum
    Viable Product for a specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: Problem Definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's say that you work for an e-commerce platform, through which your customers
    can purchase a variety of products. The merchandising department of your company
    comes up with a request to add a feature to the website – '**Addition of a slider
    that contains the 5 items that received the most positive reviews in a given calendar
    week**'.
  prefs: []
  type: TYPE_NORMAL
- en: This request is first made to the web development department since, ultimately,
    they are the ones responsible for displaying the website contents. The web development
    department realizes that, to get a review rating, the data science team needs
    to be involved. The data science team receives the request from the web development
    team – '**We need a web service that takes a string of text as input and returns
    a score that indicates the degree to which the text represents a positive sentiment**'.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data science team then refines the requirements and agrees upon the definition
    of a Minimum Viable Product (MVP) with the web development team:'
  prefs: []
  type: TYPE_NORMAL
- en: The deliverable will be a web service deployed on an AWS EC2 instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The input to the web service will be a post request containing four reviews
    (that is, a single post request to the service will contain four reviews).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output of the web service will be a set of four scores that correspond to
    each input text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output score will be on a scale from 1 to 5, with 1 being the least and
    5 being the most positive review.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data Acquisition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A big contribution toward determining the performance of any machine learning
    model is the quality and quantity of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, a data warehousing team/infrastructure team (DWH) is responsible for
    maintaining the data-related infrastructure at a company. The team takes care
    that data is never lost, that the underlying infrastructure is stable, and that
    data is always available for any team that might be interested in using it. The
    data science team, being one of the consumers of the data, contacts the DWH team,
    which grants them access to a database that contains all the reviews for various
    items in the product catalog of the company.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, there are multiple data fields/tables in the database, some of which
    may not be important for the machine learning model development.
  prefs: []
  type: TYPE_NORMAL
- en: 'A data engineer (a part of the DWH team/member of another team/member of your
    team) then connects to the database, processes the data into a tabular format,
    and generates a flat file in the **csv** format. A discussion between the data
    scientist and the data engineer at this point results in the retention of only
    three columns from the database table:'
  prefs: []
  type: TYPE_NORMAL
- en: '''Rating'': A score on the scale of 1 to 5 that indicates the degree to which
    a positive sentiment is represented'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '''Review Title'': A simple title for the review'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '''Review'': Actual review text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notice that all three fields are inputs from customers (users of your e-commerce
    platform). Additionally, fields such as '*item id*' are not retained since they
    are not required to build this machine learning model for sentiment classification.
    The removal and retention of such information is also a product of discussions
    between the DS team, data engineers, and the DWH team.
  prefs: []
  type: TYPE_NORMAL
- en: It might have been the case that the current data is devoid of sentiment ratings.
    In such a case, one common solution is to manually go through each review and
    assign it a sentiment score for the purpose of obtaining training data for the
    model. However, as you can imagine, doing so for millions of reviews is a daunting
    task. Thus, crowdsourcing services such as *Amazon Mechanical Turk* can be utilized
    to annotate the data and get training labels for it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For more information on Amazon Mechanical Turk, refer to https://www.mturk.com/.
  prefs: []
  type: TYPE_NORMAL
- en: Google Colab
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You are familiar with the intense computational requirements of deep learning
    models. On a CPU, it would take a remarkably long time to train a deep learning
    model with lots of training data. Hence, to keep training times practical, it
    is common practice to use cloud-based services that offer Graphics Processing
    Units (GPU) to speed up computations. You can expect a speedup of 10-30 times
    when compared to running the training session on a CPU. The exact amount of speedup,
    of course, depends upon the power of the GPU, the amount of data involved, and
    the processing steps.
  prefs: []
  type: TYPE_NORMAL
- en: There are many vendors offering such cloud services, such as **Amazon Web Services**
    (**AWS**), **Microsoft Azure** and others. Google offers an environment/IDE called
    **Google Colab**, which offers up to 12 hours of free GPU usage per day for anyone
    looking to train deep learning models. Additionally, the code is run on a **Jupyter**-like
    notebook. In this chapter, we will leverage the power of Google Colab to develop
    our deep learning-based sentiment classifier.
  prefs: []
  type: TYPE_NORMAL
- en: In order to familiarize yourself with Google Colab, you are urged to go through
    a tutorial for it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Before proceeding further, refer to the tutorial at https://colab.research.google.com/notebooks/welcome.ipynb#recent=true
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps should acquaint you well with Google Colab:'
  prefs: []
  type: TYPE_NORMAL
- en: To open a new blank **colab** notebook, go to https://colab.research.google.com/notebooks/welcome.ipynb,
    select '**File**' from the menu, and then select the '**New Python 3 notebook**'
    option, as shown in the screenshot:![](img/C13783_9_05.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 9.5: A new Python notebook on Google Colab'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, rename the notebook any name of your choice. Then, to use a **GPU** for
    training, we need to select a **GPU** as the runtime. To do so, choose the ''**Edit**''
    option from the menu and select ''**Notebook Settings**''.![Figure 9.6: Edit dropdown
    in Google Colab'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13783_9_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.6: Edit dropdown in Google Colab'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A menu pops up with a ''**Hardware Accelerator**'' field, which is set to ''**None**''
    by default:![Figure 9.7: Notebook settings for Google Colab'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13783_9_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.7: Notebook settings for Google Colab'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A dropdown can be used at this point to select ''**GPU**'' as the option:![Figure
    9.8: GPU hardware accelerator'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13783_9_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.8: GPU hardware accelerator'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To check whether the GPU has, in fact, been allotted to your notebook, run
    the following snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of running this snippet should indicate the GPU''s availability:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.9: Screenshot for GPU device name'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_9_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.9: Screenshot for GPU device name'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The output is the GPU device name.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, the data needs to be made accessible within the notebook. There are a
    number of ways to do this. One way to accomplish this task is by moving the data
    to a personal Google Drive location. It''s better to move the data in a zipped
    format to avoid using up too much space on the drive. Go ahead and create a new
    folder on Google Drive and move the zipped CSV data file within the folder. Next,
    we mount the Google Drive onto the Colab notebook machine to make the drive data
    available for use within the Colab notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The snippet we just mentioned would return a weblink for authorization. Upon
    clicking on that link, a new browser tab opens up containing an authorization
    code that should be copied and pasted onto the notebook prompt:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.10: Screenshot for importing data from Google Drive'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_9_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.10: Screenshot for importing data from Google Drive'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: At this point, all the data within your Google Drive is available for use within
    the Colab notebook.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, navigate to the folder location where the zipped data is present:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Confirm that you have navigated to the desired location by issuing a ''`pwd`''
    command in the notebook cell:![Figure 9.11: Data imported on the Colab notebook
    from Google Drive'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13783_9_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.11: Data imported on the Colab notebook from Google Drive'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, unzip the zipped data file using the `unzip` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.12: Unzipping a data file on a Colab notebook'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_9_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.12: Unzipping a data file on a Colab notebook'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The '**MACOSX**' output lines are operating system-specific and may not be the
    same for everyone. Anyhow, an unzipped data file, '**data.csv**' is now available
    for use within the Colab notebook.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have the data available and the environment to use the GPU is set,
    we can start coding up the model. We will import the required packages first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will write a preprocessing function that turns all the text to lowercase
    and removes any numbers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that we are using pandas for reading and processing texts. Let''s run
    this function with the path of our CSV file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now examine the contents of the dataframe:![Figure 9.13: Screenshot
    of dataframe contents'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13783_9_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.13: Screenshot of dataframe contents'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As expected, we have three fields. Also, we see that the ''`review`'' column
    has much more text than the ''`title`'' column. So, we choose to use only the
    ''`review`'' column for developing the model. We''ll now proceed with tokenizing
    the text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we have restricted the feature count to 2,000 words. We then apply the
    tokenizer with the maximum features to the 'review' column of the data. We also
    pad the sequence length to 250 words.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The `X` variable looks as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.14: Screenshot of the X variable array'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_9_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.14: Screenshot of the X variable array'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The X variable is a `NumPy` array with 3,000,000 rows and 250 columns. This
    is because there are 3,000,000 reviews available and each review has a fixed length
    of 250 words after padding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We''ll now prepare the target variable for training. We define the problem
    as a five-class classification problem where each class corresponds to a rating.
    Since the rating (sentiment score) is on a scale of 1-5, there are 5 outputs of
    the classifier. (You could also model this as a regression problem). We use the
    `get_dummies` function from pandas to get the five outputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `y_train` variable is a `NumPy` array with 3,000,000 rows and 5 columns
    with values, as shown:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.15: y_train output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_9_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.15: y_train output'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We have now preprocessed the text and prepared the target variable. Let''s
    now define the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We choose 128 embedding dimensions for input. We also choose an LSTM as the
    RNN unit with 100 hidden dimensions. The model summary is printed as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.16: Screenshot of the model summary'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_9_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.16: Screenshot of the model summary'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can now fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that we fit 100,000 reviews instead of 3,000,000\. Running the training
    session with this configuration takes around 90 minutes. It would take much longer
    with a complete amount of data:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.17: Screenshot of the training session'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_9_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.17: Screenshot of the training session'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The validation accuracy for this 5-class problem is 48%. This isn't a good result,
    but for the purpose of demonstration, we can go ahead and deploy it.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We now have the model that we wish to deploy. Now, we need to save the model
    file and the tokenizer that will be used in the production environment to get
    predictions on the new reviews:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'These files now need to be downloaded from the Google Colab environment to
    the local drive:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This snippet will download the tokenizer and model files to the local computer.
    We are now ready to use the model for predictions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Flask
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will use the Flask microserver framework provided by Python
    to make a web application that provides predictions. We will get a RESTful API
    that we can query to get our results. Before commencing, we need to install Flask
    (use `pip`):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by importing the packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s write a function that loads the trained model and `tokenizer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `make_predict_function()` is a hack that allows using `keras` models with
    Flask.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we''ll define preprocessing functions similar to the training code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Similar to the training phase, the reviews are first lowercased. Then, numbers
    are replaced with blanks. Next, the loaded tokenizer is applied and the sequences
    are padded to have a fixed length of 250 to make them consistent with the training
    input.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will now define a Flask app instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now define an endpoint that displays a fixed message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It is good practice to have a root endpoint to check whether the web service
    is up.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we''ll have a prediction endpoint, to which we can send our review strings.
    The kind of HTTP request we will use is a ''`POST`'' request:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now start the web server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We could save this file as `app.py` (any name could be used). Run this code
    from the terminal using `app.py`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'An output such as the one shown here will be produced in the terminal window:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.18: Output for Flask'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_9_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.18: Output for Flask'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'At this point, go to your browser window and enter the `http://127.0.0.1:5000/`
    address. The ''Hello World!'' message will be displayed on the screen. The output
    produced corresponds to the root endpoint we set in the code. Now, we send our
    review texts to the ''prediction'' endpoint of our Flask web service. Let''s send
    the following four reviews:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '"The book was very poor"'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '"Very nice!"'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '"The author could have done more"'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '"Amazing product!"'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can send post requests to a web service using `curl` requests. For the four
    reviews mentioned, the `curl` request can be sent through the terminal, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The list of four reviews is posted to the prediction endpoint of the web service.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The web service replies with a list of four ratings:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'So, the sentiment ratings are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"The book was very poor"- 0'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '"Very nice!"- 4'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '"The author could have done more" - 2'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '"Amazing product!" - 4'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The ratings actually make sense!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Up to this point, the data science team has a Flask web service that works
    on a local system. However, the web development team is still not in a position
    to use the service, since it only runs on a local system. So, we need to host
    this web service somewhere on a cloud platform so that it is also available for
    the web development team to use. This section provides a basic pipeline for the
    deployment to work, which can be broken down into the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Make changes to the Flask web app so that it can be deployed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use Docker to wrap the flask web application into a container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Host the container on an Amazon Web Services (AWS) EC2 instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's look at each of these steps in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Making Changes to a Flask Web App
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The flask application that was coded in the FLASK section ran on a local web
    address: `http://127.0.0.1:5000`. Since our intention is to host it on the internet,
    this address needs to be changed to: 0.0.0.0\. Additionally, since the default
    HTTP port is 80, the port also needs to be changed from 5000 to 80\. So, the address
    that needs to be queried now becomes: 0.0.0.0:80.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code snippet, this change can be accomplished simply by modifying the
    call to the `app.run` function, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the '**debug**' flag has also vanished (the default value of '**debug**'
    flag is '*False*'). This is because the application is past the debugging phase
    and is ready to be deployed to production.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The rest of the code remains exactly the same as before.
  prefs: []
  type: TYPE_NORMAL
- en: 'The application should be run again using the same command as earlier, and
    it should be verified that the same responses as earlier are received. The address
    in the curl request needs to be changed to reflect the updated web address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If a permission error is received at this point, change the port number to 5000
    in the `app.run()` command in app.py. (Port 80 is a privileged port, so change
    it to a port that isn't, for example, 5000). However, be sure to change the port
    back to 80 once it is verified that the code works.
  prefs: []
  type: TYPE_NORMAL
- en: Use Docker to Wrap the Flask Web Application into a Container
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The DS team intends to run the web service on a virtual machine hosted on a
    cloud platform (that is, AWS EC2). To isolate the EC2 operating system from the
    code environment, Docker offers containerization as a solution. We'll be using
    that here.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For a quick tutorial on the basics of Docker and how to install and use it,
    refer to https://docker-curriculum.com/.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to deploy the application onto the container:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first need a *requirements.txt* file that lists the specific packages that
    are needed to run the Python code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We need a `Dockerfile` containing instructions so that the Docker daemon can
    build the docker image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The Docker image is pulled from the Python dockerhub repository. Here, the Dockerfile
    is executed. The *app.py*, *requirements.txt*, *tokenizer pickle* file, and *trained
    model* are copied over to the Docker image using the COPY command. To change the
    working directory to the 'deploy' directory (in which the files were copied),
    the WORKDIR command is used. The `RUN` command then installs the Python packages
    mentioned in the Dockerfile. Since port 80 is required to be accessed outside
    the container, the `EXPOSE` command is used.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The Docker Hub link can be found at https://hub.docker.com/_/python.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The Docker image should next be made using the `docker build` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Don''t forget the period in this command. The output of the command is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.19: Output screenshot for docker build'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_9_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.19: Output screenshot for docker build'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: '''`app-packt`'' is the name of the Docker image generated.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The Docker image can now be run as a container by issuing the `docker run`
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `p flag` is used to do port mapping between port 80 of the local system
    to port 80 of the Docker container. (Change the port mapping part of the command
    to 5000:80 if 5000 is used locally. Please change the mapping back to 80:80 after
    verifying that the Docker container works, as explained.)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following screenshot depicts the output of the `docker run` command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.20: Output screenshot for the docker run command'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_9_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.20: Output screenshot for the docker run command'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The exact same curl request from the last section can now be issued to verify
    that the application works.
  prefs: []
  type: TYPE_NORMAL
- en: The application code is now ready to be deployed onto AWS EC2.
  prefs: []
  type: TYPE_NORMAL
- en: Host the Container on an Amazon Web Services (AWS) EC2 instance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The DS team now has a containerized application that works on their local system.
    The web development team is still not in a position to use it, as it is still
    local. As per the initial MVP definition, the DS team now goes on to use the AWS
    EC2 instance to deploy the application. The deployment will ensure that the web
    service is available for the web development team to use.
  prefs: []
  type: TYPE_NORMAL
- en: As a prerequisite, you need to have an AWS account to use the EC2 instance.
    For the purpose of demonstration, we will be using a '*t2.small*' EC2 instance
    type. This instance costs around 2 cents (USD) per hour at the time of writing.
    Note that this instance is not free-tier eligible. By default, this instance will
    not be available in your AWS region and a request needs to be raised for this
    instance to be added to your account. This usually takes a couple of hours. Alternatively,
    check the instance limits for your AWS region and select another instance with
    a minimum of 2GB RAM. A simple '*t2.micro*' instance will not work for us here,
    as it has only 1GB of memory.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The link for the AWS account can be found at https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/
  prefs: []
  type: TYPE_NORMAL
- en: To add instances and check instance limits, refer to https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the deployment process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'After logging into the AWS Management Console, search for ''**ec2**'' in the
    search bar. This takes you to the EC2 dashboard, as shown here:![Figure 9.21:
    AWS services in the AWS Management Console'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13783_9_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.21: AWS services in the AWS Management Console'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A key pair needs to be created to access AWS resources. To create one, look
    for the following pane and select ''**Key Pairs**''. This allows you to create
    a new key pair:![Figure 9.22: Network and security on the AWS console'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13783_9_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.22: Network and security on the AWS console'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A ''`.pem`'' file is downloaded, which is the key file. Be sure to save the
    `pem` file safely and change its mode using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is required to change file permissions to private.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To configure the instance, select ''**Launch Instance**'' on the EC2 dashboard:![Figure
    9.23: Resources on the AWS console'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13783_9_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.23: Resources on the AWS console'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, select the **Amazon Machine Instance** (**AMI**), which selects the OS
    that EC2 instance runs. We will work with ''**Amazon Linux 2 AMI**'':'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: For more information on Amazon Linux 2 AMI, refer to https://aws.amazon.com/amazon-linux-2/.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.24: Amazon Machine Instance (AMI)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_9_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.24: Amazon Machine Instance (AMI)'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, we select the hardware part of EC2, which is the ''**t2.small**'' instance:![Figure
    9.25: Choosing the instance type on AMI'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13783_9_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.25: Choosing the instance type on AMI'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Clicking on ''**Review and Launch**'' gets you to step 7 – the **Review Instance
    Launch** screen:![Figure 9.26: The review instance launch screen'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13783_9_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.26: The review instance launch screen'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, to make the web service reachable, the security group needs to be modified.
    To this end, a rule needs to be created. At the end, you should see the following
    screen:![Figure 9.27: Configure the security group'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13783_9_27.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.27: Configure the security group'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: More can be learned about security groups and configuration using the AWS documentation
    at https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, clicking on the ''Launch'' icon will trigger a redirection to a **Launch**
    screen:![Figure 9.28: Launch status on the AWS instance'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13783_9_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.28: Launch status on the AWS instance'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The '**View Instance**' button is to be used to navigate to a screen that displays
    the EC2 instance being launched, which is ready to be used when the instance state
    turns to 'running.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, access the EC2 using the following command from the local system terminal
    with the ''`public-dns-name`'' field replaced with your EC2 instance name (of
    the form: ec2–x–x–x–x.compute-1.amazonaws.com) and the path of the key pair `pem`
    file that was saved earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This command will take you to the prompt of the EC2 instance where Docker needs
    to be installed first. Docker installation is required for the workflow since
    the Docker image will be built within the EC2 instance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For Amazon Linux 2 AMI, the following commands should be used to accomplish
    this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: For an explanation of the commands, check out the documentation at https://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-basics.html.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The '`exit`' command should be used to log out of the instance. Next, log back
    in using the `ssh` command that was used earlier. Verify that Docker is working
    by issuing the '`docker info`' command. Open another local terminal window for
    the next steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, copy the files that are needed to build the Docker image within the EC2
    instance. Issue the command from the local terminal (not from within EC2!):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following files should be copied to build the Docker image, as was done
    earlier: *requirements.txt*, *app.py*, *trained_model.h5*, *trained_tokenizer.pkl*,
    and *Dockerfile*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, log in to the EC2 instance, issue the '`ls`' command to see whether the
    copied files exist, and build and run the Docker image using the same commands
    that were used in the local system (ensure that you use port 80 at all locations
    in the code/commands).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enter the home endpoint from the local browser using the public DNS name to
    see the ''**Hello World!**'' message:![Figure 9.29: Screenshot for the home endpoint'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13783_9_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.29: Screenshot for the home endpoint'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now you can send a curl request to the web service from a local terminal with
    the test sample data after replacing the `public-dns-name` with yours:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should return the same review ratings as the ones obtained locally.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This concludes the simple deployment process.
  prefs: []
  type: TYPE_NORMAL
- en: The DS team now shares this `curl` request with the web development team, which
    can consume the web service with their test samples.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When the web service is not required, stop or terminate the EC2 instance to
    avoid getting charged.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.30: Stopping the AWS EC2 instance'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_9_30.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.30: Stopping the AWS EC2 instance'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From an MVP point of view, the deliverables are now complete!
  prefs: []
  type: TYPE_NORMAL
- en: Improvements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The workflow described in this chapter is only meant to introduce a basic workflow
    using certain tools (Flask, Colab, Docker, and AWS EC2) and inspire an example
    plan for a deep learning project in an organization. This is, however, only an
    MVP, which could be improved in many ways for future iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we saw the journey of a deep learning project as it flows through
    an organization. We also learned about Google Colab notebooks to leverage GPUs
    for faster training. Additionally, we developed a Flask-based web service using
    Docker and deployed it to a cloud environment, hence enabling the stakeholders
    to obtain predictions for a given input.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter concludes our efforts toward learning how to leverage deep learning
    techniques to solve problems in the domain of natural language processing. Almost
    every aspect discussed in this chapter and the previous ones is a topic of research
    and is being improved upon continuously. The only way to stay informed is to keep
    learning about the new and exciting ways to tackle problems. Some common ways
    to do so are by following discussions on social media, following the work of top
    researchers/deep learning practitioners, and being on the constant lookout for
    organizations that are doing cutting-edge work when it comes to this domain.
  prefs: []
  type: TYPE_NORMAL
