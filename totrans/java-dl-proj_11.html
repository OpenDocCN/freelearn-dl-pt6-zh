<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Discussion, Current Trends, and Outlook</h1>
                </header>
            
            <article>
                
<p>Deep neural networks being at the core of <strong>deep learning</strong> (<strong>DL</strong>) allow computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art stuff in speech recognition, multimedia (image/audio/video) analytics, NLP, image processing and segmentation, visual object recognition, object detection, and many other domains in life sciences, such as cancer genomics, drug discovery, personalized medicine, and biomedical imaging.</p>
<p>Throughout this book, we have seen how to use JVM-based DL libraries to develop some applications covering these areas. I confess that some projects were not so comprehensive and cannot be deployed commercially but need some extra effort. Nonetheless, showing how to deploy such models was not within the scope of this book. However, at least these should provide us with some core insights.</p>
<p>Now that we've come to the end of our little journey of deep learning with different Java libraries, it is time to wrap up everything. But before that, in this chapter, we'll discuss the completed projects and some abstract takeaways. Then we'll provide some suggestions for <span>improvement</span>. Additionally, we'll cover some extension guidelines for other real-life deep learning projects. In summary, the following topics will be covered in this chapter:</p>
<ul>
<li>Discussions on projects, outlook, future improvement, and extension</li>
<li>Current trends on supervised and unsupervised deep learning algorithms</li>
<li>Frequently asked questions (FAQs)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Discussion and outlook</h1>
                </header>
            
            <article>
                
<p>Throughout this book, we have covered 10 end-to-end projects. We started our journey with an introduction to deep learning and finished at a factorization-machine based movie recommendation system project. In this section, we'll briefly review these projects, discuss potential limitations, and provide some future directions toward improvement and extension.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Discussion on the completed projects</h1>
                </header>
            
            <article>
                
<p>In between, we tried to cover several real-life projects from diverse domains such as healthcare, sentiment analysis in NLP, transfer learning, image and video classification, distributed deep learning and training, reinforcement learning, online trading, and real-life object detection from video. These are outlined as follows:</p>
<ul>
<li class="mce-root">Titanic survival prediction using MLP and LSTM networks</li>
<li class="mce-root">Cancer types prediction using recurrent type networks</li>
<li class="mce-root">Multi-label Image classification using convolutional neural networks</li>
<li>Sentiment analysis using Word2Vec and LSTM network</li>
<li>Image classification using transfer learning</li>
<li>Real-time object detection using YOLO, JavaCV, and DL4J</li>
<li>Stock price prediction using the LSTM network</li>
<li>Distributed deep learning for video classification using convolutional LSTM</li>
<li>Using deep reinforcement learning for a GridWorld game</li>
<li>Developing a movie recommendation system using factorization machines</li>
</ul>
<p class="mce-root">Now we will discuss the pros, cons, and future directions for improving these projects for possible extension.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Titanic survival prediction using MLP and LSTM networks</h1>
                </header>
            
            <article>
                
<p>In this project, our main goal was to get familiar with Apache Spark ML library, followed by a basic introduction to machine learning, deep learning their types, architectures, and frameworks.</p>
<p>We could not achieve higher accuracy. Then, in <a href="e27fb252-7892-4659-81e2-2289de8ce570.xhtml" target="_blank">Chapter 2</a>, <em>Cancer Types Prediction Using Recurrent Type Networks</em>, we revisited the same project but using a robust recurrent LSTM network, which shows higher accuracy. The takeaway was learning how to prepare the dataset by considering most of the features and feeding into Spark and a DL4J-based MLP classifier.</p>
<p>In addition, this dataset is not so high dimensional, so applying DL methods is not a good idea. Therefore, I would recommend using other tree ensembles such as random forest and gradient-boosted trees for modeling and deployment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cancer type prediction using recurrent type networks</h1>
                </header>
            
            <article>
                
<p>We solved an interesting project, where we successfully classified cancer patients based on cancer types. For this, we used the LSTM network. We used a very high-dimensional gene expression dataset. We converted the dataset into sequence format and trained the LSTM net for each sample per time step.</p>
<p>This project also shows the robustness of deep architecture such as LSTM, demonstrating that even without applying dimensionality reduction, the model can handle a very high-dimensional dataset.</p>
<p>One of the potential limitations of this approach was that we considered only a gene expression dataset, so it cannot be deployed for a real-life prognosis and diagnosis, whereas other datasets such as <strong>Copy Number Variation</strong> (<strong>CNV</strong>), DNA methylation, and survival-related clinical outcomes, have to be considered as well. Nonetheless, domain expertise from biomedical engineers and doctors is needed to come up with an integrated solution.</p>
<p>Finally, the takeaway is that at least it shows how to handle at least one type of cancer genomics dataset. Therefore, the same techniques can be applied to other data types too. Then, a multimodal network has to be developed by taking the input from domain experts before deploying, such as an AI expert system.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image classification using convolutional neural networks</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this project, we saw how to solve a multi-label image classification problem. We used real Yelp images. Then we trained a CNN to predict the classes for each tagged image. In this project, the most challenging part was feature engineering, as we had to deal with not only images but also different tags and metadata. Unfortunately, we could not achieve very high accuracy.</p>
<p>The takeaway would be that similar approaches can be applied to solve other image datasets having multi-labels. Yet, a multiclass classification problem can be solved with minimal effort as well. All you need is to prepare the dataset such that a CNN-based model can consume it. Apart from this outlook, the project in <a href="2d4fc6f2-3753-456c-8774-3f41dbe13cfb.xhtml" target="_blank">Chapter 5</a>, <em>Image Classification using Transfer Learning,</em> can be extended to solve similar problems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sentiment analysis using Word2Vec and the LSTM network</h1>
                </header>
            
            <article>
                
<p>In this project, we saw how to develop a sentiment analysis application using Word2Vec and LSTM. We also discussed how to convert unstructured texts into neural word embedding, and later on transform them into the sequence form required to train the LSTM network. Then we trained LSTM with the sequence of the corresponding texts at each time step. </p>
<p>This project also addressed a binary classification problem with very high accuracy. In the same line, this application can be extended for classifying other problems, such as spam versus ham for messages and email, and movie or product reviews. Finally, in the FAQ section, we discussed how to solve the same problem with CNN, which can achieve similar accuracy to LSTM. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image classification using transfer learning</h1>
                </header>
            
            <article>
                
<p>In this chapter, we solved an interesting dog versus cat classification problem using the transfer learning technique. We used a pre-trained VGG16 model and its weights, and subsequently we fine-tuned the training with a real-life cat versus dog dataset from Kaggle.</p>
<p>Once the training was completed, we saved the trained model for model persistence and later reuse. We saw that the trained model could successfully detect and differentiate between cat and dog images with very different sizes, quality, and shapes.</p>
<p>The trained model/classifier can be used to solve cat versus dog problems in real life. Finally, the lesson was that this similar technique with minimal efforts, can be extended and used to solve similar image classification problems; this applies for both binary and multiclass classification problems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Real-time object detection using YOLO, JavaCV, and DL4J</h1>
                </header>
            
            <article>
                
<p>In this project, we again used the transfer learning technique to solve another interesting problem, which is real-time object detection from video clips. We used a pre-trained YOLOv2 model, JavaCV, and DL4J libraries to solve this problem.</p>
<p>As stated in the chapter, we extended the image recognition idea to solve this problem. That is, our technique generates video frames as images and then recognizes objects from the frame using the bounding box approach. The takeaway is that although we used a video clip to show the evaluation, it still showed very good accuracy. And from the provided demo, anyone can observe that most of the objects in the clip were accurately identified. Thus, a similar technique, can be extended for real-time object detection.</p>
<p>In this regard, we saw some tips to collect real-time videos from a webcam or video camera (or even a mobile phone) and feed them into our YOLOv2 model using the JavaCV library.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stock price prediction using LSTM network</h1>
                </header>
            
            <article>
                
<p>In this project, we saw how to develop a demo project for predicting the stock prices for five categories: OPEN, CLOSE, LOW, HIGH, and VOLUME. However, this result also lacks the actual signal; all your network has to do is produce a value <span>similar </span>to the last input of the price.</p>
<p>If we took your prediction as the input for the next prediction, we saw that the results were quite bad. I know there are some serious drawbacks of this approach. Nevertheless, we had not used enough data, which potentially limits the performance of such a model.</p>
<p>Knowing the drawback of this project, the biggest takeaway is extending Bitcoin or another cryptocurrency price prediction. As suggested in the FAQ section, historical Bitcoin data can be downloaded from Kaggle. Then, similar feature engineering as we used in this project can be used to prepare the sequence dataset. Nevertheless, CNN-based approaches can be used too.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Distributed deep learning â€“ video classification using a convolutional-LSTM network</h1>
                </header>
            
            <article>
                
<p>In this project, we developed a complete deep learning application that classifies a large collection of a video dataset from the UCF101 dataset. We applied a combined CNN-LSTM network with <strong>deeplearning4j</strong> (<strong>DL4J</strong>) that overcame the limitations of standalone CNN or RNN <strong>Long Short-Term Memory</strong> (<strong>LSTM</strong>) networks.</p>
<p>Finally, we saw how to perform training in both a parallel and distributed manner across multiple devices (CPUs and GPUs) on AWS EC2 AMI 9.0. We performed parallel and distributed training on a <kbd>p2.8xlarge</kbd> instance with 8 GPUs, 32 computing cores, and 488 GB of RAM. </p>
<p>One of the greatest takeaways from this chapter was that this end-to-end project can be treated as a primer for human activity recognition from video. Secondly, we did not achieve high accuracy because we had not trained the model with all the available video clips.</p>
<p>Therefore, having the network trained with the a full video dataset and hyperparameter tuning definitely <span>increases </span>accuracy. In that case, deploying the improved model is commercially possible. Finally, if you want to make the training even faster, configuring a Hadoop cluster and distributing the training on both GPUs and Spark is possible. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using deep reinforcement learning for GridWorld</h1>
                </header>
            
            <article>
                
<p>In this project, we saw how to develop a demo GridWorld game using DL4J and Neural QLearning, which acts as the Q function. We also provided some basic theoretical background for developing DQN to play a GridWorld game. However, we did not develop a module for visualizing the moves of the agent for the entire episodes. I confess that it was the biggest drawback of this project. However, I discussed some improvements in the FAQ section.</p>
<p>The takeaway from this project is extending this application with a visualization model or even developing other RL-based games, such as Doom and ALE would be a good idea. Secondly and finally, we can also think of developing another interesting RL project for online trading.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Movie recommender system using factorization machines</h1>
                </header>
            
            <article>
                
<p>In this project, we saw how to develop a movie recommendation system using factorization machines (FMs) that are a set of algorithms that enhance the performance of linear models by incorporating second-order feature interactions that are absent in matrix factorization algorithms in a supervised way.</p>
<p>Nevertheless, we saw some theoretical background of recommendation systems using matrix factorization and collaborative filtering before diving the project implementation using RankSys library-based FMs. This project not only covered movie-rating prediction by individual users but also discussed ranking predictions. Consequently, we used FM for predicting rankings for movies too.</p>
<p>However, the potential limitation is that this library is not scalable and well structured, I would say. Therefore, trying Python-based FM libraries would be a better idea. Finally, the biggest takeaway is extending this application with Python-based FM libraries for even larger movie datasets from MovieLens or IMDb, which is recommended.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Current trends and outlook</h1>
                </header>
            
            <article>
                
<p>As a researcher, I work as a <strong>Program Committee</strong> (<strong>PC</strong>) member for conferences such as WWW'2018, ISWC'2018, ESWC'2017/2018, and ESWC SemDeep'2018 international workshops. Apart from these, I am also a guest editor for International Semantic Web Journal, Journal of Cloud Computing, and Briefings in Bioinformatics.</p>
<p>While reviewing numerous papers for these conferences and journals, I found that researchers have not limited themselves to developing emerging use cases and analytical solutions using original RNN, CNN, DBN or autoencoders. They are coming up with ideas across new architectures by combining them for diverse domains.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Current trends</h1>
                </header>
            
            <article>
                
<p>As discussed in <a href="fba163f0-88c0-4278-a4a1-df5389cc3a10.xhtml" target="_blank">Chapter 1</a>, <em>Getting Started with Deep Learning</em>, researchers have recently proposed so many emergent DL architectures. These <span>include </span>not only improving CNN/RNN and their variants but also some other special types of architecture: <strong>Deep SpatioTemporal Neural Networks</strong> (<strong>DST-NNs</strong>), <strong>Multi-Dimensional Recurrent Neural Networks</strong> (<strong>MD-RNNs</strong>), <strong>Convolutional AutoEncoders</strong> (<strong>CAEs</strong>), deep embedding clustering, and so on.</p>
<p>Nevertheless, there are a few more emerging networks, such as CapsNets, which is an improved version of a CNN designed to remove the drawbacks of regular CNNs as proposed by Hinton et al. Then we have residual neural networks for image recognition and <strong>Generative Adversarial Networks</strong> (<strong>GANs</strong>) for simple image generation.</p>
<p>Apart from these trends and use cases, different deep and emerging architectures are being used in multimedia analytics; computer vision (especially semantic image segmentation); anomaly detection from IoT, image, and network traffic; neural machine translation for NLP; and integration of knowledge graphs with neural networks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Outlook on emergent DL architectures</h1>
                </header>
            
            <article>
                
<p>In this subsection, we'll discuss some emergent architectures and their variants, focusing on some use cases.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Residual neural networks</h1>
                </header>
            
            <article>
                
<p class="mce-root">Because of the millions of billions of hyperparameters and other practical aspects associated with them, it is difficult to train deep neural networks. To overcome this limitation, Kaiming He et al. (see <a href="https://arxiv.org/abs/1512.03385v1">https://arxiv.org/abs/1512.03385v1</a>) proposed a <strong>residual learning framework</strong> (<strong>RNN</strong>) to ease the training of networks that are substantially deeper than those used previously. Now, according to the original paper:</p>
<div class="packt_quote">"In this network setting, instead of hoping each stack of layers directly fits a desired underlying mapping, we explicitly let these layers fit a residual mapping. The original mapping is recast into F(x)+x. We hypothesize that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers."</div>
<p class="mce-root">This way, RNNs are easier to optimize and can gain accuracy from considerably increased depth compared to other DNN architectures. The downside is that building a network by simply stacking residual blocks inevitably limits its optimization ability. To overcome this limitation, Ke Zhang et al. also proposed using a multilevel residual networks (see at <a href="https://arxiv.org/abs/1608.02908">https://arxiv.org/abs/1608.02908</a>). </p>
<p>Consequently, residual networks are in use to solve many emerging use cases, including:</p>
<ul>
<li><em>Skeleton-Based Action Recognition with Spatial Reasoning and Temporal Stack Learning</em> (see more at <a href="https://arxiv.org/pdf/1805.02335" target="_blank">https://arxiv.org/pdf/1805.02335</a>).</li>
<li>Recently, Yuan et al. proposed <em>Hyperspectral Image Denoising Employing a Spatial-Spectral Deep Residual Convolutional Neural Network</em> (see <a href="https://arxiv.org/pdf/1806.00183">https://arxiv.org/pdf/1806.00183</a>).</li>
<li><em>A Dynamic Model for Traffic Flow Prediction Using Improved DRN</em> (see more at <a href="https://arxiv.org/pdf/1805.00868">https://arxiv.org/pdf/1805.00868</a>)</li>
<li><em>Classification of simulated radio signals using Wide Residual Networks for use in the search for extra-terrestrial intelligence</em> (see more at <a href="https://arxiv.org/pdf/1803.08624">https://arxiv.org/pdf/1803.08624</a>) </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">GANs</h1>
                </header>
            
            <article>
                
<p>GANs are deep neural net architectures that consist of two networks pitted against each other (hence the name adversarial). Ian Goodfellow et al. introduced GANs in a paper (see more at <a href="https://arxiv.org/abs/1406.2661v1">https://arxiv.org/abs/1406.2661v1</a>). GAN is one of the best research outcomes in AI that can learn to mimic any distribution of data. This is such that a trained GAN can be deployed to create worlds similar to our own, especially for images, music, speech, or prose.</p>
<p>Although the original GAN paper targeted simple image generation such as DCGAN, BEGAN, and so on, people are extending the idea for font generation, anime character generation, interactive image generation, text-to-image generation, 2D object generation, human pose estimation, and so on. A few concrete research-oriented use cases as outlined as follows:</p>
<ul>
<li>For generating an image sequence from the description with LSTM (see more at <a href="https://arxiv.org/pdf/1806.03027">https://arxiv.org/pdf/1806.03027</a>)</li>
<li>Generative adversarial networks for <strong>electroencephalographic</strong> (<strong>EEG</strong>) brain signals (see more at <a href="https://arxiv.org/pdf/1806.01875">https://arxiv.org/pdf/1806.01875</a>)</li>
<li>For natural language generation for electronic health records (see more at <a href="https://arxiv.org/pdf/1806.01353">https://arxiv.org/pdf/1806.01353</a>)</li>
<li>For chest X-ray segmentation (see more at <a href="https://arxiv.org/pdf/1806.00600">https://arxiv.org/pdf/1806.00600</a>)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Capsule networks (CapsNet)</h1>
                </header>
            
            <article>
                
<p>As discussed in <a href="fba163f0-88c0-4278-a4a1-df5389cc3a10.xhtml" target="_blank">Chapter 1</a>, <em>Getting Started with Deep Learning</em>, CNNs perform well at classifying good quality images. However, if the images have rotation, tilt, or any other different orientation, CNNs give very poor performance. Even the pooling operation in CNNs cannot help much with positional invariance.</p>
<p>To overcome the this limitation of CNN, Geoffrey Hinton et al. come up with a ground breaking idea called <strong>CapsuleNetworks (CapsNet)</strong> that are particularly good at handling different types of visual stimulus and encoding things such as pose (position, size, and orientation), deformation, velocity, albedo, hue, texture.</p>
<p>In a regular DNN, we keep on adding layers (more layers means a deeper network). In CapsNet, the idea is to add more layers inside a single layer. This way, a CapsNet is a nested set of neural layers. In CapsNet, the limitation of max-pooling layer is overcome and replaced with <strong>routing by agreement (RBA)</strong> to capture low-level visual information.</p>
<p>Unfortunately, the original paper is too theoretical. Therefore, researchers are trying to extend the idea of CapsNet for different AI and data science projects including image classification, GAN improvement and at improving RL-based gaming experience. A few example use cases are listed as follows:</p>
<ul>
<li><em>Object Localization and Motion Transfer learning with Capsules</em> (see <a href="https://arxiv.org/pdf/1805.07706">https://arxiv.org/pdf/1805.07706</a>)</li>
<li><em>An attention-based Bi-GRU-CapsNet model for hypernymy detection between compound entities</em> (see more at <a href="https://arxiv.org/pdf/1805.04827">https://arxiv.org/pdf/1805.04827</a>)</li>
<li><em>Brain Tumor Type Classification via Capsule Networks</em> (see more at <a href="https://arxiv.org/pdf/1802.10200">https://arxiv.org/pdf/1802.10200</a>)</li>
<li><em>CapsuleGAN: Generative Adversarial Capsule Network</em> (see more at <a href="https://arxiv.org/pdf/1802.06167">https://arxiv.org/pdf/1802.06167</a>)</li>
<li><em>Deep Reinforcement Learning using Capsules in Advanced Game Environments</em> (see more at <a href="https://arxiv.org/pdf/1801.09597">https://arxiv.org/pdf/1801.09597</a>)</li>
</ul>
<p>Apart from these emerging architectures, people are trying to use GAN architectures for image synthesis using CapsNet (see <a href="https://arxiv.org/pdf/1806.03796">https://arxiv.org/pdf/1806.03796</a>). Researchers have also used adversarial autoencoders for speech-based emotion recognition (see <a href="https://arxiv.org/pdf/1806.02146">https://arxiv.org/pdf/1806.02146</a>). Finally, I would suggest readers to know the recent trends in artificial intelligence, machine learning, and deep learning from <a href="https://arxiv.org/list/cs.AI/recent">https://arxiv.org/list/cs.AI/recent</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Semantic image segmentation</h1>
                </header>
            
            <article>
                
<p>Image segmentation is the way to partition an image into several coherent parts, but <em>without</em> any attempt at understanding what these parts represent. Semantic segmentation, on the other hand, attempts to partition the image into semantically meaningful parts and to classify each part into one of the predetermined classes.</p>
<p>Such semantic segmentation of raw brain images into gray matter, white matter, and cerebrospinal fluid helps classify them based on the segmented areas. Deep-learning-based techniques are being in use and very successful such as <strong>Stacked Denoising Autoencoders</strong> (<strong>SDAE</strong>).</p>
<p>Nevertheless, the recurrent-based fully convolutional network is being in use for semantic segmentation of high-resolution remote sensing images (see more at <a href="https://arxiv.org/pdf/1805.02091">https://arxiv.org/pdf/1805.02091</a>). This types of image segmentation are being in use in emerging use cases such as Pelvic MRImage analysis, object detection for self-driving cars, geospatial image classification (see <a href="http://www.semantic-web-journal.net/system/files/swj1862.pdf">http://www.semantic-web-journal.net/system/files/swj1862.pdf</a>), and many more.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep learning for clustering analysis</h1>
                </header>
            
            <article>
                
<p>Clustering analysis is one of the most widely used data-driven task. To date, existing clustering analysis techniques use classical clustering algorithms such as k-means, bisecting k-means, or the Gaussian mixture model. In particular, the k-means clustering algorithm and its several variants have been proposed to address issues with higher-dimensional input spaces.</p>
<p>However, they are fundamentally limited to linear embedding. Hence, they cannot model nonlinear relationships. Nevertheless, fine-tuning in these approaches is based on only cluster assignment hardening loss. Therefore, a fine-grained clustering accuracy cannot be achieved. </p>
<p>In short, relatively less research has focused on deep-learning-based representation learning and clustering analysis. However, the quality of k-means is dependent on the data distribution. Deep architecture can help the model learn a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective.</p>
<p>Considering these limitations and motivations, researchers have come up with deep-learning-based clustering techniques for clustering very-high-dimensional data and nonlinear objects. In these approaches, k-means is incorporated with deep architectures, where both the clustering-assignment-hardening loss (from k-means) and reconstruction loss (from DNN) are optimized simultaneously. These approaches include:</p>
<ul>
<li><em>Unsupervised Deep Embedding for Clustering Analysis</em> by Xie et al. ( see <a href="https://arxiv.org/pdf/1511.06335.pdf">https://arxiv.org/pdf/1511.06335.pdf</a>)</li>
<li><em>Neural Networks-based Clustering using Pairwise Constraints</em> by Hsu et al. (see more at <a href="https://arxiv.org/abs/1511.06321">https://arxiv.org/abs/1511.06321</a>)</li>
<li><em>Discriminatively Boosted Clustering (DBC)</em> by Liu et al. (see more at <a href="http://dataclustering.cse.msu.edu/papers/boost_cluster.pdf">http://dataclustering.cse.msu.edu/papers/boost_cluster.pdf</a>)</li>
<li><em>Clustering with Deep Learning: Taxonomy and New Methods</em> by Elie et al. (see more at <a href="https://arxiv.org/abs/1801.07648">https://arxiv.org/abs/1801.07648</a>)</li>
<li><em>Recurrent Deep Embedding Networks for Genotype Clustering and Ethnicity Prediction</em> by Karim et al. (see more at <a href="https://arxiv.org/pdf/1805.12218.pdf">https://arxiv.org/pdf/1805.12218.pdf</a>)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Frequently asked questions (FAQs)</h1>
                </header>
            
            <article>
                
<p>We have analyzed the completed projects and looked at recent trends. Based on these, there might be several questions in your mind. In this section, I will try to devise some such questions and provide sample answers:</p>
<ol>
<li>In this chapter, we argued that using GAN, we could solve many research problems. Is there any GAN implementation in DL4J?</li>
<li>In this chapter, we argued that using CapsNet is a better idea for handling images having different shapes and orientation. Is there any implementation for CapsNet in <span>DL4J</span>?</li>
<li>In <a href="fba163f0-88c0-4278-a4a1-df5389cc3a10.xhtml" target="_blank">Chapter 1</a>, <em>Getting Started with Deep Learning</em>, we discussed DBNs and restricted Boltzmann machines as their basic building blocks. However, we have not used DBNs in any of the completed projects. What is the reason for this?</li>
<li>In this chapter, we argued that using unsupervised anomaly detection from IoT sensor data or images is an emerging research use case. Are there any examples of this in DL4J?</li>
<li>Are there any examples of developing recommendation engines with DL4J?</li>
</ol>
<ol start="6">
<li>Considering the fact that smartphones nowadays are very powerful, can we develop image/object detection applications on a smartphone?</li>
<li class="mce-root">How could I wrap-up a deep learning application as a web app?</li>
<li>I am having issues while running the projects. In addition, I am experiencing issues while configuring the development environment (for example, on Eclipse/IntelliJ IDEA and configuring CUDA/CuDNN). What can I do?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Answers to questions</h1>
                </header>
            
            <article>
                
<p><strong>Answer</strong> <strong>to question 1</strong>: There is an inactive issue on this. Interested readers can look at <a href="https://github.com/deeplearning4j/deeplearning4j/issues/1737">https://github.com/deeplearning4j/deeplearning4j/issues/1737</a> to know the current update. However, the discussion loop is not very active.</p>
<p><strong>Answer</strong> <strong>to question 2</strong>: As far as I know, there is no CapsNet implementation in DL4J. Also, I didn't see any open discussion/issues on this topic. I asked in the DL4j Gitter channel but nobody replied.</p>
<p><strong>Answer to question 3</strong>: Both unsupervised pre-training and supervised fine-tuning can be performed using DBN. That means this probabilistic network is an intelligent choice if we do not have enough labeled data but still want to perform NN-based training.</p>
<p><strong>Answer</strong> <strong>to question 4</strong>: Yes, there is an example of anomaly detection using a variational autoencoder with reconstruction probability for MNIST data. Take a look at DL4J examples at <a href="https://github.com/deeplearning4j/dl4j-examples/tree/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/unsupervised/anomalydetection">https://github.com/deeplearning4j/dl4j-examples/tree/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/unsupervised/anomalydetection</a>. However, this example can be extended for other datasets too.</p>
<p class="mce-root"><strong>Answer to question 5</strong>: There is an example for well-dressed recommendation engine in this link <a href="https://deeplearning4j.org/welldressed-recommendation-engine">https://deeplearning4j.org/welldressed-recommendation-engine</a>.</p>
<p class="mce-root"><strong>Answer to question 6</strong>: Deep learning and neural networks can be deployed in Android devices too. For more information, refer to <a href="https://deeplearning4j.org/android">https://deeplearning4j.org/android</a>.</p>
<p class="mce-root"><strong>Answer to question 7</strong>: Once the NN is trained, the network can be used for inference, or making predictions about the data it sees. Inference happens to be a much less compute-intensive process. Then, Spring Boot or another framework can be used to wrap up the application as a web app. See some guidelines at <a href="https://deeplearning4j.org/build_vgg_webapp">https://deeplearning4j.org/build_vgg_webapp</a>.</p>
<p><strong>Answer to question 8</strong>: You should follow the instructions I provided in the chapters. In addition, the code of this book is available on GitHub, so feel free to PR or create new issues and I will try to fix them as soon as possible. About any new issues, you can ask through the DL4J Gitter live channel at <a href="https://gitter.im/deeplearning4j/deeplearning4j">https://gitter.im/deeplearning4j/deeplearning4j</a>.</p>


            </article>

            
        </section>
    </body></html>