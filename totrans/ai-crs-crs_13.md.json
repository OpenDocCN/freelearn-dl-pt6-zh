["```py\n# Importing the libraries   #4\nimport numpy as np   #5\nimport pygame as pg   #6 \n```", "```py\n# Initializing the Environment class   #8\nclass Environment():   #9\n    #10\n    def __init__(self, waitTime):   #11\n        #12\n        # Defining the parameters   #13\n        self.width = 880            # width of the game window   #14\n        self.height = 880           # height of the game window   #15\n        self.nRows = 10             # number of rows in our board   #16\n        self.nColumns = 10          # number of columns in our board   #17\n        self.initSnakeLen = 2       # initial length of the snake   #18\n        self.defReward = -0.03      # reward for taking an action - The Living Penalty   #19\n        self.negReward = -1\\.        # reward for dying   #20\n        self.posReward = 2\\.         # reward for collecting an apple   #21\n        self.waitTime = waitTime    # slowdown after taking an action   #22\n        #23\n        if self.initSnakeLen > self.nRows / 2:   #24\n            self.initSnakeLen = int(self.nRows / 2)   #25\n        #26\n        self.screen = pg.display.set_mode((self.width, self.height))   #27\n        #28\n        self.snakePos = list()   #29\n        #30\n        # Creating the array that contains mathematical representation of the game's board   #31\n        self.screenMap = np.zeros((self.nRows, self.nColumns))   #32\n        #33\n        for i in range(self.initSnakeLen):   #34\n            self.snakePos.append((int(self.nRows / 2) + i, int(self.nColumns / 2)))   #35\n            self.screenMap[int(self.nRows / 2) + i][int(self.nColumns / 2)] = 0.5   #36\n            #37\n        self.applePos = self.placeApple()   #38\n        #39\n        self.drawScreen()   #40\n        #41\n        self.collected = False   #42\n        self.lastMove = 0   #43 \n```", "```py\n # Building a method that gets new, random position of an apple\n    def placeApple(self):\n        posx = np.random.randint(0, self.nColumns)\n        posy = np.random.randint(0, self.nRows)\n        while self.screenMap[posy][posx] == 0.5:\n            posx = np.random.randint(0, self.nColumns)\n            posy = np.random.randint(0, self.nRows)\n\n        self.screenMap[posy][posx] = 1\n\n        return (posy, posx) \n```", "```py\n # Making a function that draws everything for us to see\n    def drawScreen(self):\n\n        self.screen.fill((0, 0, 0))\n\n        cellWidth = self.width / self.nColumns\n        cellHeight = self.height / self.nRows\n\n        for i in range(self.nRows):\n            for j in range(self.nColumns):\n                if self.screenMap[i][j] == 0.5:\n                    pg.draw.rect(self.screen, (255, 255, 255), (j*cellWidth + 1, i*cellHeight + 1, cellWidth - 2, cellHeight - 2))\n                elif self.screenMap[i][j] == 1:\n                    pg.draw.rect(self.screen, (255, 0, 0), (j*cellWidth + 1, i*cellHeight + 1, cellWidth - 2, cellHeight - 2))\n\n        pg.display.flip() \n```", "```py\n # A method that updates the snake's position\n    def moveSnake(self, nextPos, col):\n\n        self.snakePos.insert(0, nextPos)\n\n        if not col:\n            self.snakePos.pop(len(self.snakePos) - 1)\n\n        self.screenMap = np.zeros((self.nRows, self.nColumns))\n\n        for i in range(len(self.snakePos)):\n            self.screenMap[self.snakePos[i][0]][self.snakePos[i][1]] = 0.5\n\n        if col:\n            self.applePos = self.placeApple()\n            self.collected = True\n\n        self.screenMap[self.applePos[0]][self.applePos[1]] = 1 \n```", "```py\n # The main method that updates the environment\n    def step(self, action):\n        # action = 0 -> up\n        # action = 1 -> down\n        # action = 2 -> right\n        # action = 3 -> left\n\n        # Resetting these parameters and setting the reward to the living penalty\n        gameOver = False\n        reward = self.defReward\n        self.collected = False\n\n        for event in pg.event.get():\n            if event.type == pg.QUIT:\n                return\n\n        snakeX = self.snakePos[0][1]\n        snakeY = self.snakePos[0][0]\n\n        # Checking if an action is playable and if not then it is changed to the playable one\n        if action == 1 and self.lastMove == 0:\n            action = 0\n        if action == 0 and self.lastMove == 1:\n            action = 1\n        if action == 3 and self.lastMove == 2:\n            action = 2\n        if action == 2 and self.lastMove == 3:\n            action = 3 \n```", "```py\n # Checking what happens when we take this action\n        if action == 0:\n            if snakeY > 0:\n                if self.screenMap[snakeY - 1][snakeX] == 0.5:\n                    gameOver = True\n                    reward = self.negReward\n                elif self.screenMap[snakeY - 1][snakeX] == 1:\n                    reward = self.posReward\n                    self.moveSnake((snakeY - 1, snakeX), True)\n                elif self.screenMap[snakeY - 1][snakeX] == 0:\n                    self.moveSnake((snakeY - 1, snakeX), False)\n            else:\n                gameOver = True\n                reward = self.negReward \n```", "```py\n elif action == 1:\n            if snakeY < self.nRows - 1:\n                if self.screenMap[snakeY + 1][snakeX] == 0.5:\n                    gameOver = True\n                    reward = self.negReward\n                elif self.screenMap[snakeY + 1][snakeX] == 1:\n                    reward = self.posReward\n                    self.moveSnake((snakeY + 1, snakeX), True)\n                elif self.screenMap[snakeY + 1][snakeX] == 0:\n                    self.moveSnake((snakeY + 1, snakeX), False)\n            else:\n                gameOver = True\n                reward = self.negReward\n\n        elif action == 2:\n            if snakeX < self.nColumns - 1:\n                if self.screenMap[snakeY][snakeX + 1] == 0.5:\n                    gameOver = True\n                    reward = self.negReward\n                elif self.screenMap[snakeY][snakeX + 1] == 1:\n                    reward = self.posReward\n                    self.moveSnake((snakeY, snakeX + 1), True)\n                elif self.screenMap[snakeY][snakeX + 1] == 0:\n                    self.moveSnake((snakeY, snakeX + 1), False)\n            else:\n                gameOver = True\n                reward = self.negReward \n\n        elif action == 3:\n            if snakeX > 0:\n                if self.screenMap[snakeY][snakeX - 1] == 0.5:\n                    gameOver = True\n                    reward = self.negReward\n                elif self.screenMap[snakeY][snakeX - 1] == 1:\n                    reward = self.posReward\n                    self.moveSnake((snakeY, snakeX - 1), True)\n                elif self.screenMap[snakeY][snakeX - 1] == 0:\n                    self.moveSnake((snakeY, snakeX - 1), False)\n            else:\n                gameOver = True\n                reward = self.negReward \n```", "```py\n # Drawing the screen, updating last move and waiting the wait time specified\n        self.drawScreen()\n\n        self.lastMove = action\n\n        pg.time.wait(self.waitTime) \n```", "```py\n # Returning the new frame of the game, the reward obtained and whether the game has ended or not\n        return self.screenMap, reward, gameOver \n```", "```py\n # Making a function that resets the environment\n    def reset(self):\n        self.screenMap  = np.zeros((self.nRows, self.nColumns))\n        self.snakePos = list()\n\n        for i in range(self.initSnakeLen):\n            self.snakePos.append((int(self.nRows / 2) + i, int(self.nColumns / 2)))\n            self.screenMap[int(self.nRows / 2) + i][int(self.nColumns / 2)] = 0.5\n\n        self.screenMap[self.applePos[0]][self.applePos[1]] = 1\n\n        self.lastMove = 0 \n```", "```py\n# Importing the libraries\nimport keras\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\nfrom keras.optimizers import Adam \n```", "```py\n# Creating the Brain class\nclass Brain():\n\n    def __init__(self, iS = (100,100,3), lr = 0.0005):\n\n        self.learningRate = lr\n        self.inputShape = iS\n        self.numOutputs = 4\n        self.model = Sequential() \n```", "```py\n # Adding layers to the model   #20\n        self.model.add(Conv2D(32, (3,3), activation = 'relu', input_shape = self.inputShape))   #21\n        #22\n        self.model.add(MaxPooling2D((2,2)))   #23\n        #24\n        self.model.add(Conv2D(64, (2,2), activation = 'relu'))   #25\n        #26\n        self.model.add(Flatten())   #27\n        #28\n        self.model.add(Dense(units = 256, activation = 'relu'))   #29\n        #30\n        self.model.add(Dense(units = self.numOutputs))   #31 \n```", "```py\n # Compiling the model\n        self.model.compile(loss = 'mean_squared_error', optimizer = Adam(lr = self.learningRate)) \n```", "```py\n # Making a function that will load a model from a file\n    def loadModel(self, filepath):\n        self.model = load_model(filepath)\n        return self.model \n```", "```py\n# AI for Games - Beat the Snake game\n# Implementing Deep Q-Learning with Experience Replay\n\n# Importing the libraries\nimport numpy as np\n\n# IMPLEMENTING DEEP Q-LEARNING WITH EXPERIENCE REPLAY\n\nclass Dqn(object):\n\n    # INTRODUCING AND INITIALIZING ALL THE PARAMETERS AND VARIABLES OF THE DQN\n    def __init__(self, max_memory = 100, discount = 0.9):\n        self.memory = list()\n        self.max_memory = max_memory\n        self.discount = discount\n\n    # MAKING A METHOD THAT BUILDS THE MEMORY IN EXPERIENCE REPLAY\n    def remember(self, transition, game_over):\n        self.memory.append([transition, game_over])\n        if len(self.memory) > self.max_memory:\n            del self.memory[0]\n\n    # MAKING A METHOD THAT BUILDS TWO BATCHES OF INPUTS AND TARGETS BY EXTRACTING TRANSITIONS FROM THE MEMORY\n    def get_batch(self, model, batch_size = 10):\n        len_memory = len(self.memory)\n        num_inputs = self.memory[0][0][0].shape[1]\n        num_outputs = model.output_shape[-1]\n        inputs = np.zeros((min(len_memory, batch_size), num_inputs))\n        targets = np.zeros((min(len_memory, batch_size), num_outputs))\n        for i, idx in enumerate(np.random.randint(0, len_memory, size = min(len_memory, batch_size))):\n            current_state, action, reward, next_state = self.memory[idx][0]\n            game_over = self.memory[idx][1]\n            inputs[i] = current_state\n            targets[i] = model.predict(current_state)[0]\n            Q_sa = np.max(model.predict(next_state)[0])\n            if game_over:\n                targets[i, action] = reward\n            else:\n                targets[i, action] = reward + self.discount * Q_sa\n        return inputs, targets \n```", "```py\n num_inputs = self.memory[0][0][0].shape[1] \n```", "```py\n inputs = np.zeros((min(len_memory, batch_size), num_inputs)) \n```", "```py\n inputs = np.zeros((min(len_memory, batch_size), self.memory[0][0][0].shape[1],self.memory[0][0][0].shape[2],self.memory[0][0][0].shape[3])) \n```", "```py\n# Importing the libraries\nfrom environment import Environment\nfrom brain import Brain\nfrom DQN import Dqn\nimport numpy as np\nimport matplotlib.pyplot as plt \n```", "```py\n# Defining the parameters\nmemSize = 60000\nbatchSize = 32\nlearningRate = 0.0001\ngamma = 0.9\nnLastStates = 4\n\nepsilon = 1.\nepsilonDecayRate = 0.0002\nminEpsilon = 0.05\n\nfilepathToSave = 'model2.h5' \n```", "```py\n# Creating the Environment, the Brain and the Experience Replay Memory\nenv = Environment(0)\nbrain = Brain((env.nRows, env.nColumns, nLastStates), learningRate)\nmodel = brain.model\ndqn = Dqn(memSize, gamma) \n```", "```py\n# Making a function that will initialize game states   #30\ndef resetStates():   #31\n    currentState = np.zeros((1, env.nRows, env.nColumns, nLastStates))   #32\n    #33\n    for i in range(nLastStates):   #34\n        currentState[:,:,:,i] = env.screenMap   #35\n    #36\n    return currentState, currentState   #37 \n```", "```py\n# Starting the main loop\nepoch = 0\nscores = list()\nmaxNCollected = 0\nnCollected = 0.\ntotNCollected = 0 \n```", "```py\nwhile True:\n    # Resetting the environment and game states\n    env.reset()\n    currentState, nextState = resetStates()\n    epoch += 1\n    gameOver = False \n```", "```py\n # Starting the second loop in which we play the game and teach our AI\n    while not gameOver: \n\n        # Choosing an action to play\n        if np.random.rand() < epsilon:\n            action = np.random.randint(0, 4)\n        else:\n            qvalues = model.predict(currentState)[0]\n            action = np.argmax(qvalues) \n```", "```py\n # Updating the environment\n        state, reward, gameOver = env.step(action) \n```", "```py\n # Adding new game frame to the next state and deleting the oldest frame from next state\n        state = np.reshape(state, (1, env.nRows, env.nColumns, 1))\n        nextState = np.append(nextState, state, axis = 3)\n        nextState = np.delete(nextState, 0, axis = 3) \n```", "```py\n # Remembering the transition and training our AI\n        dqn.remember([currentState, action, reward, nextState], gameOver)\n        inputs, targets = dqn.get_batch(model, batchSize)\n        model.train_on_batch(inputs, targets) \n```", "```py\n # Checking whether we have collected an apple and updating the current state\n        if env.collected:\n            nCollected += 1\n\n        currentState = nextState \n```", "```py\n # Checking if a record of apples eaten in a around was beaten and if yes then saving the model\n    if nCollected > maxNCollected and nCollected > 2:\n        maxNCollected = nCollected\n        model.save(filepathToSave)\n\n    totNCollected += nCollected\n    nCollected = 0 \n```", "```py\n # Showing the results each 100 games\n    if epoch % 100 == 0 and epoch != 0:\n        scores.append(totNCollected / 100)\n        totNCollected = 0\n        plt.plot(scores)\n        plt.xlabel('Epoch / 100')\n        plt.ylabel('Average Score')\n        plt.savefig('stats.png')\n        plt.close() \n```", "```py\n # Lowering the epsilon\n    if epsilon > minEpsilon:\n        epsilon -= epsilonDecayRate \n```", "```py\n # Showing the results each game\n    print('Epoch: ' + str(epoch) + ' Current Best: ' + str(maxNCollected) + ' Epsilon: {:.5f}'.format(epsilon)) \n```", "```py\n# Importing the libraries\nfrom environment import Environment\nfrom brain import Brain\nimport numpy as np \n```", "```py\n# Defining the parameters\nnLastStates = 4\nfilepathToOpen = 'model.h5'\nslowdown = 75 \n```", "```py\n# Creating the Environment and the Brain\nenv = Environment(slowdown)\nbrain = Brain((env.nRows, env.nColumns, nLastStates))\nmodel = brain.loadModel(filepathToOpen) \n```", "```py\n# Making a function that will reset game states\ndef resetStates():\n    currentState = np.zeros((1, env.nRows, env.nColumns, nLastStates))\n\n    for i in range(nLastStates):\n        currentState[:,:,:,i] = env.screenMap\n\n    return currentState, currentState \n```", "```py\n# Starting the main loop\nwhile True:\n    # Resetting the game and the game states\n    env.reset()\n    currentState, nextState = resetStates()\n    gameOver = False \n```", "```py\n # Playing the game\n    while not gameOver: \n\n        # Choosing an action to play\n        qvalues = model.predict(currentState)[0]\n        action = np.argmax(qvalues) \n```", "```py\n # Updating the environment\n        state, _, gameOver = env.step(action) \n```", "```py\n # Adding new game frame to next state and deleting the oldest one from next state\n        state = np.reshape(state, (1, env.nRows, env.nColumns, 1))\n        nextState = np.append(nextState, state, axis = 3)\n        nextState = np.delete(nextState, 0, axis = 3) \n```", "```py\n # Updating current state\n        currentState = nextState \n```", "```py\nconda create -n snake python=3.6 \n```", "```py\nconda activate snake \n```", "```py\nconda install -c conda-forge keras \n```", "```py\nconda activate snake \n```", "```py\npython train.py \n```", "```py\npython test.py \n```"]