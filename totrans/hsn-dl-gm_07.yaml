- en: Introducing DRL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Deep reinforcement learning** (**DRL**) is currently taking the world by
    storm and is seen as the "it" of machine learning technologies, the it goal of
    reaching some form of general AI. Perhaps it is because DRL approaches the cusp
    of general AI or what we perceive as general intelligence. It is also likely to
    be one of the main reasons you are reading this book. Fortunately, this chapter,
    and the majority of the rest of the book, focuses deeply on **reinforcement learning**
    (**RL**) and its many variations. In this chapter, we start learning the basics
    of RL and how it can be adapted to **deep learning** (**DL**). We will explore
    the **OpenAI Gym** environment, a great RL playground, and see how to use it with
    some simple DRL techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind, this is a hands-on book, so we will be keeping technical theory
    to a minimum, and instead we will explore plenty of working examples. Some readers
    may feel lost without the theoretical background and feel the need to explore
    the more theoretical side of RL on their own.
  prefs: []
  type: TYPE_NORMAL
- en: For other readers not familiar with the theoretical background of RL, we will
    cover several core concepts, but this is the abridged version, so it is recommended
    you seek theoretical knowledge from other sources when you are ready.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will start learning about DRL, a topic that will carry
    through to many chapters. We will start with the basics and then look to explore
    some working examples adapted to DL. Here is what we will cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Q-learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running the OpenAI gym
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first DRL with Deep Q-Network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RL experiments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For those of you who like to jump around books: yes, it is OK to start this
    book from this chapter. However, you may need to go back to previous chapters
    in order to complete some exercises. We will also assume that your Python environment
    is configured with TensorFlow and Keras, but if you are unsure, check out the
    `requirements.txt` file in the project folder.'
  prefs: []
  type: TYPE_NORMAL
- en: All the projects in this book are built with Visual Studio 2017 (Python), and
    it is the recommended editor for the examples in this book. If you use VS 2017
    with Python, you can easily manage the samples by opening the chapter solution
    file. Of course, there are plenty of other excellent Python editors and tools,
    so use what you are comfortable with.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RL currently leads the pack in advances compared to other machine learning methodologies.
    Note the use of the word *methodology* and not *technology*. RL is a methodology
    or algorithm that applies a principle we can use with neural networks, whereas,
    neural networks are a machine learning technology that can be applied to several
    methodologies. Previously, we looked at other methodologies that blended with
    DL, but we focused more on the actual implementation. However, RL introduces a
    new methodology that requires us to understand more of the inner and outer workings
    before we understand how to apply it.
  prefs: []
  type: TYPE_NORMAL
- en: RL was popularized by Richard Sutton, a Canadian, and current professor at the
    University of Alberta. Sutton has also assisted in the development of RL at Google's
    DeepMind, and is quite often regarded as the father of RL.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the heart of any machine learning system is the need for training. Often,
    the AI agent/brain knows nothing, and then we feed it data through some automated
    process for it to learn. As we have seen, the most common way of doing this is
    called **supervised training**. This is when we first label our training data.
    We have also looked at **unsupervised training**, where our **Generative Adversarial
    Networks** (**GANs**) were trained by competing against each other. However, neither
    system replicated the type of learning or training we see in **Biology**, and
    that is often referred to as **rewards** or RL: the type of learning that lets
    you teach your dog to bark for a treat, fetch the paper, and use the outdoors
    for nature''s calling, a type of learning that lets an agent explore its own environment
    and learn for itself. This is not unlike the type of learning a general AI would
    be expected to use; after all, RL is likely similar to the system we use, or so
    we believe.'
  prefs: []
  type: TYPE_NORMAL
- en: David Silver, a former student of Prof Sutton's and now head of DeepMind, has
    an excellent video series on the theoretical background of RL. The first five
    videos are quite interesting and recommended viewing, but the later content gets
    quite deep and may not be for everyone. Here's the link for the videos: [https://www.youtube.com/watch?v=2pWv7GOvuf0](https://www.youtube.com/watch?v=2pWv7GOvuf0)
  prefs: []
  type: TYPE_NORMAL
- en: 'RL defines its own type of training called by the same name. This form of reward-based
    training is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b176d0d-9b93-423d-9d0c-cb1998759379.png)'
  prefs: []
  type: TYPE_IMG
- en: Reinforcement learning
  prefs: []
  type: TYPE_NORMAL
- en: The diagram shows an agent in an environment. That agent reads the state of
    the environment and then decides and performs an action. This action may, or may
    not, give a reward, and that reward could be good or bad. After each action and
    possible reward, the agent collects the state of the environment again. The process
    repeats itself until the agent reaches a terminal or end state. That is, until
    it reaches the goal; perhaps it dies or just gets tired. It is important to note
    a couple of subtle things about the preceding diagram. First, the agent doesn't
    always receive a reward, meaning rewards could be delayed, until some future goal
    is reached. This is quite different from the other forms of learning we explored
    earlier, which provided immediate feedback to our training networks. Rewards can
    be good or bad, and it is often just as effective to negatively train agents this
    way, but less so for humans.
  prefs: []
  type: TYPE_NORMAL
- en: Now, as you might expect with any powerful learning model, the mathematics can
    be quite complex and certainly daunting to the newcomer. We won't go too far into
    the theoretical details other than to describe some of the foundations of RL in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The multi-armed bandit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The diagram we saw earlier describes the full RL problem as we will use for
    most of the rest of this book. However, we often teach a simpler one-step variation
    of this problem called the **multi-armed bandit**. The armed bandit is in reference
    to the Vegas slot machine and nothing more nefarious. We use these simpler scenarios
    in order to explain the basics of RL in the form of a one-step or one-state problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of the multi-armed bandit, picture a fictional multi-armed Vegas
    slot machine that awards different prizes based on which arm is pulled, but the
    prize for each arm is always the same. The agent''s goal in this scenario would
    be to figure out the correct arm to pull every time. We could further model this
    in an equation such as the one shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cce0e6d0-2ea0-4ea0-b7e2-6e13a93eb5fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Consider the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfabb87d-9a06-4b96-8661-72ef51f3bb0a.png) = vector of values (1,2,3,4)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/77c8a845-a682-4c89-b796-9055f0bd6b1f.png) = action'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f7eea114-134d-4145-b713-94d4fb0ba75e.png) = alpha = learning rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/728ee055-bd4e-4791-be8b-c26fb766e47a.png) = reward'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This equation calculates the value (*V*), a vector, for each action the agent
    takes. Then, it feeds back these values into itself, subtracted from the reward
    and multiplied by a learning rate. This calculated value can be used to determine
    which arm to pull, but first the agent needs to pull each arm at least once. Let''s
    quickly model this in code, so as game/simulation programmers, we can see how
    this works. Open the `Chapter_5_1.py` code and follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this exercise is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This code creates the required setup variables, the `arms` (`gold`, `silver`,
    and `bronze`), and the value vector `v` (all zeros). Then, the code loops through
    a number of iterations (`10`) where each arm is pulled and the value, `v`, is
    calculated and updated based on the equation. Note that the reward value is replaced
    by the value of the arm pull, which is the term `arms[a][1]`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the example, and you will see the output generated showing the value for
    each action, or in this case an arm pull.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we saw, with a simple equation, we were able to model the multi-armed bandit
    problem and arrive at a solution that will allow an agent to consistently pull
    the correct arm. This sets the foundation for RL, and in the next section, we
    take the next step and look at **contextual bandits**.
  prefs: []
  type: TYPE_NORMAL
- en: Contextual bandits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can now elevate the single multi-armed bandit problem into a problem with
    multiple multi-armed bandits, each with its own set of arms. Now our problem introduces
    context or state into the equation. With each bandit defining its own context/state,
    now we evaluate our equation in terms of quality and action. Our modified equation
    is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c461e128-42f5-4362-9277-a839f0c1589d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Consider the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/259fdfe3-f4e2-4625-8dc9-c073c9051569.png) = table/matrix of values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[1,2,3,4'
  prefs: []
  type: TYPE_NORMAL
- en: 2,3,4,5
  prefs: []
  type: TYPE_NORMAL
- en: 4,2,1,4]
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/276ce14a-f047-46d2-afcb-e45b7a3caa43.png) = state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/dea129da-958b-4492-8437-591af763c9b2.png) = action'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/ca094865-f756-450b-b7c2-9ebc383af35f.png) = alpha = learning rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/3083521e-5c5f-4684-bc1c-2453b810c4c3.png) = reward'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s open up `Chapter_5_2.py` and observe the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the code up, as follows, and follow the changes made from the previous
    sample:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This code sets up a number of multi-armed bandits, each with its own set of
    arms. It then iterates through a number of iterations, but this time as it loops,
    it also loops through each bandit. During each loop, it picks a random arm to
    pull and evaluates the quality.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the sample and look at the output of `q`. Note how, even after selecting
    random arms, the equation again consistently selected the gold arm, the arm with
    the highest reward, to pull.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feel free to play around with this sample some more and look to the exercises
    for additional inspiration. We will expand on the complexity of our RL problems
    when we discuss Q-Learning. However, before we get to that section, we will take
    a quick diversion and look at setting up the OpenAI Gym in order to conduct more
    RL experiments.
  prefs: []
  type: TYPE_NORMAL
- en: RL with the OpenAI Gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RL has become so popular that there is now a race to just build tools that help
    build RL algorithms. The two major competitors in this area right now are **OpenAI
    Gym** and **Unity**. Unity has quickly become the RL racing machine we will explore
    extensively later. For now, we will put our training wheels on and run OpenAI
    Gym to explore the fundamentals of RL further.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to install the OpenAI Gym toolkit before we can continue, and installation
    may vary greatly depending on your operating system. As such, we will focus on
    the Windows installation instructions here, as it is likely other OS users will
    have less difficulty. Follow the next steps to install OpenAI Gym on Windows:'
  prefs: []
  type: TYPE_NORMAL
- en: Install a C++ compiler; if you have Visual Studio 2017 installed, you may already
    have a recommended one. You can find other supported compilers here: [https://wiki.python.org/moin/WindowsCompilers](https://wiki.python.org/moin/WindowsCompilers).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Be sure to have Anaconda installed, and open an Anaconda command prompt and
    run the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: For our purposes, in the short term, we don't need to install any other Gym
    modules. Gym has plenty of example environments, Atari games and MuJoCo (robotics
    simulator) being some of the most fun to work with. We will take a look at the
    Atari games module later in this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That should install the Gym environment for your system. Most of what we need
    will work with minimal setup. If you decide to do more with Gym, then you will
    likely want to install other modules; there are several. In the next section,
    we are going to test this new environment as we learn about Q-Learning.
  prefs: []
  type: TYPE_NORMAL
- en: A Q-Learning model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RL is deeply entwined with several mathematical and dynamic programming concepts
    that could fill a textbook, and indeed there are several. For our purposes, however,
    we just need to understand the key concepts in order to build our DRL agents.
    Therefore, we will choose not to get too burdened with the math, but there are
    a few key concepts that you will need to understand to be successful. If you covered
    the math in the [Chapter 1](108dd4cb-0332-4f3b-963b-fbfb49f2c8f0.xhtml), *Deep
    Learning for Games*, this section will be a breeze. For those that didn't, just
    take your time, but you can't miss this one.
  prefs: []
  type: TYPE_NORMAL
- en: In order to understand the Q-Learning model, which is a form of RL, we need
    to go back to the basics. In the next section, we talk about the importance of
    the **Markov decision process** and the **Bellman** e**quation**.
  prefs: []
  type: TYPE_NORMAL
- en: Markov decision process and the Bellman equation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the heart of RL is the **Markov decision process** (**MDP**). An MDP is
    often described as a discrete time stochastic control process. In simpler terms,
    this just means it is a control program that functions by time steps to determine
    the probability of actions, provided each action leads to a reward. This process
    is already used for most automation control of robotics, drones, networking, and
    of course RL. The classic way we picture this process is shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/434ef2cd-a9d0-4c70-9cd8-e0cddd217734.png)'
  prefs: []
  type: TYPE_IMG
- en: The Markov decision process
  prefs: []
  type: TYPE_NORMAL
- en: 'Where represent an MDP as a tuple or vector ![](img/2f668c37-4a3c-4fb3-bea6-89ee1da65240.png),
    using the following variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db442408-6913-47ba-8e98-f0dc1ac4d37e.png) - being a finite set of states,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/fd0b1ca8-560b-4a27-baa6-603d0ce3229d.png) - being a finite set of actions,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/02393b19-30c2-43a5-b8d6-8e27fee233c9.png) - the probability that action
    ![](img/bf5cc02b-c6f0-43ba-a6d4-bfd93a7b3364.png) in state ![](img/612e1b85-51ac-4943-bf9f-d3043356022d.png) at
    time ![](img/2e0c7122-44b2-490a-9541-7005d2b217bd.png) will lead to state ![](img/db1fa5b6-7a59-4f45-904c-928e1e782c1f.png) at
    time ![](img/ed7e16b3-966f-43cd-8e6b-2cbccf8ab57a.png),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/a964622c-26ef-4841-865b-40ccc8197365.png) - is the immediate reward'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/aaa15109-a497-4e52-a731-a66acb40c59d.png) - gamma is a discount factor
    we apply in order to discount the significance or provide significance to future
    rewards'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The diagram works by picturing yourself as an agent in one of the states. You
    then determine actions based on the probability, always taking a random action.
    As you move to the next state, the action gives you a reward and you update the
    probability based on the reward. Again, David Silver covers this piece very well
    in his lectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the preceding process works, but another variation came along that provided
    for better future reward evaluation, and that was done by introducing the **Bellman
    Equation** and the concept of a policy/value iteration. Whereas before we had
    a value, ![](img/d62b7e72-e9f9-470c-aafa-95bf8254b13f.png), we now have a policy
    (![](img/8aedee8a-b54e-4b76-96ec-ebd39c12acc0.png)) for a value called ![](img/6ae870a0-2615-43ca-a224-1b08d456a58f.png), and
    this yields us a new equation, shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16cb44c4-34b7-4914-b933-ce07125fabf0.png)'
  prefs: []
  type: TYPE_IMG
- en: We won't cover much more about this equation other than to say to keep the concept
    of quality iteration in mind. In the next section, we will see how we can reduce
    this equation back to a quality indicator of each action and use that for Q-Learning.
  prefs: []
  type: TYPE_NORMAL
- en: Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the introduction of quality iteration methods, the derivation of a finite
    state method called **Q-learning** or **quality learning** was derived. Q uses
    the technique of quality iteration for a given finite state problem to determine
    the best course of action for an agent. The equation we saw in the previous section
    can now be represented as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/649eaf34-9453-4328-9183-5f8437af651f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Consider the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/74182c8a-795c-46a2-bd88-8679475b3ddd.png) current state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/a02d73e5-10d7-4937-b5bf-3b0d34f2807c.png) current action'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/8fe87e76-4098-487c-8033-e01ad9688d4b.png) next action'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/1d73936e-4c41-408e-8249-949594662494.png) current reward'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/6035c5ee-320c-4ab2-865c-5294f55549c2.png) learning rate (alpha)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/57f56e14-a30f-4531-9da1-e5991077f44b.png) reward discount factor (gamma)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: 'The Q value is now updated alliteratively, as the agent roams through its environment.
    Nothing demonstrates these concepts better than an example. Open up `Chapter_5_3.py`
    and follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with the various imports and set them up as shown in the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: These imports just load the basic libraries we need for this example. Remember,
    you will need to install `Gym` to run this sample.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we set up a new environment; in this example, we use the basic `FrozenLake-v0`
    sample, a perfect example to test on Q-learning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we set up the AI environment (`env`) and a number of other parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this section of the code, we set up a number of variables that we will get
    to shortly. For this sample, we are using a wrapper tool to monitor the environment,
    and this is useful for determining any potential training issues. The other thing
    to note is the setup of the `q_table` array, defined by the environment `observation_space`
    (state) and `action_space` (action); spaces define arrays and not just vectors.
    In this particular example, the `action_space` is a vector, but it could be a
    multi-dimensional array or tensor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pass over the next section of functions and skip to the end, where the training
    iteration occurs and is shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Most of the preceding code is relatively straightforward and should be easy
    to follow. Look at how the `env` (environment) is using the `action` generated
    from the `act` function; this is used to step or conduct an action on the agent.
    The output of the `step` function is `next_state`, `reward`, and `done`, which
    we use to determine the optimum Q policy by using the `learn` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before we get into the action and learning functions, run the sample and watch
    how the agent trains. It may take a while to train, so feel free to return to
    the book.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following is an example of the OpenAI Gym FrozenLake environment running
    our Q-learning model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2cbf3e7-ae6a-4e0c-a5c8-3961aa6119c6.png)'
  prefs: []
  type: TYPE_IMG
- en: FrozenLake Gym environment
  prefs: []
  type: TYPE_NORMAL
- en: As the sample runs, you will see a simple text output showing the environment.
    `S` represents the start, `G` the goal, `F` a frozen section, and `H` a hole.
    The goal for the agent is to find its way through the environment, without falling
    in a hole, and reach the goal. Pay special attention to how the agent moves and
    finds it way around the environment. In the next section, we unravel the `learn`
    and `act` functions and understand the importance of exploration.
  prefs: []
  type: TYPE_NORMAL
- en: Q-learning and exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One problem we face with the policy iterative models such as Q-learning is
    the problem of exploration versus exploitation. The Q-model equation assumes the
    use of maximum quality to determine an action and we refer to this as exploitation
    (exploiting the model). The problem with this is that it can often corner an agent
    into a solution that only looks for the best short-term benefits. Instead, we
    need to allow the agent some flexibility to explore the environment and learn
    on its own. We do this by introducing a dissolving exploration factor into the
    training. Let''s see how this looks by again opening up the `Chapter_5_3.py` example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scroll down to the `act` and `is_explore` functions as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note that in the `act` function, it first tests whether the agent wants to or
    needs to explore with `is_explore()`. In the `is_explore` function, we can see
    that the global `epsilon` value is decayed over each iteration with `epsilon_decay`
    to a global minimum value, `epsilon_min`. When the agent starts an episode, their
    exploration `epsilon` is high, making them more probable to explore. Over time,
    as the episode progresses, the `epsilon` decreases. We do in with the assumption
    that over time the agent will need to explore less and less. This trade-off between
    exploration and exploitation is quite important and something to understand with
    respect to the size of the environment state. We will see this trade-off explored
    more throughout this book.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that the agent uses an exploration function and just selects a random action.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we get to the `learn` function. This function is where the `Q` value
    is calculated, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here, the equation is broken up and simplified, but this is the step that calculates
    the value the agent will use when exploiting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keep the agent running until it finishes. We just completed the first full reinforcement
    learning problem, albeit the one that had a finite state. In the next section,
    we greatly expand our horizons and look at deep learning combined with reinforcement
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: First DRL with Deep Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we understand the reinforcement learning process in detail, we can
    look to adapt our Q-learning model to work with deep learning. This, as you could
    likely guess, is the culmination of our efforts and where the true power of RL
    shines. As we learned through earlier chapters, deep learning is essentially a
    complex system of equations that can map inputs through a non-linear function
    to generate a trained output.
  prefs: []
  type: TYPE_NORMAL
- en: A neural network is just another, simpler method of solving a non-linear equation.
    We will look at how to use DNN to solve other equations later, but for now we
    will focus on using it to solve the Q-learning equation we saw in the previous
    section.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the **CartPole** training environment from the OpenAI Gym toolkit.
    This environment is pretty much the standard used to learn **Deep Q-learning**
    (**DQN**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up `Chapter_5_4.py` and follow the next steps to see how we convert our
    solver to use deep learning:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we look at the imports and some initial starting parameters, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we are going to create a class this time to contain the functionality
    of the DQN agent. The `__init__` function is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Most of the parameters have already been covered, but note a new one called
    `memory`, which is a **deque** collection that holds that last 2,000 steps. This
    allows us to batch train our neural network in a sort of replay mode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we look at how the neural network model is built with the `_build_model`
    function, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This builds a fairly simple model, compared to others we have already seen,
    with three **dense** layers outputting a value for each action. The input into
    this network is the state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Jump down to the bottom of the file and look at the training iteration loop,
    shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In this sample, our training takes place in a real-time `render` loop. The
    important sections of the code are highlighted, showing the reshaping of the state
    and calling the `agent.remember` function. The `agent.replay` function at the
    end is where the network trains. The `remember` function is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This function just stores the `state`, `action`, `reward`, `next_state`,  and
    `done` parameters for the replay training. Scroll down more to the `replay` function,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `replay` function is where the network training occurs. We first define
    a `minibatch`, which is defined from a random sampling of previous experiences
    grouped by `batch_size`. Then, we loop through the batches setting `reward` to
    the `target` and if not `done` calculating a new target based on the model prediction
    on the `next_state`. After that, we use the `model.predict` function on the `state`
    to determine the final target. Finally, we use the `model.fit` function to backpropagate
    the trained target back into the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As this section is important, let's reiterate. Note the line where the variable
    `target` is calculated and set. These lines of code may look familiar, as they
    match the Q value equation we saw earlier. This `target` value is the value that
    should be predicted for the current action. This is the value that is backpropagated
    back for the current action and set by the returned `reward`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the sample and watch the agent train to balance the pole on the cart. The
    following shows the environment as it is being trained:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/48db2129-1bae-40e8-8dd2-1c61deb1dbc1.png)'
  prefs: []
  type: TYPE_IMG
- en: CartPole OpenAI Gym environment
  prefs: []
  type: TYPE_NORMAL
- en: The example environment uses the typical first environment, CartPole, we use
    to learn to build our first DRL model. In the next section, we will look at how
    to use the DQNAgent in other scenarios and other models supplied through the Keras-RL
    API.
  prefs: []
  type: TYPE_NORMAL
- en: RL experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning is quickly advancing, and the DQN model we just looked
    at has quickly become outpaced by more advanced algorithms. There are several
    variations and advancements in RL algorithms that could fill several chapters,
    but most of that material would be considered academic. As such, we will instead
    look at some more practical examples of the various RL models the Keras RL API
    provides.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first simple example we can work with is changing our previous example
    to work with a new `gym` environment. Open up `Chapter_5_5.py` and follow the
    next exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Change the environment name in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we are going to use the `MountainCar` environment, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e4ed1377-b31b-4cc7-adf3-e86c57db2247.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of MountainCar environment
  prefs: []
  type: TYPE_NORMAL
- en: Run the code as you normally would and see how the DQNAgent solves the hill-climbing
    problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can see how quickly we were able to switch environments and test the DQNAgent
    in another environment. In the next section, we look at training Atari games with
    the various RL algorithms that the Keras-RL API provides.
  prefs: []
  type: TYPE_NORMAL
- en: Keras RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Keras provides a very useful RL API that wraps several variations such as DQN,
    DDQN, SARSA, and so on. We won''t get into the details of those various RL variations
    right now, but we will cover the important parts later, as we get into more complex
    models. For now, though, we are going to look at how you can quickly build a DRL
    model to play Atari games. Open up `Chapter_5_6.py` and follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first need to install several dependencies with `pip`; open a command shell
    or Anaconda window, and enter the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This will install the Keras RL API, `Pillow`, an image framework, and the Atari
    environment for `gym`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the example code as you normally would. This sample does take script arguments,
    but we don''t need to use them here. An example of the rendered Atari Breakout
    environment follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3f3e5a5c-4ad3-419f-8f5b-4a57934c63c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Atari Breakout environment
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, you cannot see the game run as the agent plays, because all
    the action takes place in the background, but let the agent run until it completes
    and saves the model. Here''s how we would run the sample:'
  prefs: []
  type: TYPE_NORMAL
- en: You can rerun the sample using `--mode test` as an argument to let the agent
    run over 10 episodes and see the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As the sample runs, look through the code and pay special attention to the
    model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Note how our model is using `Convolution`, with pooling. This is because this
    example reads each screen/frame of the game as input (state) and responds accordingly.
    In this case, the model state is massive, and this demonstrates the real power
    of DRL. In this case, we are still training to a state model, but in future chapters,
    we will look at training a policy, rather than a model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This was a simple introduction to RL, and we have omitted several details that
    can get lost on newcomers. As we plan to cover several more chapters on RL, and
    in particular the **Proximal Policy Optimization** (**PPO**)in more detail in [Chapter
    8](1393797c-79cd-46c3-8e43-a09a7750fc92.xhtml), *Understanding PPO*, don't fret
    too much about differences such as policy and model-based RL.
  prefs: []
  type: TYPE_NORMAL
- en: There is an excellent example of this same DQN in TensorFlow at this GitHub
    link: [https://github.com/floodsung/DQN-Atari-Tensorflow](https://github.com/floodsung/DQN-Atari-Tensorflow).
    The code may be a bit dated, but it is a simple and excellent example that is
    worth taking a look at.
  prefs: []
  type: TYPE_NORMAL
- en: We won't look any further at the code, but the reader is certainly invited to.
    Now let's try some exercises.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As always, use the exercises in this section to get a better understanding
    of the material you learn. Try to work through at least two or three exercises
    in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: Return to the example `Chapter_5_1.py` and change the **alpha** (`learning_rate`)
    variable and see what effect this has on the values calculated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Return to the example `Chapter_5_2.py` and alter the arm positions on the various
    bandits.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the learning rate on the example `Chapter_5_2.py` and see what effect
    this has on the Q results output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alter the gamma reward discount factor in the `Chapter_5_3.py` example, and
    see what effect this has on agent training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the exploration epsilon in the `Chapter_5_3.py` to different values and
    rerun the sample. See what effect altering the various exploration parameters
    has on training the agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alter the various parameters (**exploration**, **alpha**, and **gamma**) in
    the `Chapter_5_4.py` example and see what effect this has on training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alter the size of the memory in the `Chapter_5_4.py` example, either higher
    or lower, and see what effect this has on training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try to use different Gym environments in the DQNAgent example from `Chapter_5_5.py`.
    You can do a quick Google search to see the other possible environments you can
    choose from.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `Chapter_5_6.py` example currently uses a form-exploration policy called
    `LinearAnnealedPolicy`; change the policy to use the `BoltzmannQPolicy` policy
    as mentioned in the code comments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Be sure to download and run other Keras-RL examples from [https://github.com/keras-rl/keras-rl](https://github.com/keras-rl/keras-rl).
    Again, you may have to install other Gym environments to get them working.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are plenty of other examples, videos, and other materials to study with
    respect to RL. Learn as much as you can, as this material is extensive and complex
    and not something you will pick up overnight.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RL is the machine learning technology currently dominating the interest of many
    researchers. It is typically appealing to us, because it fits well with games
    and simulations. In this chapter, we covered some of the foundations of RL by
    starting with the fundamental introductory problems of the multi-armed and contextual
    bandits. Then, we quickly looked at installing the OpenAI Gym RL toolkit. We then
    looked at Q-learning and how to implement that in code and train it on an OpenAI
    Gym environment. Finally, we looked at how we could conduct various other experiments
    with Gym by loading a couple of other environments, including the Atari games
    simulator.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we look at the quickly evolving a cutting-edge RL platform
    that Unity is currently developing.
  prefs: []
  type: TYPE_NORMAL
