- en: Preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From here, all our chapters will mostly contain code. I want to remind all my
    readers to run and develop the code at their end. Let's start the coding ninja
    journey.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be learning how to do preprocessing according to the
    different NLP applications. We will learn the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Handling corpus-raw text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling corpus-raw sentences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic preprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practical and customized preprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling corpus-raw text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will see how to get the raw text and, in the following section,
    we will preprocess text and identify the sentences.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process for this section is given in *Figure 4.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85eda8bb-a6e5-4c06-9569-9b370ab22d78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: Process of handling corpus-raw text'
  prefs: []
  type: TYPE_NORMAL
- en: Getting raw text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will use three sources where we can get the raw text data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the data sources:'
  prefs: []
  type: TYPE_NORMAL
- en: Raw text file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define raw data text inside a script in the form of a local variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use any of the available corpus from `nltk`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s begin:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Raw text file access: I have a `.txt` file saved on my local computer which
    contains text data in the form of a paragraph. I want to read the content of that
    file and then load the content as the next step. I will run a sentence tokenizer
    to get the sentences out of it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Define raw data text inside a script in the form of a local variable: If we
    have a small amount of data, then we can assign the data to a local string variable.
    For example: **Text = This is the sentence, this is another example**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use an available corpus from `nltk`: We can import an available corpus such
    as the `brown` corpus, `gutenberg` corpus, and so on from `nltk` and load the
    content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I have defined three functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`fileread()`: This reads the content of a file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`localtextvalue()`: This loads locally defined text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`readcorpus()`: This reads the `gutenberg` corpus content'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to the code snippet given in *Figure 4.2*, which describes all the three
    cases previously defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa9692ee-16ab-4ad2-be27-3a636801b560.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Various ways to get the raw data'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code by clicking on the GitHub link: [https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_1_processrawtext.py](https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_1_processrawtext.py)'
  prefs: []
  type: TYPE_NORMAL
- en: Lowercase conversion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Converting all your data to lowercase helps in the process of preprocessing
    and in later stages in the NLP application, when you are doing parsing.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, converting the text to its lowercase format is quite easy. You can find
    the code on this GitHub link: [https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_4_wordtokenization.py](https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_4_wordtokenization.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code snippet in *Figure 4.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ab65a96-ba98-434d-925a-2309c62f409f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Converting data to lowercase'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the preceding code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Sentence tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In raw text data, data is in paragraph form. Now, if you want the sentences
    from the paragraph, then you need to tokenize at sentence level.
  prefs: []
  type: TYPE_NORMAL
- en: Sentence tokenization is the process of identifying the boundary of the sentences.
    It is also called **sentence boundary detection** or **sentence segmentation**
    or **sentence boundary disambiguation**. This process identifies the sentences
    starting and ending points.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the specialized cases need a customized rule for the sentence tokenizer
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following open source tools are available for performing sentence tokenization:'
  prefs: []
  type: TYPE_NORMAL
- en: OpenNLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stanford CoreNLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GATE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: nltk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here we are using the `nltk` sentence tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are using `sent_tokenize` from `nltk` and will import it as `st`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sent_tokenize(rawtext)`: This takes a raw data string as an argument'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`st(filecontentdetails)`: This is our customized raw data, which is provided
    as an input argument'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can find the code on this GitHub Link: [https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_1_processrawtext.py](https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_1_processrawtext.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the code in the following code snippet in *Figure 4.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b94d00c-8a6e-480b-bebf-722e48dc8940.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Code snippet for nltk sentence tokenizer'
  prefs: []
  type: TYPE_NORMAL
- en: Challenges of sentence tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At first glance, you would ask, what's the big deal about finding out the sentence
    boundary from the given raw text?
  prefs: []
  type: TYPE_NORMAL
- en: Sentence tokenization varies from language to language.
  prefs: []
  type: TYPE_NORMAL
- en: 'Things get complicated when you have the following scenarios to handle. We
    are using examples to explain the cases:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If there is small letter after a dot, then the sentence should not split after
    the dot. The following is an example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sentence: He has completed his Ph.D. degree. He is happy.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In the preceding example, the sentence tokenizer should split the sentence after
    **degree**, not after **Ph.D.**
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If there is a small letter after the dot, then the sentence should be split
    after the dot. This is a common mistake. Let''s take an example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sentence: This is an apple.an apple is good for health.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In the preceding example, the sentence tokenizer should split the sentence after
    **apple**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If there is an initial name in the sentence, then the sentence should not split
    after the initials:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sentence: Harry Potter was written by J.K. Rowling. It is an entertaining one.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In the preceding example, the sentence should not split after **J.** It should
    ideally split after **Rowling**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grammarly Inc., the grammar correction software, customized a rule for the
    identification of sentences and achieves high accuracy for sentence boundary detection.
    See the blog link:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://tech.grammarly.com/blog/posts/How-to-Split-Sentences.html](https://tech.grammarly.com/blog/posts/How-to-Split-Sentences.html).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To overcome the previous challenges, you can take the following approaches,
    but the accuracy of each approach depends on the implementation. The approaches
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can develop a rule-based system to increase the performance of the sentence
    tokenizer:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the previous approach, you can use **name entity recognition** (**NER**)
    tools, POS taggers, or parsers, and then analyze the output of the described tools,
    as well as the sentence tokenizer output and rectify where the sentence tokenizer
    went wrong. With the help of NER tools, POS taggers, and parsers, can you fix
    the wrong output of the sentence tokenizer. In this case, write a rule, then code
    it, and check whether the output is as expected.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Test your code! You need to check for exceptional cases. Does your code perform
    well? If yes, great! And, if not, change it a bit:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can improve the sentence tokenizer by using **machine learning** (**ML**)
    or deep learning techniques:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have enough data that is annotated by a human, then you can train the
    model using an annotated dataset. Based on that trained model, we can generate
    a new prediction from where the sentence boundary should end.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In this method, you need to check how the model will perform.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Stemming for raw text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in [Chapter 3](f65e61fc-1d20-434f-b606-f36cf401fc41.xhtml), *Understanding
    Structure of Sentences*, stemming is the process of converting each word of the
    sentence to its root form by deleting or replacing suffixes.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will apply the `Stemmer` concept on the raw text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have code where we are using the `PorterStemmer` available in `nltk`.
    Refer to *Figure 4.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c3b44ed-01b0-4319-a4d1-9548c1221033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: PorterStemmer code for raw text'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the preceding code is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'When you compare the preceding output with the original text, then we can see
    the following changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If you want to see the difference, then you can refer to the highlighted words
    to see the difference.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges of stemming for raw text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Initially, stemming tools were made for the English language. The accuracy of
    stemming tools for the English language is high, but for languages such as Urdu
    and Hebrew, stemming tools do not perform well. So, to develop stemming tools
    for other languages is quite challenging. It is still an open research area.
  prefs: []
  type: TYPE_NORMAL
- en: Lemmatization of raw text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lemmatization is the process that identifies the correct intended **part-of-speech**
    (**POS**) and the meaning of words that are present in sentences.
  prefs: []
  type: TYPE_NORMAL
- en: In lemmatization, we remove the inflection endings and convert the word into
    its base form, present in a dictionary or in the vocabulary. If we use vocabulary
    and morphological analysis of all the words present in the raw text properly,
    then we can get high accuracy for lemmatization.
  prefs: []
  type: TYPE_NORMAL
- en: Lemmatization transforms words present in the raw text to its lemma by using
    a tagged dictionary such as WordNet.
  prefs: []
  type: TYPE_NORMAL
- en: Lemmatization is closely related to stemming.
  prefs: []
  type: TYPE_NORMAL
- en: In lemmatization, we consider POS tags, and in stemming we do not consider POS
    tags and the context of words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take some examples to make the concepts clear. The following are the
    sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentence 1: It is better for you.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a word **better** present in sentence 1\. So, the lemma of word **better**
    is as **good** as a lemma. But stemming is missing as it requires a dictionary
    lookup.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sentence 2: Man is walking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The word **walking** is derived from the base word walk and here, stemming and
    lemmatization are both the same.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sentence 3: We are meeting tomorrow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, to meet is the base form. The word **meeting** is derived from the base
    form. The base form meet can be a noun or it can be a verb. So it depends on the
    context it will use. So, lemmatization attempts to select the right lemma based
    on their POS tags.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to the code snippet in *Figure 4.6* for the lemmatization of raw text:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/e48d0681-646c-4cf2-b4de-42c921b1f663.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: Stemming and lemmatization of raw text'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the preceding code is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The given input is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In lemmatization, we use different POS tags. The abbreviation description is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`v` stands for verbs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n` stands for nouns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`a` stands for adjectives'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`s` stands for satellite adjectives'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r` stands for adverbs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can see that, inside the `lemmatizer()`function, I have used all the described
    POS tags.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the code from the GitHub link at: [https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_2_rawtext_Stemmers.py](https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_2_rawtext_Stemmers.py).'
  prefs: []
  type: TYPE_NORMAL
- en: Challenges of lemmatization of raw text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lemmatization uses a tagged dictionary such as WordNet. Mostly, it's a human-tagged
    dictionary. So human efforts and the time it takes to make WordNet for different
    languages is challenging.
  prefs: []
  type: TYPE_NORMAL
- en: Stop word removal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stop word removal is an important preprocessing step for some NLP applications,
    such as sentiment analysis, text summarization, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Removing stop words, as well as removing commonly occurring words, is a basic
    but important step. The following is a list of stop words which are going to be
    removed. This list has been generated from `nltk`. Refer to the following code
    snippet in Figure 4.7:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1982b360-e918-4f5b-b9cd-f476c91600cd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: Code to see the list of stop words for the English language'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the preceding code is a list of stop words available in `nltk`,
    refer to *Figure 4.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6fba5fc2-d654-4d6a-b4de-e384e8b48194.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: Output of nltk stop words list for the English language'
  prefs: []
  type: TYPE_NORMAL
- en: The `nltk` has a readily available list of stop words for the English language.
    You can also customize which words you want to remove according to the NLP application
    that you are developing.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the code snippet for removing customized stop words in *Figure
    4.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/deeb5441-8493-4495-a956-3c8034eba7c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.9: Removing customized stop words'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the code given in *Figure 4.9* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The code snippet in *Figure 4.10* performs actual stop word removal from raw
    text and this raw text is in the English language:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e99189e3-380d-4a09-9297-c824f24be3ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.10: Stop words removal from raw text'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the preceding code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Exercise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Take a file which is placed in the data folder with the name `rawtextcorpus.txt`,
    open the file in read mode, load the content, and then remove the stop words by
    using the nltk stop word list. Please analyze the output to get a better idea
    of how things are working out.
  prefs: []
  type: TYPE_NORMAL
- en: Up until this section, we have analyzed raw text. In the next section, we will
    do preprocessing on sentence levels and word levels.
  prefs: []
  type: TYPE_NORMAL
- en: Handling corpus-raw sentences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we were processing on raw text and looked at concepts
    at the sentence level. In this section, we are going to look at the concepts of
    tokenization, lemmatization, and so on at the word level.
  prefs: []
  type: TYPE_NORMAL
- en: Word tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word tokenization is defined as the process of chopping a stream of text up
    into words, phrases, and meaningful strings. This process is called **word tokenization**.
    The output of the process are words that we will get as an output after tokenization.
    This is called a **token**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the code snippet given in *Figure 4.11* of tokenized words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6b59dcf-3062-40f4-8d65-d0203b591ff3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.11: Word tokenized code snippet'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the code given in *Figure 4.11* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The input for word tokenization is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for word tokenization is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Challenges for word tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you analyze the preceding output, then you can observe that the word `don't`
    is tokenized as `do, n't know`. Tokenizing these kinds of words is pretty painful
    using the `word_tokenize` of `nltk`.
  prefs: []
  type: TYPE_NORMAL
- en: To solve the preceding problem, you can write exception codes and improvise
    the accuracy. You need to write pattern matching rules, which solve the defined
    challenge, but are so customized and vary from application to application.
  prefs: []
  type: TYPE_NORMAL
- en: Another challenge involves some languages such as Urdu, Hebrew, Arabic, and
    so on. They are quite difficult in terms of deciding on the word boundary and
    find out meaningful tokens from the sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Word lemmatization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word lemmatization is the same concept that we defined in the first section.
    We will just do a quick revision of it and then we will implement lemmatization
    on the word level.
  prefs: []
  type: TYPE_NORMAL
- en: Word lemmatization is converting a word from its inflected form to its base
    form. In word lemmatization, we consider the POS tags and, according to the POS
    tags, we can derive the base form which is available to the lexical WordNet.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code snippet in *Figure 4.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6822ae40-ec0e-4d7b-bb99-5a1ceedd8318.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.12: Word lemmatization code snippet'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the word lemmatization is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Challenges for word lemmatization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is time consuming to build a lexical dictionary. If you want to build a lemmatization
    tool that can consider a larger context, taking into account the context of preceding
    sentences, it is still an open area in research.
  prefs: []
  type: TYPE_NORMAL
- en: Basic preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In basic preprocessing, we include things that are simple and easy to code but
    seek our attention when we are doing preprocessing for NLP applications.
  prefs: []
  type: TYPE_NORMAL
- en: Regular expressions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will begin some of the interesting concepts of preprocessing, which are
    the most useful. We will look at some of the advanced levels of regular expression.
  prefs: []
  type: TYPE_NORMAL
- en: For those who are new to regular expression, I want to explain the basic concept
    of **regular expression** (**regex**).
  prefs: []
  type: TYPE_NORMAL
- en: Regular expression is helpful to find or find-replace specific patterns from
    a sequence of characters. There is particular syntax which you need to follow
    when you are making regex.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many online tools available which can give you the facility to develop
    and test your regex. One of my favorite online regex development tool links is
    given here: [https://regex101.com/](https://regex101.com/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also refer to the Python regex library documentation at: [https://docs.python.org/2/library/re.html](https://docs.python.org/2/library/re.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Basic level regular expression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regex is a powerful tool when you want to do customized preprocessing or when
    you have noisy data with you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, I''m presenting some of the basic syntax and then we will see the actual
    implementation on Python. In Python, the `re` library is available and by using
    this library we can implement regex. You can find the code on this GitHub link:
    [https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_5_regualrexpression.py](https://github.com/jalajthanaki/NLPython/blob/master/ch4/4_5_regualrexpression.py)'
  prefs: []
  type: TYPE_NORMAL
- en: Basic flags
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The basic flags are `I`, `L`, `M`, `S`, `U`, `X`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`re.I`: This flag is used for ignoring casing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`re.M`: This flag is useful if you want to find patterns throughout multiple
    lines'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`re.L`: This flag is used to find a local dependent'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`re.S`: This flag is used to find dot matches'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`re.U`: This flag is used to work for unicode data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`re.X`: This flag is used for writing regex in a more readable format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have mainly used `re.I`, `re.M`, `re.L`, and `re.U` flags.
  prefs: []
  type: TYPE_NORMAL
- en: We are using the `re.match()` and `re.search()` functions. Both are used to
    find the patterns and then you can process them according to the requirements
    of your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the differences between `re.match()` and `re.search()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`re.match()`: This checks for a match of the string only at the beginning of
    the string. So, if it finds the pattern at the beginning of the input string then
    it returns the matched pattern, otherwise; it returns a noun.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`re.search()`: This checks for a match of the string anywhere in the string.
    It finds all the occurrences of the pattern in the given input string or data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to the code snippet given in *Figure 4.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f086497-45e5-4f5c-8820-635c14dfa4e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.13: Code snippet to see the difference between re.match() versus re.search()'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the code snippet of *Figure 4.13* is given in *Figure 4.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40421689-c806-48dc-89a5-8d7fc8d6814c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.14: Output of the re.match() versus re.search()'
  prefs: []
  type: TYPE_NORMAL
- en: 'The syntax is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Find the single occurrence of character `a` and `b`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Find characters except `a` and `b`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Find the character range of `a` to `z`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Find range except to `z`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Find all the characters `a` to `z` as well as `A` to `Z`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Any single character:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Any whitespace character:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Any non-whitespace character:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Any digit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Any non-digit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Any non-words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Any words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Either match `a` or `b`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Occurrence of `a` is either zero or one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Occurrence of `a` is zero time or more than that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Occurrence of `a` is one time or more than that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Exactly match three occurrences of `a`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Match simultaneous occurrences of `a` with `3` or more than `3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Match simultaneous occurrences of `a` between `3` to `6`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Starting of the string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Ending of the string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Match word boundary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Non-word boundary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The basic code snippet is given in *Figure 4.15*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e4e39a15-d25a-497a-b904-8a9ccc4da46c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.15: Basic regex functions code snippet'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the code snippet of *Figure 4.15* is given in *Figure 4.16*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a83361e-378b-4931-b811-311f6be3cce7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.16: Output of the basic regex function code snippet'
  prefs: []
  type: TYPE_NORMAL
- en: Advanced level regular expression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are advanced concepts of regex which will be very useful.
  prefs: []
  type: TYPE_NORMAL
- en: The **lookahead** and **lookbehind** are used to find out substring patterns
    from your data. Let's begin. We will understand the concepts in the basic language.
    Then we will look at the implementation of them.
  prefs: []
  type: TYPE_NORMAL
- en: Positive lookahead
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Positive lookahead matches the substring from a string if the defined pattern
    is followed by the substring. If you don''t understand, then let''s look at the
    following example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a sentence: I play on the playground.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, you want to extract *play* as a pattern but only if it follows *ground*.
    In this situation, you can use positive lookahead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The syntax of positive lookahead is `(?=pattern)`
  prefs: []
  type: TYPE_NORMAL
- en: The regex `rplay(?=ground)` matches *play*, but only if it is followed by *ground*.
    Thus, the first *play* in the text string won't be matched.
  prefs: []
  type: TYPE_NORMAL
- en: Positive lookbehind
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Positive lookbehind matches the substring from a string if the defined pattern
    is preceded by the substring. Refer to the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the sentence: I play on the playground. It is the best ground.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now you want to extract *ground*, if it is preceded by the string *play*. In
    this case, you can use positive lookbehind.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The syntax of positive lookbehind is `(?<=pattern)`
  prefs: []
  type: TYPE_NORMAL
- en: The regex `r(?<=play)ground` matches *ground*, but only if it is preceded by
    *play*.
  prefs: []
  type: TYPE_NORMAL
- en: Negative lookahead
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Negative lookahead matches the string which is definitely not followed by the
    pattern which we have defined in the regex pattern part.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s give an example to understand the negative lookahead:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the sentence: I play on the playground. It is the best ground.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now you want to extract *play* only if it is not followed by the string *ground*.
    In this case, you can use negative lookahead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The syntax of negative lookahead is `(?!pattern)`
  prefs: []
  type: TYPE_NORMAL
- en: The regex `r play(?!ground)` matches *play*, but only if it is not followed
    by *ground*. Thus the *play* just before *on* is matched.
  prefs: []
  type: TYPE_NORMAL
- en: Negative lookbehind
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Negative lookbehind matches the string which is definitely not preceded by the
    pattern which we have defined in the regex pattern part.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see an example to understand the negative lookbehind:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the sentence: I play on the playground. It is the best ground.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now you want to extract *ground* only if it is not preceded by the string *play*.
    In this case, you can use negative lookbehind.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The syntax of negative lookbehind is `(?<!pattern)`
  prefs: []
  type: TYPE_NORMAL
- en: The regex `r(?<!play)ground` matches *ground*, but only if it is not preceded
    by *play*.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the code snippet which is an implementation of `advanceregex()`
    in *Figure 4.17*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/74f4c83a-883b-4b81-89ca-d529aa3f8f23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.17: Advanced regex code snippet'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the code snippet of *Figure 4.17* is given in *Figure 4.18*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6efff615-cc21-4535-9207-b0fd8f01e31d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.18: Output of code snippet of advanced regex'
  prefs: []
  type: TYPE_NORMAL
- en: Practical and customized preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we start preprocessing for NLP applications, sometimes you need to do some
    customization according to your NLP application. At that time, it might be possible
    that you need to think about some of the points which I have described as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Decide by yourself
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section is a discussion of how to approach preprocessing when you don't
    know what kind of preprocessing is required for developing an NLP application.
    In this kind of situation, what you can do is simply ask the following questions
    to yourself and make a decision.
  prefs: []
  type: TYPE_NORMAL
- en: What is your NLP application and what kind of data do you need to build the
    NLP application?
  prefs: []
  type: TYPE_NORMAL
- en: Once you have understood the problem statement, as well as having clarity on
    what your output should be, then you are in a good situation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you know about the problem statement and the expected output, now think
    what all the data points are that you need from your raw data set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To understand the previous two points, let's take an example. If you want to
    make a text summarization application, suppose you are using a news articles that
    are on the web, which you want to use for building news text summarization application.
    Now, you have built a scraper that scrapes news articles from the web. This raw
    news article dataset may contain HTML tags, long texts, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For news text summarization, how will we do preprocessing? In order to answer
    that, we need to ask ourselves a few questions. So, let's jump to a few questions
    about preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Is preprocessing required?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now you have raw-data for text summarization and your dataset contains HTML
    tags, repeated text, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your raw-data has all the content that I described in the first point, then
    preprocessing is required and, in this case, we need to remove HTML tags and repeated
    sentences; otherwise, preprocessing is not required.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You also need to apply lowercase convention.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After that, you need to apply sentence tokenizer on your text summarization
    dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, you need to apply word tokenizer on your text summarization dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether your dataset needs preprocessing depends on your problem statement and
    what data your raw dataset contains.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can see the flowchart in *Figure 4.19:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36192456-01df-4e30-91bc-3261e57a43e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.19: Basic flowchart for performing preprocessing of text-summarization'
  prefs: []
  type: TYPE_NORMAL
- en: What kind of preprocessing is required?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our example of text summarization, if a raw dataset contains HTML tags,
    long text, repeated text, then during the process of developing your application,
    as well as in your output, you don''t need the following data:'
  prefs: []
  type: TYPE_NORMAL
- en: You don't need HTML tags, so you can remove them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You don't need repeated sentences, so you can remove them as well
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there is long text content then if you can find stop words and high frequency
    small words, you should remove them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding case studies of preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whatever I have explained here regarding customized preprocessing will make
    more sense to you when you have some real life case studies explained.
  prefs: []
  type: TYPE_NORMAL
- en: Grammar correction system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You are making a grammar correction system. Now, think of the sub-task of it.
    You want to build a system which predicts the placement of articles a, an, and
    the in a particular sentence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For this kind of system, if you are thinking I need to remove stop words every
    time, then, OOPs, you are wrong because this time we really can't remove all the
    stop words blindly. In the end, we need to predict the articles a, an, and the.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can remove words which are not meaningful at all, such as when your dataset
    contains math symbols, then you can remove them. But this time, you need to do
    a detailed analysis as to whether you can remove the small length words, such
    as abbreviations, because your system also needs to predict which abbreviations
    don't take an article and which do.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's look at a system where you can apply all the preprocessing techniques
    that we have described here. Let's follow the points inside sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sentiment analysis is all about evaluating the reviews of your customers and
    categorizing them into positive, negative, and neutral categories:'
  prefs: []
  type: TYPE_NORMAL
- en: For this kind of system, your dataset contains user reviews so user writing
    generally contains casual language.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data contains informal language so we need to remove stop words such as
    Hi, Hey, Hello, and so on. We do not use Hi, Hey, How are u? to conclude whether
    the user review is positive, negative, or neutral.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from that, you can remove the repeated reviews.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also preprocess data by using word tokenization and lemmatization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine translation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Machine translation is also one of the widely used NLP applications. In machine
    translation, our goal is to translate one language to another language in a logical
    manner. So, if we want to translate the English language to the German language,
    then you may the apply the following preprocessing steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We can apply convert to the whole dataset to be converted into lowercase.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply sentence splitter on the dataset so you can get the boundary for each
    of the sentences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, suppose you have corpus where all English sentences are in `English_Sentence_File`
    and all German sentence are in `German_Sentence_File`. Now, you know for each
    English sentence there is a corresponding German sentence present in `German_Sentence_File`.
    This kind of corpus is called **parallel** corpus. So in this case, you also need
    to check that all sentences in both files are aligned appropriately.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can also apply stemming for each of the words of the sentences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spelling correction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spelling correction can be a very useful tool for preprocessing as well, as
    it helps to improve your NLP application.
  prefs: []
  type: TYPE_NORMAL
- en: Approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of spelling correction came from the concept of how much similarity
    is contained by two strings. This concept is used to compare two strings. The
    same concept has been used everywhere nowadays. We will consider some examples
    to better understand how this concept of checking the similarity of two strings
    can be helpful to us.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you search on Google, if you make a typing mistake in your search query,
    then you get a suggestion on the browser, Did you mean: with your corrected query
    with the right spelling. This mechanism rectifies your spelling mistake and Google
    has its own way of providing almost perfect results every time. Google does not
    just do a spelling correction, but is also indexes on your submitted query and
    displays the best result for you. So, the concept behind the spelling correction
    is the similarity between two strings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Take another example: If you are developing a machine translation system, then
    when you see the string translated by the machine, your next step is probably
    to validate your output. So now you will compare the output of the machine with
    a human translator and situation, which may not be perfectly similar to the output
    of the machine.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the machine translated string is: **She said could she help me?**, the human
    string translated would say: **She asked could she help me?** When you are checking
    the similarity between a system string and a human string, you may find that *said*
    is replace by asked.'
  prefs: []
  type: TYPE_NORMAL
- en: So, this concept of the similarity of two strings can be used in many applications,
    including speech recognition, NLP applications, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: There are three major operations when we are talking about measuring the similarity
    of two strings. The operations are insertion, deletion, and substitution. These
    operations are used for the implementation of the spelling correction operation.
    Right now, to avoid complexity, we are not considering transpose and long string
    editing operations.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with the operations and then we will look at the algorithm specifically
    for the spelling correction.
  prefs: []
  type: TYPE_NORMAL
- en: '**Insertion operation**'
  prefs: []
  type: TYPE_NORMAL
- en: If you have an incorrect string, now after inserting one or more characters,
    you will get the correct string or expected string.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see an example.
  prefs: []
  type: TYPE_NORMAL
- en: If I have entered a string `aple`,then after inserting `p` we will get `apple`,
    which is right. If you have entered a string `staemnt` then after inserting `t`
    and `e` you will get `statement`, which is right.
  prefs: []
  type: TYPE_NORMAL
- en: '**Deletion operation**'
  prefs: []
  type: TYPE_NORMAL
- en: You may have an incorrect string which can be converted into a correct string
    after deleting one or more characters of the string.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If I have entered `caroot`, then to get the correct string we need to delete
    one `o`. After that, we will get the correct string `carrot`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Substitution operation**'
  prefs: []
  type: TYPE_NORMAL
- en: If you get the correct string by substituting one or more characters, then it
    is called a **substitution operation**.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you have a string `implemantation`. To make it correct, you need to
    substitute the first `a` to `e` and you will get the correct string `implementation`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithms for spelling corrections**'
  prefs: []
  type: TYPE_NORMAL
- en: We are using the minimum edit distance algorithm to understand spelling corrections.
  prefs: []
  type: TYPE_NORMAL
- en: '**Minimum edit distance**'
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm is for converting one string `X` into another string `Y` and
    we need to find out what the minimum edit cost is to convert string `X` to string
    `Y`. So, here you can either do insertion, deletion, or substitution operations
    to convert string `X` to `Y` with the minimum possible sequences of the character
    edits.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you have a string `X` with a length of `n`,and string `Y` with a length
    of `m`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow the steps of the algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Set `n` to a length of P.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set `m` to a length of Q.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If `n = 0`, return `m` and exit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If `m = 0`, return `n` and exit.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Create a matrix containing 0..*m* rows and 0..*n* columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the first row to 0..*n*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize first column to 0..*m*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Iterate each character of P (`i` from 1 to *n*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate each character of Q (`j` from 1 to *m*).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If P[i] equals Q[j], the cost is 0.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If Q[i] doesn't equal Q[j], the cost is 1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Set the value at cell `v[i,j]` of the matrix equal to the minimum of all three
    of the following  points:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The cell immediately previous plus 1: `v[i-1,j] + 1`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The cell immediately to the left plus 1: `v[i,j-1] + 1`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The cell diagonally previous and to the left plus the cost: `v[i-1,j-1] +1`
    for minimum edit distance. If you are using the Levenshtein distance then `v[i-1,j-1]
    +` cost should be considered'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the iteration in *step 7* to *step 9* has been completed, the distance
    is found in cell `v[n,m]`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The previous steps are the basic algorithm to develop the logic of spelling
    corrections but we can use probability distribution of words and take a consideration
    of that as well. This kind of algorithmic approach is based on dynamic programing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s convert the string `tutour` to `tutor` by understanding that we need
    to delete `u`. The edit distance is therefore 1\. The table which is developed
    by using the defined algorithm is shown in *Figure 4.20* for computing the minimum
    edit distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3044f3f6-aa26-4af7-857e-846d55589d53.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.20: Computing minimum edit distance'
  prefs: []
  type: TYPE_NORMAL
- en: '**Implementation**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, for the spelling correction, we need to add a dictionary or extract the
    words from the large documents. So, in the implementation, we have used a big
    document from where we have extracted words. Apart from that, we have used the
    probability of occurring words in the document to get an idea about the distribution.
    You can see more details regarding the implementation part by clicking on this
    link: [http://norvig.com/spell-correct.html](http://norvig.com/spell-correct.html)'
  prefs: []
  type: TYPE_NORMAL
- en: We have implemented the spelling correction for the minimum edit distance 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'See the implementation of the spelling correction in *Figure 4.21*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aa9bb806-49ed-4658-8f9e-8acb601484be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.21: Implementation of spelling correction'
  prefs: []
  type: TYPE_NORMAL
- en: See the output of the spelling correction in *Figure 4.22*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are providing the string `aple`, which is converted to `apple` successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d23d7f2-847a-4f5d-a2f6-def409aaface.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.22: Output of spelling correction'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have looked at all kinds of preprocessing techniques which
    will be useful to you when you are developing an NLP system or an NLP application.
    We have also touched upon a spelling correction system which you can consider
    as part of the preprocessing technique because it will be useful for many of the
    NLP applications that you develop in the future. By the way, you can access the
    code on GitHub by clicking the following link: [https://github.com/jalajthanaki/NLPython/tree/master/ch4](https://github.com/jalajthanaki/NLPython/tree/master/ch4)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will look at the most important part for any NLP system:
    feature engineering. The performance of an NLP system mainly depends on what kind
    of data we provide to the NLP system. Feature engineering is an art and skill
    which you are going to adopt from the next chapter onwards and, trust me, it is
    the most important ingredient in developing the NLP systems, so read it and definitely
    implement it to enrich your skills.'
  prefs: []
  type: TYPE_NORMAL
