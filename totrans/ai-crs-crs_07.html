<html><head></head><body>
		<div id="_idContainer224">
			<h1 class="chapterNumber sigil_not_in_toc">7</h1>
			<h1 class="chapterTitle" xml:lang="en-GB" id="sigil_toc_id_91" lang="en-GB"><a id="_idTextAnchor131"/>Welcome to Q-Learning</h1>
</div>

			<p class="normal">Ladies and gentlemen, things are about to get even 
more interesting than before. The next model we are about to tackle is 
at the heart of many AIs built today; robots, autonomous vehicles, and 
even AI players of video games. They all use Q-learning at the core of 
their model. Some of them even combine Q-learning with deep learning, 
making a highly advanced version of Q-learning called deep Q-learning, 
which we will cover in <em class="italics">Chapter 9</em>, <em class="italics">Going Pro with Artificial Brains – Deep Q-Learning</em>.</p>
			<p class="normal">All of the AI fundamentals still apply<a id="_idIndexMarker170"/> to Q-learning, as follows:</p>
			<ol>
				<li class="list">Q-learning is a Reinforcement Learning model.</li>
				<li class="list">Q-learning works on the inputs (states) and outputs (actions) principle.</li>
				<li class="list">Q-learning works on a predefined environment, including the states (the inputs), the actions (the outputs), and the rewards.</li>
				<li class="list">Q-learning is modeled by a Markov decision process.</li>
				<li class="list">Q-learning uses a training mode, during which the parameters that are learned are called the Q-values, and an inference mode.</li>
			</ol>
			<p class="normal">Now we can add two more fundamentals, this time specific to Q-learning:</p>
			<ol>
				<li class="list" value="1">There are a finite number of states (there is not an infinity of possible inputs).</li>
				<li class="list">There are a finite number of actions (only a certain number of actions can be performed).</li>
			</ol>
			<p class="normal">That's all! There are no more fundamentals to keep 
in mind; now we can really dig into Q-learning, which you'll see is not 
that hard and really quite intuitive.</p>
			<p class="normal">To explain Q-learning, we'll use an example so that you won't get lost inside pure theory, and so that you can <a id="_idIndexMarker171"/>visualize what's happening. On that note: welcome to the Maze<a id="_idTextAnchor132"/>.</p>
			<h2 class="title" xml:lang="en-GB" id="sigil_toc_id_92" lang="en-GB"><a id="_idTextAnchor133"/>The Maze</h2>
			<p class="normal">You are going to learn how Q-learning <a id="_idIndexMarker172"/>works inside a maze. Let's draw our <a id="_idIndexMarker173"/>maze right away; here it is:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_07_01.png" alt=""/></figure>
			<p class="packt_figref">Figure 1: The Maze</p>
			<p class="normal">I know, it's the simplest maze you have ever seen. 
That's important for the sake of simplicity, so that you can mostly 
focus on how the AI works its magic. Imagine if you got lost in this 
chapter because of the maze and not because of the AI formulas! The 
important thing is that you have a clear maze, and you can visualize how
 the AI might manage to find its way from the beginning to the end.</p>
			<p class="normal">Speaking of the beginning and the end, imagine a little robot inside this maze, starting at point <strong class="bold">E</strong> (Entrance). Its goal is to find the quickest way to point <strong class="bold">G</strong>
 (Goal). We humans can figure that out in no time, but that's only 
because our maze is so simple. What you are going to build is an AI that
 can go from a starting point to an ending point, regardless of how 
complex the maze is. Let's get starte<a id="_idTextAnchor134"/>d!</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_93" lang="en-GB"><a id="_idTextAnchor135"/>Beginnings</h3>
			<p class="normal">Here is a question for you: what do you think is<a id="_idIndexMarker174"/> going to be the very first <a id="_idIndexMarker175"/>step?</p>
			<p class="normal">I'll give you three possible answers:</p>
			<ol>
				<li class="list" value="1">We start writing some math equations.</li>
				<li class="list">We build the environment.</li>
				<li class="list">We try to make it work with Thompson Sampling (the AI model of the previous chapter).</li>
			</ol>
			<p class="normal">The correct answer is…</p>
			<p class="normal">2. We build the environment.</p>
			<p class="normal">That was easy, but I wanted to highlight that in a 
question to make sure you keep in mind that this must always be the 
first step when building an AI. After clearly understanding the problem,
 the first step of building your AI solution is always to set up the 
environment.</p>
			<p class="normal">That begs a further question:</p>
			<p class="normal">What steps, exactly, are you going to take when building that environment?</p>
			<p class="normal">Try to remember the answer—you've already learned this—and then read on for a recap.</p>
			<ol>
				<li class="list" value="1">Firstly, you'll define the states (the inputs of your AI).</li>
				<li class="list">Secondly, you'll define the actions that can be performed (the outputs of your AI).</li>
				<li class="list">Thirdly, you'll define the rewards. Remember, the reward is what the AI gets after performing an action in a certain state.</li>
			</ol>
			<p class="normal">Now we've secured the basics, so you can tackle that first step of defining the environm<a id="_idTextAnchor136"/>ent.</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_94" lang="en-GB"><a id="_idTextAnchor137"/>Building the environment</h3>
			<p class="normal">To build the environment, we need to <a id="_idIndexMarker176"/>define the states, the actions, and the rew<a id="_idTextAnchor138"/>ards.</p>
			<h4 class="title" xml:lang="en-GB" id="sigil_toc_id_95" lang="en-GB"><a id="_idTextAnchor139"/>The states</h4>
			<p class="normal">Let's begin with the states. What do you<a id="_idIndexMarker177"/>
 think are going to be the states for this problem? Remember, the states
 are the inputs of your AI. And they should contain enough information 
for the AI to be able to take an action that will lead it to its final 
goal (reaching point E).</p>
			<p class="normal">In this model, we don't have too<a id="_idIndexMarker178"/>
 much of a choice. The state, at a specific time or specific iteration, 
is simply going to be the position of the AI at that time. In other 
words, it is going to be the letter of the location, from <strong class="bold">A</strong> to <strong class="bold">L</strong>, where the AI is in at a specific time.</p>
			<p class="normal">As you might guess, the next step after building 
the environment will be writing the mathematical equations at the heart 
of the AI, and to help you with that, it makes it much easier to encode 
the states into unique integers instead of keeping them as letters. 
That's exactly what we are going to do, with the following mapping:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_07_02.png" alt=""/></figure>
			<p class="packt_figref">Figure 2: Location to state mapping</p>
			<p class="normal">Notice that we abide by the first specific fundamental of Q-learning, that is: <strong class="bold">there are a finite number of states</strong>.</p>
			<p class="normal">Let's move on to the ac<a id="_idTextAnchor140"/>tions.</p>
			<h4 class="title" xml:lang="en-GB" id="sigil_toc_id_96" lang="en-GB"><a id="_idTextAnchor141"/>The actions</h4>
			<p class="normal">The actions are simply going to<a id="_idIndexMarker179"/> be the next moves the AI can make to go from one location to the next. For example, let's say the AI is in location <strong class="bold">J</strong>; the possible actions that the AI can perform are to go to <strong class="bold">I</strong>, to <strong class="bold">F</strong>, or to <strong class="bold">K</strong>. Again, since you'll be working with math equations, you can encode these actions with the same indexes as for the states.</p>
			<p class="normal">Following the example where the AI is in location <strong class="bold">J</strong> at a specific time, the possible actions that the AI can perform are <strong class="bold">5</strong>, <strong class="bold">8</strong>, and <strong class="bold">10</strong>, according to our previous mapping above: the index <strong class="bold">5</strong> corresponds to <strong class="bold">F</strong>, the index <strong class="bold">8</strong> corresponds to <strong class="bold">I</strong>, and the index <strong class="bold">10</strong> corresponds to <strong class="bold">K</strong>.</p>
			<p class="normal">Hence, the possible actions are simply the indexes of the different locations that can be reached:</p>
			<p class="normal">Possible actions = {0,1,2,3,4,5,6,7,8,9,10,11}</p>
			<p class="normal">Notice that again, we abide by the second specific<a id="_idIndexMarker180"/> fundamental of Q-learning, that is: <strong class="bold">there are a finite number of actions</strong>.</p>
			<p class="normal">Now obviously, when in a specific location, there 
are some actions that the AI cannot perform. Taking the same previous 
example, if the AI is in location <strong class="bold">J</strong>, it can perform the actions <strong class="bold">5</strong>, <strong class="bold">8</strong>, and <strong class="bold">10</strong>,
 but it cannot perform the other actions. You can make sure to specify 
that by attributing a 0 reward to the actions it cannot perform, and a 1
 reward to the actions it can perform. That brings us to the <a id="_idTextAnchor142"/>rewards.</p>
			<h4 class="title" xml:lang="en-GB" id="sigil_toc_id_97" lang="en-GB"><a id="_idTextAnchor143"/>The rewards</h4>
			<p class="normal">You're almost done with <a id="_idIndexMarker181"/>your
 environment—last, but not least, you have to define a system of 
rewards. More specifically, you have to define a reward function <em class="italics">R</em> that takes as input a state <em class="italics">s</em> and an action <em class="italics">a</em>, and returns a numerical reward <em class="italics">r</em> that the AI will get by performing the action <em class="italics">a</em> in the state <em class="italics">s</em>:</p>
			<p style="text-align: center">R: (s, a) <img src="../Images/B14110_07_001.png" alt=""/> <img src="../Images/B14110_07_002.png" alt=""/></p>
			<p class="normal">So, how can you build such a function for our case 
study? Here, it is simple. Since there are a discrete and finite number 
of states (the indexes from 0 to 11), as well as a discrete and finite 
number of actions (same indexes from 0 to 11), the best way to build 
your reward function <em class="italics">R</em> is to simply make a matrix. </p>
			<p class="normal">Your reward function will be a matrix of exactly 12
 rows and 12 columns, where the rows correspond to the states, and the 
columns correspond to the actions. That way, in your function R: (s, a) <img src="../Images/B14110_07_003.png" alt=""/> <img src="../Images/B14110_07_004.png" alt=""/>, <em class="italics">s</em> will be the row index of the matrix, <em class="italics">a</em> will be the column index of the matrix, and <em class="italics">r</em> will be the cell of index (<em class="italics">s</em>, <em class="italics">a</em>) in the matrix.</p>
			<p class="normal">To build this reward matrix, what you first have to
 do is attribute, for each of the 12 locations, a 0 reward to the 
actions that the<a id="_idIndexMarker182"/> robot cannot perform, and
 a 1 reward to the actions the robot can perform. By doing that for each
 of the 12 locations, you will end up with a matrix of rewards. Let's 
build it step by step, starting with the first location: location <strong class="bold">A</strong>.</p>
			<p class="normal">When in location <strong class="bold">A</strong>, the robot can only go to location <strong class="bold">B</strong>. Therefore, since location <strong class="bold">A</strong> has index 0 (first row of the matrix) and location <strong class="bold">B</strong>
 has index 1 (second column of the matrix), the first row of the matrix 
of rewards will get a 1 on the second column, and a 0 on all the other 
columns, like so:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_07_03.png" alt=""/></figure>
			<p class="packt_figref">Figure 3: Rewards matrix – Step 1</p>
			<p class="normal">Let's move on to location <strong class="bold">B</strong>. When in location <strong class="bold">B</strong>, the robot can only go to three different locations: <strong class="bold">A</strong>, <strong class="bold">C</strong>, and <strong class="bold">F</strong>. Since <strong class="bold">B</strong> has index 1 (second row), and <strong class="bold">A</strong>, <strong class="bold">C</strong>, and <strong class="bold">F</strong> have
 respective indexes 0, 2, and 5 (1st, 3rd, and 6th column), then the 
second row of the matrix of rewards will get a 1 on the 1st, 3rd, and 
6th columns, and 0 on all the other columns:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_07_04.png" alt=""/></figure>
			<p class="packt_figref">Figure 4: Rewards matrix – Step 2</p>
			<p class="normal"><strong class="bold">C</strong> (of index 2) is only <a id="_idIndexMarker183"/>connected to <strong class="bold">B</strong> and <strong class="bold">G</strong> (of indexes 1 and 6) so the third row of the matrix of rewards<a id="_idTextAnchor144"/> is:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_07_05.png" alt=""/></figure>
			<p class="packt_figref">Figure 5: Rewards matrix – Step 3</p>
			<p class="normal">By doing the same<a id="_idIndexMarker184"/> for all the other locations, you eventually get your final matrix of rewards:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_07_06.png" alt=""/></figure>
			<p class="packt_figref">Figure 6: Rewards matrix - Step 4</p>
			<p class="normal">And that's how you initialize the matrix of rewards.</p>
			<p class="normal">But wait—you're not actually finished. There is one
 final thing you need to do. It's a step that's crucial to understand. 
In fact, let me ask you another question, the ultimate one, which will 
check if your intuition is already shaping up:</p>
			<p class="normal"><strong class="bold">How can you let the AI know that it has to go to that top priority location G?</strong></p>
			<p class="normal">It's easy—you do it simply by playing with the 
rewards. You must keep in mind that with Reinforcement Learning, 
everything works from the rewards. If you attribute a high reward to 
location <strong class="bold">G</strong>, for example 1000, then the AI 
will automatically try to go and catch that high reward, simply because 
it is larger than the rewards of the other locations.</p>
			<p class="normal">In short, and it's a fundamental point to understand and remember in Reinforcement Learning in general, <strong class="bold">the AI is always looking for the highest reward</strong>. That's why the trick to reach location <strong class="bold">G</strong> is simply to attribute it a higher reward than the other locations.</p>
			<p class="normal">For now, manually put <a id="_idIndexMarker185"/>a high reward (1000) inside the cell corresponding to location <strong class="bold">G</strong>, because it is the goal location where we want our AI to go. Since location <strong class="bold">G</strong> has an index of 6, we put a 1000 reward on the cell of row 6 and column 6. Accordingly, our matrix of rewards becomes:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_07_07.png" alt=""/></figure>
			<p class="packt_figref">Figure 7: Rewards matrix - Step 5</p>
			<p class="normal">You have defined the rewards! You did it by simply 
building this matrix of rewards. It is important to understand that this
 is usually the way we define the system of rewards when doing 
Q-learning. </p>
			<p class="normal">In <em class="italics">Chapter 9</em>, <em class="italics">Going Pro with Artificial Brains – Deep Q-Learning,</em>
 which is about deep Q-learning, you will see that we will proceed very 
differently and build the environment much more easily. In fact, deep 
Q-learning is the advanced version of Q-learning that is widely used 
today in AI, far more than the simple Q-learning model itself. But you 
have to tackle Q-learning first, in depth, in order to be ready for deep
 Q-learning.</p>
			<p class="normal">Since you've defined the states, the actions, and 
the rewards, you have finished building the environment. This means you 
are ready to tackle the next step, where you will build the AI itself 
that will do its magic inside this environment that you've just def<a id="_idTextAnchor145"/>ined.</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_98" lang="en-GB"><a id="_idTextAnchor146"/>Building the AI</h3>
			<p class="normal">Now that you have built an <a id="_idIndexMarker186"/>environment
 in which you clearly defined the goal with a relevant system of 
rewards, it's time to build the AI. I hope you're ready for a little 
math.</p>
			<p class="normal">I'll break down this second step into several 
sub-steps, leading you to the final Q-learning model. To that end, we'll
 cover three important concepts at the heart of Q-learning, in the 
following order:</p>
			<ol>
				<li class="list" value="1">The Q-value</li>
				<li class="list">The temporal difference</li>
				<li class="list">The Bellman equation</li>
			</ol>
			<p class="normal">Let's get started by learning about the Q-<a id="_idTextAnchor147"/>value.</p>
			<h4 class="title" xml:lang="en-GB" id="sigil_toc_id_99" lang="en-GB"><a id="_idTextAnchor148"/>The Q-value</h4>
			<p class="normal">Before you start getting into the<a id="_idIndexMarker187"/> details of Q-learning, I need to explain the concept of the Q-value. Here's how it works:</p>
			<p class="normal">To each couple of state and action (<em class="italics">s</em>, <em class="italics">a</em>), we are going to associate a numeric value <em class="italics">Q</em>(<em class="italics">s</em>, <em class="italics">a</em>):</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_07_005.png" style="height: 2em;" alt=""/></figure>
			<p class="normal">We will say that <em class="italics">Q</em>(<em class="italics">s</em>, <em class="italics">a</em>) is "the Q-value of the action <em class="italics">a</em> performed in the state <em class="italics">s</em>."</p>
			<p class="normal">Now I know the sort of questions you might be 
asking in your head: What does this Q-value mean? What does it 
represent? How do I even compute it? These were some of the questions I 
had in my mind when I first learned Q-learning.</p>
			<p class="normal">In order to answer these questions, I need to introduce the temporal diffe<a id="_idTextAnchor149"/>rence.</p>
			<h4 class="title" xml:lang="en-GB" id="sigil_toc_id_100" lang="en-GB"><a id="_idTextAnchor150"/>The temporal difference</h4>
			<p class="normal">This is where the math really comes in. Let's say <a id="_idIndexMarker188"/>we are in a specific state <span class="mediaobject"><img src="../Images/B14110_04_001.png" alt=""/></span>, at a specific time <em class="italics">t</em>. Let's just perform an action randomly, any of them. That brings us to the next state <span class="mediaobject"><img src="../Images/B14110_07_045.png" alt=""/></span> and we get the reward <span class="mediaobject"><img src="../Images/B14110_07_008.png" alt=""/></span>.</p>
			<p class="normal">The temporal difference at time <em class="italics">t</em>, denoted by <img src="../Images/B14110_07_038.png" alt=""/>, is the difference between:</p>
			<ol>
				<li class="list" value="1"><span class="mediaobject"><img src="../Images/B14110_07_010.png" alt=""/></span>, that is, the reward <img src="../Images/B14110_07_011.png" alt=""/> obtained by performing the action <img src="../Images/B14110_07_012.png" alt=""/> in the state <img src="../Images/B14110_07_013.png" alt=""/>, plus the Q-value of the best action performed in the future state <img src="../Images/B14110_07_045.png" alt=""/>, discounted by a factor <img src="../Images/B14110_07_015.png" alt=""/>, called the discount factor</li>
				<li class="list">and <img src="../Images/B14110_07_016.png" alt=""/>, that is, the Q-value<a id="_idIndexMarker189"/> of the action <img src="../Images/B14110_07_017.png" alt=""/> performed in the state <span class="mediaobject"><img src="../Images/B14110_07_018.png" alt=""/></span>.</li>
			</ol>
			<p class="normal">This leads to:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_07_019.png" style="height: 2.5em;" alt=""/></figure>
			<p class="normal">You might think that's great, that you understand 
all the terms, but you're probably also thinking "But what does that all
 mean?" Don't worry—that's exactly what I was thinking when I was 
learning this.</p>
			<p class="normal">I'm going to explain while at the<a id="_idIndexMarker190"/>
 same time improving your AI intuition. The first thing to understand is
 that the temporal difference represents how well the AI is learning. 
Here's how it works exactly, with respect to the training process 
(during which the Q-values are learned):</p>
			<ol>
				<li class="list" value="1">At the beginning of the training, the 
Q-values are set to zero. Since the AI is looking to get the good 
rewards (here 1 or 1000), it is looking for the high temporal 
differences (see the formula of TD). Accordingly, if in the first 
iterations, <span class="mediaobject"><img src="../Images/B14110_07_020.png" alt=""/></span> is high, the AI gets a "pleasant surprise" because that means the AI was able to find a good reward. On the other hand, if <span class="mediaobject"><img src="../Images/B14110_07_021.png" alt=""/></span> is small, the AI gets a "frustration."</li>
				<li class="list">When the AI gets a great reward, the specific 
Q-value of the (state, action) that led to that great reward increases, 
so the AI can remember how it got to that high reward (you'll see 
exactly how it increases in the next section). For example, let's say 
that it was the action <span class="mediaobject"><img src="../Images/B14110_07_022.png" alt=""/></span> performed in the state <span class="mediaobject"><img src="../Images/B14110_07_018.png" alt=""/></span> that led to that high reward <img src="../Images/B14110_07_024.png" alt=""/>. That would mean the Q-value <span class="mediaobject"><img src="../Images/B14110_07_025.png" alt=""/></span>
 increases automatically (remember, you'll see how in the next section).
 Those increased Q-values are important information, because they 
indicate to the AI which transitions lead to the good rewards.</li>
				<li class="list">The next step of the AI is not only to look for the
 great rewards, but also to look at the same time for the high Q-values.
 Why? Because the high Q-values are the ones that lead to the great 
reward. In fact, the high Q-values are the ones that lead to higher 
Q-values, themselves leading to even higher Q-values, themselves leading
 eventually to the highest reward (1000). That's the role of <img src="../Images/B14110_07_026.png" alt=""/>
 in the temporal difference formula. Everything will become crystal 
clear when you put this into practice. The AI looks for the high 
Q-values, and as soon as it finds them, the Q-values of the (state, 
action) that led to these high Q-values will increase again, since they 
indicate the right path towards the goal.</li>
				<li class="list">At some point, the AI will know all the transitions
 that lead to the good rewards and high Q-values. Since the Q-values of 
these transitions have already been increased over time, the temporal 
differences decrease in the end. In fact, the closer we get to the final
 goal, the smaller the temporal differences become.</li>
			</ol>
			<p class="normal">In conclusion, the temporal difference is like a temporary intrinsic reward, of which the AI will try to find the large values <a id="_idIndexMarker191"/>at
 the beginning of the training. Eventually, the AI will minimize this 
reward as it gets to the end of the training—that is, as it gets closer 
to the final goal.</p>
			<p class="normal">That's exactly the intuition of the temporal 
difference you must have in mind, because it will really help you 
understand the magic of Q-learning. Speaking of that magic, we are about
 to reveal the last piece of the puzzle.</p>
			<p class="normal">Now you understand that the AI will iterate some 
updates of the Q-values towards the high temporal differences, which are
 ultimately decreased. But how does it do that? There is a specific 
answer to that question—the Bellman equation, the most famous equation 
in Reinforcement Learning.</p>
			<h4 class="title" xml:lang="en-GB" id="sigil_toc_id_101" lang="en-GB">The Bellma<a id="_idTextAnchor151"/>n equation</h4>
			<p class="normal">In order to perform better and better <a id="_idIndexMarker192"/>actions
 that will lead the AI to reach its goal, you have to increase the 
Q-values of actions when you find high temporal differences. Only one 
question remains: How will the AI update these Q-values? Richard 
Bellman, a pioneer of Reinforcement Learning, created the answer. At 
each iteration, you update the Q-values from time t-1 (previous 
iteration) to t (current iteration) through the following equation, 
called the Bellman equation:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_07_027.png" style="height: 1.75em;" alt=""/></figure>
			<p class="normal">where <img src="../Images/B14110_07_028.png" alt=""/>
 is the learning rate, which dictates how fast the learning of the 
Q-values goes. Its value is usually between 0 and 1, for example, 0.75. 
The lower the value of <img src="../Images/B14110_07_029.png" alt=""/>,
 the smaller the updates of the Q-values, and the longer the Q-learning 
will take. The higher its value, the bigger the updates of the Q-values 
and the faster the Q-learning will be. As you can clearly see in this 
equation, when the <a id="_idIndexMarker193"/>temporal difference <span class="mediaobject"><img src="../Images/B14110_07_030.png" alt=""/></span> is high, the Q-value <img src="../Images/B14110_07_031.png" alt=""/> increases.</p>
			<h4 class="title" xml:lang="en-GB" id="sigil_toc_id_102" lang="en-GB">Reinforcement<a id="_idTextAnchor152"/> intuition</h4>
			<p class="normal">Now you have all the elements of Q-learning—congratulations, by the way—let's connect the dots between <a id="_idIndexMarker194"/>all these elements to reinforce your AI intuition.</p>
			<p class="normal">The Q-values measure the accumulation of "good surprise" or "frustration" associated with the couple of action and state <span class="mediaobject"><img src="../Images/B14110_07_032.png" alt=""/></span>.</p>
			<p class="normal">In the "good surprise" case of a high temporal 
difference, the AI is reinforced, and in the "frustration" case of a low
 temporal difference, the AI is weakened.</p>
			<p class="normal">We want to learn the Q-values that will give the AI
 the maximum "good surprise," and that's exactly what the Bellman 
equation does by updating the Q-values at each iteration.</p>
			<p class="normal">You've learned quite a lot of new information, and 
even though you've finished with an intuition section that connects the 
dots, that's not enough to get a really solid grasp of Q-learning. The 
next step is to take a step back, and the best way to do that is to go 
through the whole Q-learning process from start to finish so that it 
becomes crystal clear in your head.</p>
			<h2 class="title" xml:lang="en-GB" id="sigil_toc_id_103" lang="en-GB"><a id="_idTextAnchor153"/>The whole Q-learning process</h2>
			<p class="normal">Let's summarize the different steps of the<a id="_idIndexMarker195"/>
 whole Q-learning process. To be clear, the only purpose of this process
 is to update the Q-values over a certain number of iterations until 
they are no longer updated (we refer to that point as convergence).</p>
			<p class="normal">The number of iterations depends on the complexity 
of the problem. For our problem, 1,000 will be enough, but for more 
complex problems you might want to consider higher numbers such as 
10,000. In short, the Q-learning process is the part where we train our 
AI, and it's called Q-learning because it's the process during which the
 Q-values are learned. Then I'll explain what <a id="_idIndexMarker196"/>happens
 for the inference part (pure predictions), which comes, as always, 
after the training. The full Q-learning process starts with training 
mode.</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_104" lang="en-GB"><a id="_idTextAnchor154"/>Training mode</h3>
			<p class="normal"><strong class="bold">Initialization (First iteration)</strong>:</p>
			<p class="normal">For all couples of states <em class="italics">s</em> and actions <em class="italics">a</em>, the Q-values <a id="_idIndexMarker197"/>are initialized to 0.</p>
			<p class="normal"><strong class="bold">Next iterations</strong>:</p>
			<p class="normal">At each iteration <em class="italics">t</em> ≥ 1, you repeat for a certain number of times (chosen by you the developer) the following steps:</p>
			<ol>
				<li class="list" value="1">You select a random state <img src="../Images/B14110_07_033.png" alt=""/> from the possible states.</li>
				<li class="list">From that state, you perform a random action <img src="../Images/B14110_07_034.png" alt=""/> that can lead to a next possible state, that is, such that <span class="mediaobject"><img src="../Images/B14110_07_035.png" alt=""/></span>.

</li>
				<li class="list">You reach the next state <span class="mediaobject"><img src="../Images/B14110_07_0451.png" alt=""/></span> and you get the reward <span class="mediaobject"><img src="../Images/B14110_07_037.png" alt=""/></span>.</li>
				<li class="list">You compute the temporal difference <span class="mediaobject"><img src="../Images/B14110_07_0381.png" alt=""/></span>:
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_07_039.png" style="height: 2.75em;" alt=""/></figure>
</li>
				<li class="list">You update the Q-value by applying the Bellman equation:
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_07_027.png" style="height: 2em;" alt=""/></figure>
</li>
</ol>
			<p class="normal">At the end of this process, you have obtained Q-values that no longer update. That means only one thing; you are<a id="_idIndexMarker198"/> ready to hack the maze by going into inference mode.</p>
			<h3 class="title" xml:lang="en-GB" id="sigil_toc_id_105" lang="en-GB"><a id="_idTextAnchor155"/>Inference mode</h3>
			<p class="normal">Th<a id="_idTextAnchor156"/>e training is complete, and now <a id="_idIndexMarker199"/>begins
 the inference. To remind you, the inference part is when you have a 
fully trained model with which you are going to make predictions. In our
 maze, the predictions that you are going to make are the actions to 
perform to take you from start (Location <strong class="bold">E</strong>) to finish (Location <strong class="bold">G</strong>). So, the question is:</p>
			<p class="normal">How are you going to use the learned Q-values to perform the actions?</p>
			<p class="normal">Good news; for Q-learning this is very simple. When in a certain state <img src="../Images/B14110_07_018.png" alt=""/>, you simply perform the action <img src="../Images/B14110_07_042.png" alt=""/> that has the highest Q-value for that state <span class="mediaobject"><img src="../Images/B14110_04_001.png" alt=""/></span>:</p>
			<figure class="mediaobject" xml:lang="en-GB" lang="en-GB"><img src="../Images/B14110_07_044.png" style="height: 2.5em;" alt=""/></figure>
			<p class="normal">That's all—by doing this at each location (each 
state), you get to your final destination through the shortest route. 
We'll implement this and see the result in the practical activities or 
the next chapter.</p>
			<h2 class="title" xml:lang="en-GB" id="sigil_toc_id_106" lang="en-GB"><a id="_idTextAnchor157"/>Summary</h2>
			<p class="normal">In this chapter we studied the Q-learning model, 
which is only applied to environments that have a finite number of input
 states and a finite number of possible actions to perform.</p>
			<p class="normal">When performing Q-learning, the AI learns Q-values 
through an iterative process, so that the higher the Q-value of a 
(state, action) pair, the closer the AI gets to the top reward.</p>
			<p class="normal">At each iteration the Q-values are updated through 
the Bellman equation, which simply consists of adding the temporal 
difference, discounted by a learning rate factor. We will get to work on
 a full practical Q-learning activity in the next chapter, applied to a 
real-world business problem.</p>
</body></html>