["```py\nt1 <- \"I'm not a huge $AAPL fan but $160 stock closes down $0.60 for the day on huge volume isn't really bearish\"\nt2 <- \"$AAPL $BAC not sure what more dissapointing: the new iphones or the presentation for the new iphones?\"\nt3 <- \"IMO, $AAPL animated emojis will be the death of $SNAP.\"\nt4 <- \"$AAPL get on board. It's going to 175\\. I think wall st will have issues as aapl pushes 1 trillion dollar valuation but 175 is in the cards\"\nt5 <- \"In the AR vs. VR battle, $AAPL just put its chips behind AR in a big way.\"\n```", "```py\ntweets <- c(t1, t2, t3, t4, t5)\ntoken <- text_tokenizer(num_words = 10) %>%    \n         fit_text_tokenizer(tweets)\ntoken$index_word[1:3]\n$`1`\n[1] \"the\"\n\n$`2`\n[1] \"aapl\"\n\n$`3`\n[1] \"in\"\n```", "```py\nseq <- texts_to_sequences(token, tweets)\nseq\n[[1]]\n[1] 4 5 6 2 7 8 1 9 6\n\n[[2]]\n[1] 2 4 1 1 8 1\n\n[[3]]\n[1] 2 1\n\n[[4]]\n[1] 2 9 2 7 3 1\n\n[[5]]\n[1] 3 1 2 3 5\n\n```", "```py\npad_seq <- pad_sequences(seq, maxlen = 5)\npad_seq\n [,1] [,2] [,3] [,4] [,5]\n[1,]    7    8    1    9    6\n[2,]    4    1    1    8    1\n[3,]    0    0    0    2    1\n[4,]    9    2    7    3    1\n[5,]    3    1    2    3    5\n```", "```py\npad_seq <- pad_sequences(seq, maxlen = 5, padding = 'post')\npad_seq \n [,1] [,2] [,3] [,4] [,5]\n[1,]    7    8    1    9    6\n[2,]    4    1    1    8    1\n[3,]    2    1    0    0    0\n[4,]    9    2    7    3    1\n[5,]    3    1    2    3    5\n```", "```py\nlibrary(syuzhet) \nget_nrc_sentiment(tweets) \n  anger anticipation disgust fear joy sadness surprise trust negative positive\n1     1            0       0    1   0       0        0     0        0        0\n2     0            0       0    0   0       0        0     0        0        0\n3     1            1       1    1   1       1        1     0        1        1\n4     0            1       0    0   0       0        0     0        0        0\n5     1            0       0    0   0       0        0     0        1        0\n```", "```py\nget_nrc_sentiment('bearish') \n  anger anticipation disgust fear joy sadness surprise trust negative positive\n1     1            0       0    1   0       0        0     0        0        0\n\nget_nrc_sentiment('death') \n  anger anticipation disgust fear joy sadness surprise trust negative positive\n1     1            1       1    1   0       1        1     0        1        0\n\nget_nrc_sentiment('animated') \n  anger anticipation disgust fear joy sadness surprise trust negative positive\n1     0            0       0    0   1       0        0     0        0        1\n```", "```py\nget_sentiment(tweets, method=\"syuzhet\")\n[1]  0.00  0.80 -0.35  0.00 -0.25\n\nget_sentiment(tweets, method=\"bing\")\n[1] -1  0 -1 -1  0\n\nget_sentiment(tweets, method=\"afinn\")\n[1]  4  0 -2  0  0\n```", "```py\nmodel <- keras_model_sequential()\nmodel %>% layer_embedding(input_dim = 10, \n                          output_dim = 8, \n                          input_length = 5) \nsummary(model)\n\nOUTPUT\n__________________________________________________________________________________\nLayer (type)                        Output Shape                     Param # \n==================================================================================\nembedding_1 (Embedding)             (None, 5, 8)                     80 \n==================================================================================\nTotal params: 80\nTrainable params: 80\nNon-trainable params: 0\n________________________________________________________________________________\n\nprint(model$get_weights(), digits = 2)\n[[1]]\n [,1]    [,2]    [,3]   [,4]    [,5]    [,6]    [,7]    [,8]\n [1,]  0.0055 -0.0364 -0.0475  0.049 -0.0139 -0.0114 -0.0452 -0.0298\n [2,]  0.0398 -0.0143 -0.0406  0.023 -0.0496 -0.0124  0.0087 -0.0104\n [3,]  0.0370 -0.0321 -0.0491 -0.021 -0.0214  0.0391  0.0428 -0.0398\n [4,] -0.0257  0.0294  0.0433  0.048  0.0259 -0.0323 -0.0308  0.0224\n [5,] -0.0079 -0.0255  0.0164  0.023 -0.0486  0.0273  0.0245 -0.0020\n [6,]  0.0372  0.0464  0.0454 -0.020  0.0086 -0.0375 -0.0188  0.0395\n [7,]  0.0293  0.0305  0.0130  0.037 -0.0324 -0.0069 -0.0248  0.0178\n [8,] -0.0116 -0.0087 -0.0344  0.027  0.0132  0.0430 -0.0196 -0.0356\n [9,]  0.0314 -0.0315  0.0074 -0.044 -0.0198 -0.0135 -0.0353  0.0081\n[10,]  0.0426  0.0199 -0.0306 -0.049  0.0259 -0.0341 -0.0155  0.0147\n\n```", "```py\nimdb <- dataset_imdb(num_words = 500)  \nc(c(train_x, train_y), c(test_x, test_y)) %<-% imdb\nz <- NULL\nfor (i in 1:25000) {z[i] <- print(length(train_x[[i]]))}\nsummary(z)\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   11.0   130.0   178.0   238.7   291.0  2494.0 \n```", "```py\ntrain_x <- pad_sequences(train_x, maxlen = 100)\ntest_x <- pad_sequences(test_x, maxlen = 100)\n```", "```py\nmodel <- keras_model_sequential()\nmodel %>% layer_embedding(input_dim = 500, \n                          output_dim = 16, \n                          input_length = 100) %>%\n         layer_flatten() %>% \n         layer_dense(units = 16, activation = 'relu') %>%\n         layer_dense(units = 1, activation = \"sigmoid\")\nsummary(model)\n\nOUTPUT\n___________________________________________________________________\nLayer (type)                  Output Shape              Param # \n===================================================================\nembedding_12 (Embedding)      (None, 100, 16)           8000 \n___________________________________________________________________\nflatten_3 (Flatten)           (None, 1600)              0 \n___________________________________________________________________\ndense_6 (Dense)               (None, 16)                25616 \n___________________________________________________________________\ndense_7 (Dense)               (None, 1)                 17 \n===================================================================\nTotal params: 33,633\nTrainable params: 33,633\nNon-trainable params: 0\n___________________________________________________________________\n```", "```py\nmodel %>% compile(optimizer = \"rmsprop\",\n          loss = \"binary_crossentropy\",\n          metrics = c(\"acc\"))\n```", "```py\nmodel_1 <- model %>% fit(train_x, train_y,\n                         epochs = 10,\n                         batch_size = 128,\n                         validation_split = 0.2)\nplot(model_1)\n```", "```py\nmodel <- keras_model_sequential()\nmodel %>% layer_embedding(input_dim = 500, \n                          output_dim = 16, \n                          input_length = 100) %>%\n         layer_flatten() %>% \n         layer_dense(units = 16, activation = 'relu') %>%\n         layer_dense(units = 1, activation = \"sigmoid\")\nmodel %>% compile(optimizer = \"rmsprop\",\n          loss = \"binary_crossentropy\",\n          metrics = c(\"acc\"))\nmodel_2 <- model %>% fit(train_x, train_y,\n                         epochs = 10,\n                         batch_size = 512,\n                         validation_split = 0.2)\nplot(model_2)\n```", "```py\nmodel %>% evaluate(train_x, train_y)\n$loss\n[1] 0.3745659\n$acc\n[1] 0.83428\n```", "```py\npred <- model %>%   predict_classes(train_x)\ntable(Predicted=pred, Actual=imdb$train$y)\n         Actual\nPredicted     0     1\n        0 11128  2771\n        1  1372  9729\n```", "```py\nmodel %>% evaluate(test_x, test_y)\n$loss\n[1] 0.4431483\n$acc\n[1] 0.79356\n```", "```py\npred1 <- model %>%   predict_classes(test_x)\ntable(Predicted=pred1, Actual=imdb$test$y)\n         Actual\nPredicted     0     1\n        0 10586  3247\n        1  1914  9253\n```", "```py\nc(c(train_x, train_y), c(test_x, test_y)) %<-% imdb\nz <- NULL\nfor (i in 1:25000) {z[i] <- print(length(train_x[[i]]))}\nsummary(z)\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   11.0   130.0   178.0   238.7   291.0  2494.0 \n```", "```py\nimdb <;- dataset_imdb(num_words = 500)  \nc(c(train_x, train_y), c(test_x, test_y)) %<-% imdb\ntrain_x <- pad_sequences(train_x, maxlen = 200) \ntest_x <- pad_sequences(test_x, maxlen = 200)\nmodel <- keras_model_sequential()\nmodel %>% layer_embedding(input_dim = 500, \n                          output_dim = 16, \n                          input_length = 200) %>%\n         layer_flatten() %>% \n         layer_dense(units = 16, activation = 'relu') %>%\n         layer_dense(units = 1, activation = \"sigmoid\")\nmodel %>% compile(optimizer = \"adamax\",  \n                  loss = \"binary_crossentropy\",\n                  metrics = c(\"acc\"))\nmodel_3 <- model %>% fit(train_x, train_y,\n                         epochs = 10,\n                         batch_size = 512,\n                         validation_split = 0.2)\nplot(model_3)\n```", "```py\nmodel %>% evaluate(test_x, test_y)\n$loss\n[1] 0.3906249\n$acc\n[1] 0.82468\n```", "```py\npred1 <- model %>%   predict_classes(test_x)\ntable(Predicted=pred1, Actual=imdb$test$y)\n         Actual\nPredicted     0     1\n        0  9970  1853\n        1  2530 10647\n```"]