<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building a Deep Feedforward Neural Network</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Training a vanilla neural network</li>
<li>Scaling the input dataset</li>
<li>Impact of training when the majority of inputs are greater than zero</li>
<li>Impact of batch size on model accuracy</li>
<li>Building a deep neural network to improve network accuracy</li>
<li>Varying the learning rate to improve network accuracy</li>
<li>Varying the loss optimizer to improve network accuracy</li>
<li>Understanding the scenario of overfitting</li>
<li>Speeding up the training process using batch normalization</li>
</ul>
<p>In the previous chapter, we looked at the basics of the function of a neural network. We also learned that there are various hyperparameters that impact the accuracy of a neural network. In this chapter, we will get into the details of the functions of the various hyperparameters within a neural network.</p>
<p>All the codes for this chapter are available at https://github.com/kishore-ayyadevara/Neural-Networks-with-Keras-Cookbook/blob/master/Neural_network_hyper_parameters.ipynb</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Training a vanilla neural network</h1>
                </header>
            
            <article>
                
<p>To understand how to train a vanilla neural network, we will go through the task of predicting the label of a digit in the MNIST dataset, which is a popular dataset of images of digits (one digit per image) and the corresponding label of the digit that is contained in the image. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Training a neural network is done in the following steps:</p>
<ol>
<li>Import the relevant packages and datasets</li>
<li>Preprocess the targets (convert them into one-hot encoded vectors) so that we can perform optimization on top of them:
<ul>
<li>We shall be minimizing categorical cross entropy loss</li>
</ul>
</li>
<li>Create train and test datasets:
<ul>
<li>We have the train dataset so that we create a model based on it</li>
<li>The test dataset is not seen by the model:
<ul>
<li>Hence, the accuracy on the test dataset is an indicator of how well the model is likely to work on data when the model is productionalized, as data in the production scenario (which might occur a few days/weeks after building the model) cannot be seen by the model</li>
</ul>
</li>
</ul>
</li>
<li>Initialize a model</li>
<li>Define the model architecture:
<ul>
<li>Specify the number of units in a hidden layer</li>
<li>Specify the activation function that is to be performed in a hidden layer</li>
<li>Specify the number of hidden layers</li>
<li>Specify the loss function that we want to minimize</li>
<li>Provide the optimizer that will minimize the loss function</li>
</ul>
</li>
<li>Fit the model:
<ul>
<li>Mention the batch size to update weights</li>
<li>Mention the total number of epochs</li>
</ul>
</li>
<li>Test the model:
<ul>
<li>Mention the validation data, otherwise, mention the validation split, which will consider the last x% of total data as test data</li>
<li>Calculate the accuracy and loss values on top of the test dataset</li>
</ul>
</li>
<li>Check for anything interesting in the way in which loss value and accuracy values changed over an increasing number of epochs</li>
</ol>
<p>Using this strategy, let's go ahead and build a neural network model in Keras, in the following section.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Import the relevant packages and dataset, and visualize the input dataset:</li>
</ol>
<pre style="padding-left: 60px">from keras.datasets import mnist<br/>import numpy<br/>from keras.datasets import mnist<br/>from keras.models import Sequential<br/>from keras.layers import Dense<br/>from keras.layers import Dropout<br/>from keras.utils import np_utils<br/>(X_train, y_train), (X_test, y_test) = mnist.load_data()</pre>
<p style="padding-left: 60px">In the preceding code, we are importing the relevant Keras files and are also importing the MNIST dataset (which is provided as a built-in dataset in Keras). </p>
<ol start="2">
<li>The MNIST dataset contains images of digits where the images are of 28 x 28 in shape. Let's plot a few images to see what they will look like in the code here:</li>
</ol>
<pre style="padding-left: 90px">import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>plt.subplot(221)<br/>plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))<br/>plt.grid('off')<br/>plt.subplot(222)<br/>plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))<br/>plt.grid('off')<br/>plt.subplot(223)<br/>plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))<br/>plt.grid('off')<br/>plt.subplot(224)<br/>plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))<br/>plt.grid('off')<br/>plt.show()</pre>
<p style="padding-left: 60px">The following screenshot shows the output of the previous code block:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1200 image-border" src="Images/97a5b714-f5d0-493b-a367-5565bf21d112.png" style="width:25.83em;height:21.00em;" width="310" height="252"/></p>
<ol start="3">
<li>Flatten the 28 x 28 images so that the input is all the 784 pixel values. Additionally, one-hot encode the outputs. This step is key in the dataset preparation process:</li>
</ol>
<pre style="padding-left: 60px"># flatten 28*28 images to a 784 vector for each image<br/>num_pixels = X_train.shape[1] * X_train.shape[2]<br/>X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')<br/>X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')</pre>
<p style="padding-left: 60px">In the preceding step, we are reshaping the input dataset using the reshape method that converts an array of a given shape into a different shape. In this specific case, we are converting an array that has an <kbd>X_train.shape[0]</kbd> number of data points (images) where there are <kbd>X_train.shape[1]</kbd> rows and <kbd>X_train.shape[2]</kbd> columns in each image, into an array of an <span><kbd>X_train.shape[0]</kbd> number of data points (images) and <kbd>X_train.shape[1] * X_train.shape[2]</kbd> values per image. Similarly, we perform the same exercise on the test dataset:</span></p>
<pre style="padding-left: 60px"># one hot encode outputs<br/>y_train = np_utils.to_categorical(y_train)<br/>y_test = np_utils.to_categorical(y_test)<br/>num_classes = y_test.shape[1]</pre>
<p style="padding-left: 60px">Let's try to understand how one-hot encoding works. If the unique possible labels are <em>{0, 1, 2, 3}</em>, they will be one-hot encoded, as follows:</p>
<table style="border-collapse: collapse;width: 100%;border-color: #000000" border="1">
<tbody>
<tr>
<td style="width: 24%"><strong>Label</strong></td>
<td style="width: 17%"><strong>0</strong></td>
<td style="width: 17%"><strong>1</strong></td>
<td style="width: 15.5172%"><strong>2</strong></td>
<td style="width: 21.4828%"><strong>3</strong></td>
</tr>
<tr>
<td style="width: 24%"><strong>0</strong></td>
<td style="width: 17%">1</td>
<td style="width: 17%">0</td>
<td style="width: 15.5172%">0</td>
<td style="width: 21.4828%">0</td>
</tr>
<tr>
<td style="width: 24%"><strong>1</strong></td>
<td style="width: 17%">0</td>
<td style="width: 17%">1</td>
<td style="width: 15.5172%">0</td>
<td style="width: 21.4828%">0</td>
</tr>
<tr>
<td style="width: 24%"><strong>2</strong></td>
<td style="width: 17%">0</td>
<td style="width: 17%">0</td>
<td style="width: 15.5172%">1</td>
<td style="width: 21.4828%">0</td>
</tr>
<tr>
<td style="width: 24%"><strong>3</strong></td>
<td style="width: 17%">0</td>
<td style="width: 17%">0</td>
<td style="width: 15.5172%">0</td>
<td style="width: 21.4828%">1</td>
</tr>
</tbody>
</table>
<p> </p>
<p style="padding-left: 60px">Essentially, each label will occupy a unique column in the dataset, and if the label is present, the column value will be one, and every other column value will be zero.</p>
<p style="padding-left: 60px">In Keras, the one-hot encoding approach on top of labels is performed using the <kbd>to_categorical</kbd> method, which figures out the number of unique labels in the target data, and then converts them into a one-hot encoded vector.</p>
<ol start="4">
<li>Build a neural network with a hidden layer with 1,000 units:</li>
</ol>
<pre style="padding-left: 60px">model = Sequential()<br/>model.add(Dense(1000, input_dim=784, activation='relu'))<br/>model.add(Dense(10,  activation='softmax'))</pre>
<p style="padding-left: 60px">In the preceding step, we mention that the input has 784 values that are connected to 1,000 values in a hidden layer. Additionally, we are also specifying that the activation, which is to be performed in the hidden layer after the matrix multiplication of the input and the weights connecting the input and hidden layer, is the ReLu activation.</p>
<p style="padding-left: 60px">Finally, the hidden layer is connected to an output that has 10 values (as there are 10 columns in the vector created by the <kbd>to_categorical</kbd> method), and we perform softmax on top of the output so that we obtain the probability of an image belonging to a certain class.</p>
<ol start="5">
<li>The preceding model architecture can be visualized as follows:</li>
</ol>
<pre style="padding-left: 60px">model.summary()<br/><br/></pre>
<p style="padding-left: 60px">A summary of model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/02ffb258-590f-499a-a9b7-6d7f2bd3b7ee.png" style="width:35.92em;height:12.75em;" width="535" height="191"/></p>
<p style="padding-left: 60px">In the preceding architecture, the number of parameters in the first layer is 785,000, as the 784 input units are connected to 1,000 hidden units, resulting in 784 * 1,000 weight values, and 1,000 bias values, for the 1,000 hidden units, resulting in a total of 785,000 parameters.</p>
<p style="padding-left: 60px">Similarly, the output layer has 10 outputs, which are connected to each of the 1,000 hidden layers, resulting in 1,000 * 10 parameters and 10 biases—a total of 10,010 parameters.</p>
<p style="padding-left: 60px">The output layer has 10 units as there are 10 possible labels in the output. The output layer now gives us a probability value for each class for a given input image.</p>
<ol start="6">
<li>Compile the model as follows:</li>
</ol>
<pre style="padding-left: 60px">model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])</pre>
<p style="padding-left: 60px">Note that because the target variable is a one-hot encoded vector with multiple classes in it, the loss function will be a categorical cross-entropy loss.</p>
<p style="padding-left: 60px">Additionally, we are using the Adam optimizer to minimize the cost function (more on different optimizers in the <span><em><span>Varying the loss optimizer to improve network accuracy</span></em></span> recipe).</p>
<p style="padding-left: 60px">We are also noting that we will need to look at the accuracy metric while the model is getting trained.</p>
<ol start="7">
<li>Fit the model as follows:</li>
</ol>
<pre style="padding-left: 60px">history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=32, verbose=1)</pre>
<p style="padding-left: 60px">In the preceding code, we have specified the input (<kbd>X_train</kbd>) and the output (<kbd>y_train</kbd>) that the model will fit. Additionally, we also specify the input and output of the test dataset, which the model will not use to train weights; however it, will give us an idea of how different the loss value and accuracy values are between the training and the test datasets.</p>
<ol start="8">
<li>Extract the training and test loss and accuracy metrics over different epochs:</li>
</ol>
<pre style="padding-left: 60px">history_dict = history.history<br/>loss_values = history_dict['loss']<br/>val_loss_values = history_dict['val_loss']<br/>acc_values = history_dict['acc']<br/>val_acc_values = history_dict['val_acc']<br/>epochs = range(1, len(val_loss_values) + 1)</pre>
<p style="padding-left: 60px">While fitting a model, the history variable will have stored the accuracy and loss values corresponding to the model in each epoch for both the training and the test datasets. In the preceding steps, we are storing those values in a list so that we can plot the variation of accuracy and loss in both the training and test datasets over an increasing number of epochs.</p>
<ol start="9">
<li>Visualize the training and test loss and the accuracy over a different number of epochs:</li>
</ol>
<pre style="padding-left: 60px">import matplotlib.pyplot as plt<br/>%matplotlib inline <br/><br/>plt.subplot(211)<br/>plt.plot(epochs, history.history['loss'], 'rx', label='Training loss')<br/>plt.plot(epochs, val_loss_values, 'b', label='Test loss')<br/>plt.title('Training and test loss')<br/>plt.xlabel('Epochs')<br/>plt.ylabel('Loss')<br/>plt.legend()<br/>plt.show()<br/><br/>plt.subplot(212)<br/>plt.plot(epochs, history.history['acc'], 'rx', label='Training accuracy')<br/>plt.plot(epochs, val_acc_values, 'b', label='Test accuracy')<br/>plt.title('Training and test accuracy')<br/>plt.xlabel('Epochs')<br/>plt.ylabel('Accuracy')<br/>plt.gca().set_yticklabels(['{:.0f}%'.format(x*100) for x in plt.gca().get_yticks()]) <br/>plt.legend()<br/>plt.show()</pre>
<p style="padding-left: 60px">The preceding code produces the following diagram, where the first plot shows the training and test loss values over increasing epochs, and the second plot shows the training and test accuracy over increasing epochs:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1201 image-border" src="Images/3d56bf42-6d83-4844-a9b7-06ea6ad79d71.png" style="width:36.92em;height:28.58em;" width="408" height="316"/></p>
<p style="padding-left: 60px">Note that the previous network resulted in an accuracy of 97%. Also, note that loss values (and thereby, accuracy) have a step change over a different number of epochs. We will contrast this change in loss with the scenario when the input dataset is scaled in the next section.</p>
<ol start="10">
<li>Let's calculate the accuracy of the model manually:</li>
</ol>
<pre style="padding-left: 60px">preds = model.predict(X_test)</pre>
<p style="padding-left: 60px">In the preceding step, we are using the <kbd>predict</kbd> method to calculate the expected output values for a given input (<kbd>X_test</kbd> in this case) to the model. Note that we are specifying it as <kbd>model.predict</kbd>, as we have initialized a sequential model named <kbd>model</kbd>:</p>
<pre style="padding-left: 90px">import numpy as np<br/>correct = 0<br/>for i in range(len(X_test)):<br/>    pred = np.argmax(preds[i],axis=0)<br/>    act = np.argmax(y_test[i],axis=0)<br/>    if(pred==act):<br/>        correct+=1<br/>    else:<br/>        continue<br/><br/>correct/len(X_test)</pre>
<p style="padding-left: 60px">In the preceding code, we are looping all of the test predictions one at a time. For each test prediction, we are perming <kbd>argmax</kbd> to obtain the index that has the highest probability value.</p>
<p>Similarly, we perform the same exercise for the actual values of the test dataset. The prediction of the index of the highest value is the same in both the prediction and the actual values of the test dataset.</p>
<p>Finally, the number of correct predictions over the total number of data points in the test dataset is the accuracy of the model on the test dataset.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The key steps that we have performed in the preceding code are as follows:</p>
<ul>
<li>We flattened the input dataset so that each pixel is considered a variable using the <kbd>reshape</kbd> method</li>
<li>We performed one-hot encoding on the output values so that we can distinguish between different labels using the <kbd>to_categorical</kbd> method in the <kbd>np_utils</kbd> package</li>
<li>We built a neural network with a hidden layer using the sequential addition of layers</li>
<li>We compiled the neural network to minimize the categorical cross entropy loss (as the output has 10 different categories) using the <kbd>model.compile</kbd> method</li>
<li>We fitted the model with training data using the <kbd>model.fit</kbd> method</li>
<li>We extracted the training and test loss accuracies across all the epochs that were stored in the history</li>
<li>We predicted the probability of each class in the test dataset using the <kbd>model.predict</kbd> method</li>
<li>We looped through all the images in the test dataset and identified the class that has the highest probability</li>
<li>Finally, we calculated the accuracy (the number of instances in which a predicted class matches the actual class of the image out of the total number of instances)</li>
</ul>
<p>In the next section, we will look at the reasons for the step change in the loss and accuracy values, and move toward making the change more smooth.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Scaling the input dataset</h1>
                </header>
            
            <article>
                
<p>Scaling a dataset is a process where we limit the variables within a dataset to ensure they do not have a very wide range of different values. One way to achieve this is to divide each variable in the dataset by the maximum value of the variable. Typically, neural networks perform well when we scale the input datasets.</p>
<p>In this section, let's understand the reason neural networks perform better when the dataset is scaled.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>To understand the impact of the scaling input on the output, let's contrast the scenario where we check the output when the input dataset is not scaled, with the output when the input dataset is scaled.</p>
<p>Input data is not scaled:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/4c549749-a8f8-45e8-8f9c-1ab0f2077506.jpg" style="width:23.33em;height:16.58em;" width="328" height="232"/></p>
<p>In the preceding table, note that the output (sigmoid) did not vary a lot, even though the weight value varied from 0.01 to 0.9. The sigmoid function is calculated as the sigmoid value of the multiplication of the input with the weight, and then adding a bias to it:</p>
<pre>output = 1/(1+np.exp(-(w*x + b))</pre>
<p>Where <kbd>w</kbd> is the weight, <kbd>x</kbd> is the input, and <kbd>b</kbd> is the bias value.</p>
<p>The reason for no change in the sigmoid output is due to the fact that the multiplication of <span> </span><kbd>w*x</kbd><span> is a large number (as x is a large number) resulting in the sigmoid value always </span>falling in the saturated portion of the sigmoid curve (saturated value on the top-right or bottom-left of the sigmoid curve).</p>
<p>In this scenario, let's multiply different weight values by a small input number, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1203 image-border" src="Images/7de39d87-5f55-4a9c-ad30-771958d1d0ce.png" style="width:22.92em;height:16.17em;" width="337" height="239"/></p>
<p>The sigmoid output in the preceding table varies, as the input and weight values are small, resulting in a smaller value when the input and the weight are multiplied, further resulting in the sigmoid value having variation in output.</p>
<p>From this exercise, we learned about the importance of scaling the input dataset so that it results in a smaller value when the weights (provided the weights do not have a high range) are multiplied by the input values. This phenomenon results in the weight value not getting updated quickly enough.</p>
<p>Thus, to achieve the optimal weight value, we should scale our input dataset while initializing the weights to not have a huge range (typically, weights have a random value between -1 and +1 during initialization).</p>
<p>These issues hold true when the weight value is also a very big number. Hence, we are better off initializing the weight values as a small value that is closer to zero.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's go through the set up of scaling the dataset that we have used in the previous section, and compare the results with and without scaling:</p>
<ol>
<li>Import the relevant packages and datasets:</li>
</ol>
<pre style="padding-left: 60px">from keras.datasets import mnist<br/>import numpy as np<br/>from keras.datasets import mnist<br/>from keras.models import Sequential<br/>from keras.layers import Dense<br/>from keras.layers import Dropout<br/>from keras.utils import np_utils<br/><br/>(X_train, y_train), (X_test, y_test) = mnist.load_data()</pre>
<ol start="2">
<li>There are multiple ways to scale a dataset. One way is to convert all the data points to a value between zero and one (by dividing each data point with the maximum value in the total dataset, which is what we are doing in the following code). Another popular method, among the multiple other ways, is to normalize the dataset so that the values are between -1 and +1 by subtracting each data point with the overall dataset mean, and then dividing each resulting data point by the standard deviation of values in the original dataset. </li>
</ol>
<p style="padding-left: 60px">Now we will be flattening the input dataset and scaling it, as follows:</p>
<pre style="padding-left: 60px"># flatten 28*28 images to a 784 vector for each image<br/>num_pixels = X_train.shape[1] * X_train.shape[2]<br/>X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')<br/>X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')<br/><br/>X_train = X_train/255<br/>X_test = X_test/255</pre>
<p style="padding-left: 60px">In the preceding step, we have scaled the training and test inputs to a value between zero and one by dividing each value by the maximum possible value in the dataset, which is 255. Additionally, we convert the output dataset into a one-hot encoded format:</p>
<pre style="padding-left: 60px"># one hot encode outputs<br/>y_train = np_utils.to_categorical(y_train)<br/>y_test = np_utils.to_categorical(y_test)<br/>num_classes = y_test.shape[1]</pre>
<ol start="3">
<li>Build the model and compile it using the following code:</li>
</ol>
<pre style="padding-left: 60px">model = Sequential()<br/>model.add(Dense(1000, input_dim=784, activation='relu'))<br/>model.add(Dense(10,  activation='softmax'))<br/>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])</pre>
<p style="padding-left: 60px">Note that the preceding model is exactly the same as the one we built in the previous section. However, the only difference is that it will be executed on the training dataset that is scaled, whereas the previous one was not scaled.</p>
<ol start="4">
<li>Fit the model as follows:</li>
</ol>
<pre style="padding-left: 60px">history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=32, verbose=1)</pre>
<p style="padding-left: 60px">You will notice that the accuracy of the preceding model is ~98.25%.</p>
<ol start="5">
<li>Plot the training and test accuracy and the loss values over different epochs (the code to generate the following plots remains the same as the one we used in step 8 of the <em>Training a vanilla neural network</em> recipe):</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1204 image-border" src="Images/4affc74a-14b2-4085-9407-5da31fb07f4b.png" style="width:34.83em;height:26.67em;" width="418" height="320"/></p>
<p>From the preceding diagram, you should notice that training and test losses decreased smoothly over increasing epochs when compared to the non-scaled dataset that we saw in the previous section.</p>
<p>While the preceding network gave us good results in terms of a smoothly decreasing loss value, we noticed that there is a gap between the training and test accuracy/loss values, indicating that there is potential overfitting on top of the training dataset. <strong>Overfitting</strong> is the phenomenon where the model specializes on the training data that it might not work as well on the test dataset.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>The key steps that we have performed in the preceding code are as follows:</span></p>
<ul>
<li>We flattened the input dataset so that each pixel is considered a variable using the reshape method</li>
<li>Additionally, we scaled the dataset so that each variable now has a value between zero and one
<ul>
<li>We achieved the preceding by dividing the values of a variable with the maximum value of that variable</li>
</ul>
</li>
</ul>
<ul>
<li>We performed one-hot encoding on the output values so that we can distinguish between different labels using the <kbd>to_categorical</kbd> method in the <kbd>np_utils</kbd> package</li>
<li>We built a neural network with a hidden layer using the sequential addition of layers</li>
<li>We compiled the neural network to minimize categorical cross entropy loss (as the output has 10 different categories) using the <kbd>model.compile</kbd> method</li>
<li>We fitted the model with training data using the <kbd>model.fit</kbd> method</li>
<li>We extracted the training and test losses accuracies across all the epochs that were stored in the history</li>
<li>We also identified a scenario that we consider overfitting</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>In addition to scaling a variable's values by dividing the values by the maximum among the values in a variable, the other commonly used scaling methods are as follows:</p>
<ul>
<li>Min-max normalization</li>
<li>Mean normalization</li>
<li>Standardization</li>
</ul>
<p>More information about these scaling methods can be found on Wikipedia here: <a href="https://en.wikipedia.org/wiki/Feature_scaling" target="_blank">https://en.wikipedia.org/wiki/Feature_scaling</a>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Impact on training when the majority of inputs are greater than zero</h1>
                </header>
            
            <article>
                
<p>So far, in the dataset that we have considered, we have not looked at the distribution of values in the input dataset. Certain values of the input result in faster training. In this section, we will understand a scenario where weights are trained faster when the training time depends on the input values.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this section, we will follow the model-building process in exactly the same way as we did in the previous section.</p>
<p>However, we will adopt a small change to our strategy:</p>
<ul>
<li>We will invert the background color, and also the foreground color. Essentially, the background will be colored white in this scenario, and the label will be written in black.</li>
</ul>
<p>The intuition for this change impacting the model accuracy is as follows.</p>
<p>The pixels in the corner of images do not contribute toward predicting the label of an image. Given that a black pixel (original scenario) has a pixel value of zero, it is automatically taken care of, as when this input is multiplied by any weight value, the output is zero. This will result in the network learning that any change in the weight value connecting this corner pixel to a hidden layer will not have an impact on changing the loss value.</p>
<p>However, if we have a white pixel in the corner (where we already know that the corner pixels do not contribute toward predicting the label of an image), it will contribute toward certain hidden unit values, and thus the weights need to be fine-tuned until the impact of the corner pixels on the predicted label is minimal.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Load and scale the input dataset:</li>
</ol>
<pre style="padding-left: 60px">(X_train, y_train), (X_test, y_test) = mnist.load_data()<br/>num_pixels = X_train.shape[1] * X_train.shape[2]<br/>X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')<br/>X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')<br/>X_train = X_train/255<br/>X_test = X_test/255<br/>y_train = np_utils.to_categorical(y_train)<br/>y_test = np_utils.to_categorical(y_test)<br/>num_classes = y_test.shape[1]</pre>
<ol start="2">
<li>Let's look at the distribution of the input values:</li>
</ol>
<pre style="padding-left: 60px">X_train.flatten()</pre>
<p style="padding-left: 60px">The preceding code flattens all the inputs into a single list, and hence, is of the shape <span>(47,040,000), which is the same as the <kbd>28 x 28 x X_train.shape[0]</kbd>. Let's plot the distribution of all the input values:</span></p>
<pre style="padding-left: 60px">plt.hist(X_train.flatten())<br/>plt.grid('off')<br/>plt.title('Histogram of input values')<br/>plt.xlabel('Input values')<br/>plt.ylabel('Frequency of input values')</pre>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1205 image-border" src="Images/aeb37d82-2cee-490f-b933-4cfe855c3975.png" style="width:32.83em;height:22.83em;" width="394" height="274"/></p>
<p style="padding-left: 60px">We notice that the majority of the inputs are zero (you should note that all the input images have a background that is black hence, a majority of the values are zero, which is the pixel value of the color black).</p>
<ol start="3">
<li>In this section, let's explore a scenario where we invert the colors, in which the background is white and the letters are written in black, using the following code:</li>
</ol>
<pre style="padding-left: 60px">X_train = 1-X_train<br/>X_test = 1-X_test</pre>
<p style="padding-left: 60px">Let's plot the images:</p>
<pre style="padding-left: 60px">import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>plt.subplot(221)<br/>plt.imshow(X_train[0].reshape(28,28), cmap=plt.get_cmap('gray'))<br/>plt.grid('off')<br/>plt.subplot(222)<br/>plt.imshow(X_train[1].reshape(28,28), cmap=plt.get_cmap('gray'))<br/>plt.grid('off')<br/>plt.subplot(223)<br/>plt.imshow(X_train[2].reshape(28,28), cmap=plt.get_cmap('gray'))<br/>plt.grid('off')<br/>plt.subplot(224)<br/>plt.imshow(X_train[3].reshape(28,28), cmap=plt.get_cmap('gray'))<br/>plt.grid('off')<br/>plt.show()</pre>
<p>They will look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1206 image-border" src="Images/41d3b675-5acf-4a2a-830c-d911857cb189.png" style="width:27.75em;height:22.75em;" width="309" height="253"/></p>
<p style="padding-left: 60px">The histogram of the resulting images now looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1207 image-border" src="Images/c8f2c774-2d0e-47dd-9324-7e199534c934.png" style="width:36.58em;height:25.17em;" width="397" height="274"/></p>
<p style="padding-left: 60px">You should notice that the majority of the input values now have a value of one.</p>
<ol start="4">
<li>Let's go ahead and build our model using the same model architecture that we built in the S<em>caling input dataset</em> section:</li>
</ol>
<pre style="padding-left: 60px">model = Sequential()<br/>model.add(Dense(1000,input_dim=784,activation='relu'))<br/>model.add(Dense(10, activation='softmax'))<br/>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32, verbose=1)</pre>
<ol start="5">
<li>Plot the training and test accuracy and loss values over different epochs (the code to generate the following plots remains the same as the one we used in step 8 of the <em>Training a vanilla neural network</em> recipe):</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1208 image-border" src="Images/bf81c069-33e3-44f5-905f-76c8bcb34578.png" style="width:36.25em;height:28.50em;" width="396" height="312"/></p>
<p>We should note that model accuracy has now fallen to ~97%, compared to ~98% when using the same model for the same number of epochs and batch size, but on a dataset that has a majority of zeros (and not a majority of ones). Additionally, the model achieved an accuracy of 97%, considerably more slowly than in the scenario where the majority of the input pixels are zero.</p>
<p>The intuition for the decrease in accuracy, when the majority of the data points are non-zero is that, when the majority of pixels are zero, the model's task was easier (less weights had to be fine-tuned), as it had to make predictions based on a few pixel values (the minority that had a pixel value greater than zero). However, a higher number of weights need to be fine-tuned to make predictions when a majority of the data points are non-zero.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Impact of batch size on model accuracy</h1>
                </header>
            
            <article>
                
<p>In the previous sections, for all the models that we have built, we considered a batch size of 32. In this section, we will try to understand the impact of varying the batch size on accuracy.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>To understand the reason batch size has an impact on model accuracy, let's contrast two scenarios where the total dataset size is 60,000:</p>
<ul>
<li>Batch size is 30,000</li>
<li>Batch size is 32</li>
</ul>
<p>When the batch size is large, the number of times of weight update per epoch is small, when compared to the scenario when the batch size is small.</p>
<p>The reason for a high number of weight updates per epoch when the batch size is small is that less data points are considered to calculate the loss value. This results in more batches per epoch, as, loosely, in an epoch, you would have to go through all the training data points in a dataset.</p>
<p>Thus, the lower the batch size, the better the accuracy for the same number of epochs. However, while deciding the number of data points to be considered for a batch size, you should also ensure that the batch size is not too small so that it might overfit on top of a small batch of data.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In the previous recipe, we built a model with a batch size of 32. In this recipe, we will go ahead and implement the model to contrast the scenario between a low batch size and a high batch size for the same number of epochs:</p>
<ol>
<li>Preprocess the dataset and fit the model as follows:</li>
</ol>
<pre style="padding-left: 60px">(X_train, y_train), (X_test, y_test) = mnist.load_data()<br/>num_pixels = X_train.shape[1] * X_train.shape[2]<br/>X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')<br/>X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')<br/>X_train = X_train/255<br/>X_test = X_test/255<br/>y_train = np_utils.to_categorical(y_train)<br/>y_test = np_utils.to_categorical(y_test)<br/>num_classes = y_test.shape[1]<br/>model = Sequential()<br/>model.add(Dense(1000,input_dim=784,activation='relu'))<br/>model.add(Dense(10, activation='softmax'))<br/>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=30000, verbose=1)</pre>
<p style="padding-left: 60px">Note that the only change in code is the <kbd>batch_size</kbd> parameter in the model fit process.</p>
<ol start="2">
<li>Plot the training and test accuracy and loss values over different epochs (the code to generate the following plots remains the same as the code we used in step 8 of the <em>Training a vanilla neural network</em> recipe):</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1209 image-border" src="Images/8045caed-04a4-41b0-a58d-5aaafba99a70.png" style="width:33.92em;height:26.67em;" width="407" height="320"/></p>
<p>In the preceding scenario, you should notice that the model accuracy reached ~98% at a much later epoch, when compared to the model accuracy it reached when the batch size was smaller.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>You should notice that the accuracy is much lower initially and that it catches up only after a considerable number of epochs are run. The reason for a low accuracy during initial epochs is that the number of times of weight update is much lower in this scenario when compared to the previous scenario (where the batch size was smaller).</p>
<p>In this scenario, when the batch size is 30,000, and the total dataset size is 60,000, when we run the model for 500 epochs, the weight updates happens at epochs * (dataset size/ batch size) = 500 * (60,000/30,000) = 1,000 times.</p>
<p>In the previous scenario, the weight updates happens at 500 * (60,000/32) = 937,500 times.</p>
<p>Hence, the lower the batch size, the more times the weights get updated and, generally, the better the accuracy is for the same number of epochs.</p>
<p>At the same time, you should be careful not to have too few examples in the batch size, which might result in not only having a very long training time, but also a potential overfitting scenario.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building a deep neural network to improve network accuracy</h1>
                </header>
            
            <article>
                
<p>Until now, we have looked at model architectures where the neural network has only one hidden layer between the input and the output layers. In this section, we will look at the neural network where there are multiple hidden layers (and hence a deep neural network)<span>, while reusing the same MNIST training and test dataset that were scaled</span>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>A deep neural network means that there are multiple hidden layers connecting the input to the output layer. Multiple hidden layers ensure that the neural network learns a complex non-linear relation between the input and output, which a simple neural network cannot learn (due to a limited number of hidden layers).</p>
<p>A typical deep feedforward neural network looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1079 image-border" src="Images/5b116810-f567-4e06-9f94-efdbe6f013e0.png" style="width:28.58em;height:22.33em;" width="949" height="742"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>A deep neural network architecture is built by adding multiple hidden layers between input and output layers, as follows:</p>
<ol>
<li>Load the dataset and scale it:</li>
</ol>
<pre style="padding-left: 60px">(X_train, y_train), (X_test, y_test) = mnist.load_data()<br/>num_pixels = X_train.shape[1] * X_train.shape[2]<br/>X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')<br/>X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')<br/>X_train = X_train/255<br/>X_test = X_test/255<br/>y_train = np_utils.to_categorical(y_train)<br/>y_test = np_utils.to_categorical(y_test)<br/>num_classes = y_test.shape[1]</pre>
<ol start="2">
<li>Build a model with multiple hidden layers connecting the input and output layers:</li>
</ol>
<pre style="padding-left: 60px">model = Sequential()<br/>model.add(Dense(1000, input_dim=784, activation='relu'))<br/>model.add(Dense(1000,activation='relu'))<br/>model.add(Dense(1000,activation='relu'))<br/>model.add(Dense(10,  activation='softmax'))</pre>
<p style="padding-left: 60px">The preceding model architecture results in a model summary, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/f36020ac-67c3-494d-a646-f863b71527dc.png" style="width:41.67em;height:20.75em;" width="523" height="261"/></p>
<p style="padding-left: 60px">Note that the preceding model results in a higher number of parameters, as a result of deep architectures (as there are multiple hidden layers in the model).</p>
<ol start="3">
<li>Now that the model is set up, let's compile and fit the model:</li>
</ol>
<pre style="padding-left: 60px">model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])<br/>history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=250, batch_size=1024, verbose=1)</pre>
<p style="padding-left: 60px">The preceding results in a model with an accuracy of 98.6%, which is slightly better than the accuracies we observed with the model architectures that we saw earlier. The training and test loss and accuracy are as follows <span>(the code to generate the plots in the following diagram remains the same as the code we used in step 8 of the <em>Training a vanilla neural network</em> recipe)</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1210 image-border" src="Images/f11ab003-caee-4343-8b8b-eadce1391e26.png" style="width:34.33em;height:26.67em;" width="405" height="315"/></p>
<p>Note that, in this scenario, there is a considerable gap between training and test loss, indicating that the deep feedforward neural network specialized on training data. Again, in the sections on overfitting, we will learn about ways to avoid overfitting on training data.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Varying the learning rate to improve network accuracy</h1>
                </header>
            
            <article>
                
<p>So far, in the previous recipes, we used the default learning rate of the Adam optimizer, which is 0.0001.</p>
<p>In this section, we will manually set the learning rate to a higher number and see the impact of changing the learning rate on model accuracy<span>, while reusing the same MNIST training and test dataset that were scaled in the previous recipes</span>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In the previous chapter on building feedforward neural networks, we learned that the learning rate is used in updating weights and the change in weight is proportional to the amount of loss reduction.</p>
<p>Additionally, a change in a weight's value is equal to the decrease in loss multiplied by the learning rate. Hence, the lower the learning rate, the lower the change in the weight value, and vice versa.</p>
<p>You can essentially think of the weight values as a continuous spectrum where the weights are initialized randomly. When the change in the weight values is great, there is a good possibility that the various weight values in the spectrum are not considered. However, when the change in the weight value is slight, the weights might achieve a global minima, as more possible weight values could be considered.</p>
<p>To understand this further, let's consider the toy example of fitting the <em>y = 2x</em> line where the initial weight value is 1.477 and the initial bias value is zero. The feedforward and back propagation functions will remain the same as we saw in the previous chapter:</p>
<pre>def feed_forward(inputs, outputs, weights):<br/>     hidden = np.dot(inputs,weights[0])<br/>     out = hidden+weights[1]<br/>     squared_error = (np.square(out - outputs))<br/>     return squared_error<br/><br/>def update_weights(inputs, outputs, weights, epochs, lr): <br/>    for epoch in range(epochs):<br/>        org_loss = feed_forward(inputs, outputs, weights)<br/>        wts_tmp = deepcopy(weights)<br/>        wts_tmp2 = deepcopy(weights)<br/>        for ix, wt in enumerate(weights):<br/>            print(ix, wt)<br/>            wts_tmp[-(ix+1)] += 0.0001<br/>            loss = feed_forward(inputs, outputs, wts_tmp)<br/>            del_loss = np.sum(org_loss - loss)/(0.0001*len(inputs))<br/>            wts_tmp2[-(ix+1)] += del_loss*lr<br/>            wts_tmp = deepcopy(weights)<br/>        weights = deepcopy(wts_tmp2)<br/>    return wts_tmp2</pre>
<p class="mce-root">Note that the only change from the backward propagation function that we saw in the previous chapter is that we are passing the learning rate as a parameter in the preceding function. The value of weight when the learning rate is 0.01 over a different number of epochs is as follows:</p>
<pre>w_val = []<br/>b_val = []<br/>for k in range(1000):<br/>     w_new, b_new = update_weights(x,y,w,(k+1),0.01)<br/>     w_val.append(w_new)<br/>     b_val.append(b_new)</pre>
<p>The plot of the change in weight over different epochs can be obtained using the following code:</p>
<pre>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>plt.plot(w_val)<br/>plt.title('Weight value over different epochs when learning rate is 0.01')<br/>plt.xlabel('epochs')<br/>plt.ylabel('weight value')<br/>plt.grid('off')</pre>
<p>The output of the preceding code is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1211 image-border" src="Images/ff23c179-fb57-4c71-905c-512353516f30.png" style="width:33.50em;height:23.50em;" width="402" height="282"/></p>
<p>In a similar manner, the value of the weight over a different number of epochs when the learning rate is 0.1 is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1218 image-border" src="Images/226c0088-34ef-46bc-a399-46ce411fd70e.png" style="width:33.00em;height:23.17em;" width="396" height="278"/></p>
<p>This screenshot shows the value of the weight over a different number of epochs when the learning rate is 0.5:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1219 image-border" src="Images/c6883c3e-8044-4ba1-97cc-34943ece045f.png" style="width:33.33em;height:23.00em;" width="400" height="276"/></p>
<p>Note that, in the preceding scenario, there was a drastic change in the weight values initially, and the 0.1 learning rate converged, while the 0.5 learning rate did not converge to an optimal solution, and thus became stuck in a local minima.</p>
<p>In the case when the learning rate was 0.5, given the weight value was stuck in a local minima, it could not reach the optimal value of two.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Now that we understand how learning rate influences the output values, let's see the impact of the learning rate in action on the MNIST dataset we saw earlier, where we keep the same model architecture but will only be changing the learning rate parameter.</p>
<p>Note that we will be using the same data-preprocessing steps as those of step 1 and step 2 in the <em>Scaling input dataset</em> recipe.</p>
<p>Once we have the dataset preprocessed, we vary the learning rate of the model by specifying the optimizer in the next step:</p>
<ol start="1">
<li>We change the learning rate as follows:</li>
</ol>
<pre style="padding-left: 60px">from keras import optimizers<br/>adam=optimizers.Adam(lr=0.01)</pre>
<p style="padding-left: 60px">With the preceding code, we have initialized the Adam optimizer with a specified learning rate of 0.01.</p>
<ol start="2">
<li>We build, compile, and fit the model as follows:</li>
</ol>
<pre style="padding-left: 60px">model = Sequential()<br/>model.add(Dense(1000, input_dim=784, activation='relu'))<br/>model.add(Dense(10,  activation='softmax'))<br/>model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy']) <br/><br/>history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=1024, verbose=1)</pre>
<p style="padding-left: 60px">The accuracy of the preceding network is ~90% at the end of 500 epochs. Let's have a look at how loss function and accuracy vary over a different number of epochs <span>(the code to generate the plots in the following diagram remains the same as the code we used in step 8 of the <em>Training a vanilla neural network</em> recipe):</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1220 image-border" src="Images/1b584542-f5c3-48b0-9af0-1855dea2f4c7.png" style="width:38.17em;height:30.50em;" width="402" height="322"/></p>
<p>Note that when the learning rate was high (0.01 in the current scenario) compared to 0.0001 (in the scenario considered in the <em>Scaling input dataset</em> recipe), the loss decreased less smoothly when compared to the low-learning-rate model.</p>
<p>The low-learning-rate model updates the weights slowly, thereby resulting in a smoothly reducing loss function, as well as a high accuracy, which was achieved slowly over a higher number of epochs.</p>
<p>Alternatively, the step changes in loss values when the learning rate is higher are due to the loss values getting stuck in a local minima until the weight values change to optimal values. A lower learning rate gives a better possibility of arriving at the optimal weight values faster, as the weights are changed slowly, but steadily, in the right direction.</p>
<p>In a similar manner, let's explore the network accuracy when the learning rate is as high as 0.1:</p>
<pre>from keras import optimizers<br/>adam=optimizers.Adam(lr=0.1)<br/><br/>model = Sequential()<br/>model.add(Dense(1000, input_dim=784, activation='relu'))<br/>model.add(Dense(10,  activation='softmax'))<br/>model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])<br/>history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=1024, verbose=1)</pre>
<p>It is to be noted that the loss values could not decrease much further, as the learning rate was high; that is, potentially the weights got stuck in a local minima:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1221 image-border" src="Images/2c8659c9-e4e3-4524-9dfd-5f6661fd3b96.png" style="width:34.00em;height:26.75em;" width="408" height="321"/></p>
<p>Thus, it is, in general, a good idea to set the learning rate to a low value and let the network learn over a high number of epochs.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Varying the loss optimizer to improve network accuracy</h1>
                </header>
            
            <article>
                
<p>So far, in the previous recipes, we considered the loss optimizer to be the Adam optimizer. However, there are multiple other variants of optimizers, and a change in the optimizer is likely to impact the speed with which the model learns to fit the input and the output.</p>
<p>In this recipe, we will understand the impact of changing the optimizer on model accuracy.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>To understand the impact of varying the optimizer on network accuracy, let's contrast the scenario laid out in previous sections (which was the Adam optimizer) with using a <strong>stochastic gradient descent optimizer</strong> in this section, while<span> </span><span>reusing the same MNIST training and test datasets that were scaled (t</span>he same data-preprocessing steps as those of step 1 and step 2 in the <em>Scaling the dataset</em> recipe):</p>
<pre>model = Sequential()<br/>model.add(Dense(1000, input_dim=784, activation='relu'))<br/>model.add(Dense(10,  activation='softmax'))<br/>model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])<br/>history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, verbose=1)</pre>
<p>Note that when we used the stochastic gradient descent optimizer in the preceding code, the final accuracy after 100 epochs is ~98% <span>(the code to generate the plots in the following diagram remains the same as the code we used in step 8 of the <em>Training a vanilla neural network</em> recipe):</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1222 image-border" src="Images/38cdae47-275b-46d5-b662-fc29d5f85df3.png" style="width:33.25em;height:26.42em;" width="399" height="317"/></p>
<p>However, we should also note that the model achieved the high accuracy levels much more slowly when compared to the model that used Adam optimization.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Some of the other loss optimizers available are as follows:</p>
<ul>
<li>RMSprop</li>
<li>Adagrad</li>
<li>Adadelta</li>
<li>Adamax</li>
<li>Nadam</li>
</ul>
<p>You can learn more about the various optimizers here: <a href="https://keras.io/optimizers/" target="_blank">https://keras.io/optimizers/</a>.</p>
<p>Additionally, you can find the source code of each optimizer here: <a href="https://github.com/keras-team/keras/blob/master/keras/optimizers.py" target="_blank">https://github.com/keras-team/keras/blob/master/keras/optimizers.py</a>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Understanding the scenario of overfitting</h1>
                </header>
            
            <article>
                
<p>In some of the previous recipes, we have noticed that the training accuracy is ~100%, while test accuracy is ~98%, which is a case of overfitting on top of a training dataset. Let's gain an intuition of the delta between the training and the test accuracies.</p>
<p>To understand the phenomenon resulting in overfitting, let's contrast two scenarios where we compare the t<span>raining and test accuracies along with a histogram of the weights</span>:</p>
<ul>
<li>Model is run for five epochs</li>
<li>Model is run for 100 epochs</li>
</ul>
<p>The comparison-of-accuracy metric between training and test datasets between the two scenarios is as follows:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p><strong>Scenario</strong></p>
</td>
<td>
<p><strong>Training dataset</strong></p>
</td>
<td>
<p><strong>Test dataset</strong></p>
</td>
</tr>
<tr>
<td>
<p>5 epochs</p>
</td>
<td>
<p>97.59%</p>
</td>
<td>
<p>97.1%</p>
</td>
</tr>
<tr>
<td>
<p>100 epochs</p>
</td>
<td>
<p>100%</p>
</td>
<td>
<p>98.28%</p>
</td>
</tr>
</tbody>
</table>
<p>Once we plot the histogram of weights that are connecting the hidden layer to the output layer, we will notice that the 100-epochs scenario has a higher spread of weights when compared to the five-epochs scenario:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1224 image-border" src="Images/a6f6b04b-293f-4fb2-a76f-211d40b81f0c.png" style="width:35.08em;height:24.50em;" width="397" height="277"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1225 image-border" src="Images/7c5f76b7-f483-4666-9fc5-ea98573fc588.png" style="width:35.33em;height:25.08em;" width="389" height="276"/></p>
<p>From the preceding pictures, you should note that the 100 epochs scenario had a higher dispersion of weight values when compared to the five-epochs scenario. This is because of the higher amount of opportunity that the model had to overfit on top of the training dataset when the model is run for 100-epochs, when compared to when the model is run for five epochs, as the number of weight updates in the 100-epochs scenario is higher than the number of weight updates in the five-epochs scenario.</p>
<p>A high value of weight (along with a difference in the training and test dataset) is a good indication of a potential over-fitting of the model and/or a potential opportunity to scale input/weights to increase the accuracy of the model.</p>
<p>Additionally, also note that a neural network can have hundreds of thousands of weights (and millions in certain architectures) that need to be adjusted, and thus, there is always a chance that one or the other weight can get updated to a very high number to fine-tune for one outlier row of the dataset.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Overcoming over-fitting using regularization</h1>
                </header>
            
            <article>
                
<p>In the previous section, we established that a high weight magnitude is one of the reasons for over-fitting. In this section, we will look into ways to get around the problem of over-fitting, such as penalizing for high weight magnitude values.</p>
<p><strong>Regularization</strong> gives a penalty for having a high magnitude of weights in model. L1 and L2 regularizations are among the most commonly used regularization techniques and work as follows:</p>
<p class="CDPAlignLeft CDPAlign">L2 regularization minimizes the weighted sum of squares of weights at the specified layer of the neural network, in addition to minimizing the loss function (which is the sum of squared loss in the following formula):</p>
<p class="CDPAlignCenter CDPAlign"><span><img class="fm-editor-equation" src="Images/64a98b14-388a-4cbb-b604-30c56e739681.png" style="width:22.00em;height:2.17em;" width="3050" height="290"/></span></p>
<p>Where <img class="fm-editor-equation" src="Images/f9fef716-fe17-45b4-8abf-6a669c5d3cc6.png" style="width:0.83em;height:1.08em;" width="130" height="170"/><span> </span>is the<span> </span>weightage<span> </span>associated with the regularization term and is a hyperparameter that needs to be tuned, <em>y</em> is the predicted value of <img class="fm-editor-equation" src="Images/28abf306-b171-4bd7-b8de-1bdffe82639c.png" style="width:0.67em;height:1.17em;" width="100" height="170"/>, and <img class="fm-editor-equation" src="Images/f552cdd0-2311-4b72-8ec6-29e8c99fcf69.png" style="width:1.42em;height:1.08em;" width="200" height="150"/> is the weight values across all the layers of the model.</p>
<p><span>L1 regularization minimizes the weighted sum of absolute values of weights at the specified layer of the neural network in addition to minimizing the loss function (which is the sum of the squared loss in the following formula):</span></p>
<p class="CDPAlignCenter CDPAlign"><span><img class="fm-editor-equation" src="Images/9e52d2e2-aca1-4e06-a3af-007deacb2468.png" style="width:23.42em;height:2.17em;" width="3130" height="290"/></span><span>.</span></p>
<p>This way, we ensure that weights do not get customized for extreme cases in the training dataset only (and thus, not generalizing on the test data).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it</h1>
                </header>
            
            <article>
                
<p>L1/L2 regularization is implemented in Keras, as follows:</p>
<pre>model = Sequential()<br/>model.add(Dense(1000,input_dim=784,activation='relu',kernel_regularizer=l2(0.1)))model.add(Dense(10,  activation='softmax',kernel_regularizer=l2(0.1)))<br/>model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])<br/>history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=1024, verbose=1)</pre>
<p>Note that the preceding involves invoking an additional hyperparameter—<kbd>kernel_regularizer</kbd><span>—</span>and then specifying whether it is an L1/L2 regularization. Furthermore, we also specify the lambda value that gives the weight to regularization.</p>
<p>We notice that, post regularization, the training dataset accuracy does not happen to be at ~100%, while the test data accuracy is at 98%. The histogram of weights post-L2 regularization is visualized in the next graph.</p>
<p>The weights of connecting the hidden layer to the output layer are extracted as follows:</p>
<pre>model.get_weights()[0].flatten()</pre>
<p>Once the weights are extracted, they are plotted as follows:</p>
<pre>plt.hist(model.get_weights()[0].flatten())</pre>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1226 image-border" src="Images/e672e37d-2059-4520-9564-12ee59698144.png" style="width:34.08em;height:23.33em;" width="409" height="280"/></p>
<p>We notice that the majority of weights are now much closer to zero when compared to the previous scenario, thus presenting a case to avoid the overfitting issue. We would see a similar trend in the case of L1 regularization.</p>
<p>Notice that the weight values when regularization exists are much lower when compared to the weight values when regularization is performed.</p>
<p>Thus, the L1 and L2 regularizations help us to avoid the overfitting issue on top of the training dataset.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Overcoming overfitting using dropout</h1>
                </header>
            
            <article>
                
<p>In the previous section of overcoming overfitting using regularization, we used L1/ L2 regularization as a means to avoid overfitting. In this section, we will use another tool that is helpful to achieve the same—<strong>dropout</strong>.</p>
<p>Dropout can be considered a way in which only a certain percentage of weights get updated, while the others do not get updated in a given iteration of weight updates. This way, we are in a position where not all weights get updated in a weight update process, thus avoiding certain weights to achieve a very high magnitude when compared to others:</p>
<pre>model = Sequential()<br/>model.add(Dense(1000, input_dim=784, activation='relu'))<br/>model.add(Dropout(0.75))<br/>model.add(Dense(10,  activation='softmax'))<br/>model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])<br/>history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=1024, verbose=1)</pre>
<p>In the preceding code, we have given a dropout of 0.75; that is, randomly, 75% of weights do not get updated in a certain weight update iteration.</p>
<p>The preceding would result in the gap between the training and test accuracy being not as high as it is when the model was built without dropout in the previous scenario, where the spread of weights was higher.</p>
<p>Note the histogram of weights of the first layer now:</p>
<pre>plt.hist(model.get_weights()[-2].flatten())</pre>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1227 image-border" src="Images/64d666de-bbb5-4c96-94d6-79946b4e26a6.png" style="width:30.17em;height:20.67em;" width="400" height="275"/></p>
<p>Note that in the preceding scenario, the frequency count of weights that are beyond 0.2 or -0.2 is less when compared to the 100-epochs scenario.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Speeding up the training process using batch normalization</h1>
                </header>
            
            <article>
                
<p>In the previous section on the scaling dataset, we learned that optimization is slow when the input data is not scaled (that is, it is not between zero and one). </p>
<p>The hidden layer value could be high in the following scenarios:</p>
<ul>
<li>Input data values are high</li>
<li>Weight values are high</li>
<li>The multiplication of weight and input are high</li>
</ul>
<p>Any of these scenarios can result in a large output value on the hidden layer.</p>
<p>Note that the hidden layer is the input layer to output layer. Hence, the phenomenon of high input values resulting in a slow optimization holds true when hidden layer values are large as well.</p>
<p><strong>Batch normalization</strong> comes to the rescue in this scenario. We have already learned that, when input values are high, we perform scaling to reduce the input values. Additionally, we have learned that scaling can also be performed using a different method, which is to subtract the mean of the input and divide it by the standard deviation of the input. Batch normalization performs this method of scaling.</p>
<p>Typically, all values are scaled using the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/bc09c4fd-85ae-46d7-97ac-2dc608a9c27f.png" style="width:15.75em;height:3.50em;" width="2490" height="540"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/e4844bdd-acf2-4e1d-b5f7-f8288413842a.png" style="width:20.75em;height:3.25em;" width="3450" height="540"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/110c9b11-033b-4bfc-9b4f-445cc803a93e.png" style="width:8.83em;height:4.50em;" width="1200" height="610"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/aa3c6535-7868-42c7-bd2f-4056453df216.png" style="width:7.42em;height:1.42em;" width="1050" height="200"/></p>
<p><span>Notice that </span><em>γ</em><span> and </span><em>β</em><span> are </span>learned during training,<span> along with the original parameters of the network.</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In code, batch normalization is applied as follows:</p>
<p><span>Note that we will be using the same data-preprocessing steps as those we used in step 1 and step 2 in the <em>Scaling the input dataset</em> recipe.</span></p>
<ol>
<li>Import the <kbd>BatchNormalization</kbd> method as follows:</li>
</ol>
<pre style="padding-left: 60px">from keras.layers.normalization import BatchNormalization</pre>
<ol start="2">
<li>Instantiate a model and build the same architecture as we built when using the regularization technique. The only addition is that we perform batch normalization in a hidden layer:</li>
</ol>
<pre style="padding-left: 60px">model = Sequential()<br/>model.add(Dense(1000, input_dim=784,activation='relu', kernel_regularizer = l2(0.01)))<br/>model.add(BatchNormalization())<br/>model.add(Dense(10, activation='softmax', kernel_regularizer = l2(0.01)))</pre>
<ol start="3">
<li>Build, compile, and fit the model as follows:</li>
</ol>
<pre style="padding-left: 60px">from keras.optimizers import Adam<br/>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=1024, verbose=1)</pre>
<p>The preceding results in training that is much faster than when there is no batch normalization, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1228 image-border" src="Images/2f5f1e2c-8a0d-45df-8c1a-d1a7918628b9.png" style="width:34.75em;height:27.50em;" width="404" height="319"/></p>
<p>The previous graphs show the training and test loss and accuracy when there is no batch normalization, but only regularization. The following graphs show the training and test loss and accuracy with both regularization and batch normalization:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1229 image-border" src="Images/32d51718-2639-441a-8155-bd78778bed39.png" style="width:36.83em;height:29.25em;" width="409" height="325"/></p>
<p>Note that, in the preceding two scenarios, we see much faster training when we perform batch normalization (test dataset accuracy of ~97%) than compared to when we don't (test dataset accuracy of ~91%).</p>
<p>Thus, batch normalization results in much quicker training.</p>


            </article>

            
        </section>
    </div>



  </body></html>