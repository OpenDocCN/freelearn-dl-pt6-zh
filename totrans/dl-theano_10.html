<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;10.&#xA0;Predicting Times Sequences with Advanced RNN" id="2RHM01-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10" class="calibre1"/>Chapter 10. Predicting Times Sequences with Advanced RNN</h1></div></div></div><p class="calibre8">This chapter covers advanced techniques for recurrent neural networks.</p><p class="calibre8">The techniques seen in <a class="calibre1" title="Chapter 2. Classifying Handwritten Digits with a Feedforward Network" href="part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 2</a>, <span class="strong"><em class="calibre12">Classifying Handwritten Digits with a Feedforward Network</em></span>, for feedforward networks, such as going deeper with more layers, or adding a dropout layer, have been more challenging for recurrent networks and require some new design principles.</p><p class="calibre8">Since adding new layers increases the vanishing/exploding gradient issue, a new technique based on identity connections as for <a class="calibre1" title="Chapter 7. Classifying Images with Residual Networks" href="part0075_split_000.html#27GQ61-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 7</a>, <span class="strong"><em class="calibre12">Classifying Images with Residual Networks</em></span> has proved to provide state-of-the-art results.</p><p class="calibre8">The topics covered are:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Variational RNN</li><li class="listitem">Stacked RNN</li><li class="listitem">Deep Transition RNN</li><li class="listitem">Highway connections and their application to RNN</li></ul></div></div>

<div class="book" title="Chapter&#xA0;10.&#xA0;Predicting Times Sequences with Advanced RNN" id="2RHM01-ccdadb29edc54339afcb9bdf9350ba6b">
<div class="book" title="Dropout for RNN"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch10lvl1sec88" class="calibre1"/>Dropout for RNN</h1></div></div></div><p class="calibre8">The application <a id="id382" class="calibre1"/>of dropout inside neural networks has long been a subject of research, since the naïve application of dropout to the recurrent connection introduced lots more instability and difficulties to training the RNN.</p><p class="calibre8">A solution <a id="id383" class="calibre1"/>has been discovered, derived from the variational <span class="strong"><strong class="calibre2">Bayes Network</strong></span> theory. The resulting idea is very simple and consists of preserving the same dropout mask for the whole sequence on which the RNN is training, as shown in the following picture, and generating a new dropout mask at each new sequence:</p><div class="mediaobject"><img src="../images/00179.jpeg" alt="Dropout for RNN" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Such a technique is called <span class="strong"><strong class="calibre2">variational RNN.</strong></span> For the connections that have the same arrows <a id="id384" class="calibre1"/>in the preceding figure, we'll keep the noise mask constant for the all sequence.</p><p class="calibre8">For <a id="id385" class="calibre1"/>that purpose, we'll introduce the symbolic variables <code class="email">_is_training</code> and <code class="email">_noise_x</code> to add a random (variational) noise (dropout) to input, output, and recurrent connection during training:</p><div class="informalexample"><pre class="programlisting">_is_training <span class="strong"><strong class="calibre2">=</strong></span> T.iscalar('is_training')
_noise_x <span class="strong"><strong class="calibre2">=</strong></span> T.matrix('noise_x')
inputs <span class="strong"><strong class="calibre2">=</strong></span> apply_dropout(_is_training, inputs, T.shape_padright(_noise_x.T))</pre></div></div></div>
<div class="book" title="Deep approaches for RNN" id="2SG6I1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10lvl1sec89" class="calibre1"/>Deep approaches for RNN</h1></div></div></div><p class="calibre8">The core <a id="id386" class="calibre1"/>principle of deep learning to improve the representative power of a network is to add more layers. For RNN, two approaches to increase the number of layers are possible:</p><div class="book"><ul class="itemizedlist"><li class="listitem">The first one is known as <span class="strong"><strong class="calibre2">stacking</strong></span> or <span class="strong"><strong class="calibre2">stacked recurrent network</strong></span>, where the output of the hidden layer of a first recurrent net is used as input to a second recurrent net, and so on, with as many recurrent networks on top of each other:<div class="mediaobject"><img src="../images/00180.jpeg" alt="Deep approaches for RNN" class="calibre9"/></div><p class="calibre27"> </p></li></ul></div><p class="calibre8">For a <a id="id387" class="calibre1"/>depth <span class="strong"><em class="calibre12">d</em></span> and <span class="strong"><em class="calibre12">T</em></span> time steps, the maximum number of connections <a id="id388" class="calibre1"/>between input and output is <span class="strong"><em class="calibre12">d + T – 1</em></span>:</p><div class="book"><ul class="itemizedlist"><li class="listitem">The second approach is the <span class="strong"><strong class="calibre2">deep transition network</strong></span>, consisting of adding more layers to the recurrent connection:<div class="mediaobject"><img src="../images/00181.jpeg" alt="Deep approaches for RNN" class="calibre9"/><div class="caption"><p class="calibre29">Figure 2</p></div></div><p class="calibre27"> </p></li></ul></div><p class="calibre8">In this case, the maximum number of connections between input and output is <span class="strong"><em class="calibre12">d x T</em></span>, which has been proved to be a lot more powerful.</p><p class="calibre8">Both approaches provide better results.</p><p class="calibre8">However, in the second approach, as the number of layers increases by a factor, the training becomes much more complicated and unstable since the signal fades or explodes a lot faster. We'll address this problem later by tackling the principle of recurrent highway <a id="id389" class="calibre1"/>connections.</p><p class="calibre8">First, as usual, sequences of words, represented as an array of index values in the vocabulary, and of dimension (<code class="email">batch_size, num_steps</code>), are embedded into an input tensor of dimension (<code class="email">num_steps, batch_size, hidden_size</code>):</p><div class="informalexample"><pre class="programlisting">embedding <span class="strong"><strong class="calibre2">=</strong></span> shared_uniform(( config.vocab_size,config.hidden_size), config.init_scale)
params <span class="strong"><strong class="calibre2">=</strong></span> [embedding]
inputs <span class="strong"><strong class="calibre2">=</strong></span> embedding[_input_data.T]</pre></div><p class="calibre8">The symbolic input variable <code class="email">_lr</code> enables the decrease of the learning rate during training:</p><div class="informalexample"><pre class="programlisting">_lr <span class="strong"><strong class="calibre2">=</strong></span> theano.shared(cast_floatX(config.learning_rate), 'lr')</pre></div><p class="calibre8">Let's begin with the first approach, the stacked recurrent networks.</p></div>
<div class="book" title="Stacked recurrent networks"><div class="book" id="2TEN42-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10lvl1sec90" class="calibre1"/>Stacked recurrent networks</h1></div></div></div><p class="calibre8">To stack <a id="id390" class="calibre1"/>recurrent networks, we connect the hidden layer of the following recurrent network, to the input of the preceding recurrent network:</p><div class="mediaobject"><img src="../images/00182.jpeg" alt="Stacked recurrent networks" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">When the number of layers is one, our implementation is a recurrent network as in the previous chapter.</p><p class="calibre8">First we implement dropout in our simple RNN model:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">def</strong></span> model(inputs, _is_training, params, batch_size, hidden_size, drop_i, drop_s, init_scale, init_H_bias):
    noise_i_for_H = get_dropout_noise((batch_size, hidden_size), drop_i)
    i_for_H = apply_dropout(_is_training, inputs, noise_i_for_H)
    i_for_H = linear.model(i_for_H, params, hidden_size, 
                   hidden_size, init_scale, <span class="strong"><strong class="calibre2">bias_init</strong></span>=init_H_bias)

    # Dropout noise for recurrent hidden state.
    noise_s <span class="strong"><strong class="calibre2">=</strong></span> get_dropout_noise((batch_size, hidden_size), drop_s)

    <span class="strong"><strong class="calibre2">def </strong></span>step(i_for_H_t, y_tm1, noise_s):
        s_lm1_for_H = apply_dropout(_is_training,y_tm1, noise_s)
        <span class="strong"><strong class="calibre2">return </strong></span>T.tanh(i_for_H_t + linear.model(s_lm1_for_H, 
                  params, hidden_size, hidden_size, init_scale))

    y_0 <span class="strong"><strong class="calibre2">=</strong></span> shared_zeros((batch_size, hidden_size), <span class="strong"><strong class="calibre2">name</strong></span>='h0')
    y, _ <span class="strong"><strong class="calibre2">=</strong></span> theano.scan(step, <span class="strong"><strong class="calibre2">sequences</strong></span>=i_for_H, <span class="strong"><strong class="calibre2">outputs_info</strong></span>=[y_0], <span class="strong"><strong class="calibre2">non_sequences</strong></span> = [noise_s])

    y_last <span class="strong"><strong class="calibre2">=</strong></span> y[-1]
    sticky_state_updates <span class="strong"><strong class="calibre2">=</strong></span> [(y_0, y_last)]

   <span class="strong"><strong class="calibre2"> return</strong></span> y, y_0, sticky_state_updates</pre></div><p class="calibre8">We do the same <a id="id391" class="calibre1"/>in our LSTM model:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">def </strong></span>model(inputs, _is_training, params, batch_size, hidden_size, drop_i, drop_s, init_scale, init_H_bias, tied_noise):
    noise_i_for_i = get_dropout_noise((batch_size, hidden_size), drop_i)
    noise_i_for_f = get_dropout_noise((batch_size, hidden_size), drop_i)<span class="strong"><strong class="calibre2"> if not</strong></span> tied_noise <span class="strong"><strong class="calibre2">else</strong></span> noise_i_for_i
    noise_i_for_c = get_dropout_noise((batch_size, hidden_size), drop_i) <span class="strong"><strong class="calibre2">if not </strong></span>tied_noise <span class="strong"><strong class="calibre2">else </strong></span>noise_i_for_i
    noise_i_for_o = get_dropout_noise((batch_size, hidden_size), drop_i) <span class="strong"><strong class="calibre2">if not</strong></span> tied_noise <span class="strong"><strong class="calibre2">else</strong></span> noise_i_for_i

    i_for_i = apply_dropout(_is_training, inputs, noise_i_for_i)
    i_for_f = apply_dropout(_is_training, inputs, noise_i_for_f)
    i_for_c = apply_dropout(_is_training, inputs, noise_i_for_c)
    i_for_o = apply_dropout(_is_training, inputs, noise_i_for_o)

    i_for_i = linear.model(i_for_i, params, hidden_size, hidden_size, init_scale, <span class="strong"><strong class="calibre2">bias_init</strong></span>=init_H_bias)
    i_for_f = linear.model(i_for_f, params, hidden_size, hidden_size, init_scale, <span class="strong"><strong class="calibre2">bias_init</strong></span>=init_H_bias)
    i_for_c = linear.model(i_for_c, params, hidden_size, hidden_size, init_scale, <span class="strong"><strong class="calibre2">bias_init</strong></span>=init_H_bias)
    i_for_o = linear.model(i_for_o, params, hidden_size, hidden_size, init_scale, <span class="strong"><strong class="calibre2">bias_init</strong></span>=init_H_bias)

    # Dropout noise for recurrent hidden state.
    noise_s = get_dropout_noise((batch_size, hidden_size), drop_s)
    <span class="strong"><strong class="calibre2">if not</strong></span> tied_noise:
      noise_s = T.stack(noise_s, get_dropout_noise((batch_size, hidden_size), drop_s),
 get_dropout_noise((batch_size, hidden_size), drop_s), get_dropout_noise((batch_size, hidden_size), drop_s))


    <span class="strong"><strong class="calibre2">def </strong></span>step(i_for_i_t,i_for_f_t,i_for_c_t,i_for_o_t, y_tm1, c_tm1, noise_s):
        noise_s_for_i = noise_s if tied_noise else noise_s[0]
        noise_s_for_f = noise_s if tied_noise else noise_s[1]
        noise_s_for_c = noise_s if tied_noise else noise_s[2]
        noise_s_for_o = noise_s if tied_noise else noise_s[3]

        s_lm1_for_i = apply_dropout(_is_training,y_tm1, noise_s_for_i)
        s_lm1_for_f = apply_dropout(_is_training,y_tm1, noise_s_for_f)
        s_lm1_for_c = apply_dropout(_is_training,y_tm1, noise_s_for_c)
        s_lm1_for_o = apply_dropout(_is_training,y_tm1, noise_s_for_o)

        i_t = T.nnet.sigmoid(i_for_i_t + linear.model(s_lm1_for_i, params, hidden_size, hidden_size, init_scale))
        f_t = T.nnet.sigmoid(i_for_o_t + linear.model(s_lm1_for_f, params, hidden_size, hidden_size, init_scale))
        c_t = f_t * c_tm1 + i_t * T.tanh(i_for_c_t + linear.model(s_lm1_for_c, params, hidden_size, hidden_size, init_scale))
        o_t = T.nnet.sigmoid(i_for_o_t + linear.model(s_lm1_for_o, params, hidden_size, hidden_size, init_scale))
        <span class="strong"><strong class="calibre2">return </strong></span>o_t * T.tanh(c_t), c_t

    y_0 = shared_zeros((batch_size,hidden_size),<span class="strong"><strong class="calibre2"> name</strong></span>='h0')
    c_0 = shared_zeros((batch_size,hidden_size), <span class="strong"><strong class="calibre2">name</strong></span>='c0')
    [y, c], _ = theano.scan(step, <span class="strong"><strong class="calibre2">sequences</strong></span>=[i_for_i,i_for_f,i_for_c,i_for_o], <span class="strong"><strong class="calibre2">outputs_info</strong></span>=[y_0,c_0], <span class="strong"><strong class="calibre2">non_sequences</strong></span> = [noise_s])

<span class="strong"><strong class="calibre2">  </strong></span>  y_last = y[-1]
    sticky_state_updates = [(y_0, y_last)]

    <span class="strong"><strong class="calibre2">return</strong></span> y, y_0, sticky_state_updates</pre></div><p class="calibre8">Running our stacked networks:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">python</strong></span> train_stacked.py --model=<span class="strong"><strong class="calibre2">rnn</strong></span>
<span class="strong"><strong class="calibre2">python</strong></span> train_stacked.py --model=<span class="strong"><strong class="calibre2">lstm</strong></span>
</pre></div><p class="calibre8">We get 15,203,150 parameters for the RNN, with 326 <span class="strong"><strong class="calibre2">words per seconds</strong></span> (WPS) on a CPU and 4,806 WPS on a GPU.</p><p class="calibre8">For LSTM, the number of parameters is 35,882,600 with a speed of 1,445 WPS on a GPU.</p><p class="calibre8">The <a id="id392" class="calibre1"/>stacked RNN do not converge, as we might have imagined: the vanishing/exploding gradient issue is increased with depth.</p><p class="calibre8">LSTM, designed to reduce such as an issue, do converge a lot better when stacked, than as a single layer.</p></div>
<div class="book" title="Deep transition recurrent network" id="2UD7M1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10lvl1sec91" class="calibre1"/>Deep transition recurrent network</h1></div></div></div><p class="calibre8">Contrary to <a id="id393" class="calibre1"/>stacked recurrent network, a deep transition recurrent network consists of increasing the depth of the network along the time direction, by adding more layers or <span class="strong"><em class="calibre12">micro-timesteps</em></span> inside the recurrent connection.</p><p class="calibre8">To illustrate this, let us come back to the definition of a transition/recurrent connection in a recurrent network: it takes as input the previous state <span class="strong"><img src="../images/00183.jpeg" alt="Deep transition recurrent network" class="calibre23"/></span> and the input data <span class="strong"><img src="../images/00184.jpeg" alt="Deep transition recurrent network" class="calibre23"/></span> at time step <span class="strong"><em class="calibre12">t</em></span>, to predict its new state <span class="strong"><img src="../images/00185.jpeg" alt="Deep transition recurrent network" class="calibre23"/></span>.</p><p class="calibre8">In a deep transition recurrent network (figure 2), the recurrent transition is developed with more than one layer, up to a recurrency depth <span class="strong"><em class="calibre12">L</em></span>: the initial state is set to the output of the last transition:</p><div class="mediaobject"><img src="../images/00186.jpeg" alt="Deep transition recurrent network" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Furthermore, inside the transition, multiple states or steps are computed:</p><div class="mediaobject"><img src="../images/00187.jpeg" alt="Deep transition recurrent network" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">The <a id="id394" class="calibre1"/>final state is the output of the transition:</p><div class="mediaobject"><img src="../images/00188.jpeg" alt="Deep transition recurrent network" class="calibre9"/></div><p class="calibre10"> </p></div>
<div class="book" title="Highway networks design principle" id="2VBO81-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10lvl1sec92" class="calibre1"/>Highway networks design principle</h1></div></div></div><p class="calibre8">Adding <a id="id395" class="calibre1"/>more layers in the transition connections increases the vanishing or exploding gradient issue during backpropagation in long term dependency.</p><p class="calibre8">In the <a id="id396" class="calibre1"/>
<a class="calibre1" title="Chapter 4. Generating Text with a Recurrent Neural Net" href="part0051_split_000.html#1GKCM1-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 4</a>, <span class="strong"><em class="calibre12">Generating Text with a Recurrent Neural Net</em></span>, LSTM and GRU networks have been introduced as solutions to address this issue. Second order optimization techniques also help overcome this problem.</p><p class="calibre8">A more general principle, based on <span class="strong"><strong class="calibre2">identity connections</strong></span>, to improve the training in deep networks <a class="calibre1" title="Chapter 7. Classifying Images with Residual Networks" href="part0075_split_000.html#27GQ61-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 7</a>, <span class="strong"><em class="calibre12">Classifying Images with Residual Networks</em></span>, can also be applied to deep transition networks.</p><p class="calibre8">Here is the principle in theory:</p><p class="calibre8">Given an input <span class="strong"><em class="calibre12">x</em></span> to a hidden layer <span class="strong"><em class="calibre12">H</em></span> with weigh <span class="strong"><img src="../images/00189.jpeg" alt="Highway networks design principle" class="calibre23"/></span>:</p><div class="mediaobject"><img src="../images/00190.jpeg" alt="Highway networks design principle" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">A highway networks design consists of adding the original input information (with an identity layer) to the output of a layer or a group of layers, as a shortcut:</p><p class="calibre8">
<span class="strong"><em class="calibre12">y = x</em></span>
</p><p class="calibre8">Two mixing gates, the <span class="strong"><em class="calibre12">transform gate</em></span> <span class="strong"><img src="../images/00191.jpeg" alt="Highway networks design principle" class="calibre23"/></span> and the <span class="strong"><em class="calibre12">carry gate</em></span>, <span class="strong"><img src="../images/00192.jpeg" alt="Highway networks design principle" class="calibre23"/></span> learn to modulate the influence of the transformation in the hidden layer, and the amount of original information to allow to pass through:</p><div class="mediaobject"><img src="../images/00193.jpeg" alt="Highway networks design principle" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Usually, to <a id="id397" class="calibre1"/>reduce the total number of parameters in order to get faster-to-train networks, the carry gate is taken as the complementary to 1 for the transform gate:</p><div class="mediaobject"><img src="../images/00194.jpeg" alt="Highway networks design principle" class="calibre9"/></div><p class="calibre10"> </p></div>
<div class="book" title="Recurrent Highway Networks" id="30A8Q1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10lvl1sec93" class="calibre1"/>Recurrent Highway Networks</h1></div></div></div><p class="calibre8">So, let's <a id="id398" class="calibre1"/>apply the highway network design to deep transition recurrent networks, which leads to the definition of <span class="strong"><strong class="calibre2">Recurrent Highway Networks</strong></span> (<span class="strong"><strong class="calibre2">RHN</strong></span>), and predict the output <span class="strong"><img src="../images/00185.jpeg" alt="Recurrent Highway Networks" class="calibre23"/></span> given <span class="strong"><img src="../images/00183.jpeg" alt="Recurrent Highway Networks" class="calibre23"/></span> the input of the transition:</p><div class="mediaobject"><img src="../images/00186.jpeg" alt="Recurrent Highway Networks" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">The transition is built with multiple steps of highway connections:</p><div class="mediaobject"><img src="../images/00195.jpeg" alt="Recurrent Highway Networks" class="calibre9"/></div><p class="calibre10"> </p><div class="mediaobject"><img src="../images/00188.jpeg" alt="Recurrent Highway Networks" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Here the transform gate is as follows:</p><div class="mediaobject"><img src="../images/00196.jpeg" alt="Recurrent Highway Networks" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">And, to reduce the number of weights, the carry gate is taken as the complementary to the transform gate:</p><div class="mediaobject"><img src="../images/00194.jpeg" alt="Recurrent Highway Networks" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">For faster computation on a GPU, it is better to compute the linear transformation on inputs over different time steps <span class="strong"><img src="../images/00197.jpeg" alt="Recurrent Highway Networks" class="calibre23"/></span> and <span class="strong"><img src="../images/00198.jpeg" alt="Recurrent Highway Networks" class="calibre23"/></span> in a single big matrix multiplication, all-steps input matrices <span class="strong"><img src="../images/00199.jpeg" alt="Recurrent Highway Networks" class="calibre23"/></span> and <span class="strong"><img src="../images/00200.jpeg" alt="Recurrent Highway Networks" class="calibre23"/></span> at once, since the GPU will use a better <a id="id399" class="calibre1"/>parallelization, and provide these inputs to the recurrency:</p><div class="informalexample"><pre class="programlisting">y_0 = shared_zeros((batch_size, hidden_size))
y, _ = theano.scan(deep_step_fn, <span class="strong"><strong class="calibre2">sequences</strong></span> = [<span class="strong"><strong class="calibre2">i_for_H</strong></span>, i_for_T],
            <span class="strong"><strong class="calibre2">outputs_info</strong></span> = [y_0], <span class="strong"><strong class="calibre2">non_sequences</strong></span> = <span class="strong"><strong class="calibre2">[noise_s]</strong></span>)</pre></div><p class="calibre8">With a deep transition between each step:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">def </strong></span>deep_step_fn(i_for_H_t, i_for_T_t, y_tm1, noise_s):
  s_lm1 = y_tm1
  <span class="strong"><strong class="calibre2">for</strong></span> l <span class="strong"><strong class="calibre2">in</strong></span> range(transition_depth):
    <span class="strong"><strong class="calibre2">if</strong></span> l <span class="strong"><strong class="calibre2">==</strong></span> 0:
      H = T.tanh(i_for_H_t + linear(s_lm1, params, hidden_size, hidden_size, init_scale))
      Tr = T.nnet.sigmoid(i_for_T_t + linear(s_lm1, params, hidden_size, hidden_size, init_scale))
    <span class="strong"><strong class="calibre2">else</strong></span>:
      H = T.tanh(linear(s_lm1, params, hidden_size, hidden_size, init_scale, <span class="strong"><strong class="calibre2">bias_init</strong></span>=init_H_bias))
      Tr = T.nnet.sigmoid(linear(s_lm1, params, hidden_size, hidden_size, init_scale, <span class="strong"><strong class="calibre2">bias_init</strong></span>=init_T_bias))
    s_l = H <span class="strong"><strong class="calibre2">*</strong></span> Tr + s_lm1 <span class="strong"><strong class="calibre2">*</strong></span> ( 1 - Tr )
    s_lm1 = s_l
  y_t = s_l
  <span class="strong"><strong class="calibre2">return</strong></span> y_t</pre></div><p class="calibre8">The recurrent <a id="id400" class="calibre1"/>hidden state of the RHN is sticky (the last hidden state of one batch is carried over to the next batch, to be used as an initial hidden state). These states are kept in a shared variable.</p><p class="calibre8">Let's run the mode:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">python</strong></span> train_stacked.py</pre></div><p class="calibre8">The number of parameters of the stacked RHN is <span class="strong"><em class="calibre12">84,172,000</em></span>, its speed <span class="strong"><em class="calibre12">420</em></span> wps on the GPU.</p><p class="calibre8">This model is the new state-of-the-art model for recurrent neural network accuracy on texts.</p></div>
<div class="book" title="Further reading" id="318PC1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10lvl1sec94" class="calibre1"/>Further reading</h1></div></div></div><p class="calibre8">You can refer to the following topics for more insights:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><em class="calibre12">Hi</em></span><span class="strong"><em class="calibre12">ghway Networks</em></span> at: <a class="calibre1" href="https://arxiv.org/abs/1505.00387">https://arxiv.org/abs/1505.00387</a></li><li class="listitem"><span class="strong"><em class="calibre12">Depth-Gated LSTM</em></span> at: <a class="calibre1" href="https://arxiv.org/abs/1508.03790">https://arxiv.org/abs/1508.03790</a></li><li class="listitem"><span class="strong"><em class="calibre12">Learning Longer Memory in Recurrent N</em></span><span class="strong"><em class="calibre12">eural Networks</em></span> at: <a class="calibre1" href="https://arxiv.org/abs/1412.7753">https://arxiv.org/abs/1412.7753</a></li><li class="listitem"><span class="strong"><em class="calibre12">Grid Long Short-Term Memory</em></span>, Nal Kalchbrenner, Ivo Danihelka, Alex Graves</li><li class="listitem">Zilly, J, Srivastava, R, Koutnik, J, Schmidhuber, J., <span class="strong"><em class="calibre12">Recurrent Highway Networks</em></span>, 2016</li><li class="listitem">Gal, Y, <span class="strong"><em class="calibre12">A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</em></span>, 2015.</li><li class="listitem">Zaremba, W, Sutskever, I, Vinyals, O, <span class="strong"><em class="calibre12">Recurrent neural network regularization</em></span>, 2014.</li><li class="listitem">Press, O, Wolf, L, <span class="strong"><em class="calibre12">Using the Output Embedding to Improve Language Models</em></span>, 2016.</li><li class="listitem">Gated Feedback Recurrent Neural Networks: Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio 2015</li><li class="listitem">A Clockwork RNN: Jan Koutník, Klaus Greff, Faustino Gomez, Jürgen Schmidhuber 2014</li></ul></div></div>
<div class="book" title="Summary" id="3279U1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10lvl1sec95" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">A classic dropout method to improve network robustness may be applied to recurrent network sequence-wise or batch-wise to avoid instability and destruction of the recurrent transition. For example, when applied on word inputs/outputs, it is equivalent to removing the same words from the sentence, replacing them with a blank value.</p><p class="calibre8">The principle of stacking layers in deep learning to improve accuracy applies to recurrent networks that can be stacked in the depth direction without burden.</p><p class="calibre8">Applying the same principle in the transition of the recurrent nets increases the vanishing/exploding issue, but is offset by the invention of the highway networks with identity connections.</p><p class="calibre8">Advanced techniques for recurrent neural nets give state-of-the-art results in sequence prediction.</p></div></body></html>