- en: Extract, Transform, Load
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training and testing DL models requires data. Data is usually hosted on different
    distributed and remote storage systems. You need them to connect to the data sources
    and perform data retrieval so that you can start the training phase and you would
    probably need to do some preparation before feeding your model. This chapter goes
    through the phases of the **Extract**, **Transform**, **Load** (**ETL**) process
    applied to DL. It covers several use cases for which the DeepLearning4j framework
    and Spark would be used. The use cases presented here are related to batch data
    ingestion. Data streaming will be covered in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Training data ingestion through Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data ingestion from a relational database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data ingestion from a NoSQL database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data ingestion from S3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training data ingestion through Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first section of this chapter introduces the DeepLearning4j framework and
    then presents some use cases of training data ingestion from files using this
    framework along with Apache Spark.
  prefs: []
  type: TYPE_NORMAL
- en: The DeepLearning4j framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before jumping into the first example, let's quickly introduce the DeepLearning4j
    ([https://deeplearning4j.org/](https://deeplearning4j.org/)) framework. It is
    an open source (released under the Apache license 2.0 ([https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0))),
    distributed deep learning framework written for the JVM. Being integrated since
    its earliest releases with Hadoop and Spark, it takes advantage of such distributed
    computing frameworks to speed up network training. It is written in Java, so is
    compatible with any other JVM language (including Scala of course), while the
    underlying computations are written in lower level languages, such as C, C++,
    and CUDA. The DL4J API gives flexibility when composing deep neural networks.
    So it is possible to combine different network implementations as needed in a
    distributed, production-grade infrastructure on top of distributed CPUs or GPUs.
    DL4J can import neural net models from most of the major ML or DL Python frameworks
    (including TensorFlow and Caffe) via Keras ([https://keras.io/](https://keras.io/)),
    bridging the gap between the Python and the JVM ecosystems in terms of toolkits
    for data scientists in particular, but also for data engineers and DevOps. Keras
    represents the DL4J's Python API.
  prefs: []
  type: TYPE_NORMAL
- en: 'DL4J is modular. These are the main libraries that comprise this framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deeplearning4j**: The neural network platform core'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ND4J**: The NumPy ([http://www.numpy.org/](http://www.numpy.org/)) porting
    for the JVM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataVec**: A tool for ML ETL operations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keras import**: To import pre-trained Python models implemented in Keras'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Arbiter**: A dedicated library for multilayer neural networks hyperparameter
    optimization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RL4J**: The implementation of deep reinforcement learning for the JVM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are going to explore almost all of the features of DL4J and its libraries,
    starting from this chapter and across the other chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: The reference release for DL4J in this book is version 0.9.1.
  prefs: []
  type: TYPE_NORMAL
- en: Data ingestion through DataVec and transformation through Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data can come from many sources and in many types, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: Log files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tabular data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Videos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When working with neural nets, the end goal is to convert each data type into
    a collection of numerical values in a multidimensional array. Data could also
    need to be pre-processed before it can be used to train or test a net. Therefore,
    an ETL process is needed in most cases, which is a sometimes underestimated challenge
    that data scientists have to face when doing ML or DL. That's when the DL4J DataVec
    library comes to the rescue. After data is transformed through this library API,
    it comes into a format (vectors) understandable by neural networks, so DataVec
    quickly produces open standard compliant vectorized data.
  prefs: []
  type: TYPE_NORMAL
- en: DataVec supports out-of-the-box all the major types of input data (text, CSV,
    audio, video, image) with their specific input formats. It can be extended for
    specialized input formats not covered by the current release of its API. You can
    think about the DataVec input/output format system as the same way Hadoop MapReduce uses
    `InputFormat` implementations to determine the logical *InputSplits* and the `RecordReaders`
    implementation to use. It also provides `RecordReaders` to serialize data. This
    library also includes facilities for feature engineering, data cleanup, and normalization.
    They work with both static data and time series. All of the available functionalities
    can be executed on Apache Spark through the DataVec-Spark module.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to know more about the Hadoop MapReduce classes mentioned previously,
    you can have a look at the following official online Javadocs:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Class name** | **Link** |'
  prefs: []
  type: TYPE_TB
- en: '| `InputFormat` | [https://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/mapred/InputFormat.html](https://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/mapred/InputFormat.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `InputSplits` | [https://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/mapred/InputSplit.html](https://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/mapred/InputSplit.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `RecordReaders` | [https://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/mapred/RecordReader.html](https://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/mapred/RecordReader.html)
    |'
  prefs: []
  type: TYPE_TB
- en: 'Let''s see a practical code example in Scala. We want to extract data from
    a CSV file that contains some e-shop transactions and have the following columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`DateTimeString`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CustomerID`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MerchantID`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NumItemsInTransaction`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MerchantCountryCode`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TransactionAmountUSD`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FraudLabel`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we perform some transformation over them.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to import the required dependencies (Scala, Spark, DataVec, and DataVec-Spark)
    first. Here is a complete list for a Maven POM file (but, of course, you can use
    SBT or Gradle):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing to do in the Scala application is to define the input data
    schema, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If input data is numeric and appropriately formatted then a `CSVRecordReader`
    ([https://deeplearning4j.org/datavecdoc/org/datavec/api/records/reader/impl/csv/CSVRecordReader.html](https://deeplearning4j.org/datavecdoc/org/datavec/api/records/reader/impl/csv/CSVRecordReader.html))
    may be satisfactory. If, however, the input data has non-numeric fields, then
    a schema transformation will be required. DataVec uses Apache Spark to perform
    transform operations. Once we have the input schema, we can define the transformation
    we want to apply to the input data. Just a couple of transformations are described
    in this example. We can remove some columns that are unnecessary for our net,
    for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Filter the `MerchantCountryCode` column in order to get the records related
    to USA and Canada only, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: At this stage, the transformations are only defined, but not applied yet (of
    course we need to get the data from the input file first). So far, we have used
    DataVec classes only. In order to read the data and apply the defined transformations,
    the Spark and DataVec-Spark API need to be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create the `SparkContext` first, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can read the CSV input file and parse the data using a `CSVRecordReader`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then execute the transformation defined earlier, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s collect the data locally, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The input data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f7520ca-540e-4f55-b1dd-e0d37e062822.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The processed data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1080235c-2b0a-4427-b254-894a85f593e6.png)'
  prefs: []
  type: TYPE_IMG
- en: The full code of this example is part of the source code included with the book.
  prefs: []
  type: TYPE_NORMAL
- en: Training data ingestion from a database with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes data has been previously ingested and stored into a database by some
    other application, so you would need to connect to a database in order to use
    it for training or testing purposes. This section describes how to get data from
    a relational database and a NoSQL database. In both cases, Spark would be used.
  prefs: []
  type: TYPE_NORMAL
- en: Data ingestion from a relational database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose the data is stored in a table called `sparkexample` in a MySQL ([https://dev.mysql.com/](https://dev.mysql.com/))
    schema with the name `sparkdb`. This is the structure of that table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'It contains the same data as, for the example, in *Training data ingestion
    through Spark*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The dependencies to add to the Scala Spark project are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark 2.2.1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark SQL 2.2.1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The specific JDBC driver for the MySQL database release used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s now implement the Spark application in Scala. In order to connect to
    the database, we need to provide all of the needed parameters. Spark SQL also
    includes a data source that can read data from other databases using JDBC, so
    the required properties are the same as for a connection to a database through
    traditional JDBC; for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to check that the JDBC driver for the MySQL database is available,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now create a `SparkSession`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the implicit conversions, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'You can finally connect to the database and load the data from the `sparkexample`
    table to a DataFrame, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Spark automatically reads the schema from a database table and maps its types
    back to Spark SQL types. Execute the following method on the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'It returns the exact same schema as for the table `sparkexample`; for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the data is loaded into the DataFrame, it is possible to run SQL queries
    against it using the specific DSL as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'It is possible to increase the parallelism of the reads through the JDBC interface.
    We need to provide split boundaries based on the DataFrame column values. There
    are four options available (`columnname`, `lowerBound`, `upperBound`, and `numPartitions`)
    to specify the parallelism on read. They are optional, but they must all be specified
    if any of them is provided; for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: While the examples in this section refer to a MySQL database, they apply the
    same way to any commercial or open source RDBMS for which a JDBC driver is available.
  prefs: []
  type: TYPE_NORMAL
- en: Data ingestion from a NoSQL database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data can also come from a NoSQL database. In this section, we are going to explore
    the code to implement in order to consume the data from a MongoDB ([https://www.mongodb.com/](https://www.mongodb.com/))
    database.
  prefs: []
  type: TYPE_NORMAL
- en: 'The collection `sparkexample` of the `sparkmdb` database contains the same
    data as for the examples in *Data ingestion through DataVec and transformation
    through Spark* and *Data ingestion from a relational database* sections, but in
    the form of BSON documents; for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The dependencies to add to the Scala Spark project are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark 2.2.1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark SQL 2.2.1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The MongoDB connector for Spark 2.2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We need to create a Spark Session, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify the connection to the database. After the session as been created,
    it is possible to use it to load data from the `sparkexample` collection through
    the `com.mongodb.spark.MongoSpark` class, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The returned DataFrame has the same structure as for the `sparkexample` collection.
    Use the following instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'It prints the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef9e643a-a52b-499d-950f-17dc5fe4aa7a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Of course, the retrieved data is that in the DB collection, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'It returns the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also possible to run SQL queries on the DataFrame. We need first to create
    a case class to define the schema for the DataFrame, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we load the data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We must register a temporary view for the DataFrame, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we can execute an SQL statement, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'It returns the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Data ingestion from S3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nowadays, there's a big chance that the training and test data are hosted in
    some cloud storage system. In this section, we are going to learn how to ingest
    data through Apache Spark from an object storage such as Amazon S3 ([https://aws.amazon.com/s3/](https://aws.amazon.com/s3/))
    or S3-based (such as Minio, [https://www.minio.io/](https://www.minio.io/)). The
    Amazon simple storage service (which is more popularly known as Amazon S3) is
    an object storage service part of the AWS cloud offering. While S3 is available
    in the public cloud, Minio is a high performance distributed object storage server
    compatible with the S3 protocol and standards that has been designed for large-scale
    private cloud infrastructures.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to add to the Scala project the Spark core and Spark SQL dependencies,
    and also the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: They are the AWS Java JDK core and S3 libraries, plus the Apache Hadoop module
    for AWS integration.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we need to have already created one existing bucket on S3
    or Minio. For the readers not familiar with the S3 object storage, a bucket is
    similar to a file system directory, where users can store objects (data and the
    metadata that describe it). Then we need to upload a file in that bucket that
    would need to be read by Spark. The file used for this example is one generally
    available for download at the MonitorWare website ([http://www.monitorware.com/en/logsamples/apache.php](http://www.monitorware.com/en/logsamples/apache.php)).
    It contains HTTP requests log entries in ASCII format. For the purpose of this
    example, we are assuming that the name of the bucket is `dl4j-bucket` and the
    uploaded file name is `access_log`. The first thing to do in our Spark program
    is to create a `SparkSession`, as follows
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: In order to reduce noise on the output, let's set the log level for Spark to
    `WARN`, as follows
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the `SparkSession` has been created, we need to set up the S3 or Minio
    endpoint and the credentials for Spark to access it, plus some other properties,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the meaning of the properties that have been set for the minimal configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '`fs.s3a.endpoint`: The S3 or Minio endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fs.s3a.access.key`: The AWS or Minio access key ID.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fs.s3a.secret.key`: The AWS or Minio secret key.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fs.s3a.path.style.access`: Enables S3 path style access while disabling the
    default virtual hosting behavior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fs.s3a.connection.ssl.enabled`: Specifies if SSL is enabled at the endpoint.
    Possible values are `true` and `false`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fs.s3a.impl`: The implementation class of the `S3AFileSystem` that is used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We are now ready to read the `access_log` file (or any other file) from a S3
    or Minio bucket and load its content into a RDD, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also possible to convert the RDD into a DataFrame and show the content
    on the output, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This will provide the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/91d783d2-5436-41da-b551-c8a31b0b7cc0.png)'
  prefs: []
  type: TYPE_IMG
- en: Once data has been loaded from objects stored into S3 or Minio buckets, any
    operation available in Spark for RDDs and Datasets can be used.
  prefs: []
  type: TYPE_NORMAL
- en: Raw data transformation with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data coming from a source is often raw data. When we talk about raw data we
    mean data that is in a format that can't be used as is for the training or testing
    purposes of our models. So, before using, we need to make it tidy. The cleanup
    process is done through one or more transformations before giving the data as
    input for a given model.
  prefs: []
  type: TYPE_NORMAL
- en: For data transformation purposes, the DL4J DataVec library and Spark provide
    several facilities. Some of the concepts described in this section have been explored
    in the *Data ingestion through DataVec and transformation through Spark* section,
    but now we are going to add a more complex use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how to use Datavec for transformation purposes, let''s build
    a Spark application for web traffic log analysis. The dataset used is generally
    available for download at the MonitorWare website ([http://www.monitorware.com/en/logsamples/apache.php](http://www.monitorware.com/en/logsamples/apache.php)).
    They are HTTP requests log entries in ASCII format. There is one line per request,
    with the following columns:'
  prefs: []
  type: TYPE_NORMAL
- en: The host making the request. A hostname or an internet address
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A timestamp in the format *DD/Mon/YYYY:HH:MM:SS*, where *DD* is the day of the
    month, *Mon* is the name of the month, *YYYY* is the year and *HH:MM:SS* is the
    time of day using a 24-hour clock. The timezone is -*0800*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The HTTP request given in quotes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The HTTP reply code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The total of bytes in the reply
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here''s a sample of the log content used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing to do in our application is to define the schema of the input
    data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Start a Spark context, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'A web log file could contain some invalid lines that don''t follow the preceding
    schema, so we need to include some logic to discard those lines that are useless
    for our analysis, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We are applying a regular expression to filter the log lines that match the
    expected format. We can now start to parse the raw data using a DataVec `RegexLineRecordReader`
    ([https://deeplearning4j.org/datavecdoc/org/datavec/api/records/reader/impl/regex/RegexLineRecordReader.html](https://deeplearning4j.org/datavecdoc/org/datavec/api/records/reader/impl/regex/RegexLineRecordReader.html)).
    We need to define a `regex` for formatting the lines, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Through the DataVec-Spark library, it is also possible to check the quality
    of the data before defining the transformations. We can use the `AnalyzeSpark`
    ([https://deeplearning4j.org/datavecdoc/org/datavec/spark/transform/AnalyzeSpark.html](https://deeplearning4j.org/datavecdoc/org/datavec/spark/transform/AnalyzeSpark.html))
    class for this purpose, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output produced by the data quality analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'From this, we notice that on `139` lines (out of `1546`), the `replyBytes`
    field isn''t an integer as expected. Here are a couple of those lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'So, the first transformation to do is to clean up the `replyBytes` field by
    replacing any non-integer entries with the value `0`. We use the `TransformProcess`
    class as for the example in the *Data ingestion through DataVec and transformation
    through Spark* section, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can apply any other transformation, for example, grouping by host
    and pulling out summary metrics (count the number of entries, count the number
    of unique requests and HTTP reply codes, total the values in the `replyBytes`
    field); for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Rename a number of columns, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Filter out all hosts that requested fewer than 1 million bytes in total, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now execute the transformations, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also perform some analysis on the final data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The final data schema is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The following shows that the result count is two:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code shows the result of the analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter explored different ways of ingesting data from files, relational
    and NoSQL databases, and S3-based object storage systems using the DeepLearning4j
    DataVec library and the Apache Spark (core and Spark SQL modules) framework, and
    showed some examples of how to transform the raw data. All of the examples presented represent
    data ingestion and transformation in a batch fashion.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will focus on ingesting and transforming data to train or test
    your DL model in streaming mode.
  prefs: []
  type: TYPE_NORMAL
