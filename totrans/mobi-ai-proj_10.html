<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">What is Next?</h1>
                </header>
            
            <article>
                
<p>Computers are developing more and more day by day, and the form factors of devices are changing tremendously. In the past, we would only see computers in offices; however, now we see them on our home desks, on our laps, in our pockets, and on our wrists. The market is becoming increasingly varied as machines are equipped with more and more intelligence.</p>
<p>Almost every adult currently carries a device with them, and it is estimated that we look at our smartphones at least 50 times a day, whether there is a need for it or not. These machines affect our daily decision-making processes. Devices are now equipped with applications such as Siri, Google Assistant, Alexa, or Cortana—these are features that are designed to mimic human intelligence. The ability to answer any query thrown at them presents these types of technology as superior to humans. On the backend of this, these systems continuously improve by using the collective intelligence that is acquired from all users. The more you interact with virtual assistants, the better the results they give out.</p>
<p>Despite these advancements, how much closer are we to actually creating a human brain through a machine? We are in 2019 now; if science discovers a way to control the neurons of our brain, this may be possible in the near future. Machines that mimic the capabilities of a human are helping to solve complex textual, visual, and audio problems. They resemble the tasks carried out by a human brain on a daily basis; to put this into perspective, on average, the human brain makes approximately 35,000 decisions in a day.</p>
<p>While we will be able to mimic the human brain in the future, it will come at a cost. We don't have a cheaper solution for it at the moment. The magnitude of power consumption of a human brain simulation limits the development efforts in comparison to an actual human brain. The human brain consumes about 20 W of power, while a simulation program consumes about 1 MW of power or more. Neurons in the human brain operate at a speed of 200 Hz, while a typical microprocessor operates at a speed of 2 GHz, which is 10 million times more than the speed of neurons in the human brain.</p>
<p>While we are still far from cloning a human brain, we can implement an algorithm that makes conscious decisions based on previous data, as well as data from similar devices. This is where the subset of <strong>Artificial Intelligence</strong> (<strong>AI</strong>) comes in handy. With predefined algorithms that identify patterns from the complex data we have, these types of intelligence can then give us useful information.</p>
<p>When the computer starts making decisions without being instructed explicitly every time, we achieve <strong>Machine Learning</strong> (<strong>ML</strong>) capability. ML is used everywhere right now, including through features such as identifying email spam, recommending the best product to buy on an e-commerce website, tagging your face automatically on a social media photograph, and more. All of these are done using patterns identified in historical data, and also through algorithms that reduce unnecessary noise from the data and produce quality output. When the data accumulates more and more, the computers can make better decisions.</p>
<p class="mce-root">Mobile phones have become the default consumption medium for most of the digital products that are being produced today. As data consumption increases, we have to get results to the user as soon as possible. For example, when you scroll through your Facebook feed, a lot of content will be based on your interests and what your friends have liked. Since the time that users spend on these apps is limited, there are a lot of algorithms running on the server and client side, to load and organize content based on what you prefer to see on your Facebook feed. If there was the possibility of running all the algorithms on the local device itself, we wouldn't need to depend on the internet to load the content faster. This is only possible by performing the processes on the client's device itself, instead of processing in the cloud.</p>
<p class="mce-root">As the processing capability of mobile devices increases, we will be encouraged to run all ML models on the mobile device itself. There are a lot of services that are already being processed on the client's device, such as identifying a face from a photo (such as Apple's Face ID feature), which uses ML on the local device.</p>
<p class="mce-root">While multiple topics are trending such as AI, <strong>augmented reality</strong> (<strong>AR</strong>), <strong>virtual reality</strong> (<strong>VR</strong>), ML, and blockchain—ML is growing faster than others, with proper use cases evident across all sectors. ML algorithms are being applied to images, text, audio, and video in order to get the output that we desire.</p>
<p class="mce-root">If you are a beginner, then there are multiple ways to start your work, by utilizing all of the free and open source frameworks that are being built. If you are worried about building a model yourself, you can start with ML Kit for Firebase from Google. Alternatively, you can build your own model using TensorFlow, and convert that model into a TensorFlow Lite model (for Android), and a Core ML model (for iOS).</p>
<p class="mce-root">In this chapter, we will cover the following topics:</p>
<p class="mce-root"/>
<ul>
<li style="list-style-type: none">
<ul>
<li class="mce-root">Popular ML-based cloud services</li>
<li class="mce-root">Where to start when you build your first ML-based mobile app</li>
</ul>
</li>
<li>References to further reading</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Popular ML–based cloud services</h1>
                </header>
            
            <article>
                
<p>To begin your ML journey, you can use one of the existing cloud-based services on ML. <strong>ML as a Service</strong> (<strong>MLaaS</strong>) is widely used across all contemporary business sectors. Data is becoming cheaper, and data volume is growing exponentially. As a result, the processing power of devices is increasing at a much quicker rate. This trend has made way for multiple cloud-based services, such as <strong>Software as a Service</strong> (<strong>SaaS</strong>), <strong>Platform as a Service</strong> (<strong>PaaS</strong>), and <strong>Infrastructure as a Service</strong> (<strong>IaaS</strong>), now joined by MLaaS.</p>
<p>Even though we can run an ML model on our mobile device, it is still greatly impacted by limited memory, CPU and <strong>Graphics Processing Unit</strong> (<strong>GPU</strong>) resources. In this case, a cloud service comes in handy.</p>
<p>Starting with ML in the cloud, there are multiple services available, such as facial recognition, optical character recognition, image recognition, landmark recognition, data visualization, and <strong>natural language processing</strong> (<strong>NLP</strong>). All of these options are supported by deep neural networks, <strong>convolutional neural networks</strong> (<strong>CNNs</strong>), probabilistic models, and more. Most cloud providers run a business model that offers some free limits for a developer to explore, before deciding which is the best fit to develop their application.</p>
<p>The following sections will explain the four major services that are available now, and are also popular among developers and enterprises. As a beginner, you can explore the functionality under each provider and pick the one that suits your application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">IBM Watson services</h1>
                </header>
            
            <article>
                
<p>IBM Watson provides deep learning as a service through a variety of products. There is a textbot service, called AI assistant, which supports mobile platforms and chat services; and a service called Watson Studio, which is helpful for building and analyzing the model. IBM Watson also has another separate API service, which processes text, vision, and speech.</p>
<div class="packt_infobox">There is a sample application available for developing a vision application using Core ML. This can be found at <a href="https://github.com/watson-developer-cloud/visual-recognition-coreml" target="_blank">https://github.com/watson-developer-cloud/visual-recognition-coreml</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Microsoft Azure Cognitive Services</h1>
                </header>
            
            <article>
                
<p class="mce-root">Microsoft provides out-of-the-box Cognitive Services in five categories, as follows:</p>
<ul>
<li>Vision APIs</li>
<li>Speech APIs</li>
<li class="mce-root">Knowledge APIs</li>
<li>Search APIs</li>
<li>Language APIs</li>
</ul>
<p>The function of these APIs is mentioned in the following sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Vision APIs</h1>
                </header>
            
            <article>
                
<p class="mce-root">Vision APIs are algorithms used for image processing to recognize, caption, and moderate your pictures smartly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Speech APIs</h1>
                </header>
            
            <article>
                
<p class="mce-root">Through speech APIs, the spoken audio is converted to text. The APIs use voice print for verification, or for adding speaker recognition to an application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Knowledge APIs</h1>
                </header>
            
            <article>
                
<p class="mce-root">To solve tasks such as <span>intelligent recommendations and semantic searches, we use k</span>nowledge APIs that help us to map the complex information and data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Search APIs</h1>
                </header>
            
            <article>
                
<p class="mce-root">Search APIs provide Bing web search APIs to your apps. They also harnesses the ability to combine billions of web pages, images, videos, and news with a single API call. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Language APIs</h1>
                </header>
            
            <article>
                
<p class="mce-root">Language APIs allow your apps to process natural language with pre-built scripts, evaluate sentiments, and learn how to recognize what the user needs.</p>
<div class="packt_tip">There are multiple sample applications for the preceding APIs. These can be found at <a href="https://azure.microsoft.com/en-in/resources/samples/?%20sort=0&amp;sort=0" target="_blank">https://azure.microsoft.com/en-in/resources/samples/?%20sort=0&amp;sort=0</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Amazon ML</h1>
                </header>
            
            <article>
                
<p><strong>Amazon Web Services</strong> (<strong>AWS</strong>) has multiple offerings for ML-based services. All of these services are tightly coupled, in order to work efficiently in the AWS cloud. A few of the services are highlighted in the following sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Vision services</h1>
                </header>
            
            <article>
                
<p>AWS has Amazon Rekognition, which is a deep learning-based service that is designed to process images and videos. We can integrate the service on mobile devices as well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chat services</h1>
                </header>
            
            <article>
                
<p>Amazon Lex helps to build chatbots. This industry is still growing, with more and more data coming in; the service will become more intelligent, allowing it to answer queries even better.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Language services</h1>
                </header>
            
            <article>
                
<p>Some examples of language services include Amazon Comprehend, which helps to uncover insights and relationships in text; Amazon Translate, which helps with the fluent translation of text; Amazon Transcribe, which helps with automatic speech recognition; and Amazon Polly, which helps to turn natural-sounding text into speech.</p>
<div class="packt_infobox packt_tip">You can see a few sample applications at <a href="https://github.com/aws-samples/machine-learning-samples" target="_blank">https://github.com/aws-samples/machine-learning-samples</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Google Cloud ML</h1>
                </header>
            
            <article>
                
<p class="mce-root">If you want to run your model in the cloud, the Google Cloud ML Engine offers the power and flexibility of TensorFlow, scikit-learn, and XGBoost in the cloud. If this is not suitable, you can pick the API services that best fit your scenario. In Google Cloud ML, multiple APIs are available. These are classified into four major categories, as follows:</p>
<ul>
<li class="mce-root"><strong>Vision</strong>: The Cloud Vision API helps with image recognition and classification; the Cloud Video Intelligence API helps with scene-level video annotation; and AutoML Vision helps with custom image classification models.</li>
<li><strong>Conversation</strong>: Dialogflow Enterprise Edition builds conversational interfaces; the Cloud Text-to-Speech API converts text to speech; and the Cloud Speech-to-Text API converts speech to text.</li>
<li><strong>Language</strong>: The Cloud Translation API is used in language detection and translation; the Cloud Natural Language API is used in text parsing and analysis; AutoML Translation is used in custom domain-specific translation; and AutoML Natural Language helps in building custom text classification models.</li>
<li><strong>Knowledge</strong>: The Cloud Inference API derives insights from time series datasets.</li>
</ul>
<div class="mce-root packt_tip">You can find a number of Google Vision APIs at <a href="https://github.com/GoogleCloudPlatform/cloud-vision" target="_blank">https://github.com/GoogleCloudPlatform/cloud-vision</a>.</div>
<p class="mce-root"><br/>
There are also other services that are popular with developers, including <strong>Dialogflow</strong> and <strong>Wit.ai</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building your first ML model</h1>
                </header>
            
            <article>
                
<p class="mce-root">With the knowledge that you have gained from this book, you can start to develop your own model that runs on a mobile phone. You will need to identify the problem statement first. There are many use cases where you will not need an ML model; we can't unnecessarily force ML into everything. Consequently, you need to follow a step-by-step approach before you build your own model:</p>
<ol>
<li class="mce-root">Identify the problem.</li>
<li class="mce-root">Plan the effectiveness of your model; decide whether the data could be useful in predicting the output for future, similar cases. For example, collecting the purchase history for people of a similar age, gender, and location will be helpful in predicting a new customer's purchasing preferences. However, the data won't be helpful in predicting the height of a new customer, if that is the data that you are looking for.</li>
<li class="mce-root">Develop a simple model (this can be based on SQL). This will be useful for reducing the effort when building actual models.</li>
<li class="mce-root">Validate the data and throw the unnecessary data out.</li>
<li class="mce-root">Build the actual model.</li>
</ol>
<p>As data is growing exponentially across various parameters (data from multiple sensors), on the local devices as well as with cloud providers, we can build better use cases with more and more personalized content. There are many applications that are already using ML, such as mail services (Gmail and Outlook) and cab services (Uber, Ola, and Lyft).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The limitations of building your own model</h1>
                </header>
            
            <article>
                
<p>While ML is getting popular, it is not yet feasible to run ML models on mobile platforms to reach the masses. When you are building your own model for mobile apps, there are some limitations as well. While it is possible to make predictions on a local device without a cloud service, it is not advisable to build an evolving model that makes predictions based on your current actions and accumulates data on the local device itself. As of right now, we can run pre-built models and get inferences out of them on mobile devices, due to the constraints on memory and the processing power of the mobile devices. Once we have better processors on mobile devices, we can train and improve the model on the local device.</p>
<p>There are a lot of use cases related to this. Apple's Face ID is one such example, running a model on a local device that requires computations from a CPU or GPU. When the device's capability increases in the future, it will be possible to build a completely new model on the device itself.</p>
<p>Accuracy is another reason why people refrain from developing models on their mobile devices. Since we are currently unable to run heavy operations on our mobile devices, the accuracy, as compared to a cloud-based service, seems bleak, the reason for this being the limitations on both memory and computational capability. You could run the models that are available for mobile devices in the TensorFlow and Core ML libraries instead.</p>
<div class="packt_tip">The TensorFlow Lite models can be found at <a href="https://www.tensorflow.org/lite/models" target="_blank">https://www.tensorflow.org/lite/models</a>; and the Core ML models can be found at <a href="https://github.com/likedan/Awesome-CoreML-Models" target="_blank">https://github.com/likedan/Awesome-CoreML-Models</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Personalized user experience</h1>
                </header>
            
            <article>
                
<p>A personalized <strong>user experience</strong> (<strong>UX</strong>) will be the basic use case for any mobile based consumer business, to provide a more curated and personalized experience for the users of their mobile applications. This can be done by analyzing data points, such as the following:</p>
<ul>
<li>Who is your customer?</li>
<li>Where do they come from?</li>
<li>What are their interests?</li>
<li>What do they say about you?</li>
<li>Where did they find you?</li>
<li>Do they have any pain points?</li>
<li>Can they afford your products?</li>
<li>Do they have a history of purchases or searches?</li>
</ul>
<p>For example, consider the customer of a retail company or a restaurant. If we have answers to the preceding questions, we have rich data about the customers, from which we can build data models that will provide more personalized experiences (with the help of ML). We can analyze and identify similar customer, to provide a better UX for all of the users, as well as targeting the right future customers.</p>
<p>Let's take a look at these ideas in further detail in the following sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Providing better search results</h1>
                </header>
            
            <article>
                
<p>Providing better search results is one of the major use cases, especially on a mobile application, to provide more contextual results, rather than text-based results. This will help to improve the business's customer base. ML algorithms should learn from user searches and optimize the results. Even spelling corrections can be done intuitively. Moreover, collecting the user's behavioral data (concerning how they use your app) will be helpful in providing the best search results, so that you can rank the results in a way that is personalized to the user.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Targeting the right user</h1>
                </header>
            
            <article>
                
<p>Most apps capture the user's age and gender data when they install the application for the first time. This will help you to understand the common user group of your application. You will also have the user's data, which will give you the usage and frequency of how much the user utilizes the app, as well as location data, if that is permitted from the user's end. This will be helpful in predicting customer targets in the near future. For example, you will be able to see whether your user audience is coming from an age group of between 18 and 25, and is predominantly female. In that case, you could devise a strategy to pull more male users, or just stick to targeting female users only. The algorithm should be able to predict and analyze all of this data, which will be helpful in marketing to and increasing your user base.</p>
<p>There are a lot of niche use cases where ML-based mobile apps can be of great help; some of them are as follows:</p>
<ul>
<li>Automatic product tagging</li>
<li>Time estimations, as used in Pedometer, Uber, and Lyft</li>
<li>Health-based recommendations</li>
<li>Shipping cost estimations</li>
<li>Supply chain predictions</li>
<li>Money management</li>
<li>Logistics optimization</li>
<li>Increasing productivity</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>With all the basic ideas that you have gained from this book, you can start building your own application with ML capabilities. Furthermore, with all of the new ways to interact with devices such as Amazon Alexa, Google Home, or the Facebook portal, you will find more use cases to build ML applications. Ultimately, we are moving toward a world with more connected devices, bringing the connections and communications closer to us, and leading us to create better experiences with ML.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>There are a lot of ML courses that are available online. A few of these courses are listed as follows:</p>
<ul>
<li>If you are beginner, you can start with the Coursera tutorial on ML by Andrew Ng, which can be found at <a href="https://www.coursera.org/learn/machine-learning" target="_blank">https://www.coursera.org/learn/machine-learning</a></li>
<li>An ML crash course from Google can be found at <a href="https://developers.google.com/machine-learning/crash-course/" target="_blank">https://developers.google.com/machine-learning/crash-course/</a></li>
<li>One of the best (and most enlightening) ML based blog series, by Adam Geitgey, can be found at <a href="https://medium.comi@ageitgey/machine-learning-is-fun-80ea3ec3c471">https://medium.comi@ageitgey/machine-learning-is-fun-80ea3ec3c471</a></li>
<li>You can kick-start your skills in TensorFlow at <a href="https://codelabs.developers.google.com/codelabs/tensorflow-for-poets" target="_blank">https://codelabs.developers.google.com/codelabs/tensorflow-for-poets</a></li>
<li>A more thorough look at TensorFlow can be found at <a href="https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/" target="_blank">https://petewarden.com/2016/09/27/tensorflow-for-mobile-poets/</a></li>
</ul>


            </article>

            
        </section>
    </body></html>