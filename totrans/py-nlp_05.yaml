- en: Feature Engineering and NLP Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature engineering is the most important part of developing NLP applications.
    Features are the input parameters for **machine learning** (**ML**) algorithms.
    These ML algorithms generate output based on the input features. Feature engineering
    is a kind of art and skill because it generates the best possible features, and
    choosing the best algorithm to develop NLP application requires a lot of effort
    and understanding about feature engineering as well as NLP and ML algorithms.
    In [Chapter 2](837b4260-d60f-474a-a208-68a1eaa8e1bb.xhtml), *Practical Understanding
    of Corpus and Dataset,* we saw how data is gathered and what the different formats
    of data or corpus are. In [Chapter 3](f65e61fc-1d20-434f-b606-f36cf401fc41.xhtml),
    *Understanding Structure of Sentences,* we touched on some of the basic but important
    aspects of NLP and linguistics. We will use these concepts to derive features
    in this chapter. In [Chapter 4](59dc896d-b0de-44c9-9cbe-18c63e510897.xhtml), *Preprocessing,*
    we looked preprocessing techniques. Now, we will work on the corpus that we preprocessed
    and will generate features from that corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to *Figure 5.1*, which will help you understand all the stages that we
    have covered so far, as well as all the focus points of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e23efc2-689b-4d33-a782-6761259fe6bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: An overview of the features generation process'
  prefs: []
  type: TYPE_NORMAL
- en: You can refer to *Figure 1.4* in [Chapter 1](238411c1-88cf-4377-a6c6-7451e0f48daa.xhtml),
    *Introduction*. We covered the first four stages in the preceding three chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will mostly focus on the practical aspect of NLP applications.
    We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is feature engineering?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding basic features of NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic statistical features of NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As well as this, we will explore topics such as how various tools or libraries
    are developed to generate features, what the various libraries that we can use
    are, and how you can tweak open source libraries or open source tools as and when
    needed.
  prefs: []
  type: TYPE_NORMAL
- en: We will also look at the challenges for each concept. Here, we will not develop
    tools from scratch as it is out of the scope of this book, but we will walk you
    through the procedure and algorithms that are used to develop the tools. So if
    you want to try and build customized tools, this will help you, and will give
    you an idea of how to approach those kind of problem statements.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before jumping into the feature generation techniques, we need to understand
    feature engineering and its purpose.
  prefs: []
  type: TYPE_NORMAL
- en: What is feature engineering?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature engineering is the process of generating or deriving features (attributes
    or an individual measurable property of a phenomenon) from raw data or corpus
    that will help us develop NLP applications or solve NLP-related problems.
  prefs: []
  type: TYPE_NORMAL
- en: A feature can be defined as a piece of information or measurable property that
    is useful when building NLP applications or predicting the output of NLP applications.
  prefs: []
  type: TYPE_NORMAL
- en: We will use ML techniques to process the natural language and develop models
    that will give us the final output. This model is called the **machine learning
    model** (**ML model**). We will feed features for machine learning algorithms
    as input and to generate the machine learning model. After this, we will use the
    generated machine learning model to produce an appropriate output for an NLP application.
  prefs: []
  type: TYPE_NORMAL
- en: If you're wondering what information can be a feature, then the answer is that
    any attribute can be a feature as long as it is useful in order to generate a
    good ML model that will produce the output for NLP applications accurately and
    efficiently. Here, your input features are totally dependent on your dataset and
    the NLP application.
  prefs: []
  type: TYPE_NORMAL
- en: Features are derived using domain knowledge for NLP applications. This is the
    reason we have explored the basic linguistics aspect of natural language, so that
    we can use these concepts in feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: What is the purpose of feature engineering?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will look at the major features that will help us to understand
    feature engineering:'
  prefs: []
  type: TYPE_NORMAL
- en: We have raw data in natural language that the computer can't understand, and
    algorithms don't have the ability to accept the raw natural language and generate
    the expected output for an NLP application. Features play an important role when
    you are developing NLP applications using machine learning techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to generate the attributes that are representative for our corpus as
    well as those attributes that can be understood by machine learning algorithms.
    ML algorithms can understand only the language of feature for communication, and
    coming up with appropriate attributes or features is a big deal. This is the whole
    purpose of feature engineering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we have generated the feature, we then need to feed them to the machine
    learning algorithm as input, and after processing these input features, we will
    get the ML model. This ML model will be used to predict or generate the output
    for new features. The ML models, accuracy and efficiency is majorly dependent
    on features, which is why we say that features engineering is a kind of art and
    skill.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the challenges involved in feature engineering:'
  prefs: []
  type: TYPE_NORMAL
- en: Coming up with good features is difficult and sometimes complex.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After generating features, we need to decide which features we should select
    this selection of features also plays a major role when we perform machine learning
    techniques on top of that. The process of selecting appropriate feature is called
    **feature selection**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes, during the feature selection, we need to eliminate some of the less
    important features, and this elimination of features is also a critical part of
    the feature engineering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manual feature engineering is time-consuming.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering requires domain expertise or, at least, basic knowledge
    about domains.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic feature of NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from the challenges, NLP applications heavily rely on feature that are
    manually crafted based on various NLP concepts. From this point onwards, we will
    explore the basic features that are available in the NLP world. Let's dive in!
  prefs: []
  type: TYPE_NORMAL
- en: Parsers and parsing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By parsing sentences, you can derive some of the most important features that
    can be helpful for almost every NLP application.
  prefs: []
  type: TYPE_NORMAL
- en: We will explore the concept of parser and parsing. Later, we will understand
    **context-free grammar** (**CFG**) and **probabilistic context-free grammar**
    (**PCFG**). We will see how statistical parsers are developed. If you want to
    make your own parser, then we will explain the procedure to do so, or if you want
    to tweak the existing parser, then what steps you should follow. We will also
    do practical work using the available parser tools. We will look at the challenges
    later in this same section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the basics of parsers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, I'm going to explain parser in terms of the NLP domain. The parser concept
    is also present in other computer science domains, but let's focus on the NLP
    domain and start understanding parser and what it can do for us.
  prefs: []
  type: TYPE_NORMAL
- en: In the NLP domain, a parser is the program or, more specifically, tool that
    takes natural language in the form of a sentence or sequence of tokens. It breaks
    the input stream into smaller chunks. This will help us understand the syntactic
    role of each element present in the stream and the basic syntax-level meaning
    of the sentence. In NLP, a parser actually analyzes the sentences using the rules
    of context-free grammar or probabilistic context-free grammar. We have seen CFG
    in [Chapter 3](f65e61fc-1d20-434f-b606-f36cf401fc41.xhtml), *Understanding Structure
    of Sentences*.
  prefs: []
  type: TYPE_NORMAL
- en: A parser usually generates output in the form of a parser tree or abstract syntax
    tree. Let's see some of the example parser trees here. There are certain grammar
    rules that parsers use to generate the parse tree with single words or lexicon
    items.
  prefs: []
  type: TYPE_NORMAL
- en: 'See the grammar rules in *Figure 5.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ea61a64-940d-4a7e-9c11-0d302aec1cbc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: Grammar rules for parser'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s discuss the symbols first:'
  prefs: []
  type: TYPE_NORMAL
- en: '**S** stands for sentence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NP** stands for noun phrase'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VP** stands for verb phrase'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**V** stands for verb'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**N** stand for noun'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ART** stands for article a, an, or the'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See a parse tree generated using the grammar rules in *Figure 5.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e4b587a-30e3-459f-be47-c84cc79ef02b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: A parse tree as per the grammar rules defined in Figure 5.2'
  prefs: []
  type: TYPE_NORMAL
- en: Here in F*igure 5.3,* we converted our sentence into the parse tree format,
    and as you can see, each word of the sentence is expressed by the symbols of the
    grammar that we already defined in F*igure 5.2.*
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two major types of parsers. We are not going to get into the technicality
    of each type of parser here because it''s more about the compiler designing aspect.
    Instead, we will explore the different types of parsers, so you can get some clarity
    on which type of parser we generally use in NLP. Refer to *Figure 5.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d32ee1e7-1041-486a-a143-46e2dced8b8e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: Types of parser'
  prefs: []
  type: TYPE_NORMAL
- en: We will also look at the differences between top-down parsers and bottom-up
    parsers in the next section, as the difference is related to the process; this
    will be followed by each of the parsers so that we understand the difference.
  prefs: []
  type: TYPE_NORMAL
- en: Let's jump into the concept of parsing.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the concept of parsing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First of all, let''s discuss what parsing is. Let''s define the term parsing.
    Parsing is a formal analysis or a process that uses a sentence or the stream of
    tokens, and with the help of defined formal grammar rules, we can understand the
    sentence structure and meaning. So, parsing uses each of the words in the sentence
    and determines its structure using a constituent structure. What is a consistent
    structure? A constituent structure is based on the observation of which words
    combine with other words to form a sensible sentence unit. So, in the English
    language, the subject mostly comes first in the sentence; the sentence **He is
    Tom** makes sense to us, whereas the sentence **is Tom he** doesn''t make sense.
    By parsing, we actually check as well as try to obtain a sensible constituent
    structure. These are the following points that will explain what parser and parsing
    does for us:'
  prefs: []
  type: TYPE_NORMAL
- en: The parser tool performs the process of parsing as per the grammar rules and
    generates a parse tree. This parse tree structure is used to verify the syntactical
    structure of the sentence. If a parse tree of the sentence follows the grammar
    rules as well as generates a meaningful sentence, then we say that the grammar
    as well as the sentence generated using that grammar is valid.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the parsing, a parse tree is generated as output that will help
    you to detect ambiguity in the sentence because. Ambiguous sentences often result
    in multiple parse trees.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s see the difference between a top-down parser and a bottom-up parser:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Top-down parsing** | **Bottom-up parsing** |'
  prefs: []
  type: TYPE_TB
- en: '| Top-down parsing is hypothesis-driven. | Bottom-up parsing is data-driven.
    |'
  prefs: []
  type: TYPE_TB
- en: '| At each stage of parsing, the parser assumes a structure and takes a word
    sequentially from the sentence and tests whether the taken word or token fulfills
    the hypothesis or not. | In this type of parsing, the first words are taken from
    the input string, then the parser checks whether any predefined categories are
    there in order to generate a valid sentence structure, and lastly, it tries to
    combine them into acceptable structures in the grammar. |'
  prefs: []
  type: TYPE_TB
- en: '| It scans a sentence in a left-to-right manner. When grammar production rules
    derive lexical items, the parser usually checks with the input to see whether
    the right sentence is being derived or not. | This kind of parsing starts with
    the input string of terminals. This type of parsing searches for substrings of
    the working string because if any string or substring matches the right-hand side
    production rule of grammar, then it substitutes the left-hand side non-terminal
    for the matching right-hand side rule. |'
  prefs: []
  type: TYPE_TB
- en: '| It includes a backtracking mechanism. When it is determined that the wrong
    rule has been used, it backs up and tries another rule. | It usually doesn''t
    include a backtracking mechanism. |'
  prefs: []
  type: TYPE_TB
- en: You will get to know how this parser has been built in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Developing a parser from scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will try to understand the procedure of the most famous
    Stanford parser, and which algorithm has been used to develop the most successful
    statistical parser.
  prefs: []
  type: TYPE_NORMAL
- en: In order to get an idea about the final procedure, we need to first understand
    some building blocks and concepts. Then, we will combine all the concepts to understand
    the overall procedure of building a statistical parser such as the Stanford parser.
  prefs: []
  type: TYPE_NORMAL
- en: Types of grammar
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will see two types of grammar that will help us understand
    the concept of how a parser works. As a prerequisite, we will simply explain them
    and avoid getting too deep into the subject. We will make them as simple as possible
    and we will explore the basic intuition of the concepts that will be used to understand
    the procedure to develop the parser. Here we go!
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of grammar. You can refer to *Figure 5.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: Context-free grammar
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probabilistic context-free grammar
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/7bd04ffa-e515-45d9-ad77-e24594b9a306.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: Types of grammar'
  prefs: []
  type: TYPE_NORMAL
- en: Context-free grammar
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen the basic concept of context-free grammar in [Chapter 3](f65e61fc-1d20-434f-b606-f36cf401fc41.xhtml),
    *Understanding Structure of Sentences*. We have already seen the formal definition
    of the CFG to find a moment and recall it. Now we will see how the rules of grammar
    are important when we build a parser.
  prefs: []
  type: TYPE_NORMAL
- en: 'CFG is also referred to as phrase structure grammar. So, CFG and phrase structure
    grammar are two terms but refer to one concept. Now, let''s see some examples
    related to this type of grammar and then talk about the conventions that are followed
    in order to generate a more natural form of grammar rules. Refer to the grammar
    rules, lexicons, and sentences in *Figure 5.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/420f73a9-3604-49b4-a6bf-9f4ac11b87b3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: CFG rules, lexicon, and sentences'
  prefs: []
  type: TYPE_NORMAL
- en: Here, **S** is the starting point of grammar. **NP** stands for noun phrase
    and **VP** stands for verb phrase. Now we will apply top-down parsing and try
    to generate the given sentence by starting from the rule with the right-hand side
    non-terminal **S** and substitute **S** with **NP** and **VP**. Now substitute
    **NP** with **N** and **VP** with **V** and **NP**, and then substitute **N**
    with people. Substitute **V** with **fish**, **NP** with **N**, and **N** with
    **tank**. You can see the pictorial representation of the given process in *Figure
    5.7:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22964f07-06d4-4c80-854a-804aa52f7962.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7: A parse tree representation of one of the sentences generated by
    the given grammar'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now try to generate a parse tree for the second sentence on your own. If you
    play around with this grammar rule for a while, then you will soon discover that
    it is a very ambiguous one. As well as this we will also discuss the more practical
    aspect of CFG that is used by linguists in order to derive the sentence structure.
    This is a more natural form of CFG and is very similar to the formal definition
    of CFG with one minor change, that is, we define the preterminal symbols in this
    grammar. If you refer to *Figure 5.7*, you will see that symbols such as **N,
    V** are called **preterminal symbols**. Now look at the definition of the natural
    form of CFG in *Figure 5.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c26804c6-9b02-4957-9056-1ef8fa1843c4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.8: A formal representation of a more natural form of CFG'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the ***** symbol includes the existence of an empty sequence. We are starting
    from the **S** symbol, but in a statistical parser we add one more stage, which
    is TOP or ROOT. Therefore, when we generate the parse tree, the main top most
    node is indicated by **S.** Please refer to *Figure 5.7* for more information*.*
    Now we will put an extra node with the symbol ROOT or TOP before **S.**
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed one weird rule in *Figure 5.6*. **NP** can be substituted
    using **e**, which stands for an empty string, so let's see what the use of that
    empty string rule is. We will first look at see an example to get a detailed idea
    about this type of grammar as well as the empty string rule. We will begin with
    the concept of a preterminal because it may be new to you. Take a noun phrase
    in the English language--any phrase containing a determiner such as a, an, or
    the, along with the noun itself. When you substitute **NP** with the symbol **DT**
    and **NN**, you enter actual lexical terminals; where we substitute **NP** with
    **DT** and **NN**, it is called the preterminal symbol. Now let's talk about the
    empty string rule. We have included this rule because, in real life, you will
    find various instances where there are missing parts to a sentence. To handle
    these kinds of instances, we put this empty string rule in grammar. We will now
    give you an example that will help you.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have seen the word sequence, **people fish tank**. From this, you can extract
    two phrases: one is **fish tank** and the second is **people fish**. In both examples,
    there are missing nouns. We will represent these phrases as **e fish tank** and
    **people fish e**. Here, **e** stands for empty string. You will notice that in
    the first phrase, there is a missing noun at the start of the phrase; more technically,
    there is a missing subject. In the second case, there is a missing noun at the
    end of the phrase; more technically, there is a missing object. These kinds of
    situations are very common when dealing with real **natural language** (**NL**)
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: There is one last thing we need to describe, which we will use in the topic
    on **grammar transformation**. Refer to *Figure 5.6*, where you will find the
    rules. Keep referring to these grammar rules as you go. The rule that has only
    an empty string on its right-hand side is called an **empty rule**. You can see
    that there are some rules that have just one symbol on their right side as well
    as on their left side; they are called **unary rules** because you can rewrite
    one category into another category, for example, **NP -> N**. There are also some
    other rules that have two symbols on their right side, such as **VP -> V NP**.
    These kinds of rules are called **binary rules.** There are also some rules that
    have three symbols on their right-hand side; we certain apply some techniques
    to get rid of the kind of rules that have more than two symbols on the right-hand
    side. We will look at these shortly.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have looked at CFG, as well as the concepts needed to understand it.
    You will be able to connect those dots in the following sections. It's now time
    to move on to the next section, which will give you an idea about probabilistic
    CFG.
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic context-free grammar
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In probabilistic grammar, we add the concept of probability. Don't worry - it's
    one of the most simple extensions of CFG that we've seen so far. We will now look
    at **probabilistic context-free grammar** (**PCFG**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define PCFG formally and then explore a different aspect of it. Refer
    to *Figure 5.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5bb01103-db44-4d79-8ea5-3076319ed1da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9: PCFGs formal definition'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, *T*, *N*, *S*, and *R* are similar to CFG; the only new thing here is
    the probability function, so let''s look at that here, the probability function
    takes each grammar rule and gives us the probability value of each rule. This
    probability maps to a real number, *R*. The range for *R* is [0,1]. We are not
    blindly taking any probability value. We enter one constraint where we have defined
    that the sum of the probability for any non-terminal should add up to 1\. Let''s
    look at an example to understand things. You can see the grammar rules with probability
    in *Figure 5.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e01d4e08-6d4e-4144-9d03-46402eabf20c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10: Probabilities for grammar rules'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the lexical grammar rules with probability in *Figure 5.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c4be3fe-94eb-4509-9a67-a2f9808fe44d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.11: Probabilities for lexical rules'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, *Figure 5.10* has three NP rules, and if you look at the probability
    distribution, you will notice the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Its probability adds up to 1 (0.1 + 0.2 + 0.7 = 1.0)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is likely that NP is further rewritten as a noun as its probability is 0.7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the same way, you can see that the first rule at the start of the sentence
    has a value of 1.0 because of a certain event that occurred first. If you look
    carefully, you'll notice that we have removed the empty string rule to make our
    grammar less ambiguous.
  prefs: []
  type: TYPE_NORMAL
- en: So, how are we going to use these probability values? This question leads us
    to the description of calculating the probability of trees and strings.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the probability of a tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we want to calculate the probability of a tree, it is quite easy because
    you need to multiply the probability values of lexicons and grammar rules. This
    will give us the probability of a tree.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at an example to understand this calculation. Here, we will take
    two trees and the sentence for which we have generated trees, **people fish tank
    with rods**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to *Figure 5.12* and *Figure 5.13* for tree structures with their respective
    probability values before we calculate the probability of each tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3640290e-0d53-48e2-81c2-8f0bc269dba3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.12: Parse Tree'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want calculate the probability for the parse tree given in *Figure 5.12*,
    then the steps of obtaining the probability is given as follows. We start scanning
    the tree from the top, so our string point is **S**, the top most node of the
    parse tree. Here, the preposition modifies the verb:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(t1)* = 1.0 * 0.7 * 0.4 * 0.5 * 0.6 * 0.7 * 1.0 * 0.2 * 1.0 * 0.7 * 0.1 =
    0.0008232'
  prefs: []
  type: TYPE_NORMAL
- en: 'The value 0.0008232 is the probability of the tree. Now you can calculate the
    same for another parse tree given in *Figure 5.13*. In this parse tree, the preposition
    modifies the noun. Calculate the tree probability for this parse tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b83b9f10-0d93-4d19-8163-32e20cc1a3ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.13: Second parse tree'
  prefs: []
  type: TYPE_NORMAL
- en: If you calculate the parse tree probability, the value should be 0.00024696.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at the calculation of the probability of string that uses the
    concept of the probability of a tree.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the probability of a string
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Calculating the probability of a string is more complex compared to calculating
    the probability of a tree. Here, we want to calculate the probability of strings
    of words, and for that we need to consider all the possible tree structures that
    generate the string for which we want to calculate the probability. We first need
    to consider all the trees that have the string as part of the tree and then calculate
    the final probability by adding the different probabilities to generate the final
    probability value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s revisit *Figure 5.12* and *Figure 5.13*, which we used to calculate
    the tree probability. Now, in order to calculate the probability of the string,
    we need to consider both the tree and the tree probability and then add those.
    Calculate the probability of the string as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(S) = P(t1) +P(t2)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*= 0.0008232 + 0.00024696*'
  prefs: []
  type: TYPE_NORMAL
- en: '*= 0.00107016*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *t1* tree has a high probability, so a **VP**-attached sentence structure
    is more likely to be generated compared to *t2*, which has **NP** attached to
    it. The reason behind this is that *t1* has a **VP** node with *0.4*, whereas
    *t2* has two nodes, **VP** with a probability of *0.6* and **NP** with a probability
    of *0.2* probability. When you multiply this, you will get *0.12*, which is less
    than *0.4*. So, the *t1* parse tree is the most likely structure.
  prefs: []
  type: TYPE_NORMAL
- en: You should now understand the different types of grammar. Now, it's time to
    explore the concept of grammar transformation for efficient parsing.
  prefs: []
  type: TYPE_NORMAL
- en: Grammar transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Grammar transformation is a technique used to make grammar more restrictive,
    which makes the parsing process more efficient. We will use **Chomsky Normal Form**
    (**CNF**) to transform grammar rules. Let's explore CNF before looking at an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see CNF first. It states that all rules should follow the following
    rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '*X-> Y Z* or *X-> w* where *X, Y, Z* *ε N* and *w* *ε T*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The meaning of the rule is very simple. You should not have more than two non-terminals
    on the right-hand side of any grammar rule; you can include the rules where the
    right-hand side of the rule has a single terminal. To transform the existing grammar
    into CNF, there is a basic procedure that you can follow:'
  prefs: []
  type: TYPE_NORMAL
- en: Empty rules and unary rules can be removed using recursive functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*N*-ary rules are divided by introducing new non-terminals in the grammar rules.
    This applies to rules that have more than two non-terminals on the right-hand
    side. When you use CNF, you can get the same string using new transform rules,
    but its parse structure may differ. The newly generated grammar after applying
    CNF is also CFG.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s look at the intuitive example. We take the grammar rules that we defined
    earlier in *Figure 5.6* and apply CNF to transform those grammar rules. Let''s
    begin. See the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We first remove the empty rules. When you have **NP** on the right-hand side,
    you can have two rules such as **S -> NP VP**, and when you put an empty value
    for **NP**, you will get **S -> VP**. By applying this method recursively, you
    will get rid of the empty rule in the grammar.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we must try to remove unary rules. So, in this case, if you try to remove
    the first unary rule **S -> VP**, then you need to consider all the rules that
    have **VP** on their left-hand side. When you do this, you need to introduce new
    rules because **S** will immediately go to **VP**. We will introduce the rule,
    **S -> V NP**. You need to keep doing this until you get rid of the unary rules.
    When you remove all the unary rules, such as **S -> V**, then you also need to
    change your lexical entries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Refer to *Figure 5.14* for the CNF process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e586546b-4f32-4676-9efa-b238a673835f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.14: CNF steps 1 to 5'
  prefs: []
  type: TYPE_NORMAL
- en: You can see the final result of the CNF process in *Figure 5.15:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87990088-3a73-46d7-9fc3-816c1da8070c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.15: Step 6 - Final grammar rules after applying CNF'
  prefs: []
  type: TYPE_NORMAL
- en: In real life, it is not necessary to apply full CNF, and it can often be quite
    painful to do so. It just makes parsing more efficient and your grammar rules
    cleaner. In real-life applications, we keep unary rules as our grammar rules because
    they tell us whether a word is treated as a verb or noun, as well as the non-terminal
    symbol information, which means that we have the information of the POS tag.
  prefs: []
  type: TYPE_NORMAL
- en: That's enough of the boring conceptual part. Now it's time to combine all the
    basic concepts of parsers and parsing to learn the algorithm that is used to develop
    a parser.
  prefs: []
  type: TYPE_NORMAL
- en: Developing a parser with the Cocke-Kasami-Younger Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the English language, there are plenty of parsers that you can use, CNF
    if you want to build a parser for any other language, you can use the **Cocke-Kasami-Younger**
    (**CKY**) algorithm. Here, we will look at some information that will be useful
    to you in terms of making a parser. We will also look at the main logic of the
    CKY algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: We need to look at the assumption that we are considering before we start with
    the algorithm. Our technical assumption is that, here, each of the parser subtrees
    is independent. This means that if we have a tree node NP, then we just focus
    on this NP node and not on the node its, derived from; each of the subtrees act
    independently. The CKY algorithm can give us the result in cubic time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s look at the logic of the CKY algorithm. This algorithm takes words
    from the sentences and tries to generate a parse tree using bottom-up parsing.
    Here, we will define a data structure that is called a **parse triangle** or **chart**.
    Refer to *Figure 5.16*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/59c8c4e8-4528-4ba0-ba97-e1933d264000.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.16: Parse triangle for the CKY algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: 'Its bottom cells represent single words such as **fish**, **people**, **fish**,
    and **tanks**. The cells in the middle row represent the overlapped word pairs
    such as **Fish people**, **People fish**, and **fish tanks**. Its third row represents
    the pair of two words without overlapping such as **Fish people** and **fish tanks**.
    The last row represents the top or root of the sentence. To understand the algorithm,
    we first need the grammar rules of rule probability. To understand the algorithm,
    we should refer to *Figure 5.17*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e868283-8d6c-44ab-b678-3576643852a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.17: To understand the CKY algorithm (Image credit: http://spark-public.s3.amazonaws.com/nlp/slides/Parsing-Probabilistic.pdf
    page no: 36)'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 5.17*, to explain the algorithm logic, we have entered
    the basic probability values in the bottom-most cells. Here, we need to find all
    the combinations that the fulfill grammar rules. Follow the given steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We will first take **NP** from the **people** cell and **VP** from the **fish**
    cell. In the grammar rules, check whether there is any grammar rule present that
    takes the sequence, **NP VP**, which you will need to find on the right-hand side
    of the grammar rule. Here, we found that the rule is **S -> NP VP** with a probability
    of 0.9.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, calculate the probability value, and to find this, you need to multiply
    the probability value of **NP** given in the people cell, the probability value
    of **VP** in the **fish** cell, and the probability value of the grammar rule
    itself. So here, the probability value of **NP** placed in the **people** cell
    *=* 0.35, the probability value of **VP** placed in the **fish** cell *=0.06*
    and the probability of the grammar rule **S -> NP VP** *=* 0.9.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then multiply 0.35 (the probability of **NP** placed in the **people** cell)
    * 0.06 (the probability of **VP** in the **fish** cell ) * 0.9 (the probability
    of the grammar rule **S -> NP VP**). Therefore, the final multiplication value
    *= 0.35 * 0.06 * 0.9 = 0.0189*. *0.0189* is the final probability for the grammar
    rule if we expand **S** into the **NP VP** grammar rule.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the same way, you can calculate other combinations, such as **NP** from the
    **people** cell and **NP** from the **fish** cell, and find the grammar rule,
    that is, **NP NP** on the right-hand side. Here, the **NP - NP NP** rule exists.
    So we calculate the probability value, *0.35 * 0.14 * 0.2 = 0.0098*. We continue
    with this process until we generate the probability value for all the combinations,
    and then we will see for which combination we have generated the maximum probability.
    The process of finding the maximum probability is called **Viterbi max score**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the combination **S -> NP VP**, we will get the maximum probability when
    the cells generate the left-hand side non-terminal on its upward cell. So, those
    two cells generate **S**, which is a sentence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is the core logic of the CKY algorithm. Let''s look at one concrete example
    for this concept. For writing purposes, we will rotate the parse triangle 90 degrees
    clockwise. Refer to *Figure 5.18*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6680024-6a6e-416e-889e-f6de607e9c37.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.18: Step 1 for the CKY algorithm (Image credit: http://spark-public.s3.amazonaws.com)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *cell (0,1)* is for **fish** and it fills using lexical rules. We have
    put **N** -> **fish** with *0.2* probability because this is defined in our grammar
    rule. We have put **V -> fish** with *0.6* probability. Now we focus on some unary
    rules that have **N** or **V** only on the right-hand side. We have the rules
    that we need to calculate the probability by considering the grammar rules probability
    and lexical probability. So, for rule **NP -> N**, the probability is *0.7* and
    **N -> fish** has the probability value *0.2*. We need to multiply this value
    and generate the probability of the grammar rule **NP -> N** *= 0.14*. In the
    same way, we generate the probability for the rule **VP -> V**, and its value
    is *0.1 * 0.6 = 0.6*. This way, you need to fill up all the four cells.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next stage, we follow the same procedure to get the probability for
    each combination generated from the grammar rules. Refer to *Figure 5.19*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/04383ecf-5951-4e09-abc1-e8f34884570c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.19 : Stage 2 of the CKY algorithm (Image credit: http://spark-public.s3.amazonaws.com)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 5.20*, you can see the final probability values, using which you
    can decide the best parse tree for the given data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d12f49a6-e874-475a-b56b-5d6fd3063327.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.20: The final stage of the CKY algorithm (Image credit: http://spark-public.s3.amazonaws.com)'
  prefs: []
  type: TYPE_NORMAL
- en: Now you know how the parse tree has been generated we want to share with you
    some important facts regarding the Stanford parser. It is built based on this
    CKY algorithm. There are a couple of technical assumptions and improvisations
    applied to the Stanford parser, but the following are the core techniques used
    to build the parser.
  prefs: []
  type: TYPE_NORMAL
- en: Developing parsers step-by-step
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we will look at the steps required to build your own parser with the
    help of the CKY algorithm. Let''s begin summarizing:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You should have tagged the corpus that has a human-annotated parse tree: if
    it is tagged as per the Penn Treebank annotation format, then you are good to
    go.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With this tagged parse corpus, you can derive the grammar rules and generate
    the probability for each of the grammar rules.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You should apply CNF for grammar transformation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the grammar rules with probability and apply them to the large corpus; use
    the CKY algorithm with the Viterbi max score to get the most likely parse structure.
    If you are providing a large amount of data, then you can use the ML learning
    technique and tackle this problem as a multiclass classifier problem. The last
    stage is where you get the best parse tree for the given data as per the probability
    value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That's enough theory; let's now use some of the famous existing parser tools
    practically and also check what kind of features you can generate from the parse
    tree.
  prefs: []
  type: TYPE_NORMAL
- en: Existing parser tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at some of the existing parsers and how you can
    generate some cool features that can be used for ML algorithms or in rule-based
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will see two parsers:'
  prefs: []
  type: TYPE_NORMAL
- en: The Stanford parser
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The spaCy parser
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Stanford parser
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s begin with the Stanford parser. You can download it from [https://stanfordnlp.github.io/CoreNLP/](https://stanfordnlp.github.io/CoreNLP/).
    After downloading it, you just need to extract it to any location you like. The
    prerequisite of running the Stanford parser is that you should have a Java-run
    environment installed in your system. Now you need to execute the following command
    in order to start the Stanford parser service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, you can change the memory from `-mx4g` to `-mx3g`.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at the concept of dependency in the parser before can fully concentrating
    on the coding part.
  prefs: []
  type: TYPE_NORMAL
- en: The dependency structure in the parser shows which words depend on other words
    in the sentence. In the sentence, some words modify the meaning of other words;
    on the other hand, some act as an argument for other words. All of these kinds
    of relationships are described using dependencies. There are several dependencies
    in the Stanford parser. We will go through some of them. Let's take an example
    and we will explain things as we go.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sentence is: The boy put the tortoise on the rug.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this given sentence, the **head** of the sentence is *put* and it modifies
    three sections: *boy*, *tortoise*, and *on the rug*. How do you find the head
    word of the sentence?Find out by asking the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Who put it down? You get the answer: *boy*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, what thing did he put down? You get the answer: *tortoise*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Where is it put? You get the answer: *on the rug*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, the word **put** modifies three things. Now look at the word *boy*, and
    check whether there is any modifier for it. Yes, it has a modifier: **the**. Then,
    check whether there is any modifier for *tortoise*. Yes, it has a modifier: **the**.
    For the phrase *on the rug* *on* complements *rug* and *rug* acts as the head
    for this phrase, taking a modifier, *the*. Refer to *Figure 5.21*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5bf923f7-b71b-4d89-a5af-6bc2a28137eb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.21: Dependency structure of the sentence'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Stanford parser has dependencies like `nsubjpass` (passive nominal subject)
    , `auxpass` (passive auxiliary) , `prep` (prepositional modifier), `pobj` (object
    of preposition), `conj` (conjunct), and so on. We won''t go into more detail regarding
    this, but is worth mentioning that dependency parsing also follows the tree structure
    and it''s linked by binary asymmetric relations called **dependencies**. You can
    find more details about each of the dependencies by accessing the Stanford parser
    document here: [https://nlp.stanford.edu/software/dependencies_manual.pdf](https://nlp.stanford.edu/software/dependencies_manual.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the basic example in *Figure 5.22*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/026b30ae-54c5-46fc-bae7-dd3a088116a5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.22: Dependency parsing of the sentence'
  prefs: []
  type: TYPE_NORMAL
- en: Now if you want to use the Stanford parser in Python, you have to use the dependency
    named `pycorenlp`. We will use it to generate the output from the Stanford parser.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the sample code in which we have used the Stanford parser to parse
    the sentence. You can parse multiple sentences as well. You can find the code
    at the following GitHub link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/jalajthanaki/NLPython/tree/master/ch5/parserexample](https://github.com/jalajthanaki/NLPython/tree/master/ch5/parserexample).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the code snippet in *Figure 5.23*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/157cb610-6435-4dea-a3ea-32aa9fd8e12c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.23: Code snippet for the Stanford parser demo'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the output of this code in *Figure 5.24*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e8ae2b9-6d21-4567-8d1b-284e810a0ccc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.24: Output of the Stanford parser'
  prefs: []
  type: TYPE_NORMAL
- en: The spaCy parser
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This parser helps you generate the parsing for the sentence. This is a dependency
    parser. You can find the code at the following GitHub link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch5/parserexample/scpacyparserdemo.py](https://github.com/jalajthanaki/NLPython/blob/master/ch5/parserexample/scpacyparserdemo.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code snippet of this parser in *Figure 5.25*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e0fcafd-dc68-49c7-82d9-a91fa5011eeb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.25: spaCy dependency parser code'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the output of the spaCy parser in *Figure 5.26*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8589e6a8-17ff-4f81-a2a4-1085618b98d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.26: The spaCy parser output'
  prefs: []
  type: TYPE_NORMAL
- en: People used the Stanford parser because it provides good accuracy as well as
    a lot of flexibility in terms of generating output. Using the Stanford parser,
    you can generate the output in a JSON format, XML format, or text format. You
    may think that we get the parse tree using the preceding code, but the kind of
    features that we can derive from the parsing result will be discussed in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting and understanding the features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generally, using the parse result, you can derive many features such as generating
    noun phrases and POS tags inside the noun phrase; you can also derive the head
    word from phrases. You can use each word and its tag. You can use the dependency
    relationships as features. You can see the code snippet in *Figure 5.27*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2082d31e-85fd-4bd0-a6b0-5f78955069cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.27: Code to get NP from sentences'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output snippet is in *Figure 5.28*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68cef596-0bdf-4b4c-a534-8579fd709b34.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.28: Output all NP from sentences'
  prefs: []
  type: TYPE_NORMAL
- en: You can generate the stem as well as lemma from each word, as we saw in [Chapter
    3](f65e61fc-1d20-434f-b606-f36cf401fc41.xhtml), *Understanding Structure of Sentences*.
  prefs: []
  type: TYPE_NORMAL
- en: In real life, you can generate the features easily using libraries, but which
    features you need to use is critical and depends on your NLP application. Let's
    assume that you are making a grammar correction system; in that case, you need
    to consider all the phrases of the sentence as well as the POS tags of each word
    present in the phrase. If you are developing a question-answer system, then noun
    phases and verb phrases are the important features that you can select.
  prefs: []
  type: TYPE_NORMAL
- en: Features selection is a bit tricky and you will need to do some iteration to
    get an idea of which features are good for your NLP application. Try to dump your
    features in a `.csv` file so you can use the `.csv` file later on in your processing.
    Each and every feature can be a single column of the `.csv` file. For example,
    you have NP words stored in one column, lemma in the other column for all words
    in NP, and so on. Now, suppose you have more than 100 columns; in that case, you
    would need to find out which are the important columns (features) and which are
    not. Based on the problem statement and features, you can decide what the most
    important features are that help us to solve our problem. In [Chapter 8](97808151-90d2-4034-8d53-b94123154265.xhtml),
    *Machine Learning for NLP Problems,* we will look at features selection in more
    detail.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing parser tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In real life, datasets are quite complex and messy. In that case, it may be
    that the parser is unable to give you a perfect or accurate result. Let's take
    an example.
  prefs: []
  type: TYPE_NORMAL
- en: Let's assume that you want to parse a dataset that has text content of research
    papers, and these research papers belong to the chemistry domain. If you are using
    the Stanford parser in order to generate a parse tree for this dataset, then sentences
    that contain chemical symbols and equations may not get parsed properly. This
    is because the Stanford parser has been trained on the Penn TreeBank corpus, so
    it's accuracy for the generation of a parse tree for chemical symbols and equation
    is low. In this case, you have two options - either you search for a parser that
    can generate parsing for symbols and equations accurately or if you have a corpus
    that has been tagged, you have the flexibility of retraining the Stanford parser
    using your tagged data.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can follow the same tagging notation given in the Penn TreeBank data for
    your dataset, then use the following command to retrain the Stanford parser on
    your dataset, save the trained model, and use it later on. You can use the following
    command to retrain the Stanford Parser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are some of the challenges related to parsers:'
  prefs: []
  type: TYPE_NORMAL
- en: To generate a parser for languages such as Hebrew, Gujarati, and so on is difficult
    and the reason is that we don't have a tagged corpus.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing parsers for fusion languages is difficult. A fusion language means
    that you are using another language alongside the English language, with a sentence
    including more than one language. Processing these kinds of sentences is difficult.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have understood some features of the parser, we can move on to our
    next concept, which is POS tagging. This is one of the essential concepts of NLP.
  prefs: []
  type: TYPE_NORMAL
- en: POS tagging and POS taggers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss the long-awaited topic of POS tags.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the concept of POS tagging and POS taggers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: POS tagging is defined as the process of marking words in the corpus corresponding
    to their particular part of speech. The POS of the word is dependent on both its
    definition and its context. It is also called **grammatical tagging** or **word-category
    disambiguation**. POS tags of words are also dependent on their relationship with
    adjacent and related words in the given phrase, sentence, and paragraph.
  prefs: []
  type: TYPE_NORMAL
- en: 'POS tagger is the tool that is used to assign POS tags for the given data.
    Assigning the POS tags is not an easy task because POS of words is changed as
    per the sentence structure and meaning. Let''s take an example. Let''s take the
    word dogs; generally, we all know that dogs is a plural noun, but in some sentences
    it acts as a verb. See the sentence: **The sailor dogs the hatch**. Here, the
    correct POS for *dogs* is the verb and not the plural noun. Generally, many POS
    taggers use the POS tags that are generated by the University of Pennsylvania.
    You can find word-level POS tags and definitions at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now touch on some of the POS tags. There are 36 POS tags in the Penn
    Treebank POS list, such as **NN** indicates noun, **DT** for determiner words,
    and **FW** for foreign words. The word that is new to POS tags is generally assigned
    the **FW** tag. Latin names and symbols often get the **FW** tag by POS tagger.
    So, if you have a (lambda) symbol then POS may tagger suggest the **FW** POS tag
    for it. You can see some word-level POS tags in *Figure 5.29*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8084df2c-6e1f-43d9-a1ef-b21563bff63f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.29: Some word-level POS tags'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are POS tags available at the phrase-level as well as the clause-level.
    All of these tags can be found at the following GitHub link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch5/POStagdemo/POS_tags.txt](https://github.com/jalajthanaki/NLPython/blob/master/ch5/POStagdemo/POS_tags.txt).'
  prefs: []
  type: TYPE_NORMAL
- en: See each of the tags given in the file that we have specified as they are really
    useful when you evaluating your parse tree result. POS tags and their definitions
    are very straightforward, so if you know basic English grammar, then you can easily
    understand them.
  prefs: []
  type: TYPE_NORMAL
- en: You must be curious to know how POS taggers are built. Let's find out the procedure
    of making your own POS tagger.
  prefs: []
  type: TYPE_NORMAL
- en: Developing POS taggers step-by-step
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To build your own POS tagger, you need to perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: You need a tagged corpus.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform training using a decision tree classifier available in the Python library,
    `scikit-learn`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check your accuracy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try to predict the POS tags using your own trained model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A great part of this section is that we will code our own POS tagger in Python,
    so you guys will get an idea of how each of the preceding stages are performed
    in reality. If you don't know what a decision tree algorithm is, do not worry
    - we will cover this topic in more detail in [Chapter 8](97808151-90d2-4034-8d53-b94123154265.xhtml),
    *Machine Learning for NLP Applications*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will see a practical example that will help you understand the process
    of developing POS taggers. You can find the code snippet for each stage and you
    can access the code at the following GitHub link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/jalajthanaki/NLPython/tree/master/ch5/CustomPOStagger](https://github.com/jalajthanaki/NLPython/tree/master/ch5/CustomPOStagger).'
  prefs: []
  type: TYPE_NORMAL
- en: 'See the code snippet of getting the Pann TreeBank corpus in *Figure 5.30*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/259c9478-af35-4f67-91e1-b4b747bdc541.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.30: Load the Penn TreeBank data from NLTK'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the feature selection code snippet in *Figure 5.31*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8342709c-f9ea-4b81-9b00-5564421c08ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.31: Extracting features of each word'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have to extract the features for each word. You can see the code snippet
    for some basic transformation such as splitting the dataset into training and
    testing. Refer to *Figure 5.32*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c8cb7a1-beac-41ae-bd89-e4d16ae656f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.32: Splitting the data into training and testing'
  prefs: []
  type: TYPE_NORMAL
- en: 'See the code to train the model using a decision tree algorithm in *Figure
    5.33*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2d9f363-6bb4-43fd-acd6-ca74c40c98c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.33: Actual training using a decision tree algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: 'See the output prediction of POS tags for the sentence that you provided in
    *Figure 5.34*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6956da11-f9bd-4be9-b762-b947e839c581.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.34: Output of a custom POS tagger'
  prefs: []
  type: TYPE_NORMAL
- en: You should have now understood the practical aspect of making your own POS tags,
    but you can still also use some of the cool POS taggers.
  prefs: []
  type: TYPE_NORMAL
- en: Plug and play with existing POS taggers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many POS taggers available nowadays. Here, we will use the POS tagger
    available in the Stanford CoreNLP and polyglot library. There are others such
    as the Tree tagger; NLTK also has a POS tagger that you can use. You can find
    the code at the following GitHub link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch5/POStagdemo](https://github.com/jalajthanaki/NLPython/blob/master/ch5/POStagdemo/).'
  prefs: []
  type: TYPE_NORMAL
- en: A Stanford POS tagger example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can see the code snippet for the Stanford POS tagger in *Figure 5.35*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6133b79c-6dd9-463d-ae4d-05bcb4a45de9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.35: Stanford POS tagger code'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output from the Stanford POS tagger can be found in *Figure 5.36*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98fd3b01-e8cd-457a-ab3a-7143a5a0c890.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.36: POS tags generated by the Stanford POS tagger'
  prefs: []
  type: TYPE_NORMAL
- en: Using polyglot to generate POS tagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can see the code snippet for the `polyglot` POS tagger in *Figure 5.37*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/71fdba9e-3649-476b-8b26-6aad00d4a5b4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.37: Polyglot POS tagger'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output from the `polyglot` POS tagger can be found in *Figure 5.38*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b28aea32-579b-4a97-ab1d-38b5e61215f0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.38:The polyglot POS tagger output
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Try using the TreeTagger library to generate POS tagging. You can find the
    installation details at this link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/.](http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/)'
  prefs: []
  type: TYPE_NORMAL
- en: Using POS tags as features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have generated POS tags for our text data using the POS tagger,
    where can we use them? We will now look at NLP applications that can use these
    POS tags as features.
  prefs: []
  type: TYPE_NORMAL
- en: POS tags are really important when you are building a chatbot with machine learning
    algorithms. POS tag sequences are quite useful when a machine has to understand
    various sentence structures. It is also useful if you are building a system that
    identifies **multiword express** (**MWE**). Some examples of MWE phrases are be
    able to, a little bit about, you know what, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have a sentence: **He backed off from the tour plan of Paris**. Here,
    *backed off* is the MWE. To identify these kinds of MWEs in sentences, you can
    use POS tags and POS tag sequences as features. You can use a POS tag in sentiment
    analysis, and there are other applications as well.'
  prefs: []
  type: TYPE_NORMAL
- en: Challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some challenges for POS tags:'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the right POS tag for certain words in an ambiguous syntax structure
    is difficult, and if the word carries a very different contextual meaning, then
    the POS tagger may generate the wrong POS tags.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing a POS tagger for Indian languages is a bit difficult because, for
    some languages, you cannot find the tagged dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let's move on to the next section, where we will learn how to find the different
    entities in sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Name entity recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at a tool called **name entity recognition** (**NER**).
    The use of this tool is as follows. If you have a sentence, such as **Bank of
    America announced its earning today**, we as humans can understand that the *Bank
    of America* is the name of a financial institution and should be referred to as
    a single entity. However, for machine to handle and recognize that entity is quite
    challenging. There is where NER tools come into the picture to rescue us.
  prefs: []
  type: TYPE_NORMAL
- en: With the NER tool, you can find out entities like person name, organization
    name, location, and so on. NER tools have certain classes in which they classify
    the entities. Here, we are considering the words of the sentence to find out the
    entities, and if there are any entities present in the sentence. Let's get some
    more details about what kind of entities we can find in our sentence using some
    of the available NER tools.
  prefs: []
  type: TYPE_NORMAL
- en: Classes of NER
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NER tools generally segregate the entities into some predefined classes. Different
    NER tools have different types of classes. The Stanford NER tool has three different
    versions based on the NER classes:'
  prefs: []
  type: TYPE_NORMAL
- en: The first version is the three-class NER tool that can identify the entities
    - whether it's Location, Person, or Organization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second version is the four-class NER tool that can identify the Location,
    person, Organization, and Misc. Misc is referred to as a miscellaneous entity
    type. If an entity doesn't belong to Location, Person, or Organization and is
    still an entity, then you can tag it as Misc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third version is a seven-class tool that can identify Person, Location,
    Organization, Money, Percent, Date, and Time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The spaCy parser also has an NER package available with the following classes.
  prefs: []
  type: TYPE_NORMAL
- en: '`PERSON` class identifies the name of a person'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NORP` class meaning Nationality, Religious or Political groups'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FACILITY` class including buildings, airports, highways, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ORG` class for organization, institution and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GPE` class for cities, countries and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LOC` class for non-GPE locations such as mountain ranges and bodies of water'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PRODUCT` that includes objects, vehicles, food, and so on, but not services'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EVENT` class for sports events, wars, named hurricanes, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WORK_OF_ART` class for titles of books, songs, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LANGUAGE` that tags any named language'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from this, spaCy's NER package has classes such as date, time, percent,
    money, quantity, ordinal, and cardinal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now it's time to do some practical work. We will use the Stanford NER tool and
    spaCy NER in our next section.
  prefs: []
  type: TYPE_NORMAL
- en: Plug and play with existing NER tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will look at the coding part as well as information on
    how to practically use these NER tools. We will begin with the Stanford NER tool
    and then the Spacy NER. You can find the code at the following GitHub link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/jalajthanaki/NLPython/tree/master/ch5/NERtooldemo](https://github.com/jalajthanaki/NLPython/tree/master/ch5/NERtooldemo).'
  prefs: []
  type: TYPE_NORMAL
- en: A Stanford NER example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the code and output snippet as follows. You need to download the
    Stanford NER tool at [https://nlp.stanford.edu/software/CRF-NER.shtml#Download](https://nlp.stanford.edu/software/CRF-NER.shtml#Download).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the code snippet in *Figure 5.39*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0892d055-ab16-411e-8557-d21b1114c73f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.39: Stanford NER tool code'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the output snippet in *Figure 5.40*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/427a4a7c-7cfc-4f25-bc2f-c0267a910e83.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.40: Output of Stanford NER'
  prefs: []
  type: TYPE_NORMAL
- en: A Spacy NER example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find the code and output snippet as follows. You can see the code snippet
    in *Figure 5.41*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58328dac-aebe-4c68-9d52-1abe10e77d4a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.41: spaCy NER tool code snippet'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the output snippet in *Figure 5.42*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe8df511-f2dd-474a-9913-bf0535b1b44c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.42: Output of the spaCy tool'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting and understanding the features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NER tags are really important because they help you to understand sentence structure
    and help machines or NLP systems to understand the meaning of certain words in
    a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take an example. If you are building a proofreading tool, then this NER
    tool is very useful because NER tools can find a person's name, an organizations'
    name, currency-related symbols, numerical formats, and so on that will help your
    proofreading tool identify exceptional cases present in text. Then, according
    to the NER tag, the system can suggest the necessary changes. Take the sentence,
    **Bank of America announced its earning today morning**. In this case, the NER
    tool gives the tag organization for *Bank of America*, which helps our system
    better understand the meaning of the sentence and the structure of the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: NER tags are also very important if you are building a question-answer system
    as it is very crucial to extract entities in this system. Once you have generated
    the entities, you can use a syntactic relationship in order to understand questions.
    After this stage, you can process the question and generate the answer.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are certain challenges for the NER system, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: NER tools train on a closed domain dataset. So, an NER system developed for
    one domain does not typically perform well on an other domain. This requires a
    universal NER tool that can work for all domains, and after training it should
    able to generalize enough to deal with unseen situations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes you will find words which are the names of locations as well as the
    name of a person. The NER tool can't handle a case where one word can be expressed
    as the location name, person name, and organization name. This is a very challenging
    case for all NER tools. Suppose you have word TATA hospital; the single the words
    TATA can be the name of a person as well as the name of an organization. In this
    case, the NER tool can't decide whether TATA is the name of a person or the name
    of an organization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To build an NER tool specifically for microblogging web platforms is also a
    challenging task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's move on to the next section, which is about n-gram algorithms. You will
    get to learn some very interesting stuff.
  prefs: []
  type: TYPE_NORMAL
- en: n-grams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: n-gram is a very popular and widely used technique in the NLP domain. If you
    are dealing with text data or speech data, you can use this concept.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at the formal definition of n-grams. An n-gram is a continuous sequence
    of n items from the given sequence of text data or speech data. Here, items can
    be phonemes, syllables, letters, words, or base pairs according to the application
    that you are trying to solve.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some versions of n-grams that you will find very useful. If we put
    n=1, then that particular n-gram is referred to as a unigram. If we put n=2, then
    we get the bigram. If we put n=3, then that particular n-gram is referred to as
    a trigram, and if you put n=4 or n=5, then these versions of n-grams are referred
    to as four gram and five gram, respectively. Now let''s take some examples from
    different domains to get a more detailed picture of n-grams. See examples from
    NLP and computational biology to understand a unigram in *Figure 5.43*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22d797e8-3c08-4f7b-a130-a9910ea82a98.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.43: Unigram example sequence'
  prefs: []
  type: TYPE_NORMAL
- en: 'You have seen unigrams. Now we will look at the bigram. With bigrams, we are
    considering overlapped pairs, as you can see in the following example. We have
    taken the same NLP and computational biology sequences to understand bigrams.
    See *Figure 5.44*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b9aa773-e7aa-4bfb-876a-082eed4ee00d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.44: Bigram example sequence'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you understood the bigram overlapped pairing concept from the example, then
    a trigram will be easier for you to understand. A trigram is just an extension
    of the bigram, but if you are still confused, then let''s explain it for you in
    laymans'' terms. In the first three rows of *Figure 5.44*, we generated a character-based
    bigram and the fourth row is a word-based bigram. We will start from the first
    character and consider the very next character because we are considering n=2
    and the same is applicable to words as well. See the first row where we are considering
    a bigram such as *AG* as the first bigram. Now, in the next iteration, we are
    considering *G* again and generate *GC*. In the next iteration, we are considering
    *C* again and so on. For generating a trigram, see the same examples that we have
    looked at previously for. Refer to *Figure 5.45*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6cf426f-2386-4eb9-bc01-a593893c4c0e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.45: Trigram example sequence'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding examples are very much self-explanatory. You can figure out how
    we are taking up the sequencing from the number of n. Here, we are taking the
    overlapped sequences, which means that if you are taking a trigram and taking
    the words **this**, **is**, and **a** as a single pair, then next time, you are
    considering **is**, **a**, and **pen**. Here, the word *is* overlaps, but these
    kind of overlapped sequences help store context. If we are using large values
    for n-five-gram or six-gram, we can store large contexts but we still need more
    space and more time to process the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding n-gram using a practice example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we are going to implement n-gram using the `nltk` library. You can see
    the code at this GitHub link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/jalajthanaki/NLPython/tree/master/ch5/n_gram](https://github.com/jalajthanaki/NLPython/tree/master/ch5/n_gram).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the code snippet in *Figure 5.46*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06f637d3-27a3-490c-b05e-33e2b85e2a31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.46: NLTK n-gram code'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the output code snippet in *Figure 5.47*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee9c7255-a8f7-4d7f-ac85-0e3036b5a6f4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.47: Output of the n-gram'
  prefs: []
  type: TYPE_NORMAL
- en: Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will see what kinds of applications n-gram has been used
    in:'
  prefs: []
  type: TYPE_NORMAL
- en: If you are making a plagiarism tool, you can use n-gram to extract the patterns
    that are copied, because that's what other plagiarism tools do to provide basic
    features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computational biology has been using n-grams to identify various DNA patterns
    in order to recognize any unusual DNA pattern; based on this, biologists decide
    what kind of genetic disease a person may have
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now let''s move on to the next concept, which is an easy but very useful concept
    for NLP applications: Bag of words.'
  prefs: []
  type: TYPE_NORMAL
- en: Bag of words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Bag of words** (**BOW**) is the technique that is used in the NLP domain.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding BOW
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This BOW model makes our life easier because it simplifies the representation
    used in NLP. In this model, the data is in the form of text and is represented
    as the bag or multiset of its words, disregarding grammar and word order and just
    keeping words. Here, text is either a sentence or document. Let's an example to
    give you a better understanding of BOW.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take the following sample set of documents:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Text document 1: John likes to watch cricket. Chris likes cricket too.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Text document 2: John also likes to watch movies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on these two text documents, you can generate the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This list is called **BOW**. Here, we are not considering the grammar of the
    sentences. We are also not bothered about the order of the words. Now it''s time
    to see the practical implementation of BOW. BOW is often used to generate features;
    after generating BOW, we can derive the term-frequency of each word in the document,
    which can later be fed to a machine learning algorithm. For the preceding documents,
    you can generate the following frequency list:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Frequency count for Document 1: [1, 2, 1, 1, 2, 1, 1, 0, 0]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Frequency count for Document 2: [1, 1, 1, 1, 0, 0, 0, 1, 1]'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, how did we generate the list of frequency counts? In order to generate
    the frequency count of Document 1, consider the list of words and check how many
    times each of the listed words appear in Document 1\. Here, we will first take
    the word, *John*, which appears in Document 1 once; the frequency count for Document
    1 is 1\. **Frequency count for Document 1: [1]**. For the second entry, the word
    *like* appears twice in Document 1, so the frequency count is 2\. **Frequency
    count for Document 1: [1, 2].** Now, we will take the third word from our list
    and the word is *to***.** This word appears in Document 1 once, so we make the
    third entry in the frequency count as 1\. **Frequency count for Document 1: [1,
    2, 1].** We have generated the frequency count for Document 1 and Document 2 in
    the same way. We will learn more about frequency in the upcoming section, TF-IDF,
    in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding BOW using a practical example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will look at the practical implementation of BOW using
    `scikit-learn`. You can find the code at this GitHub link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch5/bagofwordsdemo/BOWdemo.py](https://github.com/jalajthanaki/NLPython/blob/master/ch5/bagofwordsdemo/BOWdemo.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'See the code snippet in *Figure 5.48*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77214fed-ef02-4e3a-99ce-7c5a3cfd8791.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.48: BOW scikit-learn implementation'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first row of the output belongs to the first document with the word, `words`,
    and the second row belongs to the document with the word, `wprds`. You can see
    the output in *Figure 5.49*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f298bed-8b6e-40c8-b447-4fdc6d00033a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.49: BOW vector representation'
  prefs: []
  type: TYPE_NORMAL
- en: Comparing n-grams and BOW
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have looked at the concepts of n-grams and BOW. So, let's now see how n-grams
    and BOW are different or related to each other.
  prefs: []
  type: TYPE_NORMAL
- en: Let's first discuss the differences. Here, the difference is in terms of their
    usage in NLP applications. In n-grams, word order is important, whereas in BOW
    it is not important to maintain word order. During the NLP application, n-gram
    is used to consider words in their real order so we can get an idea about the
    context of the particular word; BOW is used to build vocabulary for your text
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at some meaningful relationships between n-grams and BOW that
    will give you an idea of how n-grams and BOW are related to each other. If you
    are considering n-gram as a feature, then BOW is the text representation derived
    using a unigram. So, in that case, an n-gram is equal to a feature and BOW is
    equal to a representation of text using a unigram (one-gram) contained within.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's check out an application of BOW.
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will look at which applications use BOW as features in
    the NLP domain:'
  prefs: []
  type: TYPE_NORMAL
- en: If you want to make an NLP application that classifies documents in different
    categories, then you can use BOW.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BOW is also used to generate frequency count and vocabulary from a dataset.
    These derived attributes are then used in NLP applications such as sentiment analysis,
    Word2vec, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now it's time to look at some of the semantic tools that we can use if we want
    to include semantic-level information in our NLP applications.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic tools and resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Trying to get the accurate meaning of a natural language is still a challenging
    task in the NLP domain, although we do have some techniques that have been recently
    developed and resources that we can use to get semantics from natural language.
    In this section, we will try to understand these techniques and resources.
  prefs: []
  type: TYPE_NORMAL
- en: The latent semantic analysis algorithm uses t**erm frequency - inverse document
    Frequency** (**tf-idf**) and the concept of linear algebra, such as cosine similarity
    and Euclidean distance, to find words with similar meanings. These techniques
    are a part of distributional semantics. The other one is word2vec. This is a recent
    algorithm that has been developed by Google and can help us find the semantics
    of words and words that have similar meanings. We will explore word2vec and other
    techniques in [Chapter 6](c4861b9e-2bcf-4fce-94d4-f1e2010831de.xhtml), *Advance
    Features Engineering and NLP Algorithms*.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from Word2vec, another powerful resource is `WordNet`, which is the largest
    corpus available to us and it's tagged by humans. It also contains sense tags
    for each word. These databases are really helpful for finding out the semantics
    of a particular word.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can have a look at `WordNet` at the following link: [https://wordnet.princeton.edu/](https://wordnet.princeton.edu/)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have listed some of the most useful resources and tools for generating
    semantics. There is a lot of room for improvement in this area.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen most of the NLP domain-related concepts and we have also seen how
    we can derive features using these concepts and available tools. Now it's time
    to jump into the next section, which will give us information about statistical
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Basic statistical features for NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last section, we looked at most of the NLP concepts, tools, and algorithms
    that can be used to derive features. Now it's time to learn about some statistical
    features as well. Here, we will explore the statistical aspect. You will learn
    how statistical concepts help us derive some of the most useful features.
  prefs: []
  type: TYPE_NORMAL
- en: Before we jump into statistical features, as a prerequisite, you need to understand
    basic mathematical concepts, linear algebra concepts, and probabilistic concepts.
    So here, we will seek to understand these concepts first and then understand the
    statistical features.
  prefs: []
  type: TYPE_NORMAL
- en: Basic mathematics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will begin with the basics of linear algebra and probability; this is because
    we want you to recall and memorize the necessary concepts so it will help you
    in this chapter as well as the upcoming chapters. We will explain the necessary
    math concepts as and when needed.
  prefs: []
  type: TYPE_NORMAL
- en: Basic concepts of linear algebra for NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will not look at all the linear algebra concepts in great
    detail. The purpose of this section is to get familiar with the basic concepts.
    Apart from the given concepts, there are many other concepts that can be used
    in NLP applications. Here, we will cover only the much needed concepts. We will
    give you all the necessary details about algorithms and their mathematical aspects
    in upcoming chapters. Let's get started with the basics.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four main terms that you will find consistently in NLP and ML:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalars**: They are just single, the real number'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vectors**: They are a one-dimensional array of the numbers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Matrices**: They are two-dimensional arrays of the numbers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tensors**: They are n-dimensional arrays of the numbers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The pictorial representation is given in *Figure 5.50*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/967508ae-8029-42fc-8ef2-1eaa80af81c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.50: A pictorial representation of scalar, vector, matrix, and tensor
    (Image credit: http://hpe-cct.github.io/programmingGuide/img/diagram1.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Matrix manipulation operations are available in the `NumPy` library. You can
    perform vector-related operations using the `SciPy` and `scikit-learn` libraries.
    We will suggest certain libraries because their sources are written to give you
    optimal solutions and provide you with a high-level API so that you don't need
    to worry about what's going on behind the scenes. However, if you want to develop
    a customized application, then you need to know the math aspect of each manipulation.
    We will also look at the concept of linear regression, gradient descent, and linear
    algebra. If you really want to explore math that is related to machine learning
    and deep learning, then the following learning materials can help you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part one of this book will really help you:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.deeplearningbook.org/](http://www.deeplearningbook.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A cheat sheet of statistics, linear algebra, and calculus can be found at this
    link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/jalajthanaki/NLPython/tree/master/Appendix2/Cheatsheets/11_Math](https://github.com/jalajthanaki/NLPython/tree/master/Appendix2/Cheatsheets/11_Math).'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are new to math, we recommend that you check out these videos:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.khanacademy.org/math/linear-algebra](https://www.khanacademy.org/math/linear-algebra)
    [https://www.khanacademy.org/math/probability](https://www.khanacademy.org/math/probability)
    [https://www.khanacademy.org/math/calculus-home](https://www.khanacademy.org/math/calculus-home)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.khanacademy.org/math/calculus-home/multivariable-calculus](https://www.khanacademy.org/math/calculus-home/multivariable-calculus)
    [https://www.khanacademy.org/math](https://www.khanacademy.org/math) [If you want
    to see the various vector similarity concepts, then this article will help you:](https://www.khanacademy.org/math/calculus-home/multivariable-calculus)
    [http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/](http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/)'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's jump into the next section, which is all about probability. This is
    one of the core concepts of probabilistic theory.
  prefs: []
  type: TYPE_NORMAL
- en: Basic concepts of the probabilistic theory for NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at some of the concepts of probabilistic theory.
    We will also look at some examples of them so that you can understand what is
    going on. We will start with probability, then the concept of an independent event,
    and then conditional probability. At the end, we will look at the Bayes rule.
  prefs: []
  type: TYPE_NORMAL
- en: Probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Probability is a measure of the likelihood that a particular event will occur.
    Probability is quantified as a number and the range of probability is between
    0 and 1\. 0 means that the particular event will never occur and 1 indicates that
    the particular event will definitely occur. Machine learning techniques use the
    concept of probability widely. Let''s look at an example just to refresh the concept.
    Refer to *Figure 5\. 51*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8da1e11f-fd1e-4f0e-ba03-93ddc50419ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.51: Probability example (Image credit: http://www.algebra-class.com/image-files/examples-of-probability-3.gif)'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's see what dependent and independent events are.
  prefs: []
  type: TYPE_NORMAL
- en: Independent event and dependent event
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at what dependent events and independent events
    are. After that, we will see how to decide if an event is dependent or not. First,
    let's begin with definitions.
  prefs: []
  type: TYPE_NORMAL
- en: If the probability of one event doesn't affect the probability of the other
    event, then this kind of event is called an independent event. So technically,
    if you take two events, A and B, and if the fact that A occurs does not affect
    the probability of B occurring, then it's called an independent event. Flipping
    a fair coin is an independent event because it doesn't depend on any other previous
    events.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, some events affect other events. Two events are said to be dependent
    when the probability of one event occurring influences the other event's occurrence.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you were to draw two cards from a deck of 52 cards, and on your
    first draw you had an ace, the probability of drawing another ace on the second
    draw has changed because you drew an ace the first time. Let's calculate these
    different probabilities to see what's going on.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four aces in a deck of 52 cards. See *Figure 5.52*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3321a03-afb3-4ccd-9bf5-ce59dccbdb36.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.52: Equation of probability'
  prefs: []
  type: TYPE_NORMAL
- en: 'On your first draw, the probability of getting an ace is in *Figure 5.53*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93b3d6c4-1664-4737-8ad8-7a254bd3c42b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.53: Calculation step (Image credit: https://dj1hlxw0wr920.cloudfront.net/userfiles/wyzfiles/02cec729-378c-4293-8a5c-3873e0b06942.gif)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now if you don''t return this drawn card to the deck, the probability of drawing
    an ace on the second round is given in the following equation. See *Figure 5.54*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/881756b3-18bf-4985-b5ff-559bddc014a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.54: Dependent event probability equation (Image credit: https://dj1hlxw0wr920.cloudfront.net/userfiles/wyzfiles/7a45b393-0275-47ac-93e1-9669f5c31caa.gif)'
  prefs: []
  type: TYPE_NORMAL
- en: 'See the calculation step in *Figure 5.55*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5d8f123-48bf-4717-b544-97232a6d1084.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.55: Calculation step (Image credit: https://dj1hlxw0wr920.cloudfront.net/userfiles/wyzfiles/11221e29-96ea-44fb-b7b5-af614f1bec96.gif)'
  prefs: []
  type: TYPE_NORMAL
- en: 'See the final answer in *Figure 5.56*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5bf68296-740a-45d5-9aec-1c60bd3a475b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.56: Final answer of the example (Image credit: https://dj1hlxw0wr920.cloudfront.net/userfiles/wyzfiles/78fdf71e-fc1c-41d2-8fb8-bc2baeb25c25.gif)'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the preceding two probability values are different, so we say
    that the two events are dependent because the second event depends on the first
    event.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mathematical condition to check whether the events are dependent or independent
    is given as: events A and B are independent events if, and only if, the following
    condition will be satisfied:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(A ∩ B) = P(A) * P(B)*'
  prefs: []
  type: TYPE_NORMAL
- en: Otherwise, *A* and *B* are called dependent events.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's take an example to understand the defined condition.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**: A poll finds that 72% of the population of Mumbai consider themselves
    football fans. If you randomly pick two people from the population, what is the
    probability that the first person is a football fan and the second is as well?
    That the first one is and the second one isn''t?'
  prefs: []
  type: TYPE_NORMAL
- en: '**Solution**: The first person being a football fan doesn''t have any impact
    on whether the second randomly selected person is a football fan or not. Therefore,
    the events are independent.'
  prefs: []
  type: TYPE_NORMAL
- en: The probability can be calculated by multiplying the individual probabilities
    of the given events together. If the first person and second person both are football
    fans, then P(A∩B) = P(A) P(B) = .72 * .72 = .5184.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the second question: The first one is a football fan, the second one isn''t:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(A∩ not B) = P(A) P( B'' ) = .72 * ( 1 - 0.72) = 0.202*.'
  prefs: []
  type: TYPE_NORMAL
- en: In this part of the calculation, we multiplied by the complement.
  prefs: []
  type: TYPE_NORMAL
- en: Here, events *A* and *B* are independent because the equation *P(A∩B) = P(A)
    P(B)* holds true.
  prefs: []
  type: TYPE_NORMAL
- en: Now it's time to move on to the next concept called conditional probability.
  prefs: []
  type: TYPE_NORMAL
- en: Conditional probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at a concept called conditional probability. We
    will use the concept of a dependent event and independent event to understand
    the concept of conditional probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'The conditional probability of an event *B* is the probability that the event
    will occur given the knowledge that an event, *A*, has already occurred. This
    probability is written as *P(B|A)*, the notation for the probability of *B* given
    *A*. Now let''s see how this conditional probability turns out when events are
    independent. Where events *A* and *B* are independent, the conditional probability
    of event *B* given event *A* is simply the probability of event B, that is, *P(B)*.
    What if events *A* and *B* are not independent? Then, the probability of the intersection
    of *A* and *B* means that the probability that both events occur is defined by
    the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(A and B) = P(A) * P(B|A)*'
  prefs: []
  type: TYPE_NORMAL
- en: Now we will look at an example.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**: Jalaj''s two favorite food items are tea and pizza. Event A represents
    the event that I drink tea for my breakfast. B represents the event that I eat
    pizza for lunch. On randomly selected days, the probability that I drink tea for
    breakfast, P(A), is 0.6\. The probability that I eat pizza for lunch, P(B), is
    0.5 and the conditional probability that I drink tea for breakfast, given that
    I eat pizza for lunch, P(A|B) is 0.7\. Based on this, please calculate the conditional
    probability of P(B|A). P(B|A) will indicate the probability that I eat pizza for
    lunch, given that I drink tea for breakfast. In layman''s terms, find out the
    probability of having pizza for lunch when drinking tea for breakfast.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Solution**'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(A) = 0.6 , P(B) =0.5 , P(A|B) =0.7*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, two events are dependent because the probability of B being true has changed
    the probability of A being true. Now we need to calculate P(B|A).
  prefs: []
  type: TYPE_NORMAL
- en: 'See the equation *P(A and B) = P(A) * P(B|A)*. To find out P(B|A), we first
    need to calculate P(A and B):'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(A and B) = P(B) * P(A|B) = P(A) * P(B|A)*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we know that *P(B) = 0.5 and P(A|B) =0.7*
  prefs: []
  type: TYPE_NORMAL
- en: '*P(A and B) = 0.5 * 0.7 = 0.35*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(B|A) = P(A and B) / P(A) = 0.35 / 0.6 = 0.5833*'
  prefs: []
  type: TYPE_NORMAL
- en: So, we have found the conditional probability for dependent events.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have seen the basics of probability that we will use in upcoming chapters
    to understand ML algorithms. We will define additional concepts as we go. The
    `scikit-learn`, TensorFlow, SparkML, and other libraries already implement major
    probability calculation, provide us with high-level APIs, and have options that
    can change the predefined parameter and set values according to your application.
    These parameters are often called **hyperparameters**. To come up with the best
    suited values for each of the parameters is called **hyperparameter tuning**.
    This process helps us optimize our system. We will look at hyperparameter tuning
    and other major concepts in [Chapter 8](97808151-90d2-4034-8d53-b94123154265.xhtml),
    *Machine Learning for NLP Applications*.
  prefs: []
  type: TYPE_NORMAL
- en: This is the end of our prerequisite section. From this section onwards, we look
    at see some statistical concepts that help us extract features from the text.
    Many NLP applications also use them.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept TF-IDF stands for **term frequency-inverse document frequency**.
    This is in the field of numerical statistics. With this concept, we will be able
    to decide how important a word is to a given document in the present dataset or
    corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding TF-IDF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a very simple but useful concept. It actually indicates how many times
    a particular word appears in the dataset and what the importance of the word is
    in order to understand the document or dataset. Let's give you an example. Suppose
    you have a dataset where students write an essay on the topic, My Car. In this
    dataset, the word **a** appears many times; it's a high frequency word compared
    to other words in the dataset. The dataset contains other words like **car**,
    **shopping**, and so on that appear less often, so their frequency are lower and
    they carry more information compared to the word, **a**. This is the intuition
    behind TF-IDF.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explain this concept in detail. Let''s also look at its mathematical
    aspect. TF-IDF has two parts: Term Frequency and Inverse Document Frequency. Let''s
    begin with the term frequency. The term is self-explanatory but we will walk through
    the concept. The term frequency indicates the frequency of each of the words present
    in the document or dataset. So, its equation is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*TF(t) = (Number of times term t appears in a document) / (Total number of
    terms in the document)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s look at the second part - inverse document frequency. IDF actually
    tells us how important the word is to the document. This is because when we calculate
    TF, we give equal importance to every single word. Now, if the word appears in
    the dataset more frequently, then its term frequency (TF) value is high while
    not being that important to the document. So, if the word **the** appears in the
    document 100 times, then it''s not carrying that much information compared to
    words that are less frequent in the dataset. Thus, we need to define some weighing
    down of the frequent terms while scaling up the rare ones, which decides the importance
    of each word. We will achieve this with the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*IDF(t) = log[10](Total number of documents / Number of documents with term
    t in it).*'
  prefs: []
  type: TYPE_NORMAL
- en: So, our equation is calculate TF-IDF is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '*TF * IDF = [ (Number of times term t appears in a document) / (Total number
    of terms in the document) ] * log10(Total number of documents / Number of documents
    with term t in it).*'
  prefs: []
  type: TYPE_NORMAL
- en: Note that in TF-IDF, - is hyphen, not the minus symbol. In reality, TF-IDF is
    the multiplication of TF and IDF, such as *TF * IDF*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s take an example where you have two sentences and are considering
    those sentences as different documents in order to understand the concept of TF-IDF:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Document 1: This is a sample.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Document 2: This is another example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now to calculate TF-IDF, we will follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We first calculate the frequency of each word for each document.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We calculate IDF.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We multiply TF and IDF.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Refer to *Figure 5.57*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e8559f0-b3ed-4ffe-b474-6016d574dfa5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.57: TF-IDF example'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see the calculation of IDF and TF * IDF in *Figure 5.58*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c1278a75-815d-417b-b884-4e54c9344ce7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.58: TF-IDF example'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding TF-IDF with a practical example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we will use two libraries to calculate TF-IDF - textblob and scikit-learn.
    You can see the code at this GitHub link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/jalajthanaki/NLPython/tree/master/ch5/TFIDFdemo](https://github.com/jalajthanaki/NLPython/tree/master/ch5/onehotencodingdemo).'
  prefs: []
  type: TYPE_NORMAL
- en: Using textblob
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can see the code snippet in *Figure 5.59*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a5fbe677-d716-4aab-9325-fd83780e4757.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.59: TF-IDF using textblob'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the code is in *Figure 5.60*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c7946ba-5a96-4aa1-9638-93c26969e884.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.60: Output of TF-IDF for the word short'
  prefs: []
  type: TYPE_NORMAL
- en: Using scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will try to generate the TF-IDF model using a small Shakespeare dataset.
    For a new given document with a TF-IDF score model, we will suggest the top three
    keywords for the document. You can see the code snippet in *Figure 5.61*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15efd78d-2e2a-493a-ad28-797f7ca62476.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.61: Using scikit-learn to generate a TF-IDF model'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the output in *Figure 5.62*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be479c79-eef3-4acc-b3a9-874c2e4480b6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.62: Output of the TF-IDF model'
  prefs: []
  type: TYPE_NORMAL
- en: Now it's time to see where we can use this TF-IDF concept, so let's look at
    some applications.
  prefs: []
  type: TYPE_NORMAL
- en: Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will look at some cool applications that use TF-IDF:'
  prefs: []
  type: TYPE_NORMAL
- en: In general, text data analysis can be performed by TF-IDF easily. You can get
    information about the most accurate keywords for your dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are developing a text summarization application where you have a selected
    statistical approach, then TF-IDF is the most important feature for generating
    a summary for the document.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variations of the TF-IDF weighting scheme are often used by search engines to
    find out the scoring and ranking of a document's relevance for a given user query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document classification applications use this technique along with BOW.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let's look at the concept of vectorization for an NLP application.
  prefs: []
  type: TYPE_NORMAL
- en: Vectorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vectorization is an important aspect of feature extraction in the NLP domain.
    Transforming the text into a vector format is a major task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Vectorization techniques try to map every possible word to a specific integer.
    There are many available APIs that make your life easier. `scikit-learn` has `DictVectorizer`
    to convert text to a one-hot encoding form. The other API is the `CountVectorizer`,
    which converts the collection of text documents to a matrix of token counts. Last
    but not least, there are a couple of other APIs out there. We can also use word2vec
    to convert text data to the vector form. Refer to this link''s *From text* section
    for more details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text).'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at the concept of one-hot encoding for an NLP application. This
    one-hot encoding is considered as part of vectorization.
  prefs: []
  type: TYPE_NORMAL
- en: Encoders and decoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of encoding in NLP is quite old as well as useful. As we mentioned
    earlier, it is not easy to handle categorical data attributes present in our dataset.
    Here, we will explore the encoding technique named one-hot encoding, which helps
    us convert our categorical features in to a numerical format.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In an NLP application, you always get categorical data. The categorical data
    is mostly in the form of words. There are words that form the vocabulary. The
    words from this vocabulary cannot turn into vectors easily.
  prefs: []
  type: TYPE_NORMAL
- en: Consider that you have a vocabulary with the size N. The way to approximate
    the state of the language is by representing the words in the form of one-hot
    encoding. This technique is used to map the words to the vectors of length n,
    where the n^(th) digit is an indicator of the presence of the particular word.
    If you are converting words to the one-hot encoding format, then you will see
    vectors such as 0000...001, 0000...100, 0000...010, and so on. Every word in the
    vocabulary is represented by one of the combinations of a binary vector. Here,
    the nth bit of each vector indicates the presence of the nth word in the vocabulary.
    So, how are these individual vectors related to sentences or other words in the
    corpus? Let's look at an example that will help you understand this concept.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you have one sentence, *Jalaj likes NLP*. Suppose after applying
    one-hot encoding, this sentence becomes 00010 00001 10000\. This vector is made
    based on the vocabulary size and encoding schema. Once we have this vector representation,
    then we can perform the numerical operation on it. Here, we are turning words
    into vectors and sentences into matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding a practical example for one-hot encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will use `scikit-learn` to generate one-hot encoding for
    a small dataset. You can find the code at this GitHub link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/jalajthanaki/NLPython/tree/master/ch5/onehotencodingdemo](https://github.com/jalajthanaki/NLPython/tree/master/ch5/onehotencodingdemo).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the code snippet in *Figure 5.63*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79dd41bf-e6ce-4af1-8601-716404d78099.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.63: Pandas and scikit-learn to generate one-hot encoding'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the output in *Figure 5.64*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9131b126-45aa-4853-a880-8f0b1fb7c3a5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.64: Output of one-hot encoding'
  prefs: []
  type: TYPE_NORMAL
- en: Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'These techniques are very useful. Let''s see some of the basic applications
    for this mapping technique:'
  prefs: []
  type: TYPE_NORMAL
- en: Many artificial neural networks accept input data in the one-hot encoding format
    and generate output vectors that carry the sematic representation as well
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The word2vec algorithm accepts input data in the form of words and these words
    are in the form of vectors that are generated by one-hot encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now it's time to look at the decoding concept. Decoding concepts are mostly
    used in deep learning nowadays. So here, we will define the decoder in terms of
    deep learning because we will use this encoding and decoding architecture in [Chapter
    9](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml), *Deep Learning for NLU and NLG
    Problems*, to develop a translation system.
  prefs: []
  type: TYPE_NORMAL
- en: An encoder maps input data to a different feature representation; we are using
    one-hot encoding for the NLP domain. A decoder maps the feature representation
    back to the input data space. In deep learning, a decoder knows which vector represents
    which words, so it can decode words as per the given input schema. We will see
    the detailed concept of encoder-decoder when we cover the sequence-to-sequence
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at the next concept called **normalization**.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we will explain normalization in terms of linguistics as well as statistics.
    Even though they are different, the word normalization can create a lot of confusion.
    Let's resolve this confusion.
  prefs: []
  type: TYPE_NORMAL
- en: The linguistics aspect of normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The linguistics aspect of normalization includes the concept text normalization.
    Text normalization is the process of transforming the given text into a single
    canonical form. Let's take an example to understand text normalization properly.
    If you are making a search application and you want the user to enter **John**,
    then John becomes a search string and all the strings that contain the word **John**
    should also pop up. If you are preparing data to search, then people prefer to
    take the stemmed format; even if you search **flying** or **flew**, ultimately
    these are forms that are derived from the word **fly**. So, the search system
    uses the stemmed form and other derived forms are removed. If you recall Chapter
    3, *Understanding Structure of Sentences*, then you will remember that we have
    already discussed how to derive lemma, stem, and root.
  prefs: []
  type: TYPE_NORMAL
- en: The statistical aspect of normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The statistical aspect of normalization is used to do features scaling. If you
    have a dataset where one data attribute's ranges are too high and the other data
    attributes' ranges are too small, then generally we need to apply statistical
    techniques to bring all the data attributes or features into one common numerical
    range. There are many ways to perform this transformation, but here we will illustrate
    the most common and easy method of doing this called **min-max scaling**. Let's
    look at equation and mathematical examples to understand the concept.
  prefs: []
  type: TYPE_NORMAL
- en: 'Min-max scaling brings the features in the range of [0,1]. The general formula
    is given in *Figure 5.65*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d1a6acf-7595-4eea-bb16-6323a59ef1b9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.65: The min-max normalization equation'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you have features values such as *[1, 8, 22, 25]*; i you apply the preceding
    formula and calculate the value for each of the elements, then you will get the
    feature with a range of [0,1]. For the first element, *z = 1 - 1/ 25 -1 = 0*,
    for the second element, *z =8 -1 /25-1 = 0.2917*, and so on. The `scikit-learn`
    library has an API that you can use for min-max scaling on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will cover the language model.
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will discuss one of the most famous probabilistic models in NLP, which has
    been used for a variety of applications - the language model. We will look at
    the basic idea of the language model. We are not going to dive deep into this,
    but we will get an intuitive idea on how the language model works and where we
    can use it.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding probabilistic language modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two basic goals of the **language model** (**LM**):'
  prefs: []
  type: TYPE_NORMAL
- en: The goal of LM is to assign probability to a sentence or sequence of words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LM also tells us about the probability of the upcoming word, which means that
    it indicates which is the next most likely word by observing the previous word
    sequence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If any model can compute either of the preceding tasks, it is called a language
    model. LM uses the conditional probability chain rule. The chain rule of conditional
    probability is just an extension of conditional probability. We have already seen
    the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(A|B) = P(A and B) / P(B)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(A and B) = P(A,B) = P(A|B) P(B)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, P(A,B) is called **joint probability**. Suppose you have multiple events
    that are dependent, then the equation to compute joint probability becomes more
    general:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(A,B,C,D) = P(A) P(B|A)P(C|A,B)P(D|A,B,C)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(x1,x2,x3,...,xn) =P(x1)P(x2|x1)P(x3|x1,x2)...P(xn|x1,x2,x3,...xn-1)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding equation is called chain rule for conditional probability. LM
    uses this to predict the probability of upcoming words. We often calculate probability
    by counting the number of times a particular event occurs and dividing it by the
    total number of possible combinations, but we can''t apply this to language because,
    with certain words, you can generate millions of sentences. So, we are not going
    to use the probability equation; we are using an assumption called **Markov Assumption**
    to calculate probability instead. Let''s understand the concept intuitively before
    looking at a technical definition of it. If you have very a long sentence and
    you are trying to predict what the next word in the sentence sequence will be,
    then you actually need to consider all the words that are already present in the
    sentence to calculate the probability for the upcoming word. This calculation
    is very tedious, so we consider only the last one, two or three words to compute
    the probability for the upcoming word; this is called the Markov assumption. The
    assumption is that you can calculate the probability of the next word that comes
    in a sequence of the sentence by looking at the last word two. Let''s take an
    example to understand this. If you want to calculate the probability of a given
    word, then it is only dependent on the last word. You can see the equation here:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(the | its water is so transparent that) = P(the | that) or you can consider
    last two words P(the | its water is so transparent that) = P(the | transparent
    that)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple LM uses a unigram, which means that we are just considering the word
    itself and calculating the probability of an individual word; you simply take
    the probability of individual words and generate a random word sequence. If you
    take a bigram model, then you consider that one previous word will decide the
    next word in the sequence. You can see the result of the bigram model in *Figure
    5.66*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cca09d81-e3d0-4671-912d-29684c6e2954.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.66: Output using a bigram LM'
  prefs: []
  type: TYPE_NORMAL
- en: 'How can we count the n-gram probability that is a core part of LM? Let''s look
    at the bigram model. We will see the equation and then go through the example.
    See the equation in *Figure 5.67*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56ac8ba9-9e7d-4a60-a2cf-bf8afa430ee2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.67: Equation to find out the next most likely word in the sequence'
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation is easy to understand. We need to calculate how many times the
    words *wi-1* and *wi* occurred together, and we also need to count how many times
    the word *wi-1* occurred. See the example in *Figure 5.68*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d194dbd-69b3-4b63-961b-a462a8056fb5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.68: Example to find out the most likely word using LM'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we is followed by *<s>* twice in three given sentences, so we
    have *P(I|<s>) =2/3*, and for every word, we will calculate the probability. Using
    LM, we can come to know how the word pairs are described in the corpus as well
    as what the more popular word pairs that occur in the corpus are. If we use a
    four-gram or five-gram model, it will give us a good result for LM because some
    sentences have a long-dependency relationship in their syntax structure with subject
    and verbs. So, with a four-gram and five-gram model, you can build a really good
    LM.
  prefs: []
  type: TYPE_NORMAL
- en: Application of LM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LM has a lot of great applications in the NLP domain. Most NLP applications
    use LM at some point. Let''s see them:'
  prefs: []
  type: TYPE_NORMAL
- en: Machine translation systems use LM to find out the probability for each of the
    translated sentences in order to decide which translated sentence is the best
    possible translation for the given input sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To spell the correct application, we can use a bigram LM to provide the most
    likely word suggestion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use LM for text summarization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use LM in a question answering system to rank the answers as per their
    probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indexing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Indexing** is quite a useful technique. This is used to convert the categorical
    data to its numerical format. In an NLP application, you may find that the data
    attributes are categorical and you want to convert them to a certain numerical
    value. In such cases, this indexing concept can help you. We can use the SparkML
    library, which has a variety of APIs to generate indexes. SparkML has an API named
    StringIndexer that uses the frequency of the categorical data and assigns the
    index as per the frequency count. So, the most frequent category gets an index
    value of 0\. This can sometimes be a naïve way of generating indexing, but in
    some analytical applications, you may find this technique useful. You can see
    the example at this link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/ml-features.html#stringindexer](https://spark.apache.org/docs/latest/ml-features.html#stringindexer).'
  prefs: []
  type: TYPE_NORMAL
- en: 'SparkML has the API, IndexToString, which you can use when you need to convert
    your numerical values back to categorical values. You can find the example at
    this link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/ml-features.html#indextostring](https://spark.apache.org/docs/latest/ml-features.html#indextostring).'
  prefs: []
  type: TYPE_NORMAL
- en: Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are some applications which use indexing for extracting features:'
  prefs: []
  type: TYPE_NORMAL
- en: When we are dealing with a multiclass classifier and our target classes are
    in the text format and we want to convert our target class labels to a numerical
    format, we can use StingIndexer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can also generate the text of the `target` class using the IndexToString
    API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now it's time to learn about a concept called ranking.
  prefs: []
  type: TYPE_NORMAL
- en: Ranking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many applications, ranking plays a key role. The concept of **ranking** is
    used when you search anything on the web. Basically, the ranking algorithm is
    used to find the relevance of the given input and generated output.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at an example. When you search the web, the search engine takes your
    query, processes it, and generates some result. It uses the ranking algorithm
    to find the most relevant link according to your query and displays the most relevant
    link or content at the top and least relevant at the end. The same thing happens
    when you visit any online e-commerce website; when you search for a product, they
    display the relevant product list to you. To make their customer experience enjoyable,
    they display those products that are relevant to your query, whose reviews are
    good, and that have affordable prices. These all are the parameters given to the
    ranking algorithm in order to generate the most relevant products.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation of the ranking algorithm is not a part of this book. You
    can find more information that will be useful here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://medium.com/towards-data-science/learning-to-rank-with-python-scikit-learn-327a5cfd81f](https://medium.com/towards-data-science/learning-to-rank-with-python-scikit-learn-327a5cfd81f).'
  prefs: []
  type: TYPE_NORMAL
- en: Indexing and ranking are not frequently used in the NLP domain, but they are
    much more important when you are trying to build an application related to analytics
    using machine learning. It is mostly used to learn the user's preferences. If
    you are making a Google News kind of NLP application, where you need to rank certain
    news events, then ranking and indexing plays a major role. In a question answering
    system, generating ranks for the answers is the most critical task, and you can
    use indexing and ranking along with a language model to get the best possible
    result Applications such as grammar correction, proofreading, summarization systems,
    and so on don't use this concept.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen most of the basic features that we can use in NLP applications.
    We will be using most of them in Chapter 8, *Machine Learning for NLP Application*,
    where we will build some real-life NLP applications with ML algorithms. In the
    upcoming section, we will explain the advantages and challenges of features engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of features engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Features engineering is the most important aspect of the NLP domain when you
    are trying to apply ML algorithms to solve your NLP problems. If you are able
    to derive good features, then you can have many advantages, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Better features give you a lot of flexibility. Even if you choose a less optimal
    ML algorithm, you will get a good result. Good features provide you with the flexibility
    of choosing an algorithm; even if you choose a less complex model, you get good
    accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you choose good features, then even simple ML algorithms do well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Better features will lead you to better accuracy. You should spend more time
    on features engineering to generate the appropriate features for your dataset.
    If you derive the best and appropriate features, you have won most of the battle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges of features engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we will discuss the challenges of features engineering for NLP applications.
    You must be thinking that we have a lot of options available in terms of tools
    and algorithms, so what is the most challenging part? Let''s find out:'
  prefs: []
  type: TYPE_NORMAL
- en: In the NLP domain, you can easily derive the features that are categorical features
    or basic NLP features. We have to convert these features into a numerical format.
    This is the most challenging part.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An effective way of converting text data into a numerical format is quite challenging.
    Here, the trial and error method may help you.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although there are a couple of techniques that you can use, such as TF-IDF,
    one-hot encoding, ranking, co-occurrence matrix, word embedding, Word2Vec, and
    so on to convert your text data into a numerical format, there are not many ways,
    so people find this part challenging.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have seen many concepts and tools that are widely used in
    the NLP domain. All of these concepts are the basic building blocks of features
    engineering. You can use any of these techniques when you want to generate features
    in order to generate NLP applications. We have looked at how parse, POS taggers,
    NER, n-grams, and bag-of-words generate Natural Language-related features. We
    have also explored the how they are built and what the different ways to tweak
    some of the existing tools are in case you need custom features to develop NLP
    applications. Further, we have seen basic concepts of linear algebra, statistics,
    and probability. We have also seen the basic concepts of probability that will
    be used in ML algorithms in the future. We have looked at some cool concepts such
    as TF-IDF, indexing, ranking, and so on, as well as the language model as part
    of the probabilistic model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at advanced features such as word2vec, Doc2vec,
    Glove, and so on. All of these algorithms are part of word embedding techniques.
    These techniques will help us convert our text features into a numerical format
    efficiently; especially when we need to use semantics. The next chapter will provide
    you with much more detailed information about the word2Vec algorithm. We will
    cover each and every technicality behind the word2vec model. We will also understand
    how an **artificial neural network** (**ANN**) is used to generate the semantic
    relationship between words, and then we will explore an extension of this concept
    from word level, sentence level, document level, and so on. We will build an application
    that includes some awesome visualization for word2vec. We will also discuss the
    importance of vectorization, so keep reading!
  prefs: []
  type: TYPE_NORMAL
