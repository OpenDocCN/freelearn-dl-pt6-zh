- en: Audio Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapters, we learned about dealing with sequential text data.
    Audio can also be considered sequential data, with varying amplitudes over time.
    In this chapter, we will be learning about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Classifying a song by genre
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Music generation using deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transcribing audio into text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying a song by genre
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this case study, we will be classifying a song into one of 10 possible genres.
    Imagine a scenario where we are tasked to automatically classify the genre of
    a song without manually listening to it. This way, we can potentially minimize
    operational overload as far as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The strategy we''ll adopt is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Download a dataset of various audio recordings and the genre they fit into.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize and contrast a spectrogram of the audio signal for various genres.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Perform CNN operations on top of a spectrogram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that we will be performing a CNN 1D operation on a spectrogram, as the
    concept of translation does not apply in the case of audio recordings
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract features from the CNN after multiple convolution and pooling operations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Flatten the output and pass it through a dense layer that has 10 possible classes
    in an output layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Minimize categorical cross-entropy to classify the audio recording to one of
    the 10 possible classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we classify the audio, we'll plot the embeddings of each audio input so
    that similar audio recordings are grouped together. This way, we will be in a
    position to identify the genre of a new song without listening to it, thus automatically
    classifying the audio input into a genre.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The strategy discussed above is coded as follows (the code file is available
    as `Genre_classification.ipynb` in GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the dataset and import the relevant packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Loop through the audio files to extract the  `mel spectrogram` input features
    of the input audio, and store the output genre for the audio input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are loading the audio file and extracting its features.
    Additionally, we are extracting the `melspectrogram` features of the signal. Finally,
    we are storing the `mel` features as the input array and the genre as the output
    array.
  prefs: []
  type: TYPE_NORMAL
- en: 'Visualize the spectrogram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d90d784-81f5-45d9-bbae-80908b53091b.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that there is a distinct difference between the classical audio spectrogram and
    the rock audio spectrogram.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the input and output arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the output classes into one-hot encoded versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Create train-and-test datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Build and compile the method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the `Conv1D` method in the preceding code works in a manner very
    similar to that of `Conv2D`; however, it is a one-dimensional filter in `Conv1D` and
    a two-dimensional one in `Conv2D`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding code, we can see that the model classifies with an accuracy
    of ~60% on the test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the output from the pre-final layer of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code produces output at the pre-final layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reduce the dimensions of the embeddings to 2, using `t-SNE` so that we can
    now plot our work on a chart:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the `t-SNE` output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the chart for the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0e76348-4592-4fb2-b376-87ec1d393faf.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding diagram, we can see that audio recordings for similar genres
    are located together. This way, we are now in a position to classify a new song
    into one of the possible genres automatically, without manual inspection. However,
    if the probability of an audio belonging to a certain genre is not very high,
    it will potentially go to a manual review so that misclassifications are uncommon.
  prefs: []
  type: TYPE_NORMAL
- en: Generating music using deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about generating text by going through a
    novel. In this section, we will learn about generating audio from a sequence of
    audio notes.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A MIDI file typically contains information about the notes and chords of the
    audio file, whereas the note object contains information about the pitch, octave,
    and offset of the notes. The chord object contains a set of notes that are played
    at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The strategy that we''ll adopt to build a music generator is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract the notes present in audio file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assign a unique ID for each note.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take a sequence of 100 historical notes, and the 101^(st) note shall be the
    output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fit an LSTM model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The strategy discussed above is coded as follows (the code file is available
    as `Music_generation.ipynb` in GitHub) along with the recommended audio file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant packages and dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Read the content of the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code reads a stream of scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a function that reads the stream of scores and extracts the notes from
    it (along with silence, if present in the audio file):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are obtaining the notes by looping through the elements
    and, depending on whether the element is a note, a chord, or a rest (which indicates
    silence), we extract the corresponding notes, append them, and return the appended
    list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the notes from the input audio file''s stream:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'A sample note output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3f25f4f-d8de-4fc1-bcae-dee7545cfe57.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the values starting with a `#` indicate silence (the duration is the
    same as the number adjacent to `#`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the input and output dataset by creating a dictionary of the note''s
    ID and its corresponding name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a sequence of input and output arrays:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding step, we are taking a sequence of 100 notes as input and extracting
    the output at the 101st time step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we are also converting the note into its corresponding ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are reshaping the input data so that it can then be
    fed into an LSTM layer (which requires the `batch_size` shape, time steps, and
    the number of features per time step).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we are normalizing the input, and we are also converting the output
    into a one-hot encoded set of vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ee599ca-17c2-42a4-962e-f5fb91653bdd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Generate predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Note that, in the preceding code, we have chosen a random audio location, from
    where we'll sample a sequence that will be used as a seed for prediction in future
    time steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate predictions by taking a sequence of 100 notes at a time, generating
    the next prediction, appending it to the input sequence, and generating the next
    prediction (by taking the latest sequence of the last 100 notes):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are dividing the index (which is the predicted output of the model)
    by 49, as we did in the same exercise while building the model (divided by `np.max(network_input)`).
  prefs: []
  type: TYPE_NORMAL
- en: The preceding exercise is slightly different than the text generation exercise,
    where we performed embedding on top of input word IDs, as we are not performing
    embedding in this scenario. The model is still working without embedding in this
    scenario, potentially because there are fewer unique values in the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create note values based on values generated by the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note that, in the preceding code, we are offsetting each note by 0.5 seconds,
    so that the notes do not stack on top of one another while producing the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Write the generated predictions into a music stream:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Now, you should be able to listen to the music that's been generated by your
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Transcribing audio into text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 14](1f989cf7-40b3-4ecd-9457-0dd648746922.xhtml), *End-to-End Learning*,
    we learned about transcribing handwritten text images into text. In this section,
    we will be leveraging a similar end-to-end model to transcribe voices into text.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The strategy that we''ll adopt to transcribe voices is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Download a dataset that contains the audio file and its corresponding transcriptions
    (*ground truths*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Specify a sampling rate while reading the audio files:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the sampling rate is 16,000, we'll be extracting 16,000 data points per second
    of audio.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Extract a Fast Fourier Transformation of the audio array:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An FFT ensures that we have only the most important features of a signal.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: By default, the FFT gives us *n/2* number of data points, where *n* is the number
    of data points in the whole audio recording.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Sample the FFT features of the audio where we extract 320 data points at a time;
    that is, we extract 20 milliseconds (320/16000 = 1/50 seconds) of audio data at
    a time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, we will sample 20 milliseconds of data at 10-millisecond intervals.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For this exercise, we'll be working on an audio recording where the audio duration
    is, at most, 10 seconds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will store the 20 milliseconds of audio data into an array:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have already seen that we sample 20 milliseconds of data for every 10 milliseconds.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, for a one-second audio clip, we will have 100 x 320 data points, and for
    a 10- second audio clip, we'll have 1,000 x 320 = 320,000 data points.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We will initialize an empty array of 160,000 data points and overwrite the values
    with the FFT values—as we have already learned that the FFT values are one half
    of the original number of data points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each array of 1,000 x 320 data points, we'll store the corresponding transcriptions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll assign an index for each character and then convert the output into a
    list of indices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, we'll also be storing the input length (which is the predefined
    number of time steps) and the label lengths (which are the actual number of characters
    present in the output)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furthermore, we will define the CTC loss function that is based on the actual
    output, the predicted output, the number of time steps (input length), and the
    label length (the number of characters in the output)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll define the model that is a combination of `conv1D` (as this is audio data)
    and GRU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furthermore, we will ensure that we normalize data using batch normalization
    so that the gradients do not vanish
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll run the model on batches of data, where we randomly sample batches of
    data and feed them to the model that tries to minimize the CTC loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we will decode the model predictions on a new data point by using the
    `ctc_decode` method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The strategy discussed above is coded as follows (the code file is available
    as `Voice transcription.ipynb` in GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the dataset and import the relevant packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Read all the file names and their corresponding transcriptions and turn them
    into separate lists:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Store the length of the transcription into a list so that we can understand
    the maximum transcription length:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'For this exercise, to be in a position to train a model on a single GPU, we''ll
    perform this exercise on the first 2,000 audio files whose transcriptions are
    fewer than 100 characters in length:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are storing the audio name and the corresponding audio
    transcription for only those audio recordings that have a transcription length
    of fewer than 100 characters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Store the inputs as a 2D array and the corresponding outputs of only those
    audio files that have a duration of fewer than 10 seconds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an index for each unique character in the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the input and the label lengths:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are creating an input length that is 243, as the output of the
    model (which we are going to build in a later step) has 243 time steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the CTC loss function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the input and the output parameters of the CTC loss function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Build and compile the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'A summary of the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4cdf1be1-cff4-4eb0-8b4b-1f5ce3b93a33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fit the model on batches of data that are sampled from the input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are looping through and extracting batches of data
    2,500 times, normalizing the input data, and fitting the model.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we are performing a high number of epochs, as the CTC loss decreases slowly
    for this particular dataset and model combination.
  prefs: []
  type: TYPE_NORMAL
- en: 'Predict the test audio:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are specifying a model (`model2`) that takes the input
    test array and extracts the model prediction in each of the 243 time steps.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we are extracting the prediction for the 12^(th) element from
    the last of the input array (note that we have excluded the last 25 input data
    points from being considered while training the model). Furthermore, we have also
    pre-processed it in the same way that we did before, by passing the input data
    to the model-training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Decode the predictions on the new data point:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we were decoding the prediction using the `ctc_decode`
    method. Alternatively, we could have decoded the prediction in the same way as
    we extracted the prediction in the handwritten image transcription. Finally, we
    print out the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll be in a position to decode the predictions by calling the previously
    defined function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of one of the predictions is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06e5ae31-fcc7-4a45-893e-13d568a5ed0e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'While the preceding output looks as if it''s gibberish, it sounds phonetically
    similar to the actual audio, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd63b2fd-7610-4ec3-99d0-17b98214b6d8.png)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some ways in which we can further improve the accuracy of our transcriptions
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Train on more data points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporate a language model to perform fuzzy matching on the output so that
    we correct the predicted output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
