- en: Advanced Feature Engineering and NLP Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级特征工程与NLP算法
- en: In this chapter, we will look at an amazing and simple concept called **word
    to vector** (**word2vec**). This concept was developed by a team of researchers
    led by Tomas Mikolov at Google. As we all know, Google provides us with a lot
    of great products and concepts. Word2vec is one of them. In NLP, developing tools
    or techniques that can deal with the semantics of words, phrases, sentences, and
    so on are quite a big deal, and the word2vec model does a great job of figuring
    out the semantics of words, phrases, sentences, paragraphs, and documents. We
    are going to jump into this vectorization world and live our life in it for a
    while. Don't you think this is quite amazing? We will be starting from the concepts
    and we will end with some fun and practical examples. So, let's begin.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨一个既简单又令人惊叹的概念，叫做**词到向量**（**word2vec**）。这个概念是由Google的Tomas Mikolov领导的研究团队开发的。众所周知，Google为我们提供了许多伟大的产品和概念，而word2vec就是其中之一。在自然语言处理（NLP）中，开发能够处理词语、短语、句子等语义的工具或技术是一项重要任务，而word2vec模型在理解词语、短语、句子、段落和文档的语义方面做得非常出色。我们将深入这个向量化的世界，并在其中待上一段时间。你不觉得这非常神奇吗？我们将从概念开始，最终通过一些有趣且实用的例子来结束。所以，让我们开始吧。
- en: Recall word embedding
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾词嵌入
- en: We have already covered word embedding in [Chapter 5](07f71ca1-6c8a-492d-beb3-a47996e93f04.xhtml),
    *Feature Engineering and NLP Algorithms*. We have looked at language models and
    feature engineering techniques in NLP, where words or phrases from the vocabulary
    are mapped to vectors of real numbers. The techniques used to convert words into
    real numbers are called **word embedding**. We have been using vectorization,
    as well as **term frequency-inverse document frequency** (**tf-idf**) based vectorization.
    So, let's just jump into the world of word2vec.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第五章](07f71ca1-6c8a-492d-beb3-a47996e93f04.xhtml)中讨论了词嵌入，*特征工程与NLP算法*。我们已经探讨了NLP中的语言模型和特征工程技术，在这些模型中，词语或短语被映射到实数向量。将词语转换为实数的技术被称为**词嵌入**。我们一直在使用向量化技术，以及基于**词频-逆文档频率**（**tf-idf**）的向量化技术。那么，让我们直接跳入word2vec的世界吧。
- en: Understanding the basics of word2vec
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解word2vec的基础
- en: 'Here, we will try to handle semantics at word level by using word2vec. Then,
    we will expand our concepts to paragraph level and document level. By looking
    at *Figure 6.1*, you will see the different kinds of semantics that we are going
    to cover in this book:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将尝试通过使用word2vec来处理词级别的语义。接着，我们将扩展我们的概念到段落级和文档级。通过查看*图6.1*，你将看到我们在本书中将要涵盖的不同种类的语义：
- en: '![](img/87ce8e19-40b7-4644-ba45-0293e84d7887.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](img/87ce8e19-40b7-4644-ba45-0293e84d7887.png)'
- en: 'Figure 6.1: Different kinds of semantics'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：不同种类的语义
- en: Semantics is a branch that deals with meaning in the area of NLP. We have already
    covered lexical semantic in [Chapter 3](f65e61fc-1d20-434f-b606-f36cf401fc41.xhtml),
    *Understanding Structure of Sentences*. So, here we will discuss more about distributional
    semantics. There are also other techniques or types in semantics, such as formal
    semantics compositional semantics; but right now, in this book, we are not going
    to cover these types or techniques.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 语义学是处理NLP领域中意义的一门分支学科。我们已经在[第三章](f65e61fc-1d20-434f-b606-f36cf401fc41.xhtml)中讨论了词汇语义学，*理解句子结构*。在这里，我们将更深入地讨论分布语义学。语义学中还有其他技术或类型，比如形式语义学和组合语义学；但是在本书中，我们现在不会涵盖这些类型或技术。
- en: Distributional semantics
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布语义学
- en: Distributional semantics is a research area that focuses on developing techniques
    or theories that quantify and categorize semantic similarities between linguistic
    items based on their distributional properties in large samples of text data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 分布语义学是一个研究领域，专注于开发量化和分类基于大型文本数据样本中分布特性的语言项目之间语义相似性的技术或理论。
- en: I want to give an example here that gives you an idea of what I mean by distributional
    semantics. Suppose you have text data of travelling blogs. Now, you as a person
    know that pasta, noodles, burgers, and so on are edible food items, whereas juice,
    tea, coffee, and so on are drinkable items. As a human, we can easily classify
    drinkable and edible food items because we have a certain context related with
    each of them, but machines cannot really know these kind of semantics. There is
    a higher chance that all described food items come along with certain words in
    the dataset. So, here we are focusing on the distribution of words in corpus and,
    let's say, that linguistic items or words with similar distributions have similar
    meanings. This is called the **distributional hypothesis**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我想给你一个例子，让你了解我所说的分布式语义是什么意思。假设你有关于旅行博客的文本数据。现在，作为一个人，你知道意大利面、面条、汉堡等是可食用的食品，而果汁、茶、咖啡等是饮料。作为人类，我们可以轻松地将可饮用和可食用的食物分类，因为我们与每个词都有一定的语境联系，但机器并不能真正理解这些语义。所有描述食物的项目很可能会在数据集中与某些特定词汇一起出现。因此，这里我们关注的是语料库中词汇的分布，假设具有相似分布的语言项目或词汇具有相似的含义。这被称为**分布假设**。
- en: 'I will give you another example. Suppose you have a dataset of research papers.
    Some of the research papers in the dataset belong to the engineering category,
    and others belong to the legal category. Documents with words such as engineering,
    equation, methods, and so on are related to engineering, so they should be part
    of one group, and words such as legal, lawyer, law institutes, and so on are related
    to research papers of the legal domain, so they should be grouped together. By
    using distributional semantics techniques such as word2vec, we can segregate the
    different domain words by using their vector values. All words with a similar
    meaning are grouped together because they have a similar distribution on the corpus.
    You can refer to *Figure 6.2*, which shows a pictorial representation of a vector
    space of our given distributional semantics example where similar contextual words
    come together:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我再给你一个例子。假设你有一个研究论文的数据集。数据集中的一些研究论文属于工程类，另一些属于法律类。包含“工程”、“方程式”、“方法”等词汇的文档与工程相关，因此它们应归为一组；而包含“法律”、“律师”、“法学院”等词汇的文档与法律领域的研究论文相关，因此它们应归为另一组。通过使用分布式语义技术，如word2vec，我们可以通过使用它们的向量值来区分不同领域的词汇。所有具有相似含义的词汇会被归为一组，因为它们在语料库中的分布相似。你可以参考*图6.2*，它展示了我们给定的分布式语义示例的向量空间图示，类似语境的词汇聚集在一起：
- en: '![](img/4f4f3862-fed5-4b60-b815-ca1d46516406.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4f4f3862-fed5-4b60-b815-ca1d46516406.png)'
- en: 'Figure 6.2: Pictorial representation of vector space of our distributional
    semantics example'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：我们分布式语义示例的向量空间图示
- en: '*Figure 6.3*, gives you an idea about from which branch the word2vec model
    was derived:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.3*，让你了解word2vec模型是从哪个分支派生出来的：'
- en: '![](img/2dc79cdc-1778-4734-8495-8d70f8f7487b.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2dc79cdc-1778-4734-8495-8d70f8f7487b.png)'
- en: 'Figure 6.3: Major models derived from distributional semantics'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：从分布式语义中衍生出的主要模型
- en: Our main concentration in this chapter is the distributional semantics technique
    called **word2vec**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的重点是分布式语义技术，称为**word2vec**。
- en: Defining word2vec
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义word2vec
- en: Word2vec is developed by using two-layer neural networks. It takes a large amount
    of text data or text corpus as input and generates a set of vectors from the given
    text.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec是通过使用双层神经网络开发的。它以大量文本数据或文本语料库为输入，并从给定的文本中生成一组向量。
- en: In other words, we can say that it generates high-dimensional vector space.
    This vector space has several hundred dimensions.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们可以说它生成了一个高维度的向量空间。这个向量空间有数百个维度。
- en: Please don't be afraid of the high dimensionality. I make the whole concept
    of word2vec simple for you during this chapter.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 请不要害怕高维度。在本章中，我会简化word2vec的整个概念。
- en: You will really want to know what I mean here when I say that the word2vec model
    generates a set of vectors or vector space from text. Here, we are using a two-layer
    neural network, which right now a black box that performs some kind of logic and
    generates vectors in the vector space for us. In the vector space, each unique
    word in the corpus is assigned a corresponding vector. So, the vector space is
    just a vector representation of all words present in the large text corpus.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当我说word2vec模型从文本中生成一组向量或向量空间时，你真的会明白我的意思。这里，我们使用的是一个两层神经网络，当前它是一个黑箱，执行某种逻辑并为我们生成向量。在这个向量空间中，语料库中每个唯一的单词都会被分配一个对应的向量。所以，向量空间只是所有词汇在大规模文本语料库中出现的向量表示。
- en: So, I think you get it, right? On the basics of what we have learnt, you are
    able to say that word2vec is one of the models that generates word embedding.
    Please recall the vectorization section of [Chapter 5](07f71ca1-6c8a-492d-beb3-a47996e93f04.xhtml),
    *Feature Engineering and NLP Algorithms*. I also want to make a point here, by
    saying that word2vec is a powerful, unsupervised word embedding technique.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我想你明白了，对吧？基于我们所学的基础，你可以说word2vec是生成词嵌入的模型之一。请回忆一下[第5章](07f71ca1-6c8a-492d-beb3-a47996e93f04.xhtml)中关于向量化的部分，*特征工程与NLP算法*。我还想在这里强调一点，word2vec是一种强大的、无监督的词嵌入技术。
- en: Necessity of unsupervised distribution semantic model - word2vec
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督分布式语义模型的必要性——word2vec
- en: This section gives us an idea of the numerous challenges that word2vec solves
    for us. The solution of those challenges leads us to the real need for word2vec.
    So, first we will look at some challenges, and then take a look at how the word2vec
    model solves those challenges.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本节内容让我们对word2vec为我们解决的众多挑战有了一个初步了解。这些挑战的解决方案引导我们发现了word2vec的真正需求。所以，我们首先来看一些挑战，然后再看看word2vec模型是如何解决这些挑战的。
- en: Challenges
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战
- en: 'There are a couple of challenges that are listed here that we are trying to
    solve:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这里列出了一些我们正在尝试解决的挑战：
- en: When we are developing an NLP application, there is one fundamental problem--we
    know that the machine can't understand our text and we need to convert the text
    data into a numerical format.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们开发NLP应用时，有一个基本问题——我们知道机器无法理解我们的文本，因此需要将文本数据转换为数值格式。
- en: 'There are certain ways to convert text data into a numerical format, but we
    apply some naive techniques, and one of them is one-hot encoding, but the problems
    with this techniques are given as follows:'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有几种方法可以将文本数据转换为数值格式，但我们采用了一些简单的技术，其中之一就是one-hot编码，但这种技术存在以下问题：
- en: 'Suppose you have a sentence: I like apple juice. Now, suppose that you apply
    one-hot encoding for each of the words in the sentence. If you have thousands
    of sentences in your corpus, then vector dimension is equal to the entire vocabulary
    of your corpus, and if these kind of high dimensional columns have been used to
    develop an NLP application, then we need high computation power and matrix operation
    on these high-dimension columns as they take too much time.'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设你有一句话：“I like apple juice”。现在，假设你对句子中的每个单词应用one-hot编码。如果你的语料库中有成千上万的句子，那么向量的维度就等于语料库的整个词汇表，如果这些高维列被用来开发NLP应用，那我们就需要强大的计算能力，并且需要对这些高维列进行矩阵运算，因为它们会消耗大量时间。
- en: For speech recognition vocabulary, size is on average 20,000 words. If we are
    developing a machine translation system then perhaps we will use more vocabulary,
    like 500,000 words. To deal with these kinds of gigantic vectors is a big challenge.
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于语音识别词汇表，词汇量的平均大小是20,000个单词。如果我们正在开发一个机器翻译系统，那么可能会使用更多的词汇，比如50万个单词。处理这些庞大的向量是一个巨大的挑战。
- en: 'Another problem is that when you apply one-hot encoding on a particular word
    in a sentence, then the whole entry has zero values, except the one that actually
    represents the word, and that value is **1**. Suppose, for simplicity, we take
    the sentence: *I like apple juice*. For a while consider that there is only one
    sentence in our corpus. Now, if I try to apply one hot encoding on the word **apple**
    then one-hot representation of **apple**, is given as follows. Refer to *Figure
    6.4*:![](img/a8438127-5658-46e1-880e-eb3b7094488d.png)'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个问题是，当你对句子中的某个单词应用one-hot编码时，整个条目都会是零，除了实际表示该单词的那个值，而该值是**1**。为了简便起见，我们暂时考虑语句：“*I
    like apple juice*”。假设我们的语料库中只有一句话。那么，如果我试图对单词**apple**应用one-hot编码，那么**apple**的one-hot表示如下。请参考*图6.4*：![](img/a8438127-5658-46e1-880e-eb3b7094488d.png)
- en: Figure 6.4:One-hot encoding representation of the words apple and juice
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4：苹果和果汁的单热编码表示
- en: One-hot encoding doesn't reveal the facts about context similarity between words.
    To understand this, I want to give an example, if your corpus has words cat and
    cats then one-hot encoding does not reveal the fact that word cat and cats are
    very similar words.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单热编码并不揭示词之间上下文相似性的事实。为了理解这一点，我想举个例子，如果你的语料库中有单词 cat 和 cats，则单热编码并不揭示单词 cat 和
    cats 是非常相似的词。
- en: If I apply an AND operation on the one-hot encoded vectors, then it will not
    express any contextual similarity. Take an example, if I apply an AND operation
    means a dot product on the one-hot vectors of **Apple** and **juice**,then the
    answer is **0**. In reality, these words can appear together and have a strong
    contextual relationship as well, but one-hot encoding alone does not express anything
    significant about word similarity.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我对苹果和果汁的单热编码向量进行 AND 运算，那么它将不会表达任何上下文相似性。举个例子，如果我对苹果和果汁的单热编码向量进行 AND 运算，答案是
    0。事实上，这些词可以一起出现，并且具有很强的上下文关系，但单热编码本身并不能表达有关词相似性的任何重要信息。
- en: If you want to find accurate word similarities, then WordNet will not help you
    enough. WordNet is made by experts and whatever WordNet contains is more subjective
    because human users created it.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你想找到准确的词相似性，那么 WordNet 将不能为你提供足够的帮助。WordNet 是由专家制作的，而 WordNet 包含的内容更主观，因为是人类用户创建的。
- en: Using WordNet takes a lot of time and effort.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 WordNet 需要大量的时间和精力。
- en: Some new words, such as Coding Ninja, Wizard, and so on are new words for WordNet
    and may not be present on the website. Because of the absence of these kinds of
    words, we cannot derive the other semantic relationships from WordNet.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些新词，如 Coding Ninja、Wizard 等，是 WordNet 的新词，可能不在网站上。由于缺少这些类型的词，我们无法从 WordNet
    推导出其他语义关系。
- en: Each of the preceding challenges has played a major role in the development
    of the techniques to solve them. In the last two decades, there has been a lot
    of effort put into developing an efficient, concise, and relevant numerical representation
    of words. Finally, in 2013, Tomas Mikolov and his research team at Google came
    up with the word2vec model, which solves many of the previous challenges in an
    efficient way.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决这些挑战之前的每一个挑战都在这些技术的发展中起了重要作用。在过去的二十年中，已经有很多努力致力于开发一种高效、简洁和相关的词的数值表示。最终，在2013年，Google
    的托马斯·米科洛夫和他的研究团队提出了 word2vec 模型，这种模型以高效的方式解决了许多之前的挑战。
- en: Word2vec is very good at finding out word similarity, as well as preserving
    the semantic relationship between words that couldn't be handled by previous techniques,
    such as one-hot encoding or by using WordNet.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec 在发现词的相似性以及保留语义关系方面非常出色，这些是以前的技术（如单热编码或使用 WordNet）无法处理的。
- en: I have given so much background on word2vec now, so let's start understanding
    the representation, components, and other parameters of the word2vec model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经在 word2vec 上提供了很多背景，现在让我们开始理解 word2vec 模型的表示、组件和其他参数。
- en: Let's begin our magical journey!
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始我们神奇的旅程吧！
- en: Converting the word2vec model from black box to white box
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 word2vec 模型从黑箱转换为白箱
- en: From this section onwards, we are going to get an understanding of each of the
    components of the word2vec model, as well as the model's working process. So,
    in short, we are converting the black box part of word2vec into a white box.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从这一节开始，我们将了解 word2vec 模型的每个组成部分，以及模型的工作过程。简而言之，我们正在将 word2vec 的黑箱部分转化为白箱。
- en: 'We will focus on the following procedures in order to understand the word2vec
    model:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重点关注以下程序，以了解 word2vec 模型：
- en: Distributional similarity based representation
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于分布相似性的表示
- en: Understanding the components of the word2vec model
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 word2vec 模型的组成部分
- en: Understanding the logic of the word2vec model
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 word2vec 模型的逻辑
- en: Understanding the algorithms and math behind the word2vec model
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 word2vec 模型背后的算法和数学
- en: Some of the facts regarding the word2vec model
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于 word2vec 模型的一些事实
- en: Application of the word2vec model
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用 word2vec 模型
- en: Let's begin!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Distributional similarity based representation
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于分布相似性的表示
- en: 'This is quite an old and powerful idea in NLP. The notion of distributional
    similarity is that you can get a lot of value for representing the meaning of
    a word by considering the context in which that particular word appears, and it
    is highly related with that context. There is a very famous quote by a famous
    linguist John Firth:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是自然语言处理领域中一个非常古老且强大的观点。分布相似性的概念是，通过考虑某个特定单词出现的上下文，你可以为该单词的意义提供很多价值，而且它与上下文密切相关。著名语言学家约翰·费尔斯曾有一句非常著名的名言：
- en: '"You shall know the word by the company it keeps."'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: “你可以通过它所处的环境来了解一个词。”
- en: 'Let''s take an example: if I want to find the meaning of the word banking,
    I am going to collect thousands of sentences in which the word banking is included,
    and then I will start looking at the other words with the word banking and try
    to understand the context in which the word banking is used. So, look at these
    examples and understand the distributional similarity:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举个例子：如果我想要了解“银行业”一词的含义，我将收集成千上万的包含“银行业”一词的句子，然后我会开始查看与“银行业”一词一起出现的其他词汇，并尝试理解“银行业”一词所处的上下文。看一下这些例子，并理解分布相似性：
- en: 'Sentence 1: The banking sector is regulated by the government'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子 1：银行业由政府进行监管。
- en: 'Sentence 2: Banking institutions need some technology to change their traditional
    operations.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子 2：银行机构需要一些技术来改变其传统的运营模式。
- en: In the previous sentences, the word banking is included more frequently with
    words such as government, department, operations, and so on. All these words are
    very useful to understand the context and meaning of the word banking.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的句子中，"银行业"这个词与政府、部门、运营等词语频繁出现。这些词语对理解“银行业”一词的背景和意义非常有帮助。
- en: These other words are really helpful to represent the meaning of the word banking.
    You can also use the word banking to predict the most common frequently occurring
    words or phrases when the word banking is present in a sentence.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这些其他词语确实有助于表征“银行业”一词的含义。你还可以利用“银行业”一词来预测该词出现时，最常见和频繁出现的词或短语。
- en: To understand how we can better represent the meaning of a particular word,
    as well as performing predictions about other words appearing in the context of
    this given word, we need to understand the distributional representation of the
    word in question.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地表示一个特定单词的含义，并对该单词上下文中出现的其他单词进行预测，我们需要理解该单词的分布式表示。
- en: The distributional representation of a word is a vector form in which the word
    can be represented. Words are expressed in the form of a dense vector, and the
    dense vector has to be chosen so that it will be good at predicting other words
    that appear in the context of this word. Now, each of those other words that we
    are making predictions about also have other words attached to them, so we use
    a similarity measure, such as the vector dot product. This is a kind of recursive
    approach, where each word will predict the other words that can appear in the
    same context, and other predicted words also perform the same operation by predicting
    some other words. So, we need a clever algorithm to perform this kind of recursive
    operation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 单词的分布式表示是一种向量形式，其中该单词可以被表示出来。单词以稠密向量的形式表达，而这个稠密向量必须被选择得足够好，以便它能够有效预测该单词上下文中出现的其他词语。现在，我们要对每个需要预测的其他词语也进行类似的操作，这些词语也会附带其他词语。因此，我们使用相似性度量方法，比如向量点积。这是一种递归的方法，每个单词会预测出能够出现在相同上下文中的其他单词，而其他被预测的单词也会通过预测其他单词来执行同样的操作。所以，我们需要一个巧妙的算法来执行这种递归操作。
- en: Here, please do not get confused between the terminology of distributional similarity
    and distribution representation. Distributional similarity based representation
    is actually used as part of the theory of semantics, which helps us to understand
    the meaning of the word in regular life usage; whereas the distributional representation
    of a word is the representation of a word in a vector form. To generate the vector
    form of a word, we can use one hot encoding or any other techniques, but the major
    point here is to generate the vector for a word that also carries the significance
    of similarity measure so that you can understand the contextual meaning of the
    word. Word2vec comes into the picture when we talk about distributional similarity.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，请不要混淆分布相似性和分布表示术语。基于分布相似性的表示术语实际上是语义理论的一部分，它帮助我们理解单词在日常使用中的含义；而单词的分布表示是单词以向量形式的表示。要生成单词的向量形式，我们可以使用独热编码或其他任何技术，但这里的主要点是生成一个单词向量，它还带有相似性测量的意义，以便你可以理解单词的上下文含义。当我们谈论分布相似性时，word2vec就会出现在画面中。
- en: Understanding the components of the word2vec model
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解word2vec模型的组成部分
- en: 'In this section, we will get an understanding of the main three components
    of the word2vec model, which are given as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将了解word2vec模型的主要三个组成部分，如下所示：
- en: Input of word2vec
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: word2vec的输入
- en: Output of word2vec
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: word2vec的输出
- en: Construction components of the word2vec model
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: word2vec模型的构造组件
- en: Input of the word2vec
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: word2vec的输入
- en: First of all, we should be aware of our input for developing the word2vec model,
    because that is a fundamental thing, from which you can start building word2vec.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们应该意识到我们为开发word2vec模型而输入的内容，因为这是一个基础事物，你可以从中开始构建word2vec。
- en: So, I want to state that we will use a raw text corpus as an input for developing
    the word2vec model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我想声明我们将使用原始文本语料库作为开发word2vec模型的输入。
- en: In real-life applications, we use large corpora as input. For simplicity, we
    will use a fairly small corpus to understand the concepts in this chapter. In
    later parts of this chapter, we will use a big corpus to develop some cool stuff
    by using word2vec model concepts.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，我们使用大型语料库作为输入。为简单起见，在本章中我们将使用一个相当小的语料库来理解概念。在本章的后面部分，我们将使用一个大语料库来开发一些很酷的东西，通过使用word2vec模型的概念。
- en: Output of word2vec
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: word2vec的输出
- en: This section is very important for your understanding because, after this point,
    whatever you understand will be just to achieve the output that you have set here.
    So, so far, we know that we want to develop the vector representation of a word
    that carries the meaning of the word, as well as to express the distribution similarity
    measure.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这一节对于你的理解非常重要，因为在此之后，你所理解的一切都将是为了实现你在这里设定的输出。到目前为止，我们知道我们想要开发一个单词的向量表示，它承载单词的含义，并表达分布相似性测量。
- en: 'Now, I will jump towards defining our goal and output. We want to define a
    model that aims to predict a central word and words that appear in its context.
    So, we can say that we want to predict the probability of the context given the
    word. Here, we are setting up the simple prediction objective. You can understand
    this goal by referring to the following figure:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我将跳转到定义我们的目标和输出。我们想要定义一个模型，旨在预测一个中心词及其上下文中出现的词。因此，我们可以说我们想要预测给定词的上下文的概率。在这里，我们正在设置简单的预测目标。您可以通过参考以下图来理解这个目标：
- en: '![](img/6a5a1a02-fa68-4fe9-8eee-24a11697799f.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a5a1a02-fa68-4fe9-8eee-24a11697799f.png)'
- en: 'Figure 6.5: Help to understand our goal'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5：帮助理解我们的目标
- en: As you can see, there are some simple example sentences given in the preceding
    figure. If we take the word **apple** from the first sentence and, as per our
    goal, convert the word **apple** into a vector form such that, by using that vector
    form of **apple**, we can predict the probability of the word **eat** appearing
    in the context of the word **apple**. The same logic applies to the other sentences.
    For example, in the third sentence, where we try to find out the vector of the
    word **scooter**, which helps us to predict the probability of words such as **driving**
    and **work** in the context of the given word **scooter**.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，前面的图中给出了一些简单的例句。如果我们从第一句中取出单词**apple**，并根据我们的目标，将单词**apple**转换为向量形式，使得通过使用**apple**的向量形式，我们能够预测单词**eat**在单词**apple**上下文中出现的概率。相同的逻辑也适用于其他句子。例如，在第三个句子中，我们尝试找出单词**scooter**的向量，帮助我们预测像**driving**和**work**这样的单词在给定单词**scooter**的上下文中出现的概率。
- en: So, in general, our straightforward goal is that we need to convert every word
    into vector format, such that they are good at predicting the words that appear
    in their context, and by giving the context, we can predict the probability of
    the word that is best suited for the given context.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一般来说，我们的直接目标是将每个单词转换为向量形式，使得它们能够预测出在上下文中出现的单词，并且通过提供上下文，我们可以预测出最适合该上下文的单词的概率。
- en: Construction components of the word2vec model
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: word2vec模型的构建组件
- en: 'We know our input and output so far, so now you''re probably thinking: how
    can we achieve our goal by using our input. As I mentioned, we need a clever algorithm
    that will help us to achieve our goal. Researchers have done the work for us and
    concluded that we can use neural network techniques. I would like to give you
    just a brief idea of why we are going to use neural network, but if you want a
    deep insight of it, then I would encourage you to read some of these papers.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们知道了输入和输出，因此你现在可能在想：我们如何通过我们的输入来实现目标呢？正如我之前提到的，我们需要一个聪明的算法来帮助我们实现目标。研究人员已经为我们做了这项工作，并得出结论：我们可以使用神经网络技术。我想给你一个简短的介绍，解释为什么我们要使用神经网络，但如果你想深入了解它，我建议你阅读一些这些论文。
- en: '[http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html.](http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html.](http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html)'
- en: '[http://s3.amazonaws.com/academia.edu.documents/44666285/jhbs.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1497377031&Signature=CtXl5qOa4OpzF%2BAcergNU%2F6dUAU%3D&response-content-disposition=inline%3B%20filename%3DPhysio_logical_circuits_The_intellectua.pdf](http://s3.amazonaws.com/academia.edu.documents/44666285/jhbs.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1497377031&Signature=CtXl5qOa4OpzF%2BAcergNU%2F6dUAU%3D&response-content-disposition=inline%3B%20filename%3DPhysio_logical_circuits_The_intellectua.pdf)[.](http://s3.amazonaws.com/academia.edu.documents/44666285/jhbs.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1497377031&Signature=CtXl5qOa4OpzF%2BAcergNU%2F6dUAU%3D&response-content-disposition=inline%3B%20filename%3DPhysio_logical_circuits_The_intellectua.pdf)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://s3.amazonaws.com/academia.edu.documents/44666285/jhbs.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1497377031&Signature=CtXl5qOa4OpzF%2BAcergNU%2F6dUAU%3D&response-content-disposition=inline%3B%20filename%3DPhysio_logical_circuits_The_intellectua.pdf](http://s3.amazonaws.com/academia.edu.documents/44666285/jhbs.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1497377031&Signature=CtXl5qOa4OpzF%2BAcergNU%2F6dUAU%3D&response-content-disposition=inline%3B%20filename%3DPhysio_logical_circuits_The_intellectua.pdf)[.](http://s3.amazonaws.com/academia.edu.documents/44666285/jhbs.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1497377031&Signature=CtXl5qOa4OpzF%2BAcergNU%2F6dUAU%3D&response-content-disposition=inline%3B%20filename%3DPhysio_logical_circuits_The_intellectua.pdf)'
- en: '[http://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf.](http://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf.](http://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf)'
- en: 'The reason why we are using a neural network technique is because neural networks
    are good algorithms when we are trying to learn from a large amount of data. If
    you want to build a simple, scalable, and easy to train model then a neural network
    is one of the best approaches. If you read the modern research papers that I have
    listed as follows, they will tell you the same truth. Links to the papers mentioned
    are as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之所以使用神经网络技术，是因为神经网络在从大量数据中学习时表现得非常优秀。如果你想构建一个简单、可扩展且易于训练的模型，那么神经网络是最佳的方法之一。如果你阅读我接下来列出的现代研究论文，它们会告诉你同样的真相。下面是这些论文的链接：
- en: '[http://web2.cs.columbia.edu/~blei/seminar/2016_discrete_data/readings/MikolovSutskeverChenCorradoDean2013.pdf.](http://web2.cs.columbia.edu/~blei/seminar/2016_discrete_data/readings/MikolovSutskeverChenCorradoDean2013.pdf)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://web2.cs.columbia.edu/~blei/seminar/2016_discrete_data/readings/MikolovSutskeverChenCorradoDean2013.pdf](http://web2.cs.columbia.edu/~blei/seminar/2016_discrete_data/readings/MikolovSutskeverChenCorradoDean2013.pdf)'
- en: '[https://arxiv.org/abs/1301.3781.](https://arxiv.org/abs/1301.3781)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)'
- en: 'This kind of neural network creates magic in terms of generating distributional
    similarity. Google generated the word2vec model by using the large corpus of Wikipedia.
    Refer to *Figure 6.6*, which gives you an overview of the input, and some famous
    output from the Google word2vec model. For us, the word2vec model is still a powerful
    black box that generates some great results. See the black box representation
    of word2vec in the following figure:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这种神经网络在生成分布式相似性方面创造了奇迹。Google 通过使用大量 Wikipedia 语料库来生成 word2vec 模型。请参考*图 6.6*，它给你展示了输入概况，以及
    Google word2vec 模型的一些著名输出。对于我们来说，word2vec 模型仍然是一个强大的“黑盒”，能够生成一些很棒的结果。请看以下图中的 word2vec
    黑盒表示：
- en: '![](img/d942cb9e-cdf8-469d-b4c4-2080447ba467.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d942cb9e-cdf8-469d-b4c4-2080447ba467.png)'
- en: 'Figure 6.6: Google word2vec model takes Wikipedia text as input and generates
    output'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6：Google word2vec 模型以 Wikipedia 文本作为输入并生成输出
- en: 'The preceding image shows that we have provided text data as input to the word2vec
    model. The word2vec model converts our text data to the vector form so that we
    can perform mathematical operations on this vector representation of words. The
    most famous example of word2vec is: if you have the vectors of king, man, and
    woman. Then, if you apply the mathematical operation subtracting the vector value
    of man from the king vector and add the vector value of the word woman to it,
    then we will get a resultant vector that represents the same vector value of the
    word queen. Here''s the mathematical representation of this example: *king - man
    + woman = queen*.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图像显示我们已经将文本数据作为输入提供给 word2vec 模型。word2vec 模型将我们的文本数据转换为向量形式，这样我们就可以对这些词汇的向量表示进行数学运算。word2vec
    最著名的例子是：如果你有 king、man 和 woman 这三个词的向量。那么，如果你对 king 向量减去 man 向量，再加上 woman 向量，你将得到一个表示
    queen 向量的结果。以下是这个例子的数学表示：*king - man + woman = queen*。
- en: Now we need to focus on the overview of the architectural component for word2vec.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要关注 word2vec 的架构组件概述。
- en: Architectural component
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构组件
- en: Let's look at the architectural components involved in building a word2vec model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看构建 word2vec 模型所涉及的架构组件。
- en: The major architectural component for the word2vec model is its neural network.
    The neural network for a word2vec model has two layers, and in that sense, it
    is not a deep neural network. The fact is that word2vec doesn't use deep neural
    networks to generate vector forms of words.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec 模型的主要架构组件是其神经网络。word2vec 模型的神经网络有两层，从这个意义上来说，它并不是一个深度神经网络。事实上，word2vec
    并没有使用深度神经网络来生成词汇的向量表示。
- en: This is one of the critical and main components of the word2vec model and we
    need to decode its functionality to get a clear idea about how word2vec works.
    Now it's time to decode the magical logic of the word2vec model.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 word2vec 模型中的一个关键组成部分，我们需要解码它的功能，才能清楚地了解 word2vec 是如何工作的。现在是解码 word2vec 模型神奇逻辑的时刻。
- en: Understanding the logic of the word2vec model
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 word2vec 模型的逻辑
- en: 'We will start decomposing the word2vec model and try to understand the logic
    of it. word2vec is a piece of software and it uses a bunch of algorithms. Refer
    to *Figure 6.7*:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始分解 word2vec 模型，并尝试理解其逻辑。word2vec 是一款软件，使用了一堆算法。请参考*图 6.7*：
- en: '![](img/d49e251d-2ab4-4663-b447-9434f97416b2.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d49e251d-2ab4-4663-b447-9434f97416b2.png)'
- en: 'Figure 6.7: Word2vec building block (Image credit: Xin Rong)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7：Word2vec 构建模块（图片来源：Xin Rong）
- en: 'As you can see in *Figure 6.7*, there are three main building blocks. We will
    examine each of them in detail:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 6.7*所示，主要有三个构建模块。我们将详细查看每一个模块：
- en: Vocabulary builder
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词汇构建器
- en: Context builder
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文构建器
- en: Neural network with two layers
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两层神经网络
- en: Vocabulary builder
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词汇构建器
- en: The vocabulary builder is the first building block of the word2vec model. It
    takes raw text data, mostly in the form of sentences. The vocabulary builder is
    used to build vocabulary from your given text corpus. It will collect all the
    unique words from your corpus and build the vocabulary.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇构建器是 word2vec 模型的第一个构建块。它接受原始文本数据，通常以句子的形式。词汇构建器用于从给定的文本语料库中构建词汇表。它会收集语料库中的所有唯一单词并构建词汇表。
- en: 'In Python, there is a library called `gensim`. We will use `gensim` to generate
    word2vec for our corpus. There are some parameters available in `gensim` that
    we can use to build vocabulary from our corpus as per your application needs.
    The parameter list is given as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，有一个名为`gensim`的库。我们将使用`gensim`为我们的语料库生成 word2vec。在`gensim`中有一些参数可以根据你的应用需求，用来从语料库中构建词汇表。以下是参数列表：
- en: '`min_count`: This parameter is used as a threshold value. This ignores all
    words with a total frequency of lower than the specified value. So, for example,
    if you set `min_count = 5`, then the output of the vocabulary builder doesn''t
    contain words that occur less than five times. The vocabulary builder output contains
    only words that appeared in the corpus more than or equal to five times.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_count`：该参数用作阈值。它会忽略所有总频率低于指定值的单词。例如，如果你设置`min_count=5`，则词汇构建器的输出不会包含出现次数少于五次的单词。词汇构建器的输出只包含在语料库中出现五次或更多的单词。'
- en: '`build_vocab(sentences`, `keep_raw_vocab=False`, `trim_rule=None`, `progress_per=10000`,
    `update=False)`: This syntax is used to build vocabulary from a sequence of sentences
    (can be a once-only generator stream). Each sentence must be a list of unicode
    strings.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`build_vocab(sentences, keep_raw_vocab=False, trim_rule=None, progress_per=10000,
    update=False)`：此语法用于从一系列句子（可以是一次性生成的流）中构建词汇表。每个句子必须是一个 Unicode 字符串的列表。'
- en: 'There are other parameters that you can read about by clicking on this link:
    [https://radimrehurek.com/gensim/models/Word2vec.html.](https://radimrehurek.com/gensim/models/Word2vec.html)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以点击这个链接，阅读更多关于其他参数的信息：[https://radimrehurek.com/gensim/models/Word2vec.html.](https://radimrehurek.com/gensim/models/Word2vec.html)
- en: 'Each word present in the vocabulary has an association with the vocabulary
    object, which contains an index and a count. That is the output of the vocabulary
    builder. So you can refer to *Figure 6.8*, which helps you to understand the input
    and output of the vocabulary builder:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表中的每个单词都与词汇对象相关联，该对象包含一个索引和一个计数。这是词汇构建器的输出。因此，你可以参考*图 6.8*，它帮助你理解词汇构建器的输入和输出：
- en: '![](img/6d986b99-ea28-413e-9356-a7533859046f.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d986b99-ea28-413e-9356-a7533859046f.png)'
- en: 'Figure 6.8: Input and output flow of vocabulary builder'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8：词汇构建器的输入输出流程
- en: Context builder
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上下文构建器
- en: The context builder uses output of the vocabulary builder, as well as words
    that are part of the context window, as input and generates the output.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文构建器使用词汇构建器的输出，以及作为上下文窗口一部分的单词，作为输入并生成输出。
- en: First of all, let's understand the concept of a context window. This context
    window is kind of a sliding window. You can define the window size as per the
    NLP application in which you will use word2vec. Generally, NLP applications use
    the context window size of five to ten words. If you decide to go with a window
    size of five, then we need to consider the five words on the left side from the
    center word and the five words on the right side of the center word. In this way,
    we capture the information about what all the surrounding words are for our center
    word.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们理解上下文窗口的概念。这个上下文窗口类似于一个滑动窗口。你可以根据将要使用 word2vec 的自然语言处理应用程序定义窗口的大小。通常，NLP
    应用程序使用大小为五到十个单词的上下文窗口。如果你选择五个单词作为窗口大小，那么我们需要考虑中心单词左边的五个单词和右边的五个单词。通过这种方式，我们捕获了中心单词的所有周围单词的信息。
- en: 'Here, I want to state an example, and for this, the context window''s size
    is equal to one, as we have a one-sentence corpus. I have the sentence: **I like
    deep learning,** and **deep** is the center word. So then, you should consider
    the surrounding words as per our window size. So here, I need to consider the
    words **like** and **learning**. In the next iteration our center word will be
    **learning,** its surrounding words are **deep**, and at the end of the sentence
    is a **period** (**.**).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我想举一个例子，假设上下文窗口的大小为1，因为我们有一个单句语料库。我有一句话：**I like deep learning,** 其中**deep**是中心词。那么接下来，你应该根据窗口大小考虑周围的词。因此，在这里，我需要考虑**like**和**learning**这两个词。在下一次迭代中，我们的中心词将是**learning**，它的周围词是**deep**，句子的结尾是一个**句点**（**.**）。
- en: I hope the context window concept is clear in your head. Now, we need to link
    this concept and see how the context builder uses this concept and the output
    of the vocabulary builder. The vocabulary builder object has word indexes and
    the frequency of the word in the corpus. By using the index of the word, the context
    builder has an idea of which word we are looking at and, according to the context
    window size, it considers the other surrounding words. These center words and
    the other surrounding words are input to the context builder.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你已经清晰理解了上下文窗口的概念。现在，我们需要将这个概念与上下文构建器如何使用这个概念以及词汇构建器的输出联系起来。词汇构建器对象包含单词的索引以及该单词在语料库中的频率。通过使用单词的索引，上下文构建器就能知道我们在查看哪个单词，并且根据上下文窗口的大小，它会考虑周围的其他单词。这些中心词和其他周围词将作为输入传递给上下文构建器。
- en: 'Now you have a clear idea about what the inputs to the context builder are.
    Let''s try to understand the output of the context builder. This context builder
    generates the word pairing. Refer to *Figure 6.9*, to get an idea about word paring:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你对上下文构建器的输入有了清晰的了解。让我们试着理解上下文构建器的输出。这个上下文构建器生成词对。参考*图6.9*，了解词对的概念：
- en: '![](img/e49e6498-3f88-492d-b13d-cae42fbe2197.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e49e6498-3f88-492d-b13d-cae42fbe2197.png)'
- en: 'Figure 6.9: Understanding word paring'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9：理解词对
- en: These word pairings will be given to the neural network. The network is going
    to learn the basic statistics from the number of times each word pairing shows
    up. So, for example, the neural network is probably going to get many more training
    examples of (deep, learning) than it is of (deep, communication). When the training
    is finished, if you give it the word **deep** as input, then it will output a
    much higher probability for learning or network than it will for communication.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这些词对将被提供给神经网络。网络将根据每个词对出现的次数来学习基本统计信息。例如，神经网络可能会看到更多（deep, learning）这个词对的训练示例，而不是（deep,
    communication）。当训练完成后，如果你将**deep**作为输入，它将为learning或network输出更高的概率，而不是为communication输出。
- en: 'So this word pair is the output of the context builder and it will pass to
    the next component, which is a two layer neural network. Refer to *Figure 6.10*,
    to see the summary of the flow of context builder:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这个词对是上下文构建器的输出，它将传递给下一个组件，即一个两层的神经网络。参考*图6.10*，查看上下文构建器流程的概述：
- en: '![](img/2209d360-175f-4706-a845-f0399eb57340.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2209d360-175f-4706-a845-f0399eb57340.png)'
- en: 'Figure 6.10: Input and output flow of context builder'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10：上下文构建器的输入和输出流
- en: So far, we have seen two major components of the word2vec building blocks. Now,
    our next focus will be on the final component, which is the neural network.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了word2vec构建块的两个主要组件。现在，我们的下一个关注点将是最终组件——神经网络。
- en: Neural network with two layers
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 两层神经网络
- en: In this section, we will look at the input and output of the neural network.
    Apart from that, we will also focus on the structural part of the neural network,
    which will give us an idea of how a single neuron looks, how many neurons there
    should be, what an activation function is, and so on. So, now, let's get started!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将研究神经网络的输入和输出。除此之外，我们还将关注神经网络的结构部分，这将帮助我们了解单个神经元的样子，神经元的数量，激活函数是什么等等。那么，现在我们开始吧！
- en: Structural details of a word2vec neural network
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: word2vec神经网络的结构细节
- en: 'Word2vec uses the neural network for training. So, for us, it is very important
    to understand the basic structure of the neural network. The structural details
    of a neural network are given as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec 使用神经网络进行训练。所以对我们来说，理解神经网络的基本结构非常重要。神经网络的结构细节如下所示：
- en: There is one input layer
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一个输入层
- en: The second layer is the hidden layer
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二层是隐藏层
- en: The third and final layer is the output layer
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word2vec neural network layer's details
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we know, there are two layers in the neural network for generating word vectors.
    We will start to look at each of the layers and their input and output in detail.
    Here, we are not including the math behind the word2vec model in this section.
    Later in this chapter, we will also look at the math behind word2vec, and I will
    let you know at that point of time to map your dots for better interpretation.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand the task of each layer in brief:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '**Input layer**: An input layer has as many neurons as there are words in the
    vocabulary for training'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden layer**: The hidden layer size in terms of neurons is the dimensionality
    of the resulting word vectors'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output layer**: The output layer has the same number of neurons as the input
    layer'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The input for the first input layer is the word with one-hot encoding. Assume
    that our vocabulary size for learning word vectors is **V**, which means there
    are **V** numbers of different words in the corpus. In that case, the position
    of the word that represents itself is encoded as **1** and all others positions
    are encoded as **0**. Refer to *Figure 6.4* again to recall the concept of one-hot
    encoding. Suppose the dimension of these words is **N**. So, the input to the
    hidden layer connections can be represented by our input matrix **WI** (input
    matrix symbol) of size *V * N* ,with each row of the matrix **WI** representing
    a vocabulary word. Similarly, the connections from the hidden layer to the output
    layer means the output from the hidden layer can be described by the hidden layer
    output matrix **WO** (hidden layer matrix symbol). The **WO** matrix is of size
    *N * V*. In this case, each column of the **WO** matrix represents a word from
    the given vocabulary. Refer to *Figure 6.11* to get a crystal clear picture of
    the input and output. As well as this, we will also look at one short example
    to understand the concept:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3eb2989e-0e2b-4df7-b67c-c38e7e193d26.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.11: two layer neural network input and output structural representation'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s talk in terms of examples. I will take a very small set of the corpus.
    See the sentences from our small corpus given as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: the dog saw a cat
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the dog chased a cat
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the cat climbed a tree
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding three sentences have eight (8) unique words. We need to order
    them in alphabetical order, and if we want to access them, then we will refer
    to the index of each word. Refer to the following table:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '| **Words** | **Index** |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: '| a | 1 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: '| cat | 2 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
- en: '| chased | 3 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '| climbed | 4 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '| dog | 5 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: '| saw | 6 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| the | 7 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| tree | 8 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: 'So, here our value for **V** is equal to **8**. In our neural network, we need
    eight input neurons and eight output neurons. Now let''s assume we will have three
    (3) neurons in the hidden layer. So in this case, our **WI** and **WO** values
    are defined as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '*WI = [V * N] = [8 * 3]*'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*WO = [N * V] = [3 * 8]*'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before training begins, these matrices, **WI** and **WO** , are initialized
    by using small random values, as is very common in neural network training. Just
    for illustration purposes, let us assume that **WI** and **WO** are to be initialized
    to the following values:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练开始之前，这些矩阵，**WI** 和 **WO**，通过使用小的随机值进行初始化，这在神经网络训练中非常常见。仅为说明目的，我们假设 **WI**
    和 **WO** 初始化为以下值：
- en: '*WI =*'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*WI =*'
- en: '![](img/b40ddeaa-3a18-440c-9909-27d279c40262.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b40ddeaa-3a18-440c-9909-27d279c40262.png)'
- en: '*WO =*'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '*WO =*'
- en: '![](img/4de28604-a756-4429-b4da-c65dfdb7d747.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4de28604-a756-4429-b4da-c65dfdb7d747.png)'
- en: 'Image source: https://iksinc.wordpress.com'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源: https://iksinc.wordpress.com'
- en: We are targeting it so that our neural network can learn the relationship between
    the words **cat** and **climbed**. So, in other words, we can explain that the
    neural network should give high probability for the word **climbed** when the
    word **cat** is fed into the neural network as an input. So, in word embedding,
    the word **cat** is referred to as a context word and the word **climbed** is
    referred to as a target word.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是让神经网络学习单词 **cat** 和 **climbed** 之间的关系。换句话说，我们可以解释为当单词 **cat** 输入到神经网络时，神经网络应为单词
    **climbed** 提供较高的概率。因此，在词嵌入中，单词 **cat** 被称为上下文单词，而单词 **climbed** 被称为目标单词。
- en: The input vector *X* stands for the word **cat** and it will be *[0 1 0 0 0
    0 0 0]t*. Notice that only the second component of the vector is *1*. The reason
    behind this is that the input word **cat**, holds the second position in a sorted
    list of the corpus. In the same way, the target word is **climbed** and the target
    vector for **climbed** will look like *[0 0 0 1 0 0 0 0 ]t*.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 输入向量 *X* 表示单词 **cat**，它将是 *[0 1 0 0 0 0 0 0]t*。注意，向量的第二个分量是 *1*。之所以这样，是因为输入单词
    **cat** 在语料库的排序列表中排在第二位。以同样的方式，目标单词是 **climbed**，目标向量为 **climbed** 的表示将是 *[0 0
    0 1 0 0 0 0 ]t*。
- en: The input for the first layer is *[0 1 0 0 0 0 0 0]t*.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层的输入是 *[0 1 0 0 0 0 0 0]t*。
- en: 'The hidden layer output *Ht* is calculated by using the following formula:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层输出 *Ht* 通过以下公式计算：
- en: '*Ht = XtWI = [-0.490796 -0.229903 0.065460]*'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*Ht = XtWI = [-0.490796 -0.229903 0.065460]*'
- en: 'From the preceding calculation, we can figure out that, here, the output of
    the hidden neurons mimics the weights of the second row of the **WI** matrix because
    of one-hot encoding representation. Now we need to check a similar calculation
    for the hidden layer and output layer. The calculation for the hidden layer and
    output layer is defined as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的计算中，我们可以看出，由于采用了独热编码表示，这里隐藏神经元的输出类似于 **WI** 矩阵第二行的权重。现在我们需要对隐藏层和输出层进行类似的计算。隐藏层和输出层的计算定义如下：
- en: '*HtWO = [0.100934 -0.309331 -0.122361 -0.151399 0.143463 -0.051262 -0.079686
    0.112928]*'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '*HtWO = [0.100934 -0.309331 -0.122361 -0.151399 0.143463 -0.051262 -0.079686
    0.112928]*'
- en: 'Here, our final goal is to obtain probabilities for words in the output layer.
    From the output layer, we are generating probability that reflects the next word
    relationship with the context word at input. So, the mathematical representation
    is given as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们的最终目标是获取输出层中各个单词的概率。通过输出层，我们生成反映下一个单词与输入上下文单词关系的概率。因此，数学表示如下：
- en: '*Probability (wordk|wordcontext) for k = 1...V*'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*概率 (wordk|wordcontext) 对于 k = 1...V*'
- en: Here, we are talking in terms of probability, but our output is in the form
    of a set of vectors, so we need to convert our output into probability. We need
    to take care that the sum of the neuron output from the final output layer should
    be added to one. In word2vec, we are converting activation values of the output
    layer neurons to probabilities by using the softmax function.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们讨论的是概率，但我们的输出是一个向量集合，因此我们需要将输出转换为概率。我们需要确保最终输出层神经元的输出之和等于 1。在 word2vec
    中，我们通过使用 softmax 函数将输出层神经元的激活值转换为概率。
- en: Softmax function
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Softmax 函数
- en: 'In this section, we will look at the softmax function. The softmax function
    is used for converting output vectors into a probability form. We are using the
    softmax function because we want to convert out last layer output in terms of
    probability and softmax function can easily convert vector values into probability
    values. Here, the output of the *k*^(th) neuron will be computed by the following
    equation, where activation(*n*) represents the activation value of the *n*th output
    layer neuron:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f6ffc37-d899-4db6-8063-b8116a611312.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: 'By using this equation, we can calculate the probabilities for eight words
    in the corpus and the probability values are given as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '*[ 0.143073 0.094925 0.114441 0.111166 0.149289 0.122874 0.119431 0.144800
    ]*'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'You must be wondering how I got these probability values. I used the previous
    softmax probability equation and generated the final probability vector. You can
    find the Python code for the softmax function in the following code snippet:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The given code will generate the following output vector:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see, the probability *0.111166* is for the chosen target word climbed.
    As we know, the target vector is *[0 0 0 1 0 0 0 0]t*, so we can compute the error
    by prediction. To generate a prediction error or error vector, we need to subtract
    the probability vector from the target vector, and once we know the error vector
    or error values, we can adjust the weight according to that. Here, we need to
    adjust the weight values of the matrices *WI* and *WO*. The technique of propagating
    errors in the network and readjusting the weight values of *WI* and *WO* is called
    **backpropagation**.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the training can continue by taking different context-target word pairs
    from the corpus. This is the way word2vec learns relationships between words in
    order to develop a vector representation of words in the corpus.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Main processing algorithms
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Word2vec has two different versions. These versions are the main algorithms
    for word2vec. Refer to *Figure 6.12*:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/010476eb-893d-4ca5-b3cd-251cb29f3f2f.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.12: Versions of word2vec'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will look at the main two processing algorithms. Those
    algorithms names are given as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Continuous bag of words
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Skip-gram
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous bag of words
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the **continuous bag of words** (**CBOW**) algorithm, context is represented
    by multiple words for given target words. Just recall our example that we stated
    in an earlier section, where our context word was **cat** and our target word
    was **climbed.** For example, we can use **cat** as well as **tree** as context
    words to predict the word **climbed** as the target word. In this case, we need
    to change the architecture of the neural network, especially the input layer.
    Now, our input layer may not represent the single-word one-hot encode vector,
    but we need to put another input layer that represents the word **tree**.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'If you increase the context words, then you need to put an additional input
    layer to represent each of the words, and all these input layers are connected
    to the hidden layer. Refer to *Figure 6.13*:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你增加上下文词的数量，那么你需要添加一个额外的输入层来表示每个词，并且所有这些输入层都与隐藏层相连接。参考 *图 6.13*：
- en: '![](img/618b4974-58e4-4569-9ddd-bf8fd1588ad0.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/618b4974-58e4-4569-9ddd-bf8fd1588ad0.png)'
- en: 'Figure 6.13: CBOW neural network architecture (Image credit: https://www.semanticscholar.org)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.13：CBOW 神经网络架构（图片来源： https://www.semanticscholar.org）
- en: Here, the good part is that the computation formula remains the same; we just
    need to compute the *Ht* for other context words as well.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，好的一点是计算公式保持不变；我们只需要计算其他上下文词的 *Ht*。
- en: Skip-gram
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跳字模型
- en: The **skip-gram** (**SG**) model reverses the usage of target words and context
    words. Here, the target word is given as input to the input layer in the form
    of a one-hot encoded vector. The hidden layer remains the same. The output layer
    of the neural network is repeated multiple times to generate the chosen number
    of context words.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**skip-gram**（**SG**）模型颠倒了目标词和上下文词的使用。在这里，目标词作为输入以 one-hot 编码向量的形式输入到输入层。隐藏层保持不变。神经网络的输出层重复多次以生成所选数量的上下文词。'
- en: 'Let''s take an example of the words **cat** and **tree** as context words and
    the word **climbed** as a target word. The input vector in the SG model will be
    the one-hot encoded word vector of the word **climbed** *[0 0 0 1 0 0 0 0 ]t*
    and this time, our output vectors should be vectors for the word **cat** and the
    word **tree**. So, the output vector should be *[ 0 1 0 0 0 0 0 0]* for **cat**
    and *[0 0 0 0 0 0 0 1]* for **tree**. Refer to the structure of the skip-gram
    algorithm in *Figure 6.14*:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以单词 **cat** 和 **tree** 作为上下文词，以单词 **climbed** 作为目标词为例。在 SG 模型中，输入向量将是单词 **climbed**
    的 one-hot 编码词向量 *[0 0 0 1 0 0 0 0 ]t*，而这次，我们的输出向量应该是单词 **cat** 和单词 **tree** 的词向量。因此，输出向量应该是
    **cat** 的 *[ 0 1 0 0 0 0 0 0]* 和 **tree** 的 *[0 0 0 0 0 0 0 1]*。参考 *图 6.14* 中的跳字模型结构：
- en: '![](img/61189b79-4a72-4e08-a4da-39260ed5896e.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61189b79-4a72-4e08-a4da-39260ed5896e.png)'
- en: 'Figure 6.14: Skip gram neural network architecture (Image credit: https://www.semanticscholar.org)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.14：跳字模型神经网络架构（图片来源： https://www.semanticscholar.org）
- en: This time the output will not be a single vector of probability, but two different
    vectors of probability, as we have two words as the context word. Here, error
    vector will be calculated in the same manner as we defined earlier. The small
    change in skip-gram is that the error vectors from all the output layers are summed
    up to adjust the weights via backpropagation. This means we need to ensure that
    the weight of matrix *WO* for each output layer should remain identical through
    the training.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这次输出将不是一个单一的概率向量，而是两个不同的概率向量，因为我们有两个上下文词。在这里，误差向量将按照我们之前定义的方式计算。skip-gram 中的小变化是，所有输出层的误差向量会被加总，通过反向传播调整权重。这意味着我们需要确保每个输出层的矩阵
    *WO* 的权重在训练过程中保持一致。
- en: Some of the algorithmic techniques and math behind word2vec and other techniques
    will be explained in the next section. So, let's get ready to deal with some cool
    mathematical stuff!
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 一些关于 word2vec 和其他技术背后的算法技术和数学将在下一节中解释。所以，准备好处理一些很酷的数学内容吧！
- en: Understanding algorithmic techniques and the mathematics behind the word2vec
    model
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 word2vec 模型背后的算法技术和数学原理
- en: This section is very important, as we are going to discuss here the core algorithms
    that have been used in word2vec. By the end of this section, there won't be any
    secrets left in order for you to understand the concept of word2vec. Thus, this
    section is converting word2vec black box into word2vec white box. Here, I'm going
    to include the math part as well, so readers can understand the core concepts
    in a better manner. Don't worry if you don't know the math, because I will provide
    you with some resources that you may find really useful.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 本节非常重要，因为我们将在这里讨论在 word2vec 中使用的核心算法。到本节结束时，为了让你理解 word2vec 的概念，所有的秘密都将揭开。因此，本节将把
    word2vec 的黑箱变成白箱。在这里，我还将包括数学部分，以便读者能够更好地理解核心概念。即使你不懂数学也不用担心，因为我将提供一些可能对你非常有用的资源。
- en: Understanding the basic mathematics for the word2vec algorithm
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 word2vec 算法的基本数学
- en: 'To begin with, we need some of the basic mathematics concepts in place for
    a better understanding of the algorithm. The topics from mathematics that we need
    are listed as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '**Vectors**: Vectors have magnitude and direction. So, when we draw a vector
    in vector space it carries some magnitude as well as direction. You can perform
    basic math operations on vectors.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Matrices**: A matrix is a grid of numbers or frequency count of words. It
    has rows and columns. We can define the dimensions of the matrix by counting the
    number of rows and columns it contains. You can refer to this link for more information
    on matrices.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partial derivative**: If there is a function that contains more than one
    variable and we perform a derivative for this kind of function with respect to
    one of these variables and hold others constant, this is how we perform partial
    derivative. Partial derivatives are used in vector calculus.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partial derivative chain rule**: The chain rule is defined as a formula that
    is used for computing the derivative of the composition of two or more functions.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let's jump to the understanding of the techniques. I have segregated all
    concepts broadly into three sections according to the stages where each and every
    technique has been used.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'I have listed some of the reference links from where you get an in-depth idea
    about each of the given concepts. You can follow this link:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.khanacademy.org/math.](https://www.khanacademy.org/math)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the following links for vectors:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[http://emweb.unl.edu/math/mathweb/vectors/vectors.html.](http://emweb.unl.edu/math/mathweb/vectors/vectors.html)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.mathsisfun.com/algebra/vectors.html.](https://www.mathsisfun.com/algebra/vectors.html)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to this link for more information on matrix:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[https://medium.com/towards-data-science/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c.](https://medium.com/towards-data-science/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see some basic examples by using this link:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[http://mathinsight.org/partial_derivative_examples.](http://mathinsight.org/partial_derivative_examples)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.analyzemath.com/calculus/multivariable/partial_derivatives.html.](http://www.analyzemath.com/calculus/multivariable/partial_derivatives.html)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the following links for the partial derivative chain rule:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[https://math.oregonstate.edu/home/programs/undergrad/CalculusQuestStudyGuides/vcalc/chain/chain.html.](https://math.oregonstate.edu/home/programs/undergrad/CalculusQuestStudyGuides/vcalc/chain/chain.html)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube.com/watch?v=HOYA0-pOHsg.](https://www.youtube.com/watch?v=HOYA0-pOHsg)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube.com/watch?v=aZcw1kN6B8Y.](https://www.youtube.com/watch?v=aZcw1kN6B8Y)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: The preceding list is more than enough for this chapter to understand the mathematics
    behind the algorithms.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Techniques used at the vocabulary building stage
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While generating vocabulary from the dataset, you may use an optimization technique,
    and lossy counting is the one that is used the most for the word2vec model.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在从数据集中生成词汇时，你可以使用一种优化技术，其中有损计数法是最常用于word2vec模型的技术。
- en: Lossy counting
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有损计数
- en: The lossy count algorithm is used to identify elements in a dataset whose frequency
    count exceeds a user-given threshold. This algorithm takes data streams as an
    input instead of the finite set of a dataset.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 有损计数算法用于识别数据集中那些频率计数超过用户设定阈值的元素。该算法以数据流作为输入，而不是数据集的有限集合。
- en: With lossy counting, you periodically remove very low-count elements from the
    frequency table. The most frequently accessed words would almost never have low
    counts anyway, and if they did, they wouldn't be likely to stay there for long.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 使用有损计数法，你会定期从频率表中移除计数非常低的元素。最常访问的词几乎不会有低频计数，哪怕它们有，通常也不会长时间停留在那里。
- en: Here, the frequency threshold is usually defined by the user. When we give a
    parameter of `min_count = 4`, we remove the words that appear in the dataset less
    than four times and we will not consider them.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，频率阈值通常由用户定义。当我们设置参数`min_count = 4`时，我们会移除在数据集中出现次数少于四次的词，并且不再考虑它们。
- en: Using it at the stage of vocabulary building
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在词汇构建阶段使用它
- en: Lossy counting is very useful; especially when you have a large corpus and you
    don't want to consider the words that appear very rarely.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 有损计数法非常有用，特别是当你有一个大型语料库并且不想考虑那些出现非常罕见的词时。
- en: At this time, lossy counting is very useful because the user can set the minimum
    word frequency count as a threshold, so words that occur less than the threshold
    frequency count won't be included in our vocabulary.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，有损计数法非常有用，因为用户可以设置一个最小词频作为阈值，这样频率低于该阈值的词将不会被包含在我们的词汇中。
- en: So, if you have a large corpus and you want to optimize the speed of training,
    then we can use this algorithm.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果你有一个大型语料库，并且想优化训练速度，那么我们可以使用这个算法。
- en: In other words, you can say that by using this algorithm you narrow down your
    vocabulary size, thus, you can speed up the training process.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，你可以说通过使用这个算法，你缩小了词汇量，从而可以加速训练过程。
- en: Applications
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用
- en: Apart from word2vec, the lossy counting algorithm is used in network traffic
    measurements and analysis of web server logs.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 除了word2vec之外，有损计数算法还用于网络流量测量和Web服务器日志分析。
- en: Techniques used at the context building stage
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在构建上下文阶段使用的技术
- en: 'While generating word context pairs, the context builder uses the following
    techniques:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成词对上下文时，上下文构建器使用以下技术：
- en: Dynamic window scaling or dynamic context window
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态窗口缩放或动态上下文窗口
- en: Subsampling
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子抽样
- en: Pruning
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 剪枝
- en: Dynamic window scaling
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态窗口缩放
- en: As you can see, dynamic window scaling is a part of the context builder. We
    will see how it can be useful and what kind of impact it generates when we use
    it. Dynamic window scaling is also known as **dynamic context window**.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，动态窗口缩放是上下文构建器的一部分。我们将看到它如何有用，以及在使用它时会产生什么样的影响。动态窗口缩放也被称为**动态上下文窗口**。
- en: Understanding dynamic context window techniques
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解动态上下文窗口技术
- en: In the word2vec implementation, dynamic context window is an optional technique
    that may be applied to generate more accurate output. You can also consider these
    techniques as hyperparameters.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在word2vec实现中，动态上下文窗口是一种可选的技术，可以用来生成更精确的输出。你也可以将这些技术视为超参数。
- en: Dynamic context window techniques use weight schema for context words with respect
    to target words.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 动态上下文窗口技术使用权重方案来衡量上下文词与目标词之间的关系。
- en: So the intuition here is that words that are near to the target word are more
    important than other words that are far away from the target word.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里的直觉是，离目标词近的词比那些离目标词远的词更为重要。
- en: Let us see how it will be useful when we are building word pairs. Dynamic context
    window considers that nearby context words hold more importance to predicting
    the target word. Here, we are applying the weighting scheme by using uniform sampling
    on the actual window size between 1 and L. For example, suppose the context window
    size is 5 and now the weight of context words are distributed in a uniform manner,
    so the weight of most nearby words is 5/5, the very next context word weight is
    4/5, and so on. So, the final weight for context words will be 5/5, 4/5, 3/5,
    2/5, 1/5\. Thus, by providing weight, you can fine-tune the final result.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Subsampling
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Subsampling is also one of the techniques that we use when we are building word
    pairs, and as we know, these word pairs are sample training data.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Subsampling is the method that removes the most frequent words. This technique
    is very useful for removing stop words.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'These techniques also remove words randomly, and these randomly chosen words
    occur in the corpus more frequently. So, words that are removed are more frequent
    than some threshold `t` with a probability of `p`, where `f` marks the words corpus
    frequency and we use *t = 10−5* in our experiments. Refer to the following equation
    given in *Figure 6.15*:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b79f0040-3022-4c81-bf10-b2979cdd8afc.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.15: Subsampling probability equation'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: This also acts as one of the useful hyperparameters, and it is very useful because
    we are removing the most frequent and unnecessary words from the corpus, as well
    as from the context window, and that way, we are improving the quality of our
    training sample set.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Pruning
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pruning is also used when we are building our word pairs for training purposes
    using context builder. When you have a large amount of vocabulary to deal with,
    if you have included less frequent words, then you need to remove them. You can
    also restrict the size of the total vocabulary by using the `max_vocab_size` parameter
    given in the Python `gensim` library.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how useful pruning is for us in order to generate word2vec. Pruning
    is used to prune the training sample size, as well as make the quality of it better.
    If you don't prune the rarely occurred words from the dataset, then the model
    accuracy may degrade. This is a kind of hack to improve the accuracy.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms used by neural networks
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we will look at the structure of individual neurons. We will also look
    into the details about the two algorithms, thus, we will understand how word2vec
    generates vectors from words.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Structure of the neurons
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen the overall neural network structure, but we haven't yet seen what
    each neuron is made of and what the structure of the neurons is. So, in this section,
    we will look at the structure of each single input neuron.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'We will look at the following structures:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Basic neuron structure
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a single neuron
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single neuron application
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-layer neural network
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backpropagation
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mathematics behind the word2vec model
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this, we will heavily include the mathematical formulas. If you are not from
    a mathematical background, then don't worry. I will give you explanations in simple
    words so you know what is going on in each section.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Basic neuron structure
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to *Figure 6.16;* it shows the basic neuron structure:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f70b4079-ac1a-46ac-ab0e-d119336d8906.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.16: Basic neuron structure (Image credits: Xin Rong)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6.16* shows a basic neuron structure. This structure is not new. This
    structure takes an input, and there are weights also as input, and they calculate
    the weighted sum. Here, *x1* to *xk* is the input value and *w1* to *wk* is the
    corresponding weights. So, the weighted sum is expressed by the following equation
    given in *Figure 6.17*:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf4e0175-6266-422d-8ad7-a0c4a91b4e9c.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.17: Weighted sum equation'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Let's take an example to understand the usage of the equation. So, if you have
    input *x=[1 5 6]* and *w=[0.5 0.2 0.1],* the weighted sum *u* is equal to *[1
    * 0.5 + 5 * 0.2 + 6 * 0.1],* so our final answer is *u = [0.5 + 1 + 0.6] = 2.1*.
    This is just a simple example to give you some concrete practical intuition about
    the real workings of the given equation. This is all about our input. Now we will
    talk about the output.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to generate output, we can say from our basic neuron structure that
    our output is the function of weighted sum *u*. Here, *y* is the output and *f(u)*
    is the function of the weighted sum. You can see the equation given in *Figure
    6.18*:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/382e56be-de82-4c74-80ea-51cc035412db.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.18: Output y is function of weighted sum u'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'In neural networks, we can use different available functions and these functions
    are called **activation functions**. These functions are listed as follows:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Step function
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sigmoid function
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some great scientists have stated that the given functions are a good choice
    as activation functions. For this chapter, we are not getting into the details
    of activation functions, but we will look at all the details regarding the activation
    functions in [Chapter 9](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml), *Deep Learning
    for NLP and NLG Problems*. So, we will consider the given two functions as activation
    functions for word2vec. We will use either the step function or sigmoid function,
    not both at the same time, to develop word2vec. Refer to the equation of both
    of the functions in *Figure 6.19*:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8c40266-ca20-4c0a-9f9e-778caeac164f.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.19: Activation functions, the first is the step function and the second
    is the sigmoid function'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Here, *f(u)* is the step function and *f(u)* is the sigmoid function.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: When we draw a circle to represent the neuron similar to the one drawn in *Figure
    6.11*, this circle contains the weighted sum and activation function, which is
    the reason we have indicated the dotted circle in *Figure 6.16*.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see how these activation functions can be used
    and how we can calculate the errors in predicted output by using the error function.
    We will see this in detail about the usage of the activation function and error
    function. So let's begin!
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Training a simple neuron
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now it is time to see how we can use a single neuron for training by using the
    activation function, and let's understand the loss function to calculate the error
    in predicted output.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: The main idea is defined as the error function, which actually tells us the
    degree of error in our prediction; we will actually try to make our error value
    as low as possible. So, in other words, we are actually trying to improve our
    prediction. During training, we use input and calculate the error by using the
    error function and update the weight of neurons and repeat our training process.
    We will continue this process until we get the minimum error rate of maximum,
    best, and accurate output.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: 'The two most important concepts that we are going to look at are listed as
    follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Define error function (loss function)
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding of gradient descent in word2vec
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define error function
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, our input is vector `X` with vocabulary `x1` to *xk* and our output `y`
    is the output vector. So, to calculate the error `E` , we need to define the error
    function or loss function, and we are using the L2 loss function. Let's begin
    with understanding the basic concept of the L2 loss function, and then we will
    see how it will be useful in word2vec.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two type of loss functions that are mostly used across **machine
    learning** (**ML**) and **deep learning** (**DL**). By the way, we will look at
    ML and DL in upcoming chapters, which are [*Chapter 8*](97808151-90d2-4034-8d53-b94123154265.xhtml),
    *Machine Learning ( ML) for NLP Problems* and [Chapter 9](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml),
    *Deep Learning for NLP and NLG Problems*. There are two standard types of loss
    functions, but here we will only look at **least square error** (**L2**), and
    in upcoming chapters, which are [Chapter 8](97808151-90d2-4034-8d53-b94123154265.xhtml),
    *Machine Learning for NLP Problems* and [Chapter 9](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml),
    *Deep Learning for NLP and NLG Problems* we will look at various error functions
    in detail and compare those error functions as well. The two standard types of
    loss functions are:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Least absolute deviation (L1)
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Least square error (L2)
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Least square error is also called **L2 loss** function. In general, loss functions
    want to achieve their minimized value while learning from a dataset and L2 loss
    functions also want to achieve their value where the error value will be minimal.
    So, precisely, the L2 function wants to minimize the squared differences between
    the estimated and existing target values.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 'The structure of a single neuron at the time of training is given in *Figure
    6.20*:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f62cfc9f-068d-449c-9bb0-8307a712d33a.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.20: Single neuron at the time of training'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'So, when we are trying to calculate an L2 loss function of a single neuron,
    we will use the following equation given in *Figure 6.21*:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6fa6d6a-7fa3-4359-8b3f-def68caff86e.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.21: L2 loss function equation (Least Squared error function)'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Here, *t* is the target vector value, *y* is the estimated vector value or predicted
    vector value, and *E* is the error function. We have defined our L2 error function.
    I know you must be very keen to know what we will be doing with this L2 error
    function to get a least error value, and that is where we need to understand the
    concept of gradient descent.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Understanding gradient descent in word2vec
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's understand what we are going to do with the L2 function and how it
    will be useful in achieving an accurate output.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: As we said earlier, we want to minimize this function value so, we can accurately
    predict the target value, and to achieve this we will take partial derivative
    of the L2 function equation given in *Figure 6.21* with respect to *y*. The procedure
    of deriving derivation and then by using this derivation trying to minimize error
    values, is called **gradient descent**.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'In that case, the result is given in *Figure 6.22*:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01aba3d0-bbfd-46d9-a936-b7b2c64a2c36.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.22: Result of Partial derivatives of L2 loss function with respect
    to y'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that output *y* is dependent on *f(u)* and *f(u)* is dependent on input
    weight values *wi*. So we need to apply chain rules and generate the error function
    value. If we use partial derivative chain rules, then we will get the following
    equation, which will be useful in word2vec. *Figure 6.23* shows the result of
    the partial derivative chain rule:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe9277c1-6873-4076-8ba3-dd667802265c.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.23: Partial derivative chain rule result for L2 error function'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: After calculating the L2 loss function as per the value of the error, the neural
    network input weight will be updated and this kind of iteration will continue
    until we achieve the minimum error value or error rate.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have been deriving equations for a single neuron, so it will be important
    to know what this single neuron can do for us. That is our next point of discussion.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Single neuron application
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have learnt a lot of technical and mathematical stuff about single neurons,
    so I really want to walk you through the application on single neurons with respect
    to the word2vec model. So let's begin!
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to build a model that identifies the words that are edible and
    that are not, then we can use a single neuron to build the model. This kind of
    application, where we are segregating words either into edible classes or non-edible
    classes is called a **binary classification** **task**. For this kind of task,
    neurons are used to take one-hot encoded vectors as input and a single neuron
    will learn which words are related to edible items and which are not related to
    edible items. So, it will generate a look up table, which you can see in *Figure
    6.24*:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8781ddad-2f2e-4aea-9849-e68108aeb524.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.24: Single neuron can classify words into edible and non edible categories'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: The reason why this kind of application can be built with such ease is that
    when you are continuously providing the one-hot word vector and use the sigmoid
    or step functions as the activation function, then it becomes a standard classification
    problem, and this kind of problem can be solved easily by using mathematics. We
    have defined this in previous sections because for edible items, the output vector
    have the same kind of values, and for non-edible items generated vectors, they
    represent the same kind of output vector. In the end, we will be able to build
    the lookup table. This kind of standard classification problem reminds us of logistic
    regression, and we are applying the same logistic regression concepts, but here
    we are using a single neuron structure.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: We have seen enough about the single neuron structure, now it is time to explore
    the multi-layer neural network. Our next section will provide you with information
    about multilayer neural networks.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Multi-layer neural networks
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A multi-layer neural network is the structure that we are using for word2vec.
    This function takes input as a one-hot encoded word vector and this vector, as
    well as a weighted sum is passed to a hidden layer. By using the activation function,
    which is the sigmoid function in this case, output is generated from the hidden
    layer and this output is passed to the next layer, which is the output layer.
    We have already seen an example in this chapter, in the section entitled *Neural
    network with two layers*. In that section, we did not look at the mathematical
    aspect, so here I will walk you through the mathematical aspect of neural networks.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s represent what I told you in the preceding paragraph through a mathematical
    equation. See the structure of a multi-layer neural network in *Figure 6.25*:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c11fdd7-ce94-4e85-9743-d7a96834cd2a.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.25: Multi-layer neural network'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see the mathematical equations for the given neural network. Here,
    I''m going to provide you with high-level intuition, which will help you to understand
    the flow and you will get an idea about the input and output functions. Refer
    to *Figure 6.26*:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9c362c99-f5b6-4fa5-b1e6-875ac3b76707.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.26: Mathematical equation for multilayer neural networks'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'The flow of the input and output is given as follows:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: The first equation of *Figure 6.24* is a weighted sum the of input layer and
    the result of the input layer will be passed on to the hidden layer. *ui* is the
    weighted sum of the input layer. The activation function of the hidden layer is
    given in the second equation. The activation function *hi* uses the sigmoid function
    and generates the intermediate output.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weighted sum of the hidden layer will be passed to the output layer and
    the third equation shows the calculation of the hidden layer weighted sum. *u'j*
    is the weighted sum from the hidden layer and it will be passed to the output
    layer. *yj* uses the weighted sum from the hidden layer, which is *u'j* as well,
    as here also the activation function is sigmoid.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have seen the flow of input and output by using a basic mathematical representation.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Now, a major concern is how this structure is used to get trained for the word2vec
    model, and the answer to that is, we use backpropagation to train the model, which
    we will see in the next section.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation
  id: totrans-349
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already seen how errors have been calculated using the L2 loss function,
    and the L2 loss function wants to minimize the squared differences between the
    estimated and existing target values. We will apply the same concepts to the multi-layer
    neural network. So, we need to define the loss function as well as we to take
    the gradient of the function and update the weight of the neural network in order
    to generate a minimum error value. Here, our input and output is vectors.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to *Figure 6.27* to see the neural network structure, and *Figure 6.28*
    shows what equations we need to apply to calculate error functions in a multi-layer
    neural network:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95ea54bf-11b2-4027-b3de-19a570a01aa1.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.27: Multi-layer neural network for calculating error functions'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s see the derivation and mathematical calculation performed by neural
    networks. You can see the equations are given as follows, refer to *Figure 6.28*:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13b778d6-f544-46b4-8401-df6697c5e815.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.28: Equations for calculating error function for multi-layer neural
    networks'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: When you are calculating error function for multi-layer neural networks, you
    need to be very careful about the indexes, as well as which layer you are calculating
    an error function value for. As you can see in *Figure 6.28,* we will start with
    the output index, backpropagate the error to the hidden layer, and update the
    weight. In the fifth equation, you can see that we need to calculate the error
    function for the output layer so that backpropagate the error and we can update
    the weight of the input layer. To deal with indexes is a kind of a challenging
    task in multi-layer neural networks. But coding it up is quite easy because you
    just need to write a `for` loop to calculate each of the layer gradients.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will put all the mathematical equations and concepts of word2vec together
    and understand the final mathematical piece of word2vec neural networks.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: Mathematics behind the word2vec model
  id: totrans-359
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at the final piece of mathematics by combining
    all the previous equations and concepts, and we will derive the final equation
    in the form of probability. We have already seen the concept and basic intuition,
    calculation, and example in the previous section, *Word2vec neural network layers
    details*.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: The word2vec neural network is using a one-hot encoded word vector as input
    and then it passes this vector value to the next layer, which is the hidden layer,
    and this is nothing but the weighted sum values that feed into the hidden layer
    as input. The last output layer generates the vector value, but to make sense
    of the output, we will convert the vector into probability format, and with the
    help of softmax techniques, we will also convert the output word vector into probability
    format. We will see all the different techniques that are used to generate probability
    from the output vector in the upcoming section, until then, just use softmax as
    a magic function.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to *Figure 6.29* to understand the mathematics behind word2vec neural
    networks:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9eb90f9d-6eb6-4ec1-b6c2-07b13b3ad9e0.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.29: Mathematics behind word2vec model'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first equation, we can see the weighted sum of the input word vector
    and weights and get *h* in the second equation. We multiply *h* and the weighted
    sum of word vectors of the hidden layer *v''wj*. Here, weight and index has been
    changed. This multiplication is *uj* . Here, *uj* is the activation function.
    We will then generate probability by using the value of *uj* . So, the final equation
    is the softmax function. Let''s simplify the equation by replacing *uj* in the
    third equation with the input and output word vector format, then you will get
    the final equation. Refer to *Figure 6.30*:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f807cd33-cb1a-4344-a7c6-b0f8f21a41cf.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.30: Final probability equation of the word2vec model'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, our output is a softmax function, so for updating weight using backpropagation,
    we need to define the loss function. So, here, we will define the loss function
    in the form of the softmax function, so we will use minus log probability of the
    softmax function and then we will perform gradient descent. See *Figure 6.31*:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c97abf5f-5208-4dcf-988b-3722bd1de0ac.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.31: Error function gradient descent in form of minus log probability
    of the softmax function'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, I want to give some idea of how the output vector value has been updated.
    So, it is an updating rule for an output layer. You can find the equation given
    as follows. Refer to *Figure 6.32*:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a838de75-1d19-4140-ad57-d2ee51996bed.png)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.32: Rule for updating the output vector'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: In this equation, we are taking the original output vector and subtracting the
    prediction error *ej* of the output node and *h* is the value of the hidden layer.
    So, the meaning of this equation is that, if we have the word **climbed** as input,
    and we want to predict the word **cat** as output, then how we can update the
    vector value of the word **climbed** so that it will be more similar to the vector
    of the word **cat**? So in simple language we can say we will add some part of
    the vector of **climbed** to the vector of the word **cat** and apart from this,
    we also need to update the output vector of other words because all other words
    that are not our target words should update their output vector so that they will
    be less similar to the target words.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: 'The rule for updating the input vector is also useful; the equation of updating
    an input vector is given as follows. Refer to *Figure 6.33*:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06d1f10f-cf58-46c8-8604-e7f34ef5a9fd.png)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.33: Rule for updating input vector'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: This equation is a bit complex. So, here, intuition is the input vector, which
    will be subtracted from the weighted sum of the prediction errors. The meaning
    of this is that, this time, we are going to update the input vector **cat**. We
    will update the vector value of the word cat in such a manner that it will come
    close to the vector of the word **climbed**. Here, co-occurrence of the words
    play a major role.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: We are almost done with the math portion of the word2vec model. We have seen
    a lot of mathematical equations that are used in the word2vec model. Now we will
    talk about the techniques that are used to generate the final vectors and probability
    of the prediction.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: Techniques used to generate final vectors and probability prediction stage
  id: totrans-380
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will see how we will generate the final vector. We will
    also use some heuristics to generate output efficiently. So, we will talk about
    those heuristics as well.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have already seen, to generate the word vector we need to update input
    as well as the output vector. Suppose we have a million words in our vocabulary,
    so the process of updating the input and output vectors will take a lot of time
    and it will be inefficient. We have to solve this challenge. So we use some of
    the optimized ways to perform the same operations and those techniques are given
    as follows:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical softmax
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Negative sampling
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, let's start to understand these techniques.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical softmax
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In hierarchical softmax, instead of mapping each output vector to its corresponding
    word, we consider the output vector as a form of binary tree. Refer to the structure
    of hierarchical softmax in *Figure 6.34*:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98a86731-7bb0-4125-90d2-d12252c8907d.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.34: Hierarchical Softmax structure'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: 'So, here, the output vector is not making a prediction about how probable the
    word is, but it is making a prediction about which way you want to go in the binary
    tree. So, either you want to visit this branch or you want to visit the other
    branch. Refer to *Figure 6.35*:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2fccd091-58bd-422a-bfda-654878270ec9.png)'
  id: totrans-391
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.35: Prediction path using hierarchical softmax mantle representation'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: In this case, consider the red activated dot going up (light grey here) and
    the blue activated dot going downwards (dark grey here), so you can see that,
    here, we can predict the word **juice** with high probability.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: Here, the advantage is, if you want to backpropagate an error, then you just
    need to update one output vector and the error will propagate to only three nodes
    that are activated at the time of prediction. We use the Huffman binary tree construction
    to generate the binary tree.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: Negative sampling
  id: totrans-395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Negative sampling is also a kind of optimization method. In this method, we
    are going to update the vector of the output word, but we are not going to update
    all the vectors of other words. We just take the sample from the words other than
    the output vector. So, we are selecting a sample from the negative sample set
    of words, hence the name of this technique is negative sampling.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: Some of the facts related to word2vec
  id: totrans-397
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are some of the facts about the word2vec models that you should keep in
    mind when you are actually using it:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: So far, you will have realized that word2vec uses neural networks and this neural
    network is not a deep neural network. It only has two layers, but it works very
    well to find out the words similarity.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word2vec neural network uses a simple logistic activation function that does
    not use non-linear functions.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The activation function of the hidden layer is simply linear because it directly
    passes its weighted sum of inputs to the next layer.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we have seen almost all the major aspects of word2vec, so in the next section,
    we will look at the application of word2vec.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: Applications of word2vec
  id: totrans-403
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Allow me to introduce some real-life applications in which word2vec has been
    used. They are:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: Dependency parser uses word2vec to generate better and accurate dependency relationship
    between words at the time of parsing.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Name entity recognition can also use word2vec, as word2vec is very good at finding
    out similarity in **named entity recognition** (**NER**). All similar entities
    can come together and you will have better results.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis uses it to preserve semantic similarity in order to generate
    better sentiment results. Semantic similarity helps us to know which kind of phrases
    or words people use to express their opinions, and you can generate good insights
    and accuracy by using word2vec concepts in sentiment analysis.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can also build an application that predicts a person's name by using their
    writing style.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to do document classification with high accuracy and using simple
    statistics, then word2vec is for you. You can use the concept and categorize the
    documents without any human labels.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word clustering is the fundamental product of word2vec. All words carrying a
    similar meaning are clustered together.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google uses word2vec and deep learning to improve their machine translation
    product.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are so many use cases where you could use word2vec concepts. Here, we
    are going to implement some fun examples. We are going to build fun applications,
    as well as doing some visualization on them so you can understand the concept
    in a far better manner.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of simple examples
  id: totrans-413
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to implement the famous word2vec example, which
    is adding `woman` and `king` and subtracting `man`, and then the resultant vector
    shows the vector value of `queen`.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: 'We are not going to train the word2vec model, on our data and then build our
    own word2vec model because there is a huge amount of data on which Google has
    already trained their word2vec model and provided us with pre-trained models.
    Now, if you want to replicate the training process on that much data, then we
    need a lot of computational resources, so we will use pre-trained word2vec models
    from Google. You can download the pre-trained model from this link: [https://code.google.com/archive/p/Word2vec/](https://code.google.com/archive/p/Word2vec/).'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: After clicking on this link, you need to go to the section entitled pre-trained
    word and phrase vectors, download the model named `GoogleNews-vectors-negative300.bin.gz`,
    and extract it.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: We will use the `genism` library to build our famous example.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: Famous example (king - man + woman)
  id: totrans-418
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to load the binary model by using the `gensim` library and replicate
    the example. If you are running it on your computer, then it will take a few minutes,
    so don''t worry and keep the script running. Refer to *Figure 6.36*, for the code
    snippet:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03d8ba9f-98d8-416c-ae56-1706cd536ff0.png)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.36: Code snippet for example King - man + woman = queen'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the code by clicking on this GitHub link: [https://github.com/jalajthanaki/NLPython/blob/master/ch6/kingqueenexample.py](https://github.com/jalajthanaki/NLPython/blob/master/ch6/kingqueenexample.py)'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to *Figure 6.37* for the output we are generating:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/454a681c-4042-41be-9c43-6d9afd811c74.png)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.37: Output of the example King - man + woman = queen'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if you want to train the model from scratch by using data that is provided
    by Google, then download the training dataset by using the following link: [https://code.google.com/archive/p/Word2vec/](https://code.google.com/archive/p/Word2vec/).
    Go to the section entitled *Where to obtain the training data* and download all
    training datasets, then, by taking a reference from the given GitHub link [https://github.com/LasseRegin/gensim-Word2vec-model](https://github.com/LasseRegin/gensim-Word2vec-model),
    you can replicate the whole training process, but it will take a lot of time because
    this kind of training needs a lot of computational power.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of word2vec
  id: totrans-427
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have seen, word2vec is a very good technique for generating distributional
    similarity. There are other advantages of it as well, which I''ve listed here:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: Word2vec concepts are really easy to understand. They are not so complex that
    you really don't know what is happening behind the scenes.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using word2vec is simple and it has very powerful architecture. It is fast to
    train compared to other techniques.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human effort for training is really minimal because, here, human tagged data
    is not needed.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This technique works for both a small amount of datasets and a large amount
    of datasets. So it is an easy-to-scale model.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you understand the concept and algorithms, then you can replicate the whole
    concept and algorithms on your dataset as well.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It does exceptionally well on capturing semantic similarity.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As this is a kind of unsupervised approach, human effort is very minimal, so
    it is a time-saving method.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges of word2vec
  id: totrans-436
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although the word2vec concept is very efficient, there are some points that
    you may find complex or challenging. Here, I will propose the most common challenges.
    Those points are listed as follows:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: The word2vec model is easy to develop, but difficult to debug, so debug ability
    is one of the major challenges when you are developing a word2vec model for your
    dataset.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It does not handle ambiguities. So, if a word has multiple meanings, and in
    the real world we can find many of these kinds of words, then in that case, embedding
    will reflect the average of these senses in vector space.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How is word2vec used in real-life applications?
  id: totrans-440
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will give you an idea of which kinds of NLP applications use word2vec
    and how NLP applications use this concept. Apart from that, I will also discuss
    some of the most frequently-asked questions across the community in order for
    you to have a clear insight of word2vec when you try it out in real life.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: NLP applications such as document classification, sentiment analysis, and so
    on can use word2vec techniques. Especially in document classification, word2vec
    implementation gives you more good results, as it preserves semantic similarity.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: For sentiment analysis, we can apply word2vec, which gives you an idea about
    how words are spread across the dataset, and then you can use customized parameters
    such as context window size, subsampling, and so on. You should first generate
    **bag of words** (**BOW**) and then start to train word2vec on that BOW and generate
    word vectors. These vectors can be fed as input features for the ML algorithm,
    then generate sentiment analysis output.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: Now, it's time to discuss some of the questions that people usually ask when
    they are trying to understand and use word2vec techniques on their own dataset.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's fire up the questions!
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '**What kind of corpus do we need?**: Word2vec techniques can be applied on
    text datasets. As such, there is not any specific kind of text data that you cannot
    use. So, as per my view, you can apply word2vec on any dataset.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Should I always remove stop words?**: In original models of word2vec that
    were from Google, remove some of the stop words, such as **a** has been removed
    in word2vec, but the word **the** has not been removed. So it is not mandatory
    that you remove the words:'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is totally dependent on your NLP application. If you are developing a sentiment
    analysis application, then you can remove all stop words, but if you are developing
    machine translation applications, then you should remove some of the stop words;
    not all. If you are using word2vec for developing word clusters to understand
    the grammar of the language, then you should not remove any of the words.
  id: totrans-448
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Should I remove all stop words?**: This question is related to the previous
    question. The straightforward answer to this question is no. It is not compulsory
    that you should remove all stop words blindly for every NLP application. Each
    and every NLP application is different, so you should take a decision based on
    the NLP application that you are trying to build:'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you look at the Google original word2vec model, then you will see that in
    that model the word **a** is not there, which means a vector that represents the
    word **a** is not present, but a vector for the word **the** is there.
  id: totrans-450
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We will load the original Google word2vec model and, using simple lines of code,
    we will look at some of the facts regarding stop words.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to the code snippet in *Figure 6.38*:'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/ca5ab527-617d-4bd8-bd77-86f99f20ae63.png)'
  id: totrans-453
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.38: Code snippet that shows the fact of stop words'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: 'For the output that is the vector value of `the`, refer to *Figure 6.39*:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc1bd72a-bcd4-4373-9e52-65f0c300ced5.png)'
  id: totrans-456
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.39: Sample values of word vector for the word the'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: 'See the output, where you can see that word2vec doesn''t contain `a` in its
    vocabulary in *Figure 6.40*:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae68391b-9340-46cf-bdd9-0a5fd97f9a03.png)'
  id: totrans-459
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.40: Word2vec doesn''t contain the word a'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: '**Don''t you think that, here, we have generated two vectors for each word?**:
    I would like to let you know that we have generated two vectors for each word.
    The reason behind this is that word in a sentence is coming on a target word as
    well as a context word, so when a word appears as a target word, we will generate
    vectors, and when a word appears as a context word, then we also generate vectors.
    We consider the target word vector in our final output, but yes, you can use both
    vectors. How to use the two vectors and generate making sense out of that is kind
    of a million dollar question!'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When should you use word2vec?
  id: totrans-462
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word2vec captures semantic similarity; this is the most important point that
    we need to keep in mind when we are processing the answer to the preceding question.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: If you have an NLP application in which you want to use the distributional semantic,
    then word2vec is for you! Some NLP applications will use this concept to generate
    the features and the output vector from the word2vec model, or similarly, vectors
    will be used as input features for the ML algorithm.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: You should know which NLP applications can use word2vec. If you know the list
    of applications, it becomes easy for you to decide whether you should use it or
    not. Suppose you can use k-mean clustering for document classification; if you
    want document classification to carry some of the attributes of semantics, then
    you can use word2vec as well. If you want to build a question-answer system, then
    you will need techniques that differentiate questions on a semantic level. As
    we need some semantic level information, we can use word2vec.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have seen enough about the concepts and theories, so we will begin our
    favorite part, which is coding, and this time it is really fun.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: Developing something interesting
  id: totrans-467
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we are going to train our word2vec model. The dataset that I'm going to
    use is text data of *Game of Thrones*. So, our formal goal is to develop word2vec
    to explore semantic similarities between the entities of *A Song of Ice and Fire
    (from the show Game of Thrones)*. The good part is we are also doing visualization
    on top of that, to get a better understanding of the concept practically. The
    original code credit goes to Yuriy Guts. I have just created a code wrapper for
    better understanding.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: 'I have used IPython notebook. Basic dependencies are `gensim`, `scikit-learn`,
    and `nltk` to train the word2vec model on the text data of Game of Thrones. You
    can find the code on this GitHub link:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch6/gameofthrones2vec/gameofthrones2vec.ipynb.](https://github.com/jalajthanaki/NLPython/blob/master/ch6/gameofthrones2vec/gameofthrones2vec.ipynb)'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: The code contains inline comments and you can see the snippet of the output.
    We have used the t-SNE technique to reduce the dimensions of the word vector,
    so we can use the two-dimensional vector for visualization. The t-SNE technique
    takes a lot of time if you want to run on a normal computer with 2 to 4 GB RAM.
    So, you need more RAM to run t-SNE code successfully at your end and you can skip
    the visualization part if you have memory constraints. You can see the visualization
    images. Once you have saved the model on to disk, you can use it and generate
    output easily. I have given sample output in *Figures 6.41* to *Figure 6.45*.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: 'You may observe the output for the word `Stark`here:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/05b7c3fb-7cec-436a-a67d-d7dda024dec2.png)'
  id: totrans-473
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.41: Word similarity output for the word stark'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the output for the nearest words:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96971b2d-b210-4ef6-81c4-b1bc30f7c0f8.png)'
  id: totrans-476
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.42: Output for the nearest words'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will see the following figures for output of visualization:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18c6afd3-33d5-4c41-a2eb-0c1430939c6b.png)'
  id: totrans-479
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.43: After using t-SNE you can visualize vectors in 2-D space'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: Now we will zoom in and try to see which words have ended up together.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: 'See the following figure, which shows people related to Kingsguard ending up
    together:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df02963d-e682-4191-9622-d0140e0dce9e.png)'
  id: totrans-483
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.44: People names grouped together'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: 'See the following figure, which shows food products grouped together nicely:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d64d433-19ce-4ed0-8cb7-62fbc3b423d9.png)'
  id: totrans-486
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.45: Name of food items grouped together'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  id: totrans-488
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I''m a big fan of Harry Potter, and so, in this exercise, you need to generate
    Word2vec from the text data from a Harry Potter book. Don''t worry about the dataset;
    I have already provided it for you and it resides on this GitHub link: [https://github.com/jalajthanaki/NLPython/tree/master/ch6/Harrypotter2vec/HPdataset](https://github.com/jalajthanaki/NLPython/tree/master/ch6/Harrypotter2vec/HPdataset)'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: Good luck with generating HarryPotter2Vec! Happy Coding!
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: Extension of the word2vec concept
  id: totrans-491
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The word2vec concept can be extended to different levels of text. This concept
    can be applied on the paragraph level or on the document level, and apart from
    this, you can also generate the global vector, which is called **GloVe**. We will
    try to understand them. Here, we are going to get an overview of each of the concepts.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the following extended concepts built by using the word2vec concept:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: Para2vec
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doc2vec
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GloVe
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Para2Vec
  id: totrans-497
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Para2vec stands for paragraph vector. The paragraph vector is an unsupervised
    algorithm that uses fixed-length feature representation. It derives this feature
    representation from variable-length pieces of texts such as sentences, paragraphs,
    and documents.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: Para2vec can be derived by using the neural network. Most of the aspects are
    the same as Word2vec. Usually, three context words are considered and fed into
    the neural network. The neural network then tries to predict the fourth context
    word. Here, we are trying to maximize the log probability and the prediction task
    is typically performed via a multi-class classifier. The function we use is softmax.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: Please note that, here, the contexts are fixed-length and generate the context
    words by using a sliding window over the paragraph. The paragraph vector is shared
    with all contexts generated from the same paragraph, but not across paragraphs.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of Para2vec is to learn to predict the words from unlabeled data
    so that you can use these techniques when you don't have enough labeled datasets.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: Doc2Vec
  id: totrans-502
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Doc2vec** (**Document vectors**) is an extension of word2vec. It learns to
    correlate document labels and words, rather than words with other words. Here,
    you need document tags. You are able to represent an entire sentence using a fixed-length
    vector. This is also using word2vec concepts. If you feed the sentences with labels
    into the neural network, then it performs classification on a given dataset. So,
    in short, you tag your text and then use this tagged dataset as input and apply
    the Doc2vec technique on that given dataset. This algorithm will generate tag
    vectors for the given text. You can find the code at this GitHub link: [https://github.com/jalajthanaki/NLPython/blob/master/ch6/doc2vecexample.py](https://github.com/jalajthanaki/NLPython/blob/master/ch6/doc2vecexample.py)'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: 'I have used very small datasets to just give you intuition on how you can develop
    Doc2vec, so I''m ignoring the accuracy factor of the developed model. You can
    refer to the code given at this reference link: [https://github.com/jhlau/doc2vec](https://github.com/jhlau/doc2vec)'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the intuitive code in *Figure 6.46* and see the output snippet in
    *Figure 6.47*:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0277849e-707b-47fd-9eec-ccc13dba224b.png)'
  id: totrans-506
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.47: Code snippet of doc2vec'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: 'You may see the following output:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/864bb3b8-8e0f-40ff-a903-1c4ce4e0f9c3.png)'
  id: totrans-509
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.48: Sample output'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: Applications of Doc2vec
  id: totrans-511
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see the applications that can use Doc2vec:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: Document clustering can be easily implemented by using Doc2vec
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can perform sentiment analysis on larger chunks of text data, and I suppose
    you could consider a very big chunk of text and generate the sentiment output
    for that large chunk
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is also used in product recommendation
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GloVe
  id: totrans-516
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GloVe stands for global vector. GloVe is an unsupervised learning algorithm.
    This algorithm generates vector representations for words. Here, training is performed
    by using an aggregated global word-word co-occurrence matrix and other statistics
    from a corpus, and the resulting representations give you interesting linear substructures
    of the word vector space. So the co-occurrence matrix is the input of GloVe.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: 'GloVe uses cosine similarity or the Euclidean distance to get an idea of similar
    words. Glove gives you fresh aspects and proves that if you take the nearest neighbor,
    then you can see such kinds of words that are very rare in terms of their frequent
    usage. GloVe can still capture those rare words in similar clusters. Let''s look
    at the a famous example:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, here are the closest words when we have the target word frog:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: Frog
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frogs
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Toad
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Litoria
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leptodactylidae
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rana
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lizard
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eleutherodactylus
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another example is the words related to the comparative-superlative form clustered
    together, and you can see the following output if you use the visualization tool.
    See *Figure 6.48*:'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34816d1b-7ecb-4a29-af52-0e6d17b67f4c.png)'
  id: totrans-529
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.48: Result of GloVe famous example'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: 'I''m using the `GloVe` Python library to give you an intuitive practical example
    of GloVe. See the code given at the following GitHub link: [https://github.com/jalajthanaki/NLPython/blob/master/ch6/gloveexample.py](https://github.com/jalajthanaki/NLPython/blob/master/ch6/gloveexample.py)'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start, we need to download the dataset, so execute the following
    command:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-533
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can see the snippet of code in *Figure 6.49* and see the output in *Figure
    6.50*:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5789625-456b-47e2-82bc-ae3d9990f95b.png)'
  id: totrans-535
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.49: GloVe code snippet'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the output of the preceding code snippet:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41328e38-e1ee-47b4-98af-9d1b828a1494.png)'
  id: totrans-538
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.50: Sample output of GloVe'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  id: totrans-540
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This exercise is more of a reading exercise for you. You should read the research
    papers on Para2vec, Doc2vec, and GloVe. Apart from this, you can also check whether
    there is any way that you can find vector representation for continuous strings,
    such as a DNA pattern. The main purpose of this exercise is to give you an idea
    of how research work has been done. You can also think of some other aspects of
    vector representation and try to solve the challenges.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: Importance of vectorization in deep learning
  id: totrans-542
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is more of a discussion with you from my end. As we all know, computers
    can't understand NL directly, so we need to convert our NL output into numerical
    format. We have various word embedding techniques, as well as some basic statistical
    techniques such as indexing, tf-idf, one-hot encoding, and so on. By using all
    these techniques, or some of these techniques, you can convert your text input
    into numerical format. Which techniques you choose totally depends on the NLP
    applications. So, there are two major points behind why we convert NL input to
    numerical format. It is basically done because the computer can only understand
    numerical data, so we have to convert text data to numerical data and computers
    are very good at performing computation on given numerical data. These are two
    major points that come to my mind when we are converting text data.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand what deep learning is. Here, I want to give you just a brief
    idea about it. Don't worry; we will see more detail in [Chapter 9](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml),
    *Deep Learning for NLU and NLG Problems.* When a neural network is many layers
    deep, it is called **deep neural network**. When we use many-layered deep neural
    networks and use them to develop NLP applications using lots of data and lots
    of computation power, it is called **deep learning**.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: Now let's talk about vectorization. Vectorization is a solid mathematical concept
    and it is easy to understand and deal with. Nowadays, Python has a lot of good
    libraries that make our life easier when we want to deal with high dimensional
    vector forms of data. The deep learning paradigm heavily relies on the vectorization
    and matrices concepts, so in order to get a good grip on deep learning, you should
    have knowledge of vectors and matrices. Deep learning applications that deal with
    input data such as video or audio also use vectors. Videos and images are converted
    into the dense vector format, and when talk about text input, word2vec is its
    basic building block for generating vectors from words. Google TensorFlow uses
    word2vec as their basic building block and it uses these concepts and improvises
    the results of Google Machine translation, Google Speech recognition, and Google
    Vision applications. So, vectors and matrices give us a lot of freedom in terms
    of their processing and making sense out of it.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: Apart from this, I also need to give you some thoughts. I want you to focus
    on how we can improvise the way we deal with text. No doubt word2vec is one of
    the most simple and efficient approaches for converting words into vector form,
    but I would definitely encourage my readers who are interested in research work
    to extend this concept for their native languages or become creative and contribute
    to building very innovative techniques that will help the NLP community to overcome
    challenges such as word ambiguities. Well, these are all my thoughts for you!
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-547
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have seen how word2vec can be used to find semantics. The
    simple vectorization techniques help us a lot. We have seen some of the applications
    of it. We have touched upon the technicalities of the word2vec model. I have introduced
    lots of new mathematical, as well as statistical, terms to you in order to give
    you a better understanding of the model. We have converted the word2vec black
    box into the word2vec white box. I have also implemented basic as well as extended
    examples for better understanding. We have used a ton of libraries and APIs to
    develop word2vec models. We have also seen the advantages of having vectorization
    in deep learning. Then, we extended our word2vec understanding and developed the
    concepts of para2vec, doc2vec, and GloVe.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will basically give you an in-depth idea of how rule-based
    techniques are used in order to develop NLP applications and how various NLP applications
    use a very simple, but yet very effective, technique called rules or logic for
    developing basic and effective prototypes for NLP applications. Google use the
    rule-based techniques for their machine translation projects, Apple also use this
    technique, and last but not least, Google used the rule-based system to make an
    early prototype of their self-driving car. We will discuss the rule-based system
    and its architecture. We will also see what the architecture of rule-based NLP
    applications is. I will provide you with a thought process, and by using that
    thought process, you can also make rules for your NLP application. We will implement
    the basic grammar rules and pattern-based rules. We will also develop a basic
    template-based chatbot from scratch.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
