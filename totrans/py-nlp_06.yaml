- en: Advanced Feature Engineering and NLP Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will look at an amazing and simple concept called **word
    to vector** (**word2vec**). This concept was developed by a team of researchers
    led by Tomas Mikolov at Google. As we all know, Google provides us with a lot
    of great products and concepts. Word2vec is one of them. In NLP, developing tools
    or techniques that can deal with the semantics of words, phrases, sentences, and
    so on are quite a big deal, and the word2vec model does a great job of figuring
    out the semantics of words, phrases, sentences, paragraphs, and documents. We
    are going to jump into this vectorization world and live our life in it for a
    while. Don't you think this is quite amazing? We will be starting from the concepts
    and we will end with some fun and practical examples. So, let's begin.
  prefs: []
  type: TYPE_NORMAL
- en: Recall word embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already covered word embedding in [Chapter 5](07f71ca1-6c8a-492d-beb3-a47996e93f04.xhtml),
    *Feature Engineering and NLP Algorithms*. We have looked at language models and
    feature engineering techniques in NLP, where words or phrases from the vocabulary
    are mapped to vectors of real numbers. The techniques used to convert words into
    real numbers are called **word embedding**. We have been using vectorization,
    as well as **term frequency-inverse document frequency** (**tf-idf**) based vectorization.
    So, let's just jump into the world of word2vec.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the basics of word2vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we will try to handle semantics at word level by using word2vec. Then,
    we will expand our concepts to paragraph level and document level. By looking
    at *Figure 6.1*, you will see the different kinds of semantics that we are going
    to cover in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87ce8e19-40b7-4644-ba45-0293e84d7887.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: Different kinds of semantics'
  prefs: []
  type: TYPE_NORMAL
- en: Semantics is a branch that deals with meaning in the area of NLP. We have already
    covered lexical semantic in [Chapter 3](f65e61fc-1d20-434f-b606-f36cf401fc41.xhtml),
    *Understanding Structure of Sentences*. So, here we will discuss more about distributional
    semantics. There are also other techniques or types in semantics, such as formal
    semantics compositional semantics; but right now, in this book, we are not going
    to cover these types or techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Distributional semantics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Distributional semantics is a research area that focuses on developing techniques
    or theories that quantify and categorize semantic similarities between linguistic
    items based on their distributional properties in large samples of text data.
  prefs: []
  type: TYPE_NORMAL
- en: I want to give an example here that gives you an idea of what I mean by distributional
    semantics. Suppose you have text data of travelling blogs. Now, you as a person
    know that pasta, noodles, burgers, and so on are edible food items, whereas juice,
    tea, coffee, and so on are drinkable items. As a human, we can easily classify
    drinkable and edible food items because we have a certain context related with
    each of them, but machines cannot really know these kind of semantics. There is
    a higher chance that all described food items come along with certain words in
    the dataset. So, here we are focusing on the distribution of words in corpus and,
    let's say, that linguistic items or words with similar distributions have similar
    meanings. This is called the **distributional hypothesis**.
  prefs: []
  type: TYPE_NORMAL
- en: 'I will give you another example. Suppose you have a dataset of research papers.
    Some of the research papers in the dataset belong to the engineering category,
    and others belong to the legal category. Documents with words such as engineering,
    equation, methods, and so on are related to engineering, so they should be part
    of one group, and words such as legal, lawyer, law institutes, and so on are related
    to research papers of the legal domain, so they should be grouped together. By
    using distributional semantics techniques such as word2vec, we can segregate the
    different domain words by using their vector values. All words with a similar
    meaning are grouped together because they have a similar distribution on the corpus.
    You can refer to *Figure 6.2*, which shows a pictorial representation of a vector
    space of our given distributional semantics example where similar contextual words
    come together:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f4f3862-fed5-4b60-b815-ca1d46516406.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: Pictorial representation of vector space of our distributional
    semantics example'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6.3*, gives you an idea about from which branch the word2vec model
    was derived:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2dc79cdc-1778-4734-8495-8d70f8f7487b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3: Major models derived from distributional semantics'
  prefs: []
  type: TYPE_NORMAL
- en: Our main concentration in this chapter is the distributional semantics technique
    called **word2vec**.
  prefs: []
  type: TYPE_NORMAL
- en: Defining word2vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word2vec is developed by using two-layer neural networks. It takes a large amount
    of text data or text corpus as input and generates a set of vectors from the given
    text.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we can say that it generates high-dimensional vector space.
    This vector space has several hundred dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Please don't be afraid of the high dimensionality. I make the whole concept
    of word2vec simple for you during this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: You will really want to know what I mean here when I say that the word2vec model
    generates a set of vectors or vector space from text. Here, we are using a two-layer
    neural network, which right now a black box that performs some kind of logic and
    generates vectors in the vector space for us. In the vector space, each unique
    word in the corpus is assigned a corresponding vector. So, the vector space is
    just a vector representation of all words present in the large text corpus.
  prefs: []
  type: TYPE_NORMAL
- en: So, I think you get it, right? On the basics of what we have learnt, you are
    able to say that word2vec is one of the models that generates word embedding.
    Please recall the vectorization section of [Chapter 5](07f71ca1-6c8a-492d-beb3-a47996e93f04.xhtml),
    *Feature Engineering and NLP Algorithms*. I also want to make a point here, by
    saying that word2vec is a powerful, unsupervised word embedding technique.
  prefs: []
  type: TYPE_NORMAL
- en: Necessity of unsupervised distribution semantic model - word2vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section gives us an idea of the numerous challenges that word2vec solves
    for us. The solution of those challenges leads us to the real need for word2vec.
    So, first we will look at some challenges, and then take a look at how the word2vec
    model solves those challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a couple of challenges that are listed here that we are trying to
    solve:'
  prefs: []
  type: TYPE_NORMAL
- en: When we are developing an NLP application, there is one fundamental problem--we
    know that the machine can't understand our text and we need to convert the text
    data into a numerical format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are certain ways to convert text data into a numerical format, but we
    apply some naive techniques, and one of them is one-hot encoding, but the problems
    with this techniques are given as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Suppose you have a sentence: I like apple juice. Now, suppose that you apply
    one-hot encoding for each of the words in the sentence. If you have thousands
    of sentences in your corpus, then vector dimension is equal to the entire vocabulary
    of your corpus, and if these kind of high dimensional columns have been used to
    develop an NLP application, then we need high computation power and matrix operation
    on these high-dimension columns as they take too much time.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For speech recognition vocabulary, size is on average 20,000 words. If we are
    developing a machine translation system then perhaps we will use more vocabulary,
    like 500,000 words. To deal with these kinds of gigantic vectors is a big challenge.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another problem is that when you apply one-hot encoding on a particular word
    in a sentence, then the whole entry has zero values, except the one that actually
    represents the word, and that value is **1**. Suppose, for simplicity, we take
    the sentence: *I like apple juice*. For a while consider that there is only one
    sentence in our corpus. Now, if I try to apply one hot encoding on the word **apple**
    then one-hot representation of **apple**, is given as follows. Refer to *Figure
    6.4*:![](img/a8438127-5658-46e1-880e-eb3b7094488d.png)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 6.4:One-hot encoding representation of the words apple and juice
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding doesn't reveal the facts about context similarity between words.
    To understand this, I want to give an example, if your corpus has words cat and
    cats then one-hot encoding does not reveal the fact that word cat and cats are
    very similar words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If I apply an AND operation on the one-hot encoded vectors, then it will not
    express any contextual similarity. Take an example, if I apply an AND operation
    means a dot product on the one-hot vectors of **Apple** and **juice**,then the
    answer is **0**. In reality, these words can appear together and have a strong
    contextual relationship as well, but one-hot encoding alone does not express anything
    significant about word similarity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to find accurate word similarities, then WordNet will not help you
    enough. WordNet is made by experts and whatever WordNet contains is more subjective
    because human users created it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using WordNet takes a lot of time and effort.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some new words, such as Coding Ninja, Wizard, and so on are new words for WordNet
    and may not be present on the website. Because of the absence of these kinds of
    words, we cannot derive the other semantic relationships from WordNet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of the preceding challenges has played a major role in the development
    of the techniques to solve them. In the last two decades, there has been a lot
    of effort put into developing an efficient, concise, and relevant numerical representation
    of words. Finally, in 2013, Tomas Mikolov and his research team at Google came
    up with the word2vec model, which solves many of the previous challenges in an
    efficient way.
  prefs: []
  type: TYPE_NORMAL
- en: Word2vec is very good at finding out word similarity, as well as preserving
    the semantic relationship between words that couldn't be handled by previous techniques,
    such as one-hot encoding or by using WordNet.
  prefs: []
  type: TYPE_NORMAL
- en: I have given so much background on word2vec now, so let's start understanding
    the representation, components, and other parameters of the word2vec model.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin our magical journey!
  prefs: []
  type: TYPE_NORMAL
- en: Converting the word2vec model from black box to white box
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From this section onwards, we are going to get an understanding of each of the
    components of the word2vec model, as well as the model's working process. So,
    in short, we are converting the black box part of word2vec into a white box.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will focus on the following procedures in order to understand the word2vec
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: Distributional similarity based representation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the components of the word2vec model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the logic of the word2vec model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the algorithms and math behind the word2vec model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of the facts regarding the word2vec model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application of the word2vec model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: Distributional similarity based representation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is quite an old and powerful idea in NLP. The notion of distributional
    similarity is that you can get a lot of value for representing the meaning of
    a word by considering the context in which that particular word appears, and it
    is highly related with that context. There is a very famous quote by a famous
    linguist John Firth:'
  prefs: []
  type: TYPE_NORMAL
- en: '"You shall know the word by the company it keeps."'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take an example: if I want to find the meaning of the word banking,
    I am going to collect thousands of sentences in which the word banking is included,
    and then I will start looking at the other words with the word banking and try
    to understand the context in which the word banking is used. So, look at these
    examples and understand the distributional similarity:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentence 1: The banking sector is regulated by the government'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sentence 2: Banking institutions need some technology to change their traditional
    operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous sentences, the word banking is included more frequently with
    words such as government, department, operations, and so on. All these words are
    very useful to understand the context and meaning of the word banking.
  prefs: []
  type: TYPE_NORMAL
- en: These other words are really helpful to represent the meaning of the word banking.
    You can also use the word banking to predict the most common frequently occurring
    words or phrases when the word banking is present in a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: To understand how we can better represent the meaning of a particular word,
    as well as performing predictions about other words appearing in the context of
    this given word, we need to understand the distributional representation of the
    word in question.
  prefs: []
  type: TYPE_NORMAL
- en: The distributional representation of a word is a vector form in which the word
    can be represented. Words are expressed in the form of a dense vector, and the
    dense vector has to be chosen so that it will be good at predicting other words
    that appear in the context of this word. Now, each of those other words that we
    are making predictions about also have other words attached to them, so we use
    a similarity measure, such as the vector dot product. This is a kind of recursive
    approach, where each word will predict the other words that can appear in the
    same context, and other predicted words also perform the same operation by predicting
    some other words. So, we need a clever algorithm to perform this kind of recursive
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: Here, please do not get confused between the terminology of distributional similarity
    and distribution representation. Distributional similarity based representation
    is actually used as part of the theory of semantics, which helps us to understand
    the meaning of the word in regular life usage; whereas the distributional representation
    of a word is the representation of a word in a vector form. To generate the vector
    form of a word, we can use one hot encoding or any other techniques, but the major
    point here is to generate the vector for a word that also carries the significance
    of similarity measure so that you can understand the contextual meaning of the
    word. Word2vec comes into the picture when we talk about distributional similarity.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the components of the word2vec model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will get an understanding of the main three components
    of the word2vec model, which are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Input of word2vec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output of word2vec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construction components of the word2vec model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Input of the word2vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First of all, we should be aware of our input for developing the word2vec model,
    because that is a fundamental thing, from which you can start building word2vec.
  prefs: []
  type: TYPE_NORMAL
- en: So, I want to state that we will use a raw text corpus as an input for developing
    the word2vec model.
  prefs: []
  type: TYPE_NORMAL
- en: In real-life applications, we use large corpora as input. For simplicity, we
    will use a fairly small corpus to understand the concepts in this chapter. In
    later parts of this chapter, we will use a big corpus to develop some cool stuff
    by using word2vec model concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Output of word2vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section is very important for your understanding because, after this point,
    whatever you understand will be just to achieve the output that you have set here.
    So, so far, we know that we want to develop the vector representation of a word
    that carries the meaning of the word, as well as to express the distribution similarity
    measure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, I will jump towards defining our goal and output. We want to define a
    model that aims to predict a central word and words that appear in its context.
    So, we can say that we want to predict the probability of the context given the
    word. Here, we are setting up the simple prediction objective. You can understand
    this goal by referring to the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a5a1a02-fa68-4fe9-8eee-24a11697799f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.5: Help to understand our goal'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there are some simple example sentences given in the preceding
    figure. If we take the word **apple** from the first sentence and, as per our
    goal, convert the word **apple** into a vector form such that, by using that vector
    form of **apple**, we can predict the probability of the word **eat** appearing
    in the context of the word **apple**. The same logic applies to the other sentences.
    For example, in the third sentence, where we try to find out the vector of the
    word **scooter**, which helps us to predict the probability of words such as **driving**
    and **work** in the context of the given word **scooter**.
  prefs: []
  type: TYPE_NORMAL
- en: So, in general, our straightforward goal is that we need to convert every word
    into vector format, such that they are good at predicting the words that appear
    in their context, and by giving the context, we can predict the probability of
    the word that is best suited for the given context.
  prefs: []
  type: TYPE_NORMAL
- en: Construction components of the word2vec model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We know our input and output so far, so now you''re probably thinking: how
    can we achieve our goal by using our input. As I mentioned, we need a clever algorithm
    that will help us to achieve our goal. Researchers have done the work for us and
    concluded that we can use neural network techniques. I would like to give you
    just a brief idea of why we are going to use neural network, but if you want a
    deep insight of it, then I would encourage you to read some of these papers.'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html.](http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://s3.amazonaws.com/academia.edu.documents/44666285/jhbs.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1497377031&Signature=CtXl5qOa4OpzF%2BAcergNU%2F6dUAU%3D&response-content-disposition=inline%3B%20filename%3DPhysio_logical_circuits_The_intellectua.pdf](http://s3.amazonaws.com/academia.edu.documents/44666285/jhbs.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1497377031&Signature=CtXl5qOa4OpzF%2BAcergNU%2F6dUAU%3D&response-content-disposition=inline%3B%20filename%3DPhysio_logical_circuits_The_intellectua.pdf)[.](http://s3.amazonaws.com/academia.edu.documents/44666285/jhbs.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1497377031&Signature=CtXl5qOa4OpzF%2BAcergNU%2F6dUAU%3D&response-content-disposition=inline%3B%20filename%3DPhysio_logical_circuits_The_intellectua.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf.](http://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason why we are using a neural network technique is because neural networks
    are good algorithms when we are trying to learn from a large amount of data. If
    you want to build a simple, scalable, and easy to train model then a neural network
    is one of the best approaches. If you read the modern research papers that I have
    listed as follows, they will tell you the same truth. Links to the papers mentioned
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://web2.cs.columbia.edu/~blei/seminar/2016_discrete_data/readings/MikolovSutskeverChenCorradoDean2013.pdf.](http://web2.cs.columbia.edu/~blei/seminar/2016_discrete_data/readings/MikolovSutskeverChenCorradoDean2013.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://arxiv.org/abs/1301.3781.](https://arxiv.org/abs/1301.3781)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This kind of neural network creates magic in terms of generating distributional
    similarity. Google generated the word2vec model by using the large corpus of Wikipedia.
    Refer to *Figure 6.6*, which gives you an overview of the input, and some famous
    output from the Google word2vec model. For us, the word2vec model is still a powerful
    black box that generates some great results. See the black box representation
    of word2vec in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d942cb9e-cdf8-469d-b4c4-2080447ba467.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.6: Google word2vec model takes Wikipedia text as input and generates
    output'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding image shows that we have provided text data as input to the word2vec
    model. The word2vec model converts our text data to the vector form so that we
    can perform mathematical operations on this vector representation of words. The
    most famous example of word2vec is: if you have the vectors of king, man, and
    woman. Then, if you apply the mathematical operation subtracting the vector value
    of man from the king vector and add the vector value of the word woman to it,
    then we will get a resultant vector that represents the same vector value of the
    word queen. Here''s the mathematical representation of this example: *king - man
    + woman = queen*.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we need to focus on the overview of the architectural component for word2vec.
  prefs: []
  type: TYPE_NORMAL
- en: Architectural component
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's look at the architectural components involved in building a word2vec model.
  prefs: []
  type: TYPE_NORMAL
- en: The major architectural component for the word2vec model is its neural network.
    The neural network for a word2vec model has two layers, and in that sense, it
    is not a deep neural network. The fact is that word2vec doesn't use deep neural
    networks to generate vector forms of words.
  prefs: []
  type: TYPE_NORMAL
- en: This is one of the critical and main components of the word2vec model and we
    need to decode its functionality to get a clear idea about how word2vec works.
    Now it's time to decode the magical logic of the word2vec model.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the logic of the word2vec model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will start decomposing the word2vec model and try to understand the logic
    of it. word2vec is a piece of software and it uses a bunch of algorithms. Refer
    to *Figure 6.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d49e251d-2ab4-4663-b447-9434f97416b2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.7: Word2vec building block (Image credit: Xin Rong)'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in *Figure 6.7*, there are three main building blocks. We will
    examine each of them in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: Vocabulary builder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context builder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural network with two layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vocabulary builder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The vocabulary builder is the first building block of the word2vec model. It
    takes raw text data, mostly in the form of sentences. The vocabulary builder is
    used to build vocabulary from your given text corpus. It will collect all the
    unique words from your corpus and build the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, there is a library called `gensim`. We will use `gensim` to generate
    word2vec for our corpus. There are some parameters available in `gensim` that
    we can use to build vocabulary from our corpus as per your application needs.
    The parameter list is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`min_count`: This parameter is used as a threshold value. This ignores all
    words with a total frequency of lower than the specified value. So, for example,
    if you set `min_count = 5`, then the output of the vocabulary builder doesn''t
    contain words that occur less than five times. The vocabulary builder output contains
    only words that appeared in the corpus more than or equal to five times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`build_vocab(sentences`, `keep_raw_vocab=False`, `trim_rule=None`, `progress_per=10000`,
    `update=False)`: This syntax is used to build vocabulary from a sequence of sentences
    (can be a once-only generator stream). Each sentence must be a list of unicode
    strings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are other parameters that you can read about by clicking on this link:
    [https://radimrehurek.com/gensim/models/Word2vec.html.](https://radimrehurek.com/gensim/models/Word2vec.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each word present in the vocabulary has an association with the vocabulary
    object, which contains an index and a count. That is the output of the vocabulary
    builder. So you can refer to *Figure 6.8*, which helps you to understand the input
    and output of the vocabulary builder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d986b99-ea28-413e-9356-a7533859046f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.8: Input and output flow of vocabulary builder'
  prefs: []
  type: TYPE_NORMAL
- en: Context builder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The context builder uses output of the vocabulary builder, as well as words
    that are part of the context window, as input and generates the output.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, let's understand the concept of a context window. This context
    window is kind of a sliding window. You can define the window size as per the
    NLP application in which you will use word2vec. Generally, NLP applications use
    the context window size of five to ten words. If you decide to go with a window
    size of five, then we need to consider the five words on the left side from the
    center word and the five words on the right side of the center word. In this way,
    we capture the information about what all the surrounding words are for our center
    word.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, I want to state an example, and for this, the context window''s size
    is equal to one, as we have a one-sentence corpus. I have the sentence: **I like
    deep learning,** and **deep** is the center word. So then, you should consider
    the surrounding words as per our window size. So here, I need to consider the
    words **like** and **learning**. In the next iteration our center word will be
    **learning,** its surrounding words are **deep**, and at the end of the sentence
    is a **period** (**.**).'
  prefs: []
  type: TYPE_NORMAL
- en: I hope the context window concept is clear in your head. Now, we need to link
    this concept and see how the context builder uses this concept and the output
    of the vocabulary builder. The vocabulary builder object has word indexes and
    the frequency of the word in the corpus. By using the index of the word, the context
    builder has an idea of which word we are looking at and, according to the context
    window size, it considers the other surrounding words. These center words and
    the other surrounding words are input to the context builder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you have a clear idea about what the inputs to the context builder are.
    Let''s try to understand the output of the context builder. This context builder
    generates the word pairing. Refer to *Figure 6.9*, to get an idea about word paring:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e49e6498-3f88-492d-b13d-cae42fbe2197.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.9: Understanding word paring'
  prefs: []
  type: TYPE_NORMAL
- en: These word pairings will be given to the neural network. The network is going
    to learn the basic statistics from the number of times each word pairing shows
    up. So, for example, the neural network is probably going to get many more training
    examples of (deep, learning) than it is of (deep, communication). When the training
    is finished, if you give it the word **deep** as input, then it will output a
    much higher probability for learning or network than it will for communication.
  prefs: []
  type: TYPE_NORMAL
- en: 'So this word pair is the output of the context builder and it will pass to
    the next component, which is a two layer neural network. Refer to *Figure 6.10*,
    to see the summary of the flow of context builder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2209d360-175f-4706-a845-f0399eb57340.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.10: Input and output flow of context builder'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have seen two major components of the word2vec building blocks. Now,
    our next focus will be on the final component, which is the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network with two layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at the input and output of the neural network.
    Apart from that, we will also focus on the structural part of the neural network,
    which will give us an idea of how a single neuron looks, how many neurons there
    should be, what an activation function is, and so on. So, now, let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Structural details of a word2vec neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Word2vec uses the neural network for training. So, for us, it is very important
    to understand the basic structure of the neural network. The structural details
    of a neural network are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: There is one input layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second layer is the hidden layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third and final layer is the output layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word2vec neural network layer's details
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we know, there are two layers in the neural network for generating word vectors.
    We will start to look at each of the layers and their input and output in detail.
    Here, we are not including the math behind the word2vec model in this section.
    Later in this chapter, we will also look at the math behind word2vec, and I will
    let you know at that point of time to map your dots for better interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand the task of each layer in brief:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input layer**: An input layer has as many neurons as there are words in the
    vocabulary for training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden layer**: The hidden layer size in terms of neurons is the dimensionality
    of the resulting word vectors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output layer**: The output layer has the same number of neurons as the input
    layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The input for the first input layer is the word with one-hot encoding. Assume
    that our vocabulary size for learning word vectors is **V**, which means there
    are **V** numbers of different words in the corpus. In that case, the position
    of the word that represents itself is encoded as **1** and all others positions
    are encoded as **0**. Refer to *Figure 6.4* again to recall the concept of one-hot
    encoding. Suppose the dimension of these words is **N**. So, the input to the
    hidden layer connections can be represented by our input matrix **WI** (input
    matrix symbol) of size *V * N* ,with each row of the matrix **WI** representing
    a vocabulary word. Similarly, the connections from the hidden layer to the output
    layer means the output from the hidden layer can be described by the hidden layer
    output matrix **WO** (hidden layer matrix symbol). The **WO** matrix is of size
    *N * V*. In this case, each column of the **WO** matrix represents a word from
    the given vocabulary. Refer to *Figure 6.11* to get a crystal clear picture of
    the input and output. As well as this, we will also look at one short example
    to understand the concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3eb2989e-0e2b-4df7-b67c-c38e7e193d26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.11: two layer neural network input and output structural representation'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s talk in terms of examples. I will take a very small set of the corpus.
    See the sentences from our small corpus given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: the dog saw a cat
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the dog chased a cat
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the cat climbed a tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding three sentences have eight (8) unique words. We need to order
    them in alphabetical order, and if we want to access them, then we will refer
    to the index of each word. Refer to the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Words** | **Index** |'
  prefs: []
  type: TYPE_TB
- en: '| a | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| cat | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| chased | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| climbed | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| dog | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| saw | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| the | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| tree | 8 |'
  prefs: []
  type: TYPE_TB
- en: 'So, here our value for **V** is equal to **8**. In our neural network, we need
    eight input neurons and eight output neurons. Now let''s assume we will have three
    (3) neurons in the hidden layer. So in this case, our **WI** and **WO** values
    are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*WI = [V * N] = [8 * 3]*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*WO = [N * V] = [3 * 8]*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before training begins, these matrices, **WI** and **WO** , are initialized
    by using small random values, as is very common in neural network training. Just
    for illustration purposes, let us assume that **WI** and **WO** are to be initialized
    to the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '*WI =*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b40ddeaa-3a18-440c-9909-27d279c40262.png)'
  prefs: []
  type: TYPE_IMG
- en: '*WO =*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4de28604-a756-4429-b4da-c65dfdb7d747.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: https://iksinc.wordpress.com'
  prefs: []
  type: TYPE_NORMAL
- en: We are targeting it so that our neural network can learn the relationship between
    the words **cat** and **climbed**. So, in other words, we can explain that the
    neural network should give high probability for the word **climbed** when the
    word **cat** is fed into the neural network as an input. So, in word embedding,
    the word **cat** is referred to as a context word and the word **climbed** is
    referred to as a target word.
  prefs: []
  type: TYPE_NORMAL
- en: The input vector *X* stands for the word **cat** and it will be *[0 1 0 0 0
    0 0 0]t*. Notice that only the second component of the vector is *1*. The reason
    behind this is that the input word **cat**, holds the second position in a sorted
    list of the corpus. In the same way, the target word is **climbed** and the target
    vector for **climbed** will look like *[0 0 0 1 0 0 0 0 ]t*.
  prefs: []
  type: TYPE_NORMAL
- en: The input for the first layer is *[0 1 0 0 0 0 0 0]t*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hidden layer output *Ht* is calculated by using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Ht = XtWI = [-0.490796 -0.229903 0.065460]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding calculation, we can figure out that, here, the output of
    the hidden neurons mimics the weights of the second row of the **WI** matrix because
    of one-hot encoding representation. Now we need to check a similar calculation
    for the hidden layer and output layer. The calculation for the hidden layer and
    output layer is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*HtWO = [0.100934 -0.309331 -0.122361 -0.151399 0.143463 -0.051262 -0.079686
    0.112928]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, our final goal is to obtain probabilities for words in the output layer.
    From the output layer, we are generating probability that reflects the next word
    relationship with the context word at input. So, the mathematical representation
    is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Probability (wordk|wordcontext) for k = 1...V*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we are talking in terms of probability, but our output is in the form
    of a set of vectors, so we need to convert our output into probability. We need
    to take care that the sum of the neuron output from the final output layer should
    be added to one. In word2vec, we are converting activation values of the output
    layer neurons to probabilities by using the softmax function.
  prefs: []
  type: TYPE_NORMAL
- en: Softmax function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will look at the softmax function. The softmax function
    is used for converting output vectors into a probability form. We are using the
    softmax function because we want to convert out last layer output in terms of
    probability and softmax function can easily convert vector values into probability
    values. Here, the output of the *k*^(th) neuron will be computed by the following
    equation, where activation(*n*) represents the activation value of the *n*th output
    layer neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f6ffc37-d899-4db6-8063-b8116a611312.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By using this equation, we can calculate the probabilities for eight words
    in the corpus and the probability values are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*[ 0.143073 0.094925 0.114441 0.111166 0.149289 0.122874 0.119431 0.144800
    ]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'You must be wondering how I got these probability values. I used the previous
    softmax probability equation and generated the final probability vector. You can
    find the Python code for the softmax function in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The given code will generate the following output vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the probability *0.111166* is for the chosen target word climbed.
    As we know, the target vector is *[0 0 0 1 0 0 0 0]t*, so we can compute the error
    by prediction. To generate a prediction error or error vector, we need to subtract
    the probability vector from the target vector, and once we know the error vector
    or error values, we can adjust the weight according to that. Here, we need to
    adjust the weight values of the matrices *WI* and *WO*. The technique of propagating
    errors in the network and readjusting the weight values of *WI* and *WO* is called
    **backpropagation**.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the training can continue by taking different context-target word pairs
    from the corpus. This is the way word2vec learns relationships between words in
    order to develop a vector representation of words in the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Main processing algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Word2vec has two different versions. These versions are the main algorithms
    for word2vec. Refer to *Figure 6.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/010476eb-893d-4ca5-b3cd-251cb29f3f2f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.12: Versions of word2vec'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will look at the main two processing algorithms. Those
    algorithms names are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous bag of words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Skip-gram
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous bag of words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the **continuous bag of words** (**CBOW**) algorithm, context is represented
    by multiple words for given target words. Just recall our example that we stated
    in an earlier section, where our context word was **cat** and our target word
    was **climbed.** For example, we can use **cat** as well as **tree** as context
    words to predict the word **climbed** as the target word. In this case, we need
    to change the architecture of the neural network, especially the input layer.
    Now, our input layer may not represent the single-word one-hot encode vector,
    but we need to put another input layer that represents the word **tree**.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you increase the context words, then you need to put an additional input
    layer to represent each of the words, and all these input layers are connected
    to the hidden layer. Refer to *Figure 6.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/618b4974-58e4-4569-9ddd-bf8fd1588ad0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.13: CBOW neural network architecture (Image credit: https://www.semanticscholar.org)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the good part is that the computation formula remains the same; we just
    need to compute the *Ht* for other context words as well.
  prefs: []
  type: TYPE_NORMAL
- en: Skip-gram
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **skip-gram** (**SG**) model reverses the usage of target words and context
    words. Here, the target word is given as input to the input layer in the form
    of a one-hot encoded vector. The hidden layer remains the same. The output layer
    of the neural network is repeated multiple times to generate the chosen number
    of context words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take an example of the words **cat** and **tree** as context words and
    the word **climbed** as a target word. The input vector in the SG model will be
    the one-hot encoded word vector of the word **climbed** *[0 0 0 1 0 0 0 0 ]t*
    and this time, our output vectors should be vectors for the word **cat** and the
    word **tree**. So, the output vector should be *[ 0 1 0 0 0 0 0 0]* for **cat**
    and *[0 0 0 0 0 0 0 1]* for **tree**. Refer to the structure of the skip-gram
    algorithm in *Figure 6.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61189b79-4a72-4e08-a4da-39260ed5896e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.14: Skip gram neural network architecture (Image credit: https://www.semanticscholar.org)'
  prefs: []
  type: TYPE_NORMAL
- en: This time the output will not be a single vector of probability, but two different
    vectors of probability, as we have two words as the context word. Here, error
    vector will be calculated in the same manner as we defined earlier. The small
    change in skip-gram is that the error vectors from all the output layers are summed
    up to adjust the weights via backpropagation. This means we need to ensure that
    the weight of matrix *WO* for each output layer should remain identical through
    the training.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the algorithmic techniques and math behind word2vec and other techniques
    will be explained in the next section. So, let's get ready to deal with some cool
    mathematical stuff!
  prefs: []
  type: TYPE_NORMAL
- en: Understanding algorithmic techniques and the mathematics behind the word2vec
    model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section is very important, as we are going to discuss here the core algorithms
    that have been used in word2vec. By the end of this section, there won't be any
    secrets left in order for you to understand the concept of word2vec. Thus, this
    section is converting word2vec black box into word2vec white box. Here, I'm going
    to include the math part as well, so readers can understand the core concepts
    in a better manner. Don't worry if you don't know the math, because I will provide
    you with some resources that you may find really useful.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the basic mathematics for the word2vec algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To begin with, we need some of the basic mathematics concepts in place for
    a better understanding of the algorithm. The topics from mathematics that we need
    are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vectors**: Vectors have magnitude and direction. So, when we draw a vector
    in vector space it carries some magnitude as well as direction. You can perform
    basic math operations on vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Matrices**: A matrix is a grid of numbers or frequency count of words. It
    has rows and columns. We can define the dimensions of the matrix by counting the
    number of rows and columns it contains. You can refer to this link for more information
    on matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partial derivative**: If there is a function that contains more than one
    variable and we perform a derivative for this kind of function with respect to
    one of these variables and hold others constant, this is how we perform partial
    derivative. Partial derivatives are used in vector calculus.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partial derivative chain rule**: The chain rule is defined as a formula that
    is used for computing the derivative of the composition of two or more functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let's jump to the understanding of the techniques. I have segregated all
    concepts broadly into three sections according to the stages where each and every
    technique has been used.
  prefs: []
  type: TYPE_NORMAL
- en: 'I have listed some of the reference links from where you get an in-depth idea
    about each of the given concepts. You can follow this link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.khanacademy.org/math.](https://www.khanacademy.org/math)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the following links for vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://emweb.unl.edu/math/mathweb/vectors/vectors.html.](http://emweb.unl.edu/math/mathweb/vectors/vectors.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.mathsisfun.com/algebra/vectors.html.](https://www.mathsisfun.com/algebra/vectors.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to this link for more information on matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://medium.com/towards-data-science/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c.](https://medium.com/towards-data-science/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see some basic examples by using this link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://mathinsight.org/partial_derivative_examples.](http://mathinsight.org/partial_derivative_examples)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.analyzemath.com/calculus/multivariable/partial_derivatives.html.](http://www.analyzemath.com/calculus/multivariable/partial_derivatives.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the following links for the partial derivative chain rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://math.oregonstate.edu/home/programs/undergrad/CalculusQuestStudyGuides/vcalc/chain/chain.html.](https://math.oregonstate.edu/home/programs/undergrad/CalculusQuestStudyGuides/vcalc/chain/chain.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube.com/watch?v=HOYA0-pOHsg.](https://www.youtube.com/watch?v=HOYA0-pOHsg)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube.com/watch?v=aZcw1kN6B8Y.](https://www.youtube.com/watch?v=aZcw1kN6B8Y)'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding list is more than enough for this chapter to understand the mathematics
    behind the algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques used at the vocabulary building stage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While generating vocabulary from the dataset, you may use an optimization technique,
    and lossy counting is the one that is used the most for the word2vec model.
  prefs: []
  type: TYPE_NORMAL
- en: Lossy counting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The lossy count algorithm is used to identify elements in a dataset whose frequency
    count exceeds a user-given threshold. This algorithm takes data streams as an
    input instead of the finite set of a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: With lossy counting, you periodically remove very low-count elements from the
    frequency table. The most frequently accessed words would almost never have low
    counts anyway, and if they did, they wouldn't be likely to stay there for long.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the frequency threshold is usually defined by the user. When we give a
    parameter of `min_count = 4`, we remove the words that appear in the dataset less
    than four times and we will not consider them.
  prefs: []
  type: TYPE_NORMAL
- en: Using it at the stage of vocabulary building
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lossy counting is very useful; especially when you have a large corpus and you
    don't want to consider the words that appear very rarely.
  prefs: []
  type: TYPE_NORMAL
- en: At this time, lossy counting is very useful because the user can set the minimum
    word frequency count as a threshold, so words that occur less than the threshold
    frequency count won't be included in our vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: So, if you have a large corpus and you want to optimize the speed of training,
    then we can use this algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, you can say that by using this algorithm you narrow down your
    vocabulary size, thus, you can speed up the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from word2vec, the lossy counting algorithm is used in network traffic
    measurements and analysis of web server logs.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques used at the context building stage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While generating word context pairs, the context builder uses the following
    techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic window scaling or dynamic context window
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subsampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pruning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic window scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see, dynamic window scaling is a part of the context builder. We
    will see how it can be useful and what kind of impact it generates when we use
    it. Dynamic window scaling is also known as **dynamic context window**.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding dynamic context window techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the word2vec implementation, dynamic context window is an optional technique
    that may be applied to generate more accurate output. You can also consider these
    techniques as hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic context window techniques use weight schema for context words with respect
    to target words.
  prefs: []
  type: TYPE_NORMAL
- en: So the intuition here is that words that are near to the target word are more
    important than other words that are far away from the target word.
  prefs: []
  type: TYPE_NORMAL
- en: Let us see how it will be useful when we are building word pairs. Dynamic context
    window considers that nearby context words hold more importance to predicting
    the target word. Here, we are applying the weighting scheme by using uniform sampling
    on the actual window size between 1 and L. For example, suppose the context window
    size is 5 and now the weight of context words are distributed in a uniform manner,
    so the weight of most nearby words is 5/5, the very next context word weight is
    4/5, and so on. So, the final weight for context words will be 5/5, 4/5, 3/5,
    2/5, 1/5\. Thus, by providing weight, you can fine-tune the final result.
  prefs: []
  type: TYPE_NORMAL
- en: Subsampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Subsampling is also one of the techniques that we use when we are building word
    pairs, and as we know, these word pairs are sample training data.
  prefs: []
  type: TYPE_NORMAL
- en: Subsampling is the method that removes the most frequent words. This technique
    is very useful for removing stop words.
  prefs: []
  type: TYPE_NORMAL
- en: 'These techniques also remove words randomly, and these randomly chosen words
    occur in the corpus more frequently. So, words that are removed are more frequent
    than some threshold `t` with a probability of `p`, where `f` marks the words corpus
    frequency and we use *t = 105* in our experiments. Refer to the following equation
    given in *Figure 6.15*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b79f0040-3022-4c81-bf10-b2979cdd8afc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.15: Subsampling probability equation'
  prefs: []
  type: TYPE_NORMAL
- en: This also acts as one of the useful hyperparameters, and it is very useful because
    we are removing the most frequent and unnecessary words from the corpus, as well
    as from the context window, and that way, we are improving the quality of our
    training sample set.
  prefs: []
  type: TYPE_NORMAL
- en: Pruning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pruning is also used when we are building our word pairs for training purposes
    using context builder. When you have a large amount of vocabulary to deal with,
    if you have included less frequent words, then you need to remove them. You can
    also restrict the size of the total vocabulary by using the `max_vocab_size` parameter
    given in the Python `gensim` library.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how useful pruning is for us in order to generate word2vec. Pruning
    is used to prune the training sample size, as well as make the quality of it better.
    If you don't prune the rarely occurred words from the dataset, then the model
    accuracy may degrade. This is a kind of hack to improve the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms used by neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we will look at the structure of individual neurons. We will also look
    into the details about the two algorithms, thus, we will understand how word2vec
    generates vectors from words.
  prefs: []
  type: TYPE_NORMAL
- en: Structure of the neurons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen the overall neural network structure, but we haven't yet seen what
    each neuron is made of and what the structure of the neurons is. So, in this section,
    we will look at the structure of each single input neuron.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will look at the following structures:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic neuron structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a single neuron
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single neuron application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-layer neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mathematics behind the word2vec model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this, we will heavily include the mathematical formulas. If you are not from
    a mathematical background, then don't worry. I will give you explanations in simple
    words so you know what is going on in each section.
  prefs: []
  type: TYPE_NORMAL
- en: Basic neuron structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to *Figure 6.16;* it shows the basic neuron structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f70b4079-ac1a-46ac-ab0e-d119336d8906.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.16: Basic neuron structure (Image credits: Xin Rong)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6.16* shows a basic neuron structure. This structure is not new. This
    structure takes an input, and there are weights also as input, and they calculate
    the weighted sum. Here, *x1* to *xk* is the input value and *w1* to *wk* is the
    corresponding weights. So, the weighted sum is expressed by the following equation
    given in *Figure 6.17*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf4e0175-6266-422d-8ad7-a0c4a91b4e9c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.17: Weighted sum equation'
  prefs: []
  type: TYPE_NORMAL
- en: Let's take an example to understand the usage of the equation. So, if you have
    input *x=[1 5 6]* and *w=[0.5 0.2 0.1],* the weighted sum *u* is equal to *[1
    * 0.5 + 5 * 0.2 + 6 * 0.1],* so our final answer is *u = [0.5 + 1 + 0.6] = 2.1*.
    This is just a simple example to give you some concrete practical intuition about
    the real workings of the given equation. This is all about our input. Now we will
    talk about the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to generate output, we can say from our basic neuron structure that
    our output is the function of weighted sum *u*. Here, *y* is the output and *f(u)*
    is the function of the weighted sum. You can see the equation given in *Figure
    6.18*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/382e56be-de82-4c74-80ea-51cc035412db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.18: Output y is function of weighted sum u'
  prefs: []
  type: TYPE_NORMAL
- en: 'In neural networks, we can use different available functions and these functions
    are called **activation functions**. These functions are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Step function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sigmoid function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some great scientists have stated that the given functions are a good choice
    as activation functions. For this chapter, we are not getting into the details
    of activation functions, but we will look at all the details regarding the activation
    functions in [Chapter 9](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml), *Deep Learning
    for NLP and NLG Problems*. So, we will consider the given two functions as activation
    functions for word2vec. We will use either the step function or sigmoid function,
    not both at the same time, to develop word2vec. Refer to the equation of both
    of the functions in *Figure 6.19*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8c40266-ca20-4c0a-9f9e-778caeac164f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.19: Activation functions, the first is the step function and the second
    is the sigmoid function'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *f(u)* is the step function and *f(u)* is the sigmoid function.
  prefs: []
  type: TYPE_NORMAL
- en: When we draw a circle to represent the neuron similar to the one drawn in *Figure
    6.11*, this circle contains the weighted sum and activation function, which is
    the reason we have indicated the dotted circle in *Figure 6.16*.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see how these activation functions can be used
    and how we can calculate the errors in predicted output by using the error function.
    We will see this in detail about the usage of the activation function and error
    function. So let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: Training a simple neuron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now it is time to see how we can use a single neuron for training by using the
    activation function, and let's understand the loss function to calculate the error
    in predicted output.
  prefs: []
  type: TYPE_NORMAL
- en: The main idea is defined as the error function, which actually tells us the
    degree of error in our prediction; we will actually try to make our error value
    as low as possible. So, in other words, we are actually trying to improve our
    prediction. During training, we use input and calculate the error by using the
    error function and update the weight of neurons and repeat our training process.
    We will continue this process until we get the minimum error rate of maximum,
    best, and accurate output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two most important concepts that we are going to look at are listed as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Define error function (loss function)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding of gradient descent in word2vec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define error function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, our input is vector `X` with vocabulary `x1` to *xk* and our output `y`
    is the output vector. So, to calculate the error `E` , we need to define the error
    function or loss function, and we are using the L2 loss function. Let's begin
    with understanding the basic concept of the L2 loss function, and then we will
    see how it will be useful in word2vec.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two type of loss functions that are mostly used across **machine
    learning** (**ML**) and **deep learning** (**DL**). By the way, we will look at
    ML and DL in upcoming chapters, which are [*Chapter 8*](97808151-90d2-4034-8d53-b94123154265.xhtml),
    *Machine Learning ( ML) for NLP Problems* and [Chapter 9](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml),
    *Deep Learning for NLP and NLG Problems*. There are two standard types of loss
    functions, but here we will only look at **least square error** (**L2**), and
    in upcoming chapters, which are [Chapter 8](97808151-90d2-4034-8d53-b94123154265.xhtml),
    *Machine Learning for NLP Problems* and [Chapter 9](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml),
    *Deep Learning for NLP and NLG Problems* we will look at various error functions
    in detail and compare those error functions as well. The two standard types of
    loss functions are:'
  prefs: []
  type: TYPE_NORMAL
- en: Least absolute deviation (L1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Least square error (L2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Least square error is also called **L2 loss** function. In general, loss functions
    want to achieve their minimized value while learning from a dataset and L2 loss
    functions also want to achieve their value where the error value will be minimal.
    So, precisely, the L2 function wants to minimize the squared differences between
    the estimated and existing target values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The structure of a single neuron at the time of training is given in *Figure
    6.20*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f62cfc9f-068d-449c-9bb0-8307a712d33a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.20: Single neuron at the time of training'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, when we are trying to calculate an L2 loss function of a single neuron,
    we will use the following equation given in *Figure 6.21*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6fa6d6a-7fa3-4359-8b3f-def68caff86e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.21: L2 loss function equation (Least Squared error function)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *t* is the target vector value, *y* is the estimated vector value or predicted
    vector value, and *E* is the error function. We have defined our L2 error function.
    I know you must be very keen to know what we will be doing with this L2 error
    function to get a least error value, and that is where we need to understand the
    concept of gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding gradient descent in word2vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's understand what we are going to do with the L2 function and how it
    will be useful in achieving an accurate output.
  prefs: []
  type: TYPE_NORMAL
- en: As we said earlier, we want to minimize this function value so, we can accurately
    predict the target value, and to achieve this we will take partial derivative
    of the L2 function equation given in *Figure 6.21* with respect to *y*. The procedure
    of deriving derivation and then by using this derivation trying to minimize error
    values, is called **gradient descent**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In that case, the result is given in *Figure 6.22*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01aba3d0-bbfd-46d9-a936-b7b2c64a2c36.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.22: Result of Partial derivatives of L2 loss function with respect
    to y'
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that output *y* is dependent on *f(u)* and *f(u)* is dependent on input
    weight values *wi*. So we need to apply chain rules and generate the error function
    value. If we use partial derivative chain rules, then we will get the following
    equation, which will be useful in word2vec. *Figure 6.23* shows the result of
    the partial derivative chain rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe9277c1-6873-4076-8ba3-dd667802265c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.23: Partial derivative chain rule result for L2 error function'
  prefs: []
  type: TYPE_NORMAL
- en: After calculating the L2 loss function as per the value of the error, the neural
    network input weight will be updated and this kind of iteration will continue
    until we achieve the minimum error value or error rate.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have been deriving equations for a single neuron, so it will be important
    to know what this single neuron can do for us. That is our next point of discussion.
  prefs: []
  type: TYPE_NORMAL
- en: Single neuron application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have learnt a lot of technical and mathematical stuff about single neurons,
    so I really want to walk you through the application on single neurons with respect
    to the word2vec model. So let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to build a model that identifies the words that are edible and
    that are not, then we can use a single neuron to build the model. This kind of
    application, where we are segregating words either into edible classes or non-edible
    classes is called a **binary classification** **task**. For this kind of task,
    neurons are used to take one-hot encoded vectors as input and a single neuron
    will learn which words are related to edible items and which are not related to
    edible items. So, it will generate a look up table, which you can see in *Figure
    6.24*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8781ddad-2f2e-4aea-9849-e68108aeb524.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.24: Single neuron can classify words into edible and non edible categories'
  prefs: []
  type: TYPE_NORMAL
- en: The reason why this kind of application can be built with such ease is that
    when you are continuously providing the one-hot word vector and use the sigmoid
    or step functions as the activation function, then it becomes a standard classification
    problem, and this kind of problem can be solved easily by using mathematics. We
    have defined this in previous sections because for edible items, the output vector
    have the same kind of values, and for non-edible items generated vectors, they
    represent the same kind of output vector. In the end, we will be able to build
    the lookup table. This kind of standard classification problem reminds us of logistic
    regression, and we are applying the same logistic regression concepts, but here
    we are using a single neuron structure.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen enough about the single neuron structure, now it is time to explore
    the multi-layer neural network. Our next section will provide you with information
    about multilayer neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-layer neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A multi-layer neural network is the structure that we are using for word2vec.
    This function takes input as a one-hot encoded word vector and this vector, as
    well as a weighted sum is passed to a hidden layer. By using the activation function,
    which is the sigmoid function in this case, output is generated from the hidden
    layer and this output is passed to the next layer, which is the output layer.
    We have already seen an example in this chapter, in the section entitled *Neural
    network with two layers*. In that section, we did not look at the mathematical
    aspect, so here I will walk you through the mathematical aspect of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s represent what I told you in the preceding paragraph through a mathematical
    equation. See the structure of a multi-layer neural network in *Figure 6.25*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c11fdd7-ce94-4e85-9743-d7a96834cd2a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.25: Multi-layer neural network'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see the mathematical equations for the given neural network. Here,
    I''m going to provide you with high-level intuition, which will help you to understand
    the flow and you will get an idea about the input and output functions. Refer
    to *Figure 6.26*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9c362c99-f5b6-4fa5-b1e6-875ac3b76707.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.26: Mathematical equation for multilayer neural networks'
  prefs: []
  type: TYPE_NORMAL
- en: 'The flow of the input and output is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The first equation of *Figure 6.24* is a weighted sum the of input layer and
    the result of the input layer will be passed on to the hidden layer. *ui* is the
    weighted sum of the input layer. The activation function of the hidden layer is
    given in the second equation. The activation function *hi* uses the sigmoid function
    and generates the intermediate output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weighted sum of the hidden layer will be passed to the output layer and
    the third equation shows the calculation of the hidden layer weighted sum. *u'j*
    is the weighted sum from the hidden layer and it will be passed to the output
    layer. *yj* uses the weighted sum from the hidden layer, which is *u'j* as well,
    as here also the activation function is sigmoid.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have seen the flow of input and output by using a basic mathematical representation.
  prefs: []
  type: TYPE_NORMAL
- en: Now, a major concern is how this structure is used to get trained for the word2vec
    model, and the answer to that is, we use backpropagation to train the model, which
    we will see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already seen how errors have been calculated using the L2 loss function,
    and the L2 loss function wants to minimize the squared differences between the
    estimated and existing target values. We will apply the same concepts to the multi-layer
    neural network. So, we need to define the loss function as well as we to take
    the gradient of the function and update the weight of the neural network in order
    to generate a minimum error value. Here, our input and output is vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to *Figure 6.27* to see the neural network structure, and *Figure 6.28*
    shows what equations we need to apply to calculate error functions in a multi-layer
    neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95ea54bf-11b2-4027-b3de-19a570a01aa1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.27: Multi-layer neural network for calculating error functions'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s see the derivation and mathematical calculation performed by neural
    networks. You can see the equations are given as follows, refer to *Figure 6.28*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13b778d6-f544-46b4-8401-df6697c5e815.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.28: Equations for calculating error function for multi-layer neural
    networks'
  prefs: []
  type: TYPE_NORMAL
- en: When you are calculating error function for multi-layer neural networks, you
    need to be very careful about the indexes, as well as which layer you are calculating
    an error function value for. As you can see in *Figure 6.28,* we will start with
    the output index, backpropagate the error to the hidden layer, and update the
    weight. In the fifth equation, you can see that we need to calculate the error
    function for the output layer so that backpropagate the error and we can update
    the weight of the input layer. To deal with indexes is a kind of a challenging
    task in multi-layer neural networks. But coding it up is quite easy because you
    just need to write a `for` loop to calculate each of the layer gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will put all the mathematical equations and concepts of word2vec together
    and understand the final mathematical piece of word2vec neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematics behind the word2vec model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at the final piece of mathematics by combining
    all the previous equations and concepts, and we will derive the final equation
    in the form of probability. We have already seen the concept and basic intuition,
    calculation, and example in the previous section, *Word2vec neural network layers
    details*.
  prefs: []
  type: TYPE_NORMAL
- en: The word2vec neural network is using a one-hot encoded word vector as input
    and then it passes this vector value to the next layer, which is the hidden layer,
    and this is nothing but the weighted sum values that feed into the hidden layer
    as input. The last output layer generates the vector value, but to make sense
    of the output, we will convert the vector into probability format, and with the
    help of softmax techniques, we will also convert the output word vector into probability
    format. We will see all the different techniques that are used to generate probability
    from the output vector in the upcoming section, until then, just use softmax as
    a magic function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to *Figure 6.29* to understand the mathematics behind word2vec neural
    networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9eb90f9d-6eb6-4ec1-b6c2-07b13b3ad9e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.29: Mathematics behind word2vec model'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first equation, we can see the weighted sum of the input word vector
    and weights and get *h* in the second equation. We multiply *h* and the weighted
    sum of word vectors of the hidden layer *v''wj*. Here, weight and index has been
    changed. This multiplication is *uj* . Here, *uj* is the activation function.
    We will then generate probability by using the value of *uj* . So, the final equation
    is the softmax function. Let''s simplify the equation by replacing *uj* in the
    third equation with the input and output word vector format, then you will get
    the final equation. Refer to *Figure 6.30*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f807cd33-cb1a-4344-a7c6-b0f8f21a41cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.30: Final probability equation of the word2vec model'
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, our output is a softmax function, so for updating weight using backpropagation,
    we need to define the loss function. So, here, we will define the loss function
    in the form of the softmax function, so we will use minus log probability of the
    softmax function and then we will perform gradient descent. See *Figure 6.31*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c97abf5f-5208-4dcf-988b-3722bd1de0ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.31: Error function gradient descent in form of minus log probability
    of the softmax function'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, I want to give some idea of how the output vector value has been updated.
    So, it is an updating rule for an output layer. You can find the equation given
    as follows. Refer to *Figure 6.32*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a838de75-1d19-4140-ad57-d2ee51996bed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.32: Rule for updating the output vector'
  prefs: []
  type: TYPE_NORMAL
- en: In this equation, we are taking the original output vector and subtracting the
    prediction error *ej* of the output node and *h* is the value of the hidden layer.
    So, the meaning of this equation is that, if we have the word **climbed** as input,
    and we want to predict the word **cat** as output, then how we can update the
    vector value of the word **climbed** so that it will be more similar to the vector
    of the word **cat**? So in simple language we can say we will add some part of
    the vector of **climbed** to the vector of the word **cat** and apart from this,
    we also need to update the output vector of other words because all other words
    that are not our target words should update their output vector so that they will
    be less similar to the target words.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rule for updating the input vector is also useful; the equation of updating
    an input vector is given as follows. Refer to *Figure 6.33*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06d1f10f-cf58-46c8-8604-e7f34ef5a9fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.33: Rule for updating input vector'
  prefs: []
  type: TYPE_NORMAL
- en: This equation is a bit complex. So, here, intuition is the input vector, which
    will be subtracted from the weighted sum of the prediction errors. The meaning
    of this is that, this time, we are going to update the input vector **cat**. We
    will update the vector value of the word cat in such a manner that it will come
    close to the vector of the word **climbed**. Here, co-occurrence of the words
    play a major role.
  prefs: []
  type: TYPE_NORMAL
- en: We are almost done with the math portion of the word2vec model. We have seen
    a lot of mathematical equations that are used in the word2vec model. Now we will
    talk about the techniques that are used to generate the final vectors and probability
    of the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques used to generate final vectors and probability prediction stage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will see how we will generate the final vector. We will
    also use some heuristics to generate output efficiently. So, we will talk about
    those heuristics as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have already seen, to generate the word vector we need to update input
    as well as the output vector. Suppose we have a million words in our vocabulary,
    so the process of updating the input and output vectors will take a lot of time
    and it will be inefficient. We have to solve this challenge. So we use some of
    the optimized ways to perform the same operations and those techniques are given
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical softmax
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Negative sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, let's start to understand these techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical softmax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In hierarchical softmax, instead of mapping each output vector to its corresponding
    word, we consider the output vector as a form of binary tree. Refer to the structure
    of hierarchical softmax in *Figure 6.34*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98a86731-7bb0-4125-90d2-d12252c8907d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.34: Hierarchical Softmax structure'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, here, the output vector is not making a prediction about how probable the
    word is, but it is making a prediction about which way you want to go in the binary
    tree. So, either you want to visit this branch or you want to visit the other
    branch. Refer to *Figure 6.35*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2fccd091-58bd-422a-bfda-654878270ec9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.35: Prediction path using hierarchical softmax mantle representation'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, consider the red activated dot going up (light grey here) and
    the blue activated dot going downwards (dark grey here), so you can see that,
    here, we can predict the word **juice** with high probability.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the advantage is, if you want to backpropagate an error, then you just
    need to update one output vector and the error will propagate to only three nodes
    that are activated at the time of prediction. We use the Huffman binary tree construction
    to generate the binary tree.
  prefs: []
  type: TYPE_NORMAL
- en: Negative sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Negative sampling is also a kind of optimization method. In this method, we
    are going to update the vector of the output word, but we are not going to update
    all the vectors of other words. We just take the sample from the words other than
    the output vector. So, we are selecting a sample from the negative sample set
    of words, hence the name of this technique is negative sampling.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the facts related to word2vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are some of the facts about the word2vec models that you should keep in
    mind when you are actually using it:'
  prefs: []
  type: TYPE_NORMAL
- en: So far, you will have realized that word2vec uses neural networks and this neural
    network is not a deep neural network. It only has two layers, but it works very
    well to find out the words similarity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word2vec neural network uses a simple logistic activation function that does
    not use non-linear functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The activation function of the hidden layer is simply linear because it directly
    passes its weighted sum of inputs to the next layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we have seen almost all the major aspects of word2vec, so in the next section,
    we will look at the application of word2vec.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of word2vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Allow me to introduce some real-life applications in which word2vec has been
    used. They are:'
  prefs: []
  type: TYPE_NORMAL
- en: Dependency parser uses word2vec to generate better and accurate dependency relationship
    between words at the time of parsing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Name entity recognition can also use word2vec, as word2vec is very good at finding
    out similarity in **named entity recognition** (**NER**). All similar entities
    can come together and you will have better results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis uses it to preserve semantic similarity in order to generate
    better sentiment results. Semantic similarity helps us to know which kind of phrases
    or words people use to express their opinions, and you can generate good insights
    and accuracy by using word2vec concepts in sentiment analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can also build an application that predicts a person's name by using their
    writing style.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to do document classification with high accuracy and using simple
    statistics, then word2vec is for you. You can use the concept and categorize the
    documents without any human labels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word clustering is the fundamental product of word2vec. All words carrying a
    similar meaning are clustered together.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google uses word2vec and deep learning to improve their machine translation
    product.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are so many use cases where you could use word2vec concepts. Here, we
    are going to implement some fun examples. We are going to build fun applications,
    as well as doing some visualization on them so you can understand the concept
    in a far better manner.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of simple examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to implement the famous word2vec example, which
    is adding `woman` and `king` and subtracting `man`, and then the resultant vector
    shows the vector value of `queen`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are not going to train the word2vec model, on our data and then build our
    own word2vec model because there is a huge amount of data on which Google has
    already trained their word2vec model and provided us with pre-trained models.
    Now, if you want to replicate the training process on that much data, then we
    need a lot of computational resources, so we will use pre-trained word2vec models
    from Google. You can download the pre-trained model from this link: [https://code.google.com/archive/p/Word2vec/](https://code.google.com/archive/p/Word2vec/).'
  prefs: []
  type: TYPE_NORMAL
- en: After clicking on this link, you need to go to the section entitled pre-trained
    word and phrase vectors, download the model named `GoogleNews-vectors-negative300.bin.gz`,
    and extract it.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the `genism` library to build our famous example.
  prefs: []
  type: TYPE_NORMAL
- en: Famous example (king - man + woman)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to load the binary model by using the `gensim` library and replicate
    the example. If you are running it on your computer, then it will take a few minutes,
    so don''t worry and keep the script running. Refer to *Figure 6.36*, for the code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03d8ba9f-98d8-416c-ae56-1706cd536ff0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.36: Code snippet for example King - man + woman = queen'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the code by clicking on this GitHub link: [https://github.com/jalajthanaki/NLPython/blob/master/ch6/kingqueenexample.py](https://github.com/jalajthanaki/NLPython/blob/master/ch6/kingqueenexample.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to *Figure 6.37* for the output we are generating:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/454a681c-4042-41be-9c43-6d9afd811c74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.37: Output of the example King - man + woman = queen'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if you want to train the model from scratch by using data that is provided
    by Google, then download the training dataset by using the following link: [https://code.google.com/archive/p/Word2vec/](https://code.google.com/archive/p/Word2vec/).
    Go to the section entitled *Where to obtain the training data* and download all
    training datasets, then, by taking a reference from the given GitHub link [https://github.com/LasseRegin/gensim-Word2vec-model](https://github.com/LasseRegin/gensim-Word2vec-model),
    you can replicate the whole training process, but it will take a lot of time because
    this kind of training needs a lot of computational power.'
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of word2vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have seen, word2vec is a very good technique for generating distributional
    similarity. There are other advantages of it as well, which I''ve listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: Word2vec concepts are really easy to understand. They are not so complex that
    you really don't know what is happening behind the scenes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using word2vec is simple and it has very powerful architecture. It is fast to
    train compared to other techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human effort for training is really minimal because, here, human tagged data
    is not needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This technique works for both a small amount of datasets and a large amount
    of datasets. So it is an easy-to-scale model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you understand the concept and algorithms, then you can replicate the whole
    concept and algorithms on your dataset as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It does exceptionally well on capturing semantic similarity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As this is a kind of unsupervised approach, human effort is very minimal, so
    it is a time-saving method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges of word2vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although the word2vec concept is very efficient, there are some points that
    you may find complex or challenging. Here, I will propose the most common challenges.
    Those points are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The word2vec model is easy to develop, but difficult to debug, so debug ability
    is one of the major challenges when you are developing a word2vec model for your
    dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It does not handle ambiguities. So, if a word has multiple meanings, and in
    the real world we can find many of these kinds of words, then in that case, embedding
    will reflect the average of these senses in vector space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How is word2vec used in real-life applications?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will give you an idea of which kinds of NLP applications use word2vec
    and how NLP applications use this concept. Apart from that, I will also discuss
    some of the most frequently-asked questions across the community in order for
    you to have a clear insight of word2vec when you try it out in real life.
  prefs: []
  type: TYPE_NORMAL
- en: NLP applications such as document classification, sentiment analysis, and so
    on can use word2vec techniques. Especially in document classification, word2vec
    implementation gives you more good results, as it preserves semantic similarity.
  prefs: []
  type: TYPE_NORMAL
- en: For sentiment analysis, we can apply word2vec, which gives you an idea about
    how words are spread across the dataset, and then you can use customized parameters
    such as context window size, subsampling, and so on. You should first generate
    **bag of words** (**BOW**) and then start to train word2vec on that BOW and generate
    word vectors. These vectors can be fed as input features for the ML algorithm,
    then generate sentiment analysis output.
  prefs: []
  type: TYPE_NORMAL
- en: Now, it's time to discuss some of the questions that people usually ask when
    they are trying to understand and use word2vec techniques on their own dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's fire up the questions!
  prefs: []
  type: TYPE_NORMAL
- en: '**What kind of corpus do we need?**: Word2vec techniques can be applied on
    text datasets. As such, there is not any specific kind of text data that you cannot
    use. So, as per my view, you can apply word2vec on any dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Should I always remove stop words?**: In original models of word2vec that
    were from Google, remove some of the stop words, such as **a** has been removed
    in word2vec, but the word **the** has not been removed. So it is not mandatory
    that you remove the words:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is totally dependent on your NLP application. If you are developing a sentiment
    analysis application, then you can remove all stop words, but if you are developing
    machine translation applications, then you should remove some of the stop words;
    not all. If you are using word2vec for developing word clusters to understand
    the grammar of the language, then you should not remove any of the words.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Should I remove all stop words?**: This question is related to the previous
    question. The straightforward answer to this question is no. It is not compulsory
    that you should remove all stop words blindly for every NLP application. Each
    and every NLP application is different, so you should take a decision based on
    the NLP application that you are trying to build:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you look at the Google original word2vec model, then you will see that in
    that model the word **a** is not there, which means a vector that represents the
    word **a** is not present, but a vector for the word **the** is there.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We will load the original Google word2vec model and, using simple lines of code,
    we will look at some of the facts regarding stop words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to the code snippet in *Figure 6.38*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/ca5ab527-617d-4bd8-bd77-86f99f20ae63.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.38: Code snippet that shows the fact of stop words'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the output that is the vector value of `the`, refer to *Figure 6.39*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc1bd72a-bcd4-4373-9e52-65f0c300ced5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.39: Sample values of word vector for the word the'
  prefs: []
  type: TYPE_NORMAL
- en: 'See the output, where you can see that word2vec doesn''t contain `a` in its
    vocabulary in *Figure 6.40*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae68391b-9340-46cf-bdd9-0a5fd97f9a03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.40: Word2vec doesn''t contain the word a'
  prefs: []
  type: TYPE_NORMAL
- en: '**Don''t you think that, here, we have generated two vectors for each word?**:
    I would like to let you know that we have generated two vectors for each word.
    The reason behind this is that word in a sentence is coming on a target word as
    well as a context word, so when a word appears as a target word, we will generate
    vectors, and when a word appears as a context word, then we also generate vectors.
    We consider the target word vector in our final output, but yes, you can use both
    vectors. How to use the two vectors and generate making sense out of that is kind
    of a million dollar question!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When should you use word2vec?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word2vec captures semantic similarity; this is the most important point that
    we need to keep in mind when we are processing the answer to the preceding question.
  prefs: []
  type: TYPE_NORMAL
- en: If you have an NLP application in which you want to use the distributional semantic,
    then word2vec is for you! Some NLP applications will use this concept to generate
    the features and the output vector from the word2vec model, or similarly, vectors
    will be used as input features for the ML algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: You should know which NLP applications can use word2vec. If you know the list
    of applications, it becomes easy for you to decide whether you should use it or
    not. Suppose you can use k-mean clustering for document classification; if you
    want document classification to carry some of the attributes of semantics, then
    you can use word2vec as well. If you want to build a question-answer system, then
    you will need techniques that differentiate questions on a semantic level. As
    we need some semantic level information, we can use word2vec.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have seen enough about the concepts and theories, so we will begin our
    favorite part, which is coding, and this time it is really fun.
  prefs: []
  type: TYPE_NORMAL
- en: Developing something interesting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we are going to train our word2vec model. The dataset that I'm going to
    use is text data of *Game of Thrones*. So, our formal goal is to develop word2vec
    to explore semantic similarities between the entities of *A Song of Ice and Fire
    (from the show Game of Thrones)*. The good part is we are also doing visualization
    on top of that, to get a better understanding of the concept practically. The
    original code credit goes to Yuriy Guts. I have just created a code wrapper for
    better understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 'I have used IPython notebook. Basic dependencies are `gensim`, `scikit-learn`,
    and `nltk` to train the word2vec model on the text data of Game of Thrones. You
    can find the code on this GitHub link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch6/gameofthrones2vec/gameofthrones2vec.ipynb.](https://github.com/jalajthanaki/NLPython/blob/master/ch6/gameofthrones2vec/gameofthrones2vec.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: The code contains inline comments and you can see the snippet of the output.
    We have used the t-SNE technique to reduce the dimensions of the word vector,
    so we can use the two-dimensional vector for visualization. The t-SNE technique
    takes a lot of time if you want to run on a normal computer with 2 to 4 GB RAM.
    So, you need more RAM to run t-SNE code successfully at your end and you can skip
    the visualization part if you have memory constraints. You can see the visualization
    images. Once you have saved the model on to disk, you can use it and generate
    output easily. I have given sample output in *Figures 6.41* to *Figure 6.45*.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may observe the output for the word `Stark`here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/05b7c3fb-7cec-436a-a67d-d7dda024dec2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.41: Word similarity output for the word stark'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the output for the nearest words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96971b2d-b210-4ef6-81c4-b1bc30f7c0f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.42: Output for the nearest words'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will see the following figures for output of visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18c6afd3-33d5-4c41-a2eb-0c1430939c6b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.43: After using t-SNE you can visualize vectors in 2-D space'
  prefs: []
  type: TYPE_NORMAL
- en: Now we will zoom in and try to see which words have ended up together.
  prefs: []
  type: TYPE_NORMAL
- en: 'See the following figure, which shows people related to Kingsguard ending up
    together:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df02963d-e682-4191-9622-d0140e0dce9e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.44: People names grouped together'
  prefs: []
  type: TYPE_NORMAL
- en: 'See the following figure, which shows food products grouped together nicely:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d64d433-19ce-4ed0-8cb7-62fbc3b423d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.45: Name of food items grouped together'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I''m a big fan of Harry Potter, and so, in this exercise, you need to generate
    Word2vec from the text data from a Harry Potter book. Don''t worry about the dataset;
    I have already provided it for you and it resides on this GitHub link: [https://github.com/jalajthanaki/NLPython/tree/master/ch6/Harrypotter2vec/HPdataset](https://github.com/jalajthanaki/NLPython/tree/master/ch6/Harrypotter2vec/HPdataset)'
  prefs: []
  type: TYPE_NORMAL
- en: Good luck with generating HarryPotter2Vec! Happy Coding!
  prefs: []
  type: TYPE_NORMAL
- en: Extension of the word2vec concept
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The word2vec concept can be extended to different levels of text. This concept
    can be applied on the paragraph level or on the document level, and apart from
    this, you can also generate the global vector, which is called **GloVe**. We will
    try to understand them. Here, we are going to get an overview of each of the concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the following extended concepts built by using the word2vec concept:'
  prefs: []
  type: TYPE_NORMAL
- en: Para2vec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doc2vec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GloVe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Para2Vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Para2vec stands for paragraph vector. The paragraph vector is an unsupervised
    algorithm that uses fixed-length feature representation. It derives this feature
    representation from variable-length pieces of texts such as sentences, paragraphs,
    and documents.
  prefs: []
  type: TYPE_NORMAL
- en: Para2vec can be derived by using the neural network. Most of the aspects are
    the same as Word2vec. Usually, three context words are considered and fed into
    the neural network. The neural network then tries to predict the fourth context
    word. Here, we are trying to maximize the log probability and the prediction task
    is typically performed via a multi-class classifier. The function we use is softmax.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that, here, the contexts are fixed-length and generate the context
    words by using a sliding window over the paragraph. The paragraph vector is shared
    with all contexts generated from the same paragraph, but not across paragraphs.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of Para2vec is to learn to predict the words from unlabeled data
    so that you can use these techniques when you don't have enough labeled datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Doc2Vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Doc2vec** (**Document vectors**) is an extension of word2vec. It learns to
    correlate document labels and words, rather than words with other words. Here,
    you need document tags. You are able to represent an entire sentence using a fixed-length
    vector. This is also using word2vec concepts. If you feed the sentences with labels
    into the neural network, then it performs classification on a given dataset. So,
    in short, you tag your text and then use this tagged dataset as input and apply
    the Doc2vec technique on that given dataset. This algorithm will generate tag
    vectors for the given text. You can find the code at this GitHub link: [https://github.com/jalajthanaki/NLPython/blob/master/ch6/doc2vecexample.py](https://github.com/jalajthanaki/NLPython/blob/master/ch6/doc2vecexample.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'I have used very small datasets to just give you intuition on how you can develop
    Doc2vec, so I''m ignoring the accuracy factor of the developed model. You can
    refer to the code given at this reference link: [https://github.com/jhlau/doc2vec](https://github.com/jhlau/doc2vec)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the intuitive code in *Figure 6.46* and see the output snippet in
    *Figure 6.47*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0277849e-707b-47fd-9eec-ccc13dba224b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.47: Code snippet of doc2vec'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/864bb3b8-8e0f-40ff-a903-1c4ce4e0f9c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.48: Sample output'
  prefs: []
  type: TYPE_NORMAL
- en: Applications of Doc2vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see the applications that can use Doc2vec:'
  prefs: []
  type: TYPE_NORMAL
- en: Document clustering can be easily implemented by using Doc2vec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can perform sentiment analysis on larger chunks of text data, and I suppose
    you could consider a very big chunk of text and generate the sentiment output
    for that large chunk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is also used in product recommendation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GloVe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GloVe stands for global vector. GloVe is an unsupervised learning algorithm.
    This algorithm generates vector representations for words. Here, training is performed
    by using an aggregated global word-word co-occurrence matrix and other statistics
    from a corpus, and the resulting representations give you interesting linear substructures
    of the word vector space. So the co-occurrence matrix is the input of GloVe.
  prefs: []
  type: TYPE_NORMAL
- en: 'GloVe uses cosine similarity or the Euclidean distance to get an idea of similar
    words. Glove gives you fresh aspects and proves that if you take the nearest neighbor,
    then you can see such kinds of words that are very rare in terms of their frequent
    usage. GloVe can still capture those rare words in similar clusters. Let''s look
    at the a famous example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, here are the closest words when we have the target word frog:'
  prefs: []
  type: TYPE_NORMAL
- en: Frog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frogs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Toad
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Litoria
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leptodactylidae
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lizard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eleutherodactylus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another example is the words related to the comparative-superlative form clustered
    together, and you can see the following output if you use the visualization tool.
    See *Figure 6.48*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34816d1b-7ecb-4a29-af52-0e6d17b67f4c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.48: Result of GloVe famous example'
  prefs: []
  type: TYPE_NORMAL
- en: 'I''m using the `GloVe` Python library to give you an intuitive practical example
    of GloVe. See the code given at the following GitHub link: [https://github.com/jalajthanaki/NLPython/blob/master/ch6/gloveexample.py](https://github.com/jalajthanaki/NLPython/blob/master/ch6/gloveexample.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start, we need to download the dataset, so execute the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see the snippet of code in *Figure 6.49* and see the output in *Figure
    6.50*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5789625-456b-47e2-82bc-ae3d9990f95b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.49: GloVe code snippet'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the output of the preceding code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41328e38-e1ee-47b4-98af-9d1b828a1494.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.50: Sample output of GloVe'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This exercise is more of a reading exercise for you. You should read the research
    papers on Para2vec, Doc2vec, and GloVe. Apart from this, you can also check whether
    there is any way that you can find vector representation for continuous strings,
    such as a DNA pattern. The main purpose of this exercise is to give you an idea
    of how research work has been done. You can also think of some other aspects of
    vector representation and try to solve the challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Importance of vectorization in deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is more of a discussion with you from my end. As we all know, computers
    can't understand NL directly, so we need to convert our NL output into numerical
    format. We have various word embedding techniques, as well as some basic statistical
    techniques such as indexing, tf-idf, one-hot encoding, and so on. By using all
    these techniques, or some of these techniques, you can convert your text input
    into numerical format. Which techniques you choose totally depends on the NLP
    applications. So, there are two major points behind why we convert NL input to
    numerical format. It is basically done because the computer can only understand
    numerical data, so we have to convert text data to numerical data and computers
    are very good at performing computation on given numerical data. These are two
    major points that come to my mind when we are converting text data.
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand what deep learning is. Here, I want to give you just a brief
    idea about it. Don't worry; we will see more detail in [Chapter 9](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml),
    *Deep Learning for NLU and NLG Problems.* When a neural network is many layers
    deep, it is called **deep neural network**. When we use many-layered deep neural
    networks and use them to develop NLP applications using lots of data and lots
    of computation power, it is called **deep learning**.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's talk about vectorization. Vectorization is a solid mathematical concept
    and it is easy to understand and deal with. Nowadays, Python has a lot of good
    libraries that make our life easier when we want to deal with high dimensional
    vector forms of data. The deep learning paradigm heavily relies on the vectorization
    and matrices concepts, so in order to get a good grip on deep learning, you should
    have knowledge of vectors and matrices. Deep learning applications that deal with
    input data such as video or audio also use vectors. Videos and images are converted
    into the dense vector format, and when talk about text input, word2vec is its
    basic building block for generating vectors from words. Google TensorFlow uses
    word2vec as their basic building block and it uses these concepts and improvises
    the results of Google Machine translation, Google Speech recognition, and Google
    Vision applications. So, vectors and matrices give us a lot of freedom in terms
    of their processing and making sense out of it.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from this, I also need to give you some thoughts. I want you to focus
    on how we can improvise the way we deal with text. No doubt word2vec is one of
    the most simple and efficient approaches for converting words into vector form,
    but I would definitely encourage my readers who are interested in research work
    to extend this concept for their native languages or become creative and contribute
    to building very innovative techniques that will help the NLP community to overcome
    challenges such as word ambiguities. Well, these are all my thoughts for you!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have seen how word2vec can be used to find semantics. The
    simple vectorization techniques help us a lot. We have seen some of the applications
    of it. We have touched upon the technicalities of the word2vec model. I have introduced
    lots of new mathematical, as well as statistical, terms to you in order to give
    you a better understanding of the model. We have converted the word2vec black
    box into the word2vec white box. I have also implemented basic as well as extended
    examples for better understanding. We have used a ton of libraries and APIs to
    develop word2vec models. We have also seen the advantages of having vectorization
    in deep learning. Then, we extended our word2vec understanding and developed the
    concepts of para2vec, doc2vec, and GloVe.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will basically give you an in-depth idea of how rule-based
    techniques are used in order to develop NLP applications and how various NLP applications
    use a very simple, but yet very effective, technique called rules or logic for
    developing basic and effective prototypes for NLP applications. Google use the
    rule-based techniques for their machine translation projects, Apple also use this
    technique, and last but not least, Google used the rule-based system to make an
    early prototype of their self-driving car. We will discuss the rule-based system
    and its architecture. We will also see what the architecture of rule-based NLP
    applications is. I will provide you with a thought process, and by using that
    thought process, you can also make rules for your NLP application. We will implement
    the basic grammar rules and pattern-based rules. We will also develop a basic
    template-based chatbot from scratch.
  prefs: []
  type: TYPE_NORMAL
