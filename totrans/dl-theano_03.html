<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;3.&#xA0;Encoding Word into Vector" id="164MG1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03" class="calibre1"/>Chapter 3. Encoding Word into Vector</h1></div></div></div><p class="calibre8">In the previous chapter, inputs to neural nets were images, that is, vectors of continuous numeric <a id="id149" class="calibre1"/>values, the <span class="strong"><strong class="calibre2">natural language</strong></span> for neural nets. But for many other machine learning fields, inputs may be categorical and discrete.</p><p class="calibre8">In this chapter, we'll present a technique known as embedding, which learns to transform discrete input signals into vectors. Such a representation of inputs is an important first step for compatibility with the rest of neural net processing.</p><p class="calibre8">Such embedding techniques will be illustrated with an example of natural language texts, which are composed of words belonging to a finite vocabulary.</p><p class="calibre8">We will present the different aspects of embedding:</p><div class="book"><ul class="itemizedlist"><li class="listitem">The principles of embedding</li><li class="listitem">The different types of word embedding</li><li class="listitem">One hot encoding versus index encoding</li><li class="listitem">Building a network to translate text into vectors</li><li class="listitem">Training and discovering the properties of embedding spaces</li><li class="listitem">Saving and loading the parameters of a model</li><li class="listitem">Dimensionality reduction for visualization</li><li class="listitem">Evaluating the quality of embeddings</li><li class="listitem">Applications of embedding spaces</li><li class="listitem">Weight tying</li></ul></div></div>

<div class="book" title="Chapter&#xA0;3.&#xA0;Encoding Word into Vector" id="164MG1-ccdadb29edc54339afcb9bdf9350ba6b">
<div class="book" title="Encoding and embedding"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch03lvl1sec32" class="calibre1"/>Encoding and embedding</h1></div></div></div><p class="calibre8">Each <a id="id150" class="calibre1"/>word can <a id="id151" class="calibre1"/>be represented by an index in a vocabulary:</p><div class="mediaobject"><img src="../images/00043.jpeg" alt="Encoding and embedding" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Encoding words is the process of representing each word as a vector. The simplest method <a id="id152" class="calibre1"/>of encoding words is called one-hot or 1-of-K vector representation. In this method, each word is represented as an <span class="strong"><img src="../images/00044.jpeg" alt="Encoding and embedding" class="calibre23"/></span> vector with all 0s and one 1 at <a id="id153" class="calibre1"/>the index of that word in the sorted vocabulary. In this notation, |V| is the size of the vocabulary. Word vectors in this type of encoding for vocabulary {<span class="strong"><strong class="calibre2">King</strong></span>, <span class="strong"><strong class="calibre2">Queen</strong></span>, <span class="strong"><strong class="calibre2">Man</strong></span>, <span class="strong"><strong class="calibre2">Woman</strong></span>, <span class="strong"><strong class="calibre2">Child</strong></span>} appear as in the following example of encoding for the word <span class="strong"><strong class="calibre2">Queen</strong></span>:</p><div class="mediaobject"><img src="../images/00045.jpeg" alt="Encoding and embedding" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">In the one-hot vector representation method, every word is equidistant from the other. However, it fails to preserve any relationship between them and leads to data sparsity. Using word embedding does overcome some of these drawbacks.</p><p class="calibre8">Word embedding is an approach to distributional semantics that represents words as vectors of real numbers. Such representation has useful clustering properties, since it groups together words that are semantically and syntactically similar.</p><p class="calibre8">For example, the words <span class="strong"><strong class="calibre2">seaworld</strong></span> and <span class="strong"><strong class="calibre2">dolphin</strong></span> will be very close in the created space. The main aim of this step is to map each word into a continuous, low-dimensional, and real-valued vector and use it as input to a model such as a <span class="strong"><strong class="calibre2">Recurrent Neural Network</strong></span> (<span class="strong"><strong class="calibre2">RNN</strong></span>), a <span class="strong"><strong class="calibre2">Convolutional Neural Network</strong></span> (<span class="strong"><strong class="calibre2">CNN</strong></span>), and <a id="id154" class="calibre1"/>so on:</p><div class="mediaobject"><img src="../images/00046.jpeg" alt="Encoding and embedding" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Such a <a id="id155" class="calibre1"/>representation is <span class="strong"><strong class="calibre2">dense</strong></span>. We would expect synonyms and interchangeable words to be close in that space.</p><p class="calibre8">In this <a id="id156" class="calibre1"/>chapter, we will present the very popular model of word embedding, Word2Vec, which was initially developed by Mikolov et al. in 2013. Word2Vec <a id="id157" class="calibre1"/>has two different models: <span class="strong"><strong class="calibre2">Continuous Bag of Words</strong></span> (<span class="strong"><strong class="calibre2">CBOW</strong></span>) and <span class="strong"><strong class="calibre2">Skip-gram</strong></span>.</p><p class="calibre8">In the <a id="id158" class="calibre1"/>CBOW method, the goal is to predict a word given a surrounding context. A Skip-gram predicts a surrounding context of words given a single word (see the following figure):</p><div class="mediaobject"><img src="../images/00047.jpeg" alt="Encoding and embedding" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">For this <a id="id159" class="calibre1"/>chapter, we will focus on the CBOW model. We will start by presenting the dataset, then we will explain the idea behind the method. Afterwards, we will <a id="id160" class="calibre1"/>show a simple implementation of it using Theano. Finally, we will end with referring to some applications of word embedding.</p></div></div>
<div class="book" title="Dataset" id="173721-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec33" class="calibre1"/>Dataset</h1></div></div></div><p class="calibre8">Before we <a id="id161" class="calibre1"/>explain the model part, let us start by processing the text corpus by creating the vocabulary and integrating the text with it so that each word is represented as an integer. As a dataset, any text corpus can be used, such as Wikipedia or web articles, or posts from social networks such as Twitter. Frequently used datasets include PTB, text8, BBC, IMDB, and WMT datasets.</p><p class="calibre8">In this chapter, we use the <code class="email">text8</code> corpus. It consists of a pre-processed version of the first 100 million characters from a Wikipedia dump. Let us first download the corpus:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">wget</strong></span> http://mattmahoney.net/dc/text8.zip -O /sharedfiles/text8.gz
<span class="strong"><strong class="calibre2">gzip</strong></span> -d /sharedfiles/text8.gz -f</pre></div><p class="calibre8">Now, we construct the vocabulary and replace the rare words with tokens for <span class="strong"><strong class="calibre2">UNKNOWN</strong></span>. Let us start by reading the data into a list of strings:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Read the data into a list of strings:<div class="informalexample"><pre class="programlisting">words = []
<span class="strong"><strong class="calibre2">with</strong></span> open('data/text8') <span class="strong"><strong class="calibre2">as</strong></span> fin:
  <span class="strong"><strong class="calibre2">for</strong></span> line <span class="strong"><strong class="calibre2">in</strong></span> fin:
    words += [w <span class="strong"><strong class="calibre2">for</strong></span> w <span class="strong"><strong class="calibre2">in</strong></span> line.strip().lower().split()]

<span class="strong"><strong class="calibre2">data_size</strong></span> = len(words)  
<span class="strong"><strong class="calibre2">print</strong></span>('Data size:', data_size)</pre></div><p class="calibre24">From the list of strings, we can now build the dictionary. We start by counting the frequency of the words in the <code class="email">word_freq</code> dictionary. Afterwards, we replace the words that are infrequent, that have a number of ocurrences in the corpus less than <code class="email">max_df</code>, with tokens.</p></li><li class="listitem" value="2">Build the <a id="id162" class="calibre1"/>dictionary and replace rare words with the <code class="email">UNK</code> token:<div class="informalexample"><pre class="programlisting">unkown_token = '&lt;UNK&gt;'
pad_token = '&lt;PAD&gt;' # for padding the context
max_df = 5 # maximum number of freq
word_freq = [[unkown_token, -1], [pad_token, 0]]
word_freq.extend(Counter(words).most_common())
word_freq = OrderedDict(word_freq)
word2idx = {unkown_token: 0, pad_token: 1}
idx2word = {0: unkown_token, 1: pad_token}
idx = 2
<span class="strong"><strong class="calibre2">for</strong></span> w <span class="strong"><strong class="calibre2">in</strong></span> word_freq:
  f = word_freq[w]
  <span class="strong"><strong class="calibre2">if</strong></span> f <span class="strong"><strong class="calibre2">&gt;=</strong></span> max_df:
    word2idx[w] = idx
    idx2word[idx] = w
    idx += 1
  <span class="strong"><strong class="calibre2">else:</strong></span>
    word2idx[w] = 0 # map the rare word into the unkwon token
    word_freq[unkown_token] += 1 # increment the number of unknown tokens

data = [word2idx[w] <span class="strong"><strong class="calibre2">for</strong></span> w <span class="strong"><strong class="calibre2">in</strong></span> words]

<span class="strong"><strong class="calibre2">del</strong></span> words # for reduce mem use

vocabulary_size = len(word_freq)
most_common_words = list(word_freq.items())[:5]
<span class="strong"><strong class="calibre2">print(</strong></span>'Most common words (+UNK):', most_common_words)
<span class="strong"><strong class="calibre2">print(</strong></span>'Sample data:', data[:10], [idx2word[i] <span class="strong"><strong class="calibre2">for</strong></span> i <span class="strong"><strong class="calibre2">in</strong></span> data[:10]])

<span class="strong"><em class="calibre12">Data size: 17005207</em></span>
<span class="strong"><em class="calibre12">Most common words (+UNK): [('&lt;UNK&gt;', 182564), ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]</em></span>
<span class="strong"><em class="calibre12">Sample data: [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']</em></span>
</pre></div></li><li class="listitem" value="3">Now, let us <a id="id163" class="calibre1"/>define the functions of creating the dataset (that is, the contexts and targets):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">def</strong></span> get_sample(data, data_size, word_idx, pad_token, c = 1):

  idx = max(0, word_idx - c)
  context = data[idx:word_idx]
  <span class="strong"><strong class="calibre2">if</strong></span> word_idx + 1 <span class="strong"><strong class="calibre2">&lt;</strong></span> data_size<span class="strong"><strong class="calibre2">:</strong></span>
    context += data[word_idx + 1 : min(data_size, word_idx + c + 1)]
  target = data[word_idx]
  context = [w <span class="strong"><strong class="calibre2">for</strong></span> w <span class="strong"><strong class="calibre2">in</strong></span> context <span class="strong"><strong class="calibre2">if</strong></span> w != target]
  <span class="strong"><strong class="calibre2">if</strong></span> len(context) &gt; 0:
    <span class="strong"><strong class="calibre2">return</strong></span> target, context + (2 * c - len(context)) * [pad_token]
  <span class="strong"><strong class="calibre2">return</strong></span> None, None

<span class="strong"><strong class="calibre2">def</strong></span> get_data_set(data, data_size, pad_token, c=1):
  contexts = []
  targets = []
  <span class="strong"><strong class="calibre2">for</strong></span> i <span class="strong"><strong class="calibre2">in</strong></span> xrange(data_size):
    target, context =  get_sample(data, data_size, i, pad_token, c)
    <span class="strong"><strong class="calibre2">if</strong></span> not target <span class="strong"><strong class="calibre2">is</strong></span> None:
      contexts.append(context)
      targets.append(target)

  <span class="strong"><strong class="calibre2">return</strong></span> np.array(contexts, dtype='int32'), np.array(targets, dtype='int32')</pre></div></li></ol><div class="calibre13"/></div></div>
<div class="book" title="Continuous Bag of Words model"><div class="book" id="181NK2-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec34" class="calibre1"/>Continuous Bag of Words model</h1></div></div></div><p class="calibre8">The design <a id="id164" class="calibre1"/>of the neural network to predict a word given its surrounding context is shown in the following figure:</p><div class="mediaobject"><img src="../images/00048.jpeg" alt="Continuous Bag of Words model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">The input layer receives the context while the output layer predicts the target word. The model we'll use for the CBOW model has three layers: input layer, hidden layer (also called the projection layer or embedding layer), and output layer. In our setting, the vocabulary size is V and the hidden layer size is N. Adjacent units are fully connected.</p><p class="calibre8">The input and the output can be represented either by an index (an integer, 0-dimensional) or a one-hot-encoding vector (1-dimensional). Multiplying with the one-hot-encoding vector <code class="email">v</code> consists simply of taking the j-th row of the embedding matrix:</p><div class="mediaobject"><img src="../images/00049.jpeg" alt="Continuous Bag of Words model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Since the <a id="id165" class="calibre1"/>index representation is more efficient than the one-hot encoding representation in terms of memory usage, and Theano supports indexing symbolic variables, it is preferable to adopt the index representation as much as possible.</p><p class="calibre8">Therefore, input (context) will be 2-dimensional, represented by a matrix, with two dimensions: the batch size and the context length. The output (target) is 1-dimensional, represented by a vector with one dimension: the batch size.</p><p class="calibre8">Let's define the CBOW model:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">import</strong></span> theano
<span class="strong"><strong class="calibre2">import</strong></span> theano.tensor <span class="strong"><strong class="calibre2">as</strong></span> T
<span class="strong"><strong class="calibre2">import</strong></span> numpy as np
<span class="strong"><strong class="calibre2">import</strong></span> math
context = T.imatrix(<span class="strong"><strong class="calibre2">name</strong></span>='context')
target = T.ivector('target')</pre></div><p class="calibre8">The context and target variables are the known parameters of this model. The unknown parameters of the CBOW model are the connection matrix <span class="strong"><img src="../images/00050.jpeg" alt="Continuous Bag of Words model" class="calibre23"/></span>, between the input layer and the hidden layer, and the connection matrix <span class="strong"><img src="../images/00051.jpeg" alt="Continuous Bag of Words model" class="calibre23"/></span>, between the hidden layer and the output layer:</p><div class="informalexample"><pre class="programlisting">vocab_size = len(idx2word)
emb_size = 128
W_in_values = np.asarray(np.random.uniform(-1.0, 1.0, 
	(vocab_size, emb_size)),
<span class="strong"><strong class="calibre2">dtype</strong></span>=theano.config.floatX)

W_out_values = np.asarray(np.random.normal(
	<span class="strong"><strong class="calibre2">scale</strong></span>=1.0 / math.sqrt(emb_size),
   <span class="strong"><strong class="calibre2">size</strong></span>=(emb_size, vocab_size)),
   <span class="strong"><strong class="calibre2">dtype</strong></span>=theano.config.floatX)

W_in = theano.shared(<span class="strong"><strong class="calibre2">value</strong></span>=W_in_values,
                      <span class="strong"><strong class="calibre2">name</strong></span>='W_in',
                      <span class="strong"><strong class="calibre2">borrow</strong></span>=True)

W_out = theano.shared(<span class="strong"><strong class="calibre2">value</strong></span>=W_out_values,
                      <span class="strong"><strong class="calibre2">name</strong></span>='W_out',
                      <span class="strong"><strong class="calibre2">borrow</strong></span>=True)


params = [W_in, W_out]</pre></div><p class="calibre8">Each row of <span class="strong"><img src="../images/00050.jpeg" alt="Continuous Bag of Words model" class="calibre23"/></span> is the N-dimension vector representation <span class="strong"><img src="../images/00052.jpeg" alt="Continuous Bag of Words model" class="calibre23"/></span> of the associated word, <code class="email">i</code>, of the <a id="id166" class="calibre1"/>input layer, where <code class="email">N</code> is the hidden layer size. Given a context, when computing the hidden layer output, the CBOW model takes the average of the vectors of the input context words, and uses the product of the <code class="email">input -&gt; hidden</code> weight matrix and the average vector as the output:</p><div class="mediaobject"><img src="../images/00053.jpeg" alt="Continuous Bag of Words model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Here, <code class="email">C</code> is the number of words in the context, <code class="email">w1, w2, w3,..., wc</code> are the words in the context, and <span class="strong"><img src="../images/00054.jpeg" alt="Continuous Bag of Words model" class="calibre23"/></span> is the input vector of a word <span class="strong"><img src="../images/00055.jpeg" alt="Continuous Bag of Words model" class="calibre23"/></span>. The activation function of the output layer is the softmax layer. Equations 2 and 3 show how we compute the output layer:</p><div class="mediaobject"><img src="../images/00056.jpeg" alt="Continuous Bag of Words model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Here, <span class="strong"><img src="../images/00052.jpeg" alt="Continuous Bag of Words model" class="calibre23"/></span> is the j-th column of the matrix <span class="strong"><img src="../images/00051.jpeg" alt="Continuous Bag of Words model" class="calibre23"/></span> and <code class="email">V</code> is the vocabulary size. In our settings, the vocabulary size is <code class="email">vocab_size</code> and the hidden layer size is <code class="email">emb_size</code>. The loss <a id="id167" class="calibre1"/>function is as follows:</p><div class="mediaobject"><img src="../images/00057.jpeg" alt="Continuous Bag of Words model" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">  (4)</p><p class="calibre8">Now, let us translate equations 1, 2, 3, and 4 in Theano.</p><p class="calibre8">To compute the hidden (projection) layer output: <code class="email">input -&gt; hidden (eq. 1)</code>
</p><div class="informalexample"><pre class="programlisting">h = T.mean(W_in[context], <span class="strong"><strong class="calibre2">axis</strong></span>=1) 

For the hidden -&gt; output layer (eq. 2)
uj = T.dot(h, W_out) </pre></div><p class="calibre8">The softmax activation (eq. 3) :</p><div class="informalexample"><pre class="programlisting">p_target_given_contex = T.nnet.softmax(uj).dimshuffle(1, 0)</pre></div><p class="calibre8">The loss function (eq. 4):</p><div class="informalexample"><pre class="programlisting">loss = -T.mean(T.log(p_target_given_contex)[T.arange(target.shape[0]), target]) </pre></div><p class="calibre8">Update the parameters of the model using SGD:</p><div class="informalexample"><pre class="programlisting">g_params = T.grad(<span class="strong"><strong class="calibre2">cost</strong></span>=loss, <span class="strong"><strong class="calibre2">wrt</strong></span>=params)
updates = [
        (param, param - learning_rate <span class="strong"><strong class="calibre2">*</strong></span> gparam)
        <span class="strong"><strong class="calibre2">for</strong></span> param, gparam <span class="strong"><strong class="calibre2">in</strong></span> <span class="strong"><strong class="calibre2">zip</strong></span>(params, g_params)
]</pre></div><p class="calibre8">Finally, we need to define the training and evaluation functions.</p><p class="calibre8">Let's make the dataset shared to pass it to the GPU. For simplicity, we assume that we have a function called <code class="email">get_data_set</code> that returns the set of targets and its surrounding context:</p><div class="informalexample"><pre class="programlisting">contexts, targets = get_data_set(data, data_size, word2idx[pad_token], <span class="strong"><strong class="calibre2">c</strong></span>=2)

contexts = theano.shared(contexts)
targets = theano.shared(targets)

index = T.lscalar(<span class="strong"><strong class="calibre2">'</strong></span>index')

train_model = theano.function(
    <span class="strong"><strong class="calibre2">inputs</strong></span>=[index],
    <span class="strong"><strong class="calibre2">outputs</strong></span>=[loss],
    <span class="strong"><strong class="calibre2">updates</strong></span>=updates,
    <span class="strong"><strong class="calibre2">givens</strong></span>={
        context: contexts[index * batch_size: (index + 1) <span class="strong"><strong class="calibre2">*</strong></span> batch_size],
        target: targets[index * batch_size: (index + 1) <span class="strong"><strong class="calibre2">*</strong></span> batch_size]
    }
)</pre></div><p class="calibre8">The input <a id="id168" class="calibre1"/>variable of <code class="email">train_model</code> is the index of the batch, since the whole dataset has been transferred in one pass to the GPU thanks to shared variables.</p><p class="calibre8">For validation during training, we evaluate the model using the cosine similarity between a mini batch of examples and all embeddings.</p><p class="calibre8">Let's use a <code class="email">theano</code> variable to place the input to the validation model:</p><div class="informalexample"><pre class="programlisting">valid_samples = T.ivector('valid_samples') </pre></div><p class="calibre8">Normalized word embedding of the validation input:</p><div class="informalexample"><pre class="programlisting">embeddings = params[0]
norm = T.sqrt(T.sum(T.sqr(embeddings), <span class="strong"><strong class="calibre2">axis</strong></span>=1, <span class="strong"><strong class="calibre2">keepdims=True</strong></span>))
normalized_embeddings = W_in / norm


valid_embeddings = normalized_embeddings[valid_samples]</pre></div><p class="calibre8">Similarity is <a id="id169" class="calibre1"/>given by the cosine similarity function:</p><div class="informalexample"><pre class="programlisting">similarity = theano.function([valid_samples], T.dot(valid_embeddings, normalized_embeddings.T))</pre></div></div>
<div class="book" title="Training the model"><div class="book" id="190862-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec35" class="calibre1"/>Training the model</h1></div></div></div><p class="calibre8">Now we can <a id="id170" class="calibre1"/>start training the model. In this example, we chose to train the model using SGD with a batch size of 64 and 100 epochs. To validate the model, we randomly selected 16 words and used the similarity measure as an evaluation metric:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Let's begin training:<div class="informalexample"><pre class="programlisting">valid_size = 16     # Random set of words to evaluate similarity on.
valid_window = 100  # Only pick dev samples in the head of the distribution.
valid_examples = np.array(np.random.choice(valid_window, valid_size, <span class="strong"><strong class="calibre2">replace=False</strong></span>), <span class="strong"><strong class="calibre2">dtype=</strong></span>'int32')

n_epochs = 100
n_train_batches = data_size // batch_size
n_iters = n_epochs <span class="strong"><strong class="calibre2">*</strong></span> n_train_batches
train_loss = np.zeros(n_iters)
average_loss = 0

<span class="strong"><strong class="calibre2">for</strong></span> epoch <span class="strong"><strong class="calibre2">in</strong></span> range(n_epochs):
    <span class="strong"><strong class="calibre2">for</strong></span> minibatch_index <span class="strong"><strong class="calibre2">in</strong></span> range(n_train_batches):

        iteration = minibatch_index + n_train_batches * epoch
        loss = train_model(minibatch_index)
        train_loss[iteration] = loss
        average_loss += loss


        <span class="strong"><strong class="calibre2">if</strong></span> iteration <span class="strong"><strong class="calibre2">%</strong></span> 2000 == 0:

          <span class="strong"><strong class="calibre2">if</strong></span> iteration <span class="strong"><strong class="calibre2">&gt;</strong></span> 0:
            average_loss /= 2000
            # The average loss is an estimate of the loss over the last 2000 batches.
            <span class="strong"><strong class="calibre2">print</strong></span>("Average loss at step ", iteration, ": ", average_loss)
            average_loss = 0  


        # Note that this is expensive (~20% slowdown if computed every 500 steps)
        <span class="strong"><strong class="calibre2">if</strong></span> iteration <span class="strong"><strong class="calibre2">%</strong></span> 10000 == 0:

          sim = similarity(valid_examples)
          <span class="strong"><strong class="calibre2">for</strong></span> i <span class="strong"><strong class="calibre2">in</strong></span> xrange(valid_size):
              valid_word = idx2word[valid_examples[i]]
              top_k = 8 # number of nearest neighbors
              nearest = (-sim[i, :]).argsort()[1:top_k+1]
              log_str = "Nearest to %s:" % valid_word
              <span class="strong"><strong class="calibre2">for</strong></span> k <span class="strong"><strong class="calibre2">in</strong></span> xrange(top_k):
                  close_word = idx2word[nearest[k]]
                  log_str = "%s <span class="strong"><strong class="calibre2">%s</strong></span>," % (log_str, close_word)
              <span class="strong"><strong class="calibre2">print</strong></span>(log_str)</pre></div></li><li class="listitem" value="2">Lastly, let us <a id="id171" class="calibre1"/>create two generic functions that will help us save any model parameters in a reusable <code class="email">utils.py</code> utility file:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">def</strong></span> save_params(outfile, params):
    l = []
    <span class="strong"><strong class="calibre2">for</strong></span> param <span class="strong"><strong class="calibre2">in</strong></span> params:
        l = l + [ param.get_value() ]
    numpy.savez(outfile, <span class="strong"><strong class="calibre2">*</strong></span>l)
    <span class="strong"><strong class="calibre2">print</strong></span>("Saved model parameters to {}.npz".format(outfile))

<span class="strong"><strong class="calibre2">def</strong></span> load_params(path, params):
    npzfile = numpy.load(path+".npz")
    <span class="strong"><strong class="calibre2">for</strong></span> i, param <span class="strong"><strong class="calibre2">in</strong></span> enumerate(params):
        param.set_value( npzfile["arr_" +str(i)] )
    <span class="strong"><strong class="calibre2">print(</strong></span>"Loaded model parameters from {}.npz".format(path))</pre></div></li><li class="listitem" value="3">Running on a GPU, the preceding code prints the following results:<div class="informalexample"><pre class="programlisting">
<span class="strong"><em class="calibre12">Using gpu device 1: Tesla K80 (CNMeM is enabled with initial size: 80.0% of memory, cuDNN 5105)</em></span>
<span class="strong"><em class="calibre12">Data size 17005207</em></span>
<span class="strong"><em class="calibre12">Most common words (+UNK) [('&lt;UNK&gt;', 182565), ('&lt;PAD&gt;', 0), ('the', 1061396), ('of', 593677), ('and', 416629)]</em></span>
<span class="strong"><em class="calibre12">Sample data [5240, 3085, 13, 7, 196, 3, 3138, 47, 60, 157] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']</em></span>
<span class="strong"><em class="calibre12">Average loss at step  0 :  11.2959747314</em></span>
<span class="strong"><em class="calibre12">Average loss at step  2000 :  8.81626828802</em></span>
<span class="strong"><em class="calibre12">Average loss at step  4000 :  7.63789177912</em></span>
<span class="strong"><em class="calibre12">Average loss at step  6000 :  7.40699760973</em></span>
<span class="strong"><em class="calibre12">Average loss at step  8000 :  7.20080085599</em></span>
<span class="strong"><em class="calibre12">Average loss at step  10000 :  6.85602856147</em></span>
<span class="strong"><em class="calibre12">Average loss at step  12000 :  6.88123817992</em></span>
<span class="strong"><em class="calibre12">Average loss at step  14000 :  6.96217652643</em></span>
<span class="strong"><em class="calibre12">Average loss at step  16000 :  6.53794862854</em></span>
<span class="strong"><em class="calibre12">...</em></span>


<span class="strong"><em class="calibre12">Average loss at step  26552000 :  4.52319500107</em></span>
<span class="strong"><em class="calibre12">Average loss at step  26554000 :  4.55709513521</em></span>
<span class="strong"><em class="calibre12">Average loss at step  26556000 :  4.62755958384</em></span>
<span class="strong"><em class="calibre12">Average loss at step  26558000 :  4.6266620369</em></span>
<span class="strong"><em class="calibre12">Average loss at step  26560000 :  4.82731778347</em></span>
<span class="strong"><em class="calibre12">Nearest to system: systems, network, device, unit, controller, schemes, vessel, scheme,</em></span>
<span class="strong"><em class="calibre12">Nearest to up: off, out, alight, forth, upwards, down, ordered, ups,</em></span>
<span class="strong"><em class="calibre12">Nearest to first: earliest, last, second, next, oldest, fourth, third, newest,</em></span>
<span class="strong"><em class="calibre12">Nearest to nine: apq, nineteenth, poz, jyutping, afd, apod, eurocents, zoolander,</em></span>
<span class="strong"><em class="calibre12">Nearest to between: across, within, involving, with, among, concerning, through, from,</em></span>
<span class="strong"><em class="calibre12">Nearest to state: states, provincial, government, nation, gaeltachta, reservation, infirmity, slates,</em></span>
<span class="strong"><em class="calibre12">Nearest to are: were, is, aren, was, include, have, weren, contain,</em></span>
<span class="strong"><em class="calibre12">Nearest to may: might, should, must, can, could, would, will, cannot,</em></span>
<span class="strong"><em class="calibre12">Nearest to zero: hundred, pounders, hadza, cest, bureaus, eight, rang, osr,</em></span>
<span class="strong"><em class="calibre12">Nearest to that: which, where, aurea, kessai, however, unless, but, although,</em></span>
<span class="strong"><em class="calibre12">Nearest to can: could, must, cannot, should, may, will, might, would,</em></span>
<span class="strong"><em class="calibre12">Nearest to s: his, whose, its, castletown, cide, codepoint, onizuka, brooklands,</em></span>
<span class="strong"><em class="calibre12">Nearest to will: would, must, should, could, can, might, shall, may,</em></span>
<span class="strong"><em class="calibre12">Nearest to their: its, his, your, our, her, my, the, whose,</em></span>
<span class="strong"><em class="calibre12">Nearest to but: however, though, although, which, while, whereas, moreover, unfortunately,</em></span>
<span class="strong"><em class="calibre12">Nearest to not: never, indeed, rarely, seldom, almost, hardly, unable, gallaecia,</em></span>
<span class="strong"><em class="calibre12">Saved model parameters to model.npz</em></span>
</pre></div></li></ol><div class="calibre13"/></div><p class="calibre8">Let us <a id="id172" class="calibre1"/>note:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Rare words are updated only a small number of times, while frequent words appear more often in inputs and context windows. Subsampling of frequent words can remedy to this.</li><li class="listitem">All weights are updated in the output embedding, and only a few of them, those corresponding to the words in the context window, are updated positively. Negative sampling can help rebalance the positives and negatives in the update.</li></ul></div></div>
<div class="book" title="Visualizing the learned embeddings" id="19UOO1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec36" class="calibre1"/>Visualizing the learned embeddings</h1></div></div></div><p class="calibre8">Let us <a id="id173" class="calibre1"/>visualize the embedding in a 2D figure in order <a id="id174" class="calibre1"/>to get an understanding of how well they <a id="id175" class="calibre1"/>capture similarity and semantics. For <a id="id176" class="calibre1"/>that purpose, we need to reduce the number <a id="id177" class="calibre1"/>of dimension of the embedding, which is highly dimensional, to two dimensions without altering the structure of <a id="id178" class="calibre1"/>the embeddings.</p><p class="calibre8">Reducing <a id="id179" class="calibre1"/>the number of dimension is called manifold learning, and many different techniques exist, some of them linear, such as <span class="strong"><strong class="calibre2">Principal Component Analysis</strong></span> (<span class="strong"><strong class="calibre2">PCA</strong></span>), <span class="strong"><strong class="calibre2">Independent Component Analysis</strong></span> (<span class="strong"><strong class="calibre2">ICA</strong></span>), <span class="strong"><strong class="calibre2">Linear Discriminant Analysis</strong></span> (<span class="strong"><strong class="calibre2">LDA</strong></span>), and <span class="strong"><strong class="calibre2">Latent Sementic Analysis</strong></span> / <span class="strong"><strong class="calibre2">Indexing</strong></span> (<span class="strong"><strong class="calibre2">LSA</strong></span> / <span class="strong"><strong class="calibre2">LSI</strong></span>), and some are non-linear, such as <span class="strong"><strong class="calibre2">Isomap</strong></span>, <span class="strong"><strong class="calibre2">Locally Linear Embedding</strong></span> (<span class="strong"><strong class="calibre2">LLE</strong></span>), <span class="strong"><strong class="calibre2">Hessian </strong></span>
<a id="id180" class="calibre1"/>
<span class="strong"><strong class="calibre2">Eigenmapping</strong></span>, <span class="strong"><strong class="calibre2">Spectral embedding</strong></span>, <span class="strong"><strong class="calibre2">Local tangent space embedding</strong></span>, <span class="strong"><strong class="calibre2">Multi Dimensional Scaling</strong></span> (<span class="strong"><strong class="calibre2">MDS</strong></span>), and <span class="strong"><strong class="calibre2">t-distributed Stochastic Neighbor Embedding</strong></span> (<span class="strong"><strong class="calibre2">t-SNE</strong></span>).</p><p class="calibre8">To display the word embedding, let us use t-SNE, a great technique adapted to high dimensional data to reveal local structures and clusters, without crowding points together:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Visualize the embeddings:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">def</strong></span> plot_with_labels(low_dim_embs, labels, <span class="strong"><strong class="calibre2">filename</strong></span>='tsne.png'):
  <span class="strong"><strong class="calibre2">assert</strong></span> low_dim_embs.shape[0] &gt;= len(labels), "More labels than embeddings"
  plt.figure(figsize=(18, 18))  #in inches
  <span class="strong"><strong class="calibre2">for</strong></span> i, label <span class="strong"><strong class="calibre2">in</strong></span> enumerate(labels):
    x, y = low_dim_embs[i,:]
    plt.scatter(x, y)
    plt.annotate(label,
                 xy=(x, y),
                 xytext=(5, 2),
                 textcoords='offset points',
                 ha='right',
                 va='bottom')

  plt.savefig(filename)

<span class="strong"><strong class="calibre2">from</strong></span> sklearn.manifold <span class="strong"><strong class="calibre2">import</strong></span> TSNE
<span class="strong"><strong class="calibre2">import</strong></span> matplotlib.pyplot <span class="strong"><strong class="calibre2">as</strong></span> plt

tsne = TSNE(<span class="strong"><strong class="calibre2">perplexity</strong></span>=30, <span class="strong"><strong class="calibre2">n_components</strong></span>=2, <span class="strong"><strong class="calibre2">init</strong></span>='pca', <span class="strong"><strong class="calibre2">n_iter</strong></span>=5000)
plot_only = 500
low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only,:])
labels = [idx2word[i] for i in xrange(plot_only)]
plot_with_labels(low_dim_embs, labels)</pre></div><p class="calibre24">The plotted <a id="id181" class="calibre1"/>map displays the words with similar embeddings close to each other:</p><div class="mediaobject"><img src="../images/00058.jpeg" alt="Visualizing the learned embeddings" class="calibre9"/></div><p class="calibre27"> </p></li></ol><div class="calibre13"/></div></div>
<div class="book" title="Evaluating embeddings &#x2013; analogical reasoning"><div class="book" id="1AT9A2-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec37" class="calibre1"/>Evaluating embeddings – analogical reasoning</h1></div></div></div><p class="calibre8">Analogical <a id="id182" class="calibre1"/>reasoning is a simple and efficient way to evaluate embeddings by predicting syntactic and semantic relationships of the form <span class="strong"><em class="calibre12">a is to b as c is to _?</em></span>, denoted as <span class="strong"><em class="calibre12">a : b → c : ?</em></span>. The task is to identify the held-out fourth word, with only exact word matches deemed correct.</p><p class="calibre8"> For example, the word <span class="strong"><em class="calibre12">woman</em></span> is the best answer to the question <span class="strong"><em class="calibre12">king is to queen as man is to?</em></span>. Assume that <span class="strong"><img src="../images/00059.jpeg" alt="Evaluating embeddings – analogical reasoning" class="calibre23"/></span> is the representation vector for the word <span class="strong"><img src="../images/00060.jpeg" alt="Evaluating embeddings – analogical reasoning" class="calibre23"/></span> normalized to unit norm. Then, we can answer the question <span class="strong"><em class="calibre12">a : b → c : ?</em></span> , by finding the word <span class="strong"><img src="../images/00061.jpeg" alt="Evaluating embeddings – analogical reasoning" class="calibre23"/></span> with the representation closest to:</p><div class="mediaobject"><img src="../images/00062.jpeg" alt="Evaluating embeddings – analogical reasoning" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">According <a id="id183" class="calibre1"/>to cosine similarity:</p><div class="mediaobject"><img src="../images/00063.jpeg" alt="Evaluating embeddings – analogical reasoning" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Now let us implement the analogy prediction function using Theano. First, we need to define the input of the function. The analogy function receives three inputs, which are the word indices of <code class="email">a</code>, <code class="email">b</code>, and <code class="email">c</code>:</p><div class="informalexample"><pre class="programlisting">analogy_a = T.ivector('<span class="strong"><strong class="calibre2">analogy_a</strong></span>')  
analogy_b = T.ivector('<span class="strong"><strong class="calibre2">analogy_b</strong></span>')  
analogy_c = T.ivector('<span class="strong"><strong class="calibre2">analogy_c</strong></span>')</pre></div><p class="calibre8">Then, we need to map each input to the word embedding vector. Each row of <code class="email">a_emb</code>, <code class="email">b_emb</code>, <code class="email">c_emb</code> is a word's embedding vector:</p><div class="informalexample"><pre class="programlisting">a_emb = embeddings[analogy_a]  # a's embs
b_emb = embeddings[analogy_b]  # b's embs
c_emb = embeddings[analogy_c]  # c's embs</pre></div><p class="calibre8">Now we can compute the cosine distance between each target and vocab pair. We expect that d's embedding vectors on the unit hyper-sphere is near: <code class="email">c_emb + (b_emb - a_emb)</code>, which has the shape <code class="email">[bsz, emb_size]</code>. <code class="email">dist</code> has shape [<code class="email">bsz, vocab_size</code>].</p><div class="informalexample"><pre class="programlisting"> dist = T.dot(target, embeddings.T)</pre></div><p class="calibre8">In this example, we consider that the prediction function takes the top four words. Thus, we can define the function in Theano as the following:</p><div class="informalexample"><pre class="programlisting">pred_idx = T.argsort(dist, <span class="strong"><strong class="calibre2">axis</strong></span>=1)[:, -4:]
prediction = theano.function([analogy_a, analogy_b, analogy_c], pred_idx)</pre></div><p class="calibre8">To run the preceding function, we need to load the evaluation data, which is in this example the set of analogy questions defined by Google. Each question contains four words separated by spaces. The first question can be interpreted as <span class="strong"><em class="calibre12">Athens is to Greece as Baghdad is to _?</em></span> and the correct answer should be <span class="strong"><em class="calibre12">Iraq</em></span>:</p><div class="informalexample"><pre class="programlisting">Athens Greece Baghdad Iraq
Athens Greece Bangkok Thailand
Athens Greece Beijing China</pre></div><p class="calibre8">Let us load <a id="id184" class="calibre1"/>the analogy questions using the <code class="email">read_analogies</code> function that is defined in the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">def</strong></span> read_analogies(fname, word2idx):
    """Reads through the analogy question file.
    Returns:
      questions: a [n, 4] numpy array containing the analogy question's
                 word ids.
      questions_skipped: questions skipped due to unknown words.
    """
    questions = []
    questions_skipped = 0
    <span class="strong"><strong class="calibre2">with</strong></span> open(fname, "r") <span class="strong"><strong class="calibre2">as</strong></span> analogy_f:
      <span class="strong"><strong class="calibre2">for</strong></span> line <span class="strong"><strong class="calibre2">in</strong></span> analogy_f:
        <span class="strong"><strong class="calibre2">if</strong></span> line.startswith(":"):  # Skip comments.
          <span class="strong"><strong class="calibre2">continue</strong></span>
        words = line.strip().lower().split(" ")
        ids = [word2idx.get(w.strip()) for w in words]
        <span class="strong"><strong class="calibre2">if</strong></span> None <span class="strong"><strong class="calibre2">in</strong></span> ids <span class="strong"><strong class="calibre2">or</strong></span> len(ids) != 4:
          questions_skipped += 1
        <span class="strong"><strong class="calibre2">else</strong></span>:
          questions.append(np.array(ids))
    <span class="strong"><strong class="calibre2">print</strong></span>("Eval analogy file: ", fname)
    <span class="strong"><strong class="calibre2">print</strong></span>("Questions: ", len(questions))
    <span class="strong"><strong class="calibre2">print</strong></span>("Skipped: ", questions_skipped)

    <span class="strong"><strong class="calibre2">return</strong></span> np.array(questions, <span class="strong"><strong class="calibre2">dtype=</strong></span>np.int32)</pre></div><p class="calibre8">Now, we can run the evaluation model:</p><div class="informalexample"><pre class="programlisting">  """Evaluate analogy questions and reports accuracy."""

  # How many questions we get right at precision@1.
  correct = 0
  analogy_data = read_analogies(args.eval_data, word2idx)
  analogy_questions = analogy_data[:, :3]
  answers = analogy_data[:, 3]
  <span class="strong"><strong class="calibre2">del</strong></span> analogy_data
  total = analogy_questions.shape[0]
  start = 0

  <span class="strong"><strong class="calibre2">while</strong></span> start &lt; total:
    limit = start + 200
    sub_questions = analogy_questions[start:limit, :]
    sub_answers = answers[start:limit]
    idx = prediction(sub_questions[:,0], sub_questions[:,1], sub_questions[:,2])

    start = limit
    <span class="strong"><strong class="calibre2">for</strong></span> question <span class="strong"><strong class="calibre2">in</strong></span> xrange(sub_questions.shape[0]):
      <span class="strong"><strong class="calibre2">for</strong></span> j <span class="strong"><strong class="calibre2">in</strong></span> xrange(4):
        <span class="strong"><strong class="calibre2">if</strong></span> idx[question, j] == sub_answers[question]:
          # Bingo! We predicted correctly. E.g., [italy, rome, france, paris].
          correct += 1
          <span class="strong"><strong class="calibre2">break</strong></span>
        <span class="strong"><strong class="calibre2">elif</strong></span> idx[question, j] <span class="strong"><strong class="calibre2">in</strong></span> sub_questions[question]:
          # We need to skip words already in the question.
          continue
        <span class="strong"><strong class="calibre2">else:</strong></span>
          # The correct label is not the precision@1
          break
  <span class="strong"><strong class="calibre2">print</strong></span>()
  <span class="strong"><strong class="calibre2">print</strong></span>("Eval %4d/%d accuracy = %4.1f%%" % (correct, total,
                                            correct * 100.0 / total))</pre></div><p class="calibre8">This <a id="id185" class="calibre1"/>results in:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><em class="calibre12">Eval analogy file:  questions-words.txt</em></span>
<span class="strong"><em class="calibre12">Questions:  17827</em></span>
<span class="strong"><em class="calibre12">Skipped:  1717</em></span>
<span class="strong"><em class="calibre12">Eval   831/17827 accuracy =  4.7%</em></span>
</pre></div></div>
<div class="book" title="Evaluating embeddings &#x2013; quantitative analysis" id="1BRPS1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec38" class="calibre1"/>Evaluating embeddings – quantitative analysis</h1></div></div></div><p class="calibre8">A few <a id="id186" class="calibre1"/>words might be enough to indicate that the quantitative analysis of embeddings is also possible.</p><p class="calibre8">Some word similarity benchmarks propose human-based distances between concepts: Simlex999 (Hill et al., 2016), Verb-143 (Baker et al., 2014), MEN (Bruni et al., 2014), RareWord (Luong et al., 2013), and MTurk- 771 (Halawi et al., 2012).</p><p class="calibre8">Our similarity distance between embeddings can be compared to these human distances, using Spearman's rank correlation coefficient to quantitatively evaluate the quality of the learned embeddings.</p></div>
<div class="book" title="Application of word embeddings" id="1CQAE1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec39" class="calibre1"/>Application of word embeddings</h1></div></div></div><p class="calibre8">Word <a id="id187" class="calibre1"/>embeddings capture the meaning of the words. They translate a discrete input into an input that can be processed by neural nets.</p><p class="calibre8">Embeddings are the start of many applications linked to language:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Generating texts, as we'll see in the next chapter</li><li class="listitem">Translation systems, where input and target sentences are sequences of words and whose embeddings can be processed by end-to-end neural nets (<a class="calibre1" title="Chapter 8. Translating and Explaining with Encoding – decoding Networks" href="part0083_split_000.html#2F4UM2-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 8</a>, <span class="strong"><em class="calibre12">Translating and Explaining with Encoding – decoding Networks</em></span>)</li><li class="listitem">Sentiment analysis (<a class="calibre1" title="Chapter 5. Analyzing Sentiment with a Bidirectional LSTM" href="part0060_split_000.html#1P71O2-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 5</a>, <span class="strong"><em class="calibre12">Analyzing Sentiment with a Bidirectional LSTM</em></span>)</li><li class="listitem">Zero-shot learning in computer vision; the structure in the word language enables us to find classes for which no training images exist</li><li class="listitem">Image annotation/captioning</li><li class="listitem">Neuro-psychiatry, for which neural nets can predict with 100% accuracy some psychiatric disorders in human beings</li><li class="listitem">Chatbots, or answering questions from a user (<a class="calibre1" title="Chapter 9. Selecting Relevant Inputs or Memories with the Mechanism of Attention" href="part0091_split_000.html#2MP362-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 9</a>, <span class="strong"><em class="calibre12">Selecting Relevant Inputs or Memories with the Mechanism of Attention</em></span>)</li></ul></div><p class="calibre8">As with words, the principle of semantic embedding can be used on any problem with categorical variables (classes of images, sounds, films, and so on), where the learned embedding for the activation of categorical variables can be used as input to neural nets for further classification challenges.</p><p class="calibre8">As language structures our mind, word embeddings help structure or improve the performance of neural net based systems.</p></div>
<div class="book" title="Weight tying" id="1DOR01-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec40" class="calibre1"/>Weight tying</h1></div></div></div><p class="calibre8">Two <a id="id188" class="calibre1"/>weight matrices, <span class="strong"><img src="../images/00050.jpeg" alt="Weight tying" class="calibre23"/></span> and <span class="strong"><img src="../images/00051.jpeg" alt="Weight tying" class="calibre23"/></span> have been used for input or output respectively. While all weights of <span class="strong"><img src="../images/00051.jpeg" alt="Weight tying" class="calibre23"/></span> are updated at every iteration during back propagation, <span class="strong"><img src="../images/00050.jpeg" alt="Weight tying" class="calibre23"/></span>is only updated on the column corresponding to the current training input word.</p><p class="calibre8">
<span class="strong"><strong class="calibre2">Weight tying </strong></span>(<span class="strong"><strong class="calibre2">WT</strong></span>) consists of using only one matrix, W, for input and output embedding. Theano then computes the new derivatives with respect to these new weights and all weights in W are updated at every iteration. Fewer parameters leads to less overfitting.</p><p class="calibre8">In the case of Word2Vec, such a technique does not give better results for a simple reason: in the Word2Vec model, the probability of finding the input word in the context is given as:</p><div class="mediaobject"><img src="../images/00064.jpeg" alt="Weight tying" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">It should be as close to zero but cannot <a id="id189" class="calibre1"/>be zero except if W = 0.</p><p class="calibre8">But in <a id="id190" class="calibre1"/>other applications, such as in <span class="strong"><strong class="calibre2">Neural Network Language Models</strong></span> (<span class="strong"><strong class="calibre2">NNLM</strong></span>) in <a class="calibre1" title="Chapter 4. Generating Text with a Recurrent Neural Net" href="part0051_split_000.html#1GKCM1-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 4</a>, <span class="strong"><em class="calibre12">Generating Text with a Recurrent Neural Net</em></span> and <span class="strong"><strong class="calibre2">Neural Machine Translation</strong></span> (<span class="strong"><strong class="calibre2">NMT</strong></span>) in <a class="calibre1" title="Chapter 8. Translating and Explaining with Encoding – decoding Networks" href="part0083_split_000.html#2F4UM2-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 8</a>, <span class="strong"><em class="calibre12">Translating and Explaining with Encoding-decoding Networks</em></span>), it can be shown [<span class="strong"><em class="calibre12">Using the output embedding to improve the language models</em></span>] that:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Input embeddings are usually worse than output embeddings</li><li class="listitem">WT solves this problem</li><li class="listitem">The common embedding learned with WT is close in quality to the output embedding without WT</li><li class="listitem">Inserting a regularized projection matrix P before the output embedding helps the networks use the same embedding and leads to even better results under WT</li></ul></div></div>
<div class="book" title="Further reading" id="1ENBI1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec41" class="calibre1"/>Further reading</h1></div></div></div><p class="calibre8">Please refer to the following articles:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Efficient Estimation of Word Representations in Vector Space, Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, Jan 2013</li><li class="listitem">Factor-based Compositional Embedding Models, Mo Yu, 2014</li><li class="listitem">Character-level Convolutional Networks for Text Classification, Xiang Zhang, Junbo Zhao, Yann LeCun, 2015</li><li class="listitem">Distributed Representations of Words and Phrases and their Compositionality, Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean, 2013</li><li class="listitem">Using the Output Embedding to Improve Language Models, Ofir Press, Lior Wolf, Aug 2016</li></ul></div></div>
<div class="book" title="Summary" id="1FLS41-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec42" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">This chapter presented a very common way to transform discrete inputs in particular texts into numerical embeddings, in the case of natural language processing.</p><p class="calibre8">The technique to train these word representations with neural networks does not require us to label the data and infers its embedding directly from natural texts. Such training is named <span class="strong"><em class="calibre12">unsupervised learning</em></span>.</p><p class="calibre8">One of the main challenges with deep learning is to convert input and output signals into representations that can be processed by nets, in particular vectors of floats. Then, neural nets give all the tools to process these vectors, to learn, decide, classify, reason, or generate.</p><p class="calibre8">In the next chapters, we'll use these embeddings to work with texts and more advanced neural networks. The first application presented in the next chapter is about automatic text generation.</p></div></body></html>