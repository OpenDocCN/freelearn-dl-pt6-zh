["```py\nThis: {1,0,0,0}\nis: {0,1,0,0}\nan: {0,0,1,0}\n```", "```py\nThis is an: {1,1,1,0}\n```", "```py\nInput --> {1,1,1,0}\nOutput --> {0,0,0,1}\n```", "```py\n#define documents\ndocs = ['this, is','is an']\n# define class labels\nlabels = ['an','example']\n```", "```py\nfrom collections import Counter\ncounts = Counter()\nfor i,review in enumerate(docs+labels):\n      counts.update(review.split())\nwords = sorted(counts, key=counts.get, reverse=True)\nvocab_size=len(words)\nword_to_int = {word: i for i, word in enumerate(words, 1)}\n```", "```py\nprint(word_to_int)\n# {'an': 2, 'example': 4, 'is': 1, 'this': 3}\n```", "```py\nencoded_docs = []\nfor doc in docs:\n      encoded_docs.append([word_to_int[word] for word in doc.split()])\nencoded_labels = []\nfor label in labels:\n      encoded_labels.append([word_to_int[word] for word in label.split()])\nprint('encoded_docs: ',encoded_docs)\nprint('encoded_labels: ',encoded_labels)\n# encoded_docs: [[3, 1], [1, 2]]\n# encoded_labels: [[2], [4]]\n```", "```py\n# pad documents to a max length of 2 words\nmax_length = 2\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='pre')\n```", "```py\none_hot_encoded_labels = to_categorical(encoded_labels, num_classes=5)\nprint(one_hot_encoded_labels)\n# [[0\\. 0\\. 1\\. 0\\. 0.] [0\\. 0\\. 0\\. 0\\. 1.]]\n```", "```py\npadded_docs = padded_docs.reshape(2,2,1)\n```", "```py\n# define the model\nembed_length=1\nmax_length=2\nmodel = Sequential()\nmodel.add(SimpleRNN(1,activation='tanh', return_sequences=False,recurrent_initializer='Zeros',input_shape=(max_length,embed_length),unroll=True))\n```", "```py\nmodel.add(Dense(5, activation='softmax'))\n```", "```py\n# compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n# summarize the model\nprint(model.summary())\n```", "```py\nmodel.fit(padded_docs,np.array(one_hot_encoded_labels),epochs=500)\n```", "```py\nmodel.predict(padded_docs[0].reshape(1,2,1))\n```", "```py\narray([[0.3684635, 0.33566403, 0.61344165, 0.378485, 0.4069949 ]],      dtype=float32)\n```", "```py\nmodel.weights\n[<tf.Variable 'simple_rnn_2/kernel:0' shape=(1, 1) dtype=float32_ref>,\n <tf.Variable 'simple_rnn_2/recurrent_kernel:0' shape=(1, 1) dtype=float32_ref>,\n <tf.Variable 'simple_rnn_2/bias:0' shape=(1,) dtype=float32_ref>,\n <tf.Variable 'dense_2/kernel:0' shape=(1, 5) dtype=float32_ref>,\n <tf.Variable 'dense_2/bias:0' shape=(5,) dtype=float32_ref>]\n```", "```py\nmodel.get_weights()\n```", "```py\npadded_docs[0]\n#array([3, 1], dtype=int32)\n```", "```py\ninput_t0 = 3\n```", "```py\ninput_t0_kernel_bias = input_t0*model.get_weights()[0] + model.get_weights()[2]\n```", "```py\nhidden_layer0_value = np.tanh(input_t0_kernel_bias)\n```", "```py\ninput_t1 = 1\n```", "```py\ninput_t1_kernel_bias = input_t1*model.get_weights()[0] + model.get_weights()[2]\n```", "```py\ninput_t1_recurrent = hidden_layer0_value*model.get_weights()[1]\n```", "```py\ntotal_input_t1 = input_t1_kernel_bias + input_t1_recurrent\n```", "```py\noutput_t1 = np.tanh(total_input_t1)\n```", "```py\nfinal_output = output_t1*model.get_weights()[3] + model.get_weights()[4]\n```", "```py\nnp.exp(final_output)/np.sum(np.exp(final_output))\n# array([[0.3684635, 0.33566403, 0.61344165, 0.378485, 0.40699497]], dtype=float32)\n```", "```py\nfrom keras.layers import Dense, Activation\nfrom keras.layers.recurrent import SimpleRNN\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.cross_validation import train_test_split\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nimport re\nimport pandas as pd\ndata=pd.read_csv('https://www.dropbox.com/s/8yq0edd4q908xqw/airline_sentiment.csv')\ndata.head()\n```", "```py\nimport nltk\nnltk.download('stopwords')\nstop = nltk.corpus.stopwords.words('english')\ndef preprocess(text):\n    text=text.lower()\n    text=re.sub('[^0-9a-zA-Z]+',' ',text)\n    words = text.split()\n    words2=[w for w in words if (w not in stop)]\n    #words3=[ps.stem(w) for w in words]\n    words4=' '.join(words2)\n    return(words4)\ndata['text'] = data['text'].apply(preprocess)\n```", "```py\nfrom collections import Counter\ncounts = Counter()\nfor i,review in enumerate(t['text']):\n    counts.update(review.split())\nwords = sorted(counts, key=counts.get, reverse=True\n```", "```py\nnb_chars = len(words)\nword_to_int = {word: i for i, word in enumerate(words, 1)}\nint_to_word = {i: word for i, word in enumerate(words, 1)}\n```", "```py\nmapped_reviews = []\nfor review in data['text']:\n    mapped_reviews.append([word_to_int[word] for word in review.split()])\n```", "```py\nlength_sent = []\nfor i in range(len(mapped_reviews)):\n      length_sent.append(len(mapped_reviews[i]))\nsequence_length = max(length_sent)\n```", "```py\nfrom keras.preprocessing.sequence import pad_sequences\nX = pad_sequences(maxlen=sequence_length, sequences=mapped_reviews, padding=\"post\", value=0)\n```", "```py\ny=data['airline_sentiment'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30,random_state=42)\ny_train2 = to_categorical(y_train)\ny_test2 = to_categorical(y_test)\n```", "```py\nembedding_vecor_length=32\nmax_review_length=26\nmodel = Sequential()\nmodel.add(Embedding(input_dim=12533, output_dim=32, input_length = 26))\n```", "```py\nmodel.add(SimpleRNN(40, return_sequences=False))\n```", "```py\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\n```", "```py\nmodel.fit(X_train, y_train2, validation_data=(X_test, y_test2), epochs=10, batch_size=32)\n```", "```py\nembed_length=1\nmax_length=2\nmodel = Sequential()\nmodel.add(LSTM(1,activation='tanh',return_sequences=False,\nrecurrent_initializer='Zeros',recurrent_activation='sigmoid',\ninput_shape=(max_length,embed_length),unroll=True))\n```", "```py\nmodel.add(Dense(5, activation='softmax'))\n```", "```py\n# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n# summarize the model\nprint(model.summary())\n```", "```py\nmodel.fit(padded_docs.reshape(2,2,1),np.array(one_hot_encoded_labels),epochs=500)\n```", "```py\nmodel.weights[<tf.Variable 'lstm_19/kernel:0' shape=(1, 4) dtype=float32_ref>,\n <tf.Variable 'lstm_19/recurrent_kernel:0' shape=(1, 4) dtype=float32_ref>,\n <tf.Variable 'lstm_19/bias:0' shape=(4,) dtype=float32_ref>,\n <tf.Variable 'dense_18/kernel:0' shape=(1, 5) dtype=float32_ref>,\n <tf.Variable 'dense_18/bias:0' shape=(5,) dtype=float32_ref>]\n```", "```py\nmodel.get_weights()\n```", "```py\nmodel.predict(padded_docs[0].reshape(1,2,1))\n# array([[0.05610514, 0.11013522, 0.38451442, 0.0529648, 0.39628044]], dtype=float32)\n\n```", "```py\ninput_t0 = 3\ncell_state0 = 0\nforget0 = input_t0*model.get_weights()[0][0][1] + model.get_weights()[2][1]\nforget1 = 1/(1+np.exp(-(forget0)))\n```", "```py\ncell_state1 = forget1 * cell_state0\n```", "```py\ninput_t0_1 = input_t0*model.get_weights()[0][0][0] + model.get_weights()[2][0]\ninput_t0_2 = 1/(1+np.exp(-(input_t0_1)))\n```", "```py\ninput_t0_cell1 = input_t0*model.get_weights()[0][0][2] +model.get_weights()[2][2]\ninput_t0_cell2 = np.tanh(input_t0_cell1)\n```", "```py\ninput_t0_cell3 = input_t0_cell2*input_t0_2\ninput_t0_cell4 = input_t0_cell3 + cell_state1\n```", "```py\noutput_t0_1 = input_t0*model.get_weights()[0][0][3] + model.get_weights()[2][3]\noutput_t0_2 = 1/(1+np.exp(-output_t0_1))\n```", "```py\nhidden_layer_1 = np.tanh(input_t0_cell4)*output_t0_2\n```", "```py\ninput_t1 = 1\ncell_state1 = input_t0_cell4\n```", "```py\nforget21 = hidden_layer_1*model.get_weights()[1][0][1] + model.get_weights()[2][1] + input_t1*model.get_weights()[0][0][1]\nforget_22 = 1/(1+np.exp(-(forget21)))\n```", "```py\ncell_state2 = cell_state1 * forget_22\ninput_t1_1 = input_t1*model.get_weights()[0][0][0] + model.get_weights()[2][0] + hidden_layer_1*model.get_weights()[1][0][0]\ninput_t1_2 = 1/(1+np.exp(-(input_t1_1)))\ninput_t1_cell1 = input_t1*model.get_weights()[0][0][2] + model.get_weights()[2][2]+ hidden_layer_1*model.get_weights()[1][0][2]\ninput_t1_cell2 = np.tanh(input_t1_cell1)\ninput_t1_cell3 = input_t1_cell2*input_t1_2\ninput_t1_cell4 = input_t1_cell3 + cell_state2\n```", "```py\noutput_t1_1 = input_t1*model.get_weights()[0][0][3] + model.get_weights()[2][3]+ hidden_layer_1*model.get_weights()[1][0][3]\noutput_t1_2 = 1/(1+np.exp(-output_t1_1))\nhidden_layer_2 = np.tanh(input_t1_cell4)*output_t1_2\n```", "```py\nfinal_output = hidden_layer_2 * model.get_weights()[3][0] +model.get_weights()[4]\n```", "```py\nnp.exp(final_output)/np.sum(np.exp(final_output))\n```", "```py\n# array([0.05610514, 0.11013523, 0.3845144, 0.05296481, 0.39628044],dtype=float32)\n```", "```py\nembedding_vecor_length=32\nmax_review_length=26\nmodel = Sequential()\nmodel.add(Embedding(input_dim=12533, output_dim=32, input_length = 26))\n```", "```py\nmodel.add(LSTM(40, return_sequences=False))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\n```", "```py\nW = model.layers[1].get_weights()[0]\nU = model.layers[1].get_weights()[1]\nb = model.layers[1].get_weights()[2]\nprint(W.shape,U.shape,b.shape)\n```", "```py\n((32, 160), (40, 160), (160,))\n```", "```py\nunits = 40\nW_i = W[:, :units]\nW_f = W[:, units: units * 2]\nW_c = W[:, units * 2: units * 3]\nW_o = W[:, units * 3:]\n```", "```py\nU_i = U[:, :units]\nU_f = U[:, units: units * 2]\nU_c = U[:, units * 2: units * 3]\nU_o = U[:, units * 3:]\n```", "```py\nb_i = b[:units]\nb_f = b[units: units * 2]\nb_c = b[units * 2: units * 3]\nb_o = b[units * 3:]\n```", "```py\nmodel.fit(X_train, y_train2, validation_data=(X_test, y_test2), epochs=50, batch_size=32)\n```", "```py\nembedding_vecor_length=32\nmax_review_length=26\nmodel = Sequential()\nmodel.add(Embedding(input_dim=12533, output_dim=32, input_length = 26))\nmodel.add(LSTM(40, return_sequences=True))\nmodel.add(LSTM(40, return_sequences=False))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\n```", "```py\nW = model.layers[2].get_weights()[0]\nU = model.layers[2].get_weights()[1]\nb = model.layers[2].get_weights()[2]\nprint(W.shape,U.shape,b.shape)\n```", "```py\n((40, 160), (40, 160), (160,))\n```", "```py\nmodel.fit(X_train, y_train2, validation_data=(X_test, y_test2), epochs=50, batch_size=32)\n```"]