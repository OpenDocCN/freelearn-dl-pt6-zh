["```py\npublic static void main(String[] args) throws Exception {\n       Nd4j.getMemoryManager().setAutoGcWindow(10000);// see more in the FAQ section\n       wordVectors = WordVectorSerializer.loadStaticModel(new File(WORD_VECTORS_PATH)); // Word2vec path   \n       downloadAndExtractData(); // download and extract the dataset\n       networkTrainAndSaver(); // create net, train and save the model\n       networkEvaluator(); // evaluate the model on test set\n       sampleEvaluator(); // evaluate a simple review from text/file.\n}\n```", "```py\npublic static final String WORD_VECTORS_PATH = \"/Downloads/GoogleNews-vectors-negative300.bin.gz\";\n```", "```py\npublic static final String DATA_URL = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\";\n```", "```py\npublic static final String DATA_PATH = FilenameUtils.concat(System.getProperty(\"java.io.tmpdir\"), \"dl4j_w2vSentiment/\");\n```", "```py\npublic static void downloadAndExtractData() throws Exception {\n  //Create directory if required\n  File directory = new File(DATA_PATH);\n\n  if(!directory.exists()) directory.mkdir();\n  //Download file:\n  String archizePath = DATA_PATH + \"aclImdb_v1.tar.gz\";\n  File archiveFile = new File(archizePath);\n  String extractedPath = DATA_PATH + \"aclImdb\";\n  File extractedFile = new File(extractedPath);\n\n  if( !archiveFile.exists() ){\n    System.out.println(\"Starting data download (80MB)...\");\n    FileUtils.copyURLToFile(new URL(DATA_URL), archiveFile);\n    System.out.println(\"Data (.tar.gz file) downloaded to \" + archiveFile.getAbsolutePath());\n\n    //Extract tar.gz file to output directory\n    DataUtilities.extractTarGz(archizePath, DATA_PATH);\n  } else {\n    //Assume if archive (.tar.gz) exists, then data has already been extracted\n    System.out.println(\"Data (.tar.gz file) already exists at \" + archiveFile.getAbsolutePath());\n\n    if( !extractedFile.exists()){\n    //Extract tar.gz file to output directory\n      DataUtilities.extractTarGz(archizePath, DATA_PATH);\n    } else {\n      System.out.println(\"Data (extracted) already exists at \" + extractedFile.getAbsolutePath());\n    }\n  }\n}\n```", "```py\npublic static boolean downloadFile(String remoteUrl, String localPath) throws IOException {\n  boolean downloaded = false;\n\n  if (remoteUrl == null || localPath == null)\n       return downloaded;\n\n  File file = new File(localPath);\n  if (!file.exists()) {\n    file.getParentFile().mkdirs();\n    HttpClientBuilder builder = HttpClientBuilder.create();\n    CloseableHttpClient client = builder.build();\n    try (CloseableHttpResponse response = client.execute(new HttpGet(remoteUrl))) {\n      HttpEntity entity = response.getEntity();\n      if (entity != null) {\n        try (FileOutputStream outstream = new FileOutputStream(file)) {\n          entity.writeTo(outstream);\n          outstream.flush();\n          outstream.close();\n        }\n      }\n    }\n    downloaded = true;\n  }\n  if (!file.exists())\n  throw new IOException(\"File doesn't exist: \" + localPath);\n  return downloaded;\n}\n```", "```py\npublic static void extractTarGz(String inputPath, String outputPath) throws IOException {\n  if (inputPath == null || outputPath == null)\n       return;\n\n  final int bufferSize = 4096;\n  if (!outputPath.endsWith(\"\" + File.separatorChar))\n      outputPath = outputPath + File.separatorChar;\n\n  try (TarArchiveInputStream tais = new TarArchiveInputStream( new GzipCompressorInputStream(new BufferedInputStream(\n                                      new FileInputStream(inputPath))))) {\n    TarArchiveEntry entry;\n    while ((entry = (TarArchiveEntry) tais.getNextEntry()) != null) {\n      if (entry.isDirectory()) {\n        new File(outputPath + entry.getName()).mkdirs();\n      } else {\n        int count;\n        byte data[] = newbyte[bufferSize];\n        FileOutputStream fos = new FileOutputStream(outputPath + entry.getName());\n        BufferedOutputStream dest = new BufferedOutputStream(fos, bufferSize);\n        while ((count = tais.read(data, 0, bufferSize)) != -1) {\n              dest.write(data, 0, count);\n        }\n        dest.close();\n      }\n    }\n  }\n}\n```", "```py\nimport org.apache.commons.compress.archivers.tar.TarArchiveEntry;\nimport org.apache.commons.compress.archivers.tar.TarArchiveInputStream;\nimport org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream;\n```", "```py\nprivate final WordVectors wordVectors;\nprivate final int batchSize;\nprivate final int vectorSize;\nprivate final int truncateLength;\nprivate int cursor = 0;\nprivate final File[] positiveFiles;\nprivate final File[] negativeFiles;\nprivate final TokenizerFactory tokenizerFactory;\n\npublic SentimentDatasetIterator(String dataDirectory, WordVectors wordVectors, \n                                 int batchSize, int truncateLength, boolean train) throws IOException {\n  this.batchSize = batchSize;\n  this.vectorSize = wordVectors.getWordVector(wordVectors.vocab().wordAtIndex(0)).length;\n  File p = new File(FilenameUtils.concat(dataDirectory, \"aclImdb/\" + (train ? \"train\" : \"test\") \n                                         + \"/pos/\") + \"/\");\n  File n = new File(FilenameUtils.concat(dataDirectory, \"aclImdb/\" + (train ? \"train\" : \"test\")\n                                         + \"/neg/\") + \"/\");\n  positiveFiles = p.listFiles();\n  negativeFiles = n.listFiles();\n\n  this.wordVectors = wordVectors;\n  this.truncateLength = truncateLength;\n  tokenizerFactory = new DefaultTokenizerFactory();\n  tokenizerFactory.setTokenPreProcessor(new CommonPreprocessor());\n}\n```", "```py\nList<String> reviews = new ArrayList<>(num);\nboolean[] positive = newboolean[num];\n\nfor(int i=0; i<num && cursor<totalExamples(); i++ ){\n  if(cursor % 2 == 0){\n    //Load positive review\n    int posReviewNumber = cursor / 2;\n    String review = FileUtils.readFileToString(positiveFiles[posReviewNumber]);\n    reviews.add(review);\n    positive[i] = true;\n  } else {\n    //Load negative review\n    int negReviewNumber = cursor / 2;\n    String review = FileUtils.readFileToString(negativeFiles[negReviewNumber]);\n    reviews.add(review);\n    positive[i] = false;\n  }\n  cursor++;\n}\n```", "```py\nList<List<String>> allTokens = new ArrayList<>(reviews.size());\nint maxLength = 0;\n\nfor(String s : reviews){\n  List<String> tokens = tokenizerFactory.create(s).getTokens();\n  List<String> tokensFiltered = new ArrayList<>();\n for(String t : tokens ){\n if(wordVectors.hasWord(t)) tokensFiltered.add(t);\n  }\n  allTokens.add(tokensFiltered);\n  maxLength = Math.*max*(maxLength,tokensFiltered.size());\n}\n```", "```py\nif(maxLength > truncateLength) \n    maxLength = truncateLength;\n```", "```py\nINDArray features = Nd4j.create(newint[]{reviews.size(), vectorSize, maxLength}, 'f');\nINDArray labels = Nd4j.create(newint[]{reviews.size(), 2, maxLength}, 'f');\n```", "```py\nINDArray featuresMask = Nd4j.*zeros*(reviews.size(), maxLength);\nINDArray labelsMask = Nd4j.*zeros*(reviews.size(), maxLength);\n```", "```py\nfor( int i=0; i<reviews.size(); i++ ){\n  List<String> tokens = allTokens.get(i);\n  int seqLength = Math.min(tokens.size(), maxLength);\n  final INDArray vectors = wordVectors.getWordVectors(tokens.subList(0, seqLength)).transpose();\n  features.put(new INDArrayIndex[] {\n      NDArrayIndex.point(i), NDArrayIndex.all(), NDArrayIndex.interval(0, seqLength)\n    }, vectors);\n\n  featuresMask.get(new INDArrayIndex[] {NDArrayIndex.point(i), NDArrayIndex.interval(0,      \n                   seqLength)}).assign(1);\n  int idx = (positive[i] ? 0 : 1);\n  int lastIdx = Math.min(tokens.size(),maxLength);\n\n  labels.putScalar(newint[]{i,idx,lastIdx-1},1.0);\n  labelsMask.putScalar(newint[]{i,lastIdx-1},1.0);\n}\n```", "```py\nreturn new DataSet(features,labels,featuresMask,labelsMask);\n```", "```py\nSentimentDatasetIterator train = new SentimentDatasetIterator(DATA_PATH, wordVectors, \n                                                              batchSize, truncateReviewsToLength, true);\n```", "```py\n// Network hyperparameters: Truncate reviews with length greater than this\nstatic int truncateReviewsToLength = 30;\nstatic int numEpochs = 10; // number of training epochs\nstatic int batchSize = 64; //Number of examples in each minibatch\nstatic int vectorSize = 300; //Size of word vectors in Google Word2Vec\nstatic int seed = 12345; //Seed for reproducibility\nstatic int numClasses = 2; // number of classes to be predicted\nstatic int numHiddenNodes = 256;\n```", "```py\nMultiLayerConfiguration LSTMconf = new NeuralNetConfiguration.Builder()\n     .seed(seed)\n     .updater(new Adam(1e-8)) // Gradient updater with Adam\n     .l2(1e-5) // L2 regularization coefficient for weights\n     .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n     .weightInit(WeightInit.XAVIER)\n     .gradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue)\n     .gradientNormalizationThreshold(1.0)     \n     .trainingWorkspaceMode(WorkspaceMode.SEPARATE).inferenceWorkspaceMode(WorkspaceMode.SEPARATE)\n     .list()\n     .layer(0, new LSTM.Builder()\n           .nIn(vectorSize)\n           .nOut(numHiddenNodes)\n           .activation(Activation.TANH)\n           .build())\n     .layer(1, new LSTM.Builder()\n           .nIn(numHiddenNodes)\n           .nOut(numHiddenNodes)\n           .activation(Activation.TANH)\n           .build())\n     .layer(2, new RnnOutputLayer.Builder()\n          .activation(Activation.SOFTMAX)\n          .lossFunction(LossFunction.XENT)\n          .nIn(numHiddenNodes)\n          .nOut(numClasses)\n          .build())\n    .pretrain(false).backprop(true).build();\n```", "```py\nMultiLayerNetwork model = new MultiLayerNetwork(LSTMconf);\nmodel.init();\n```", "```py\nLayer[] layers = model.getLayers();\nint totalNumParams = 0;\nfor(int i=0; i<layers.length; i++ ){\n  int nParams = layers[i].numParams();\n  System.out.println(\"Number of parameters in layer \" + i + \": \" + nParams);\n  totalNumParams += nParams;\n}\nSystem.out.println(\"Total number of network parameters: \" + totalNumParams);\n\n>>\n Number of parameters in layer 0: 570,368\n Number of parameters in layer 1: 525,312\n Number of parameters in layer 2: 514\n Total number of network parameters: 1,096,194\n```", "```py\nMultiLayerNetwork net = new MultiLayerNetwork(LSTMconf);\nnet.init();\nnet.setListeners(new ScoreIterationListener(1));\nfor (int i = 0; i < numEpochs; i++) {\n  net.fit(train);\n  train.reset();\n  System.out.println(\"Epoch \" + (i+1) + \" finished ...\");\n}\nSystem.out.println(\"Training has been completed\");\n```", "```py\nFile locationToSave = new File(modelPath); //location and file format\nboolean saveUpdater = true; // we save the network updater too\nModelSerializer.writeModel(net, locationToSave, saveUpdater);\n```", "```py\npublic static void networkEvaluator() throws Exception {\n      System.out.println(\"Starting the evaluation ...\");\n      boolean saveUpdater = true;\n\n      //Load the model\n      MultiLayerNetwork restoredModel = ModelSerializer.restoreMultiLayerNetwork(modelPath, saveUpdater);\n      //WordVectors wordVectors = getWord2Vec();\n      SentimentDatasetIterator test = new SentimentDatasetIterator(DATA_PATH, wordVectors, batchSize,   \n                                                                   truncateReviewsToLength, false);\n      Evaluation evaluation = restoredModel.evaluate(test);\n      System.out.println(evaluation.stats());\n      System.out.println(\"----- Evaluation completed! -----\");\n}\n\n>>>\n ==========================Scores========================================\n # of classes: 2\n Accuracy: 0.8632\n Precision: 0.8632\n Recall: 0.8632\n F1 Score: 0.8634\n Precision, recall, and F1: Reported for positive class (class 1 -\"negative\") only\n ========================================================================\n```", "```py\nPredictions labeled as positive classified by model as positive: 10,777 times\n Predictions labeled as positive classified by model as negative: 1,723 times\n Predictions labeled as negative classified by model as positive: 1,696 times\n Predictions labeled as negative classified by model as negative: 10,804 times\n```", "```py\n// Compute Matthews correlation coefficient\nEvaluationAveraging averaging = EvaluationAveraging.*Macro*;\ndouble MCC = eval.matthewsCorrelation(averaging);\nSystem.*out*.println(\"Matthews correlation coefficient: \"+ MCC);\n\n>>\n Matthews's correlation coefficient: 0.22308172619187497\n```", "```py\nSystem.*out*.println(\"Starting the evaluation on sample texts ...\");\nboolean saveUpdater = true;\n\nMultiLayerNetwork restoredModel = ModelSerializer.*restoreMultiLayerNetwork*(*modelPath*, saveUpdater);\nSentimentDatasetIterator test = new SentimentDatasetIterator(*DATA_PATH*, *wordvectors*, *batchSize*, \n                                                             *truncateReviewsToLength*, false);\n```", "```py\nString IMDb_PositiveReview = \"Not only did it only confirm that the film would be unfunny and generic, but \n                              it also managed to give away the ENTIRE movie; and I'm not exaggerating - \n                              every moment, every plot point, every joke is told in the trailer\";\n\nString IMDb_NegativeReview = \"One character is totally annoying with a voice that gives me the feeling of \n                              fingernails on a chalkboard.\";\n\nString Amazon_PositiveReview = \"This phone is very fast with sending any kind of messages and web browsing \n                                is significantly faster than previous phones i have used\";\n\nString Amazon_NegativeReview = \"The one big drawback of the MP3 player is that the buttons on the phone's \n                             front cover that let you pause and skip songs lock out after a few seconds.\";\n\nString Yelp_PositiveReview = \"My side Greek salad with the Greek dressing was so tasty, and the pita and \n                              hummus was very refreshing.\";\n\nString Yelp_NegativeReview = \"Hard to judge whether these sides were good because we were grossed out by \n                              the melted styrofoam and didn't want to eat it for fear of getting sick.\";\n```", "```py\nString[] reviews = {IMDb_PositiveReview, IMDb_NegativeReview, Amazon_PositiveReview, \n                    Amazon_NegativeReview, Yelp_PositiveReview, Yelp_NegativeReview};\n\nString[] sentiments = {\"Positive\", \"Negative\", \"Positive\", \"Negative\", \"Positive\", \"Negative\"};\nMap<String, String> reviewMap = new HashMap<String, String>();\n\nreviewMap.put(reviews[0], sentiments[0]);\nreviewMap.put(reviews[1], sentiments[1]);\nreviewMap.put(reviews[2], sentiments[2]);\nreviewMap.put(reviews[3], sentiments[3]);\n```", "```py\nSystem.out.println(\"Starting the evaluation on sample texts ...\");         \nfor (Map.Entry<String, String> entry : reviewMap.entrySet()) {\n            String text = entry.getKey();\n            String label = entry.getValue();\n\n            INDArray features = test.loadFeaturesFromString(text, truncateReviewsToLength);\n            INDArray networkOutput = restoredModel.output(features);\n\n            int timeSeriesLength = networkOutput.size(2);\n            INDArray probabilitiesAtLastWord = networkOutput.get(NDArrayIndex.point(0), \n                              NDArrayIndex.all(), NDArrayIndex.point(timeSeriesLength - 1));\n\n            System.out.println(\"-------------------------------\");\n            System.out.println(\"\\n\\nProbabilities at last time step: \");\n            System.out.println(\"p(positive): \" + probabilitiesAtLastWord.getDouble(0));\n            System.out.println(\"p(negative): \" + probabilitiesAtLastWord.getDouble(1));\n\n            Boolean flag = false;\n            if(probabilitiesAtLastWord.getDouble(0) > probabilitiesAtLastWord.getDouble(1))\n                flag = true;\n            else\n                flag = false;\n            if (flag == true) {\n                System.out.println(\"The text express a positive sentiment, actually it is \" + label);\n            } else {\n                System.out.println(\"The text express a negative sentiment, actually it is \" + label);\n            }\n        }\n    System.out.println(\"----- Sample evaluation completed! -----\");\n    }\n```", "```py\npublic INDArray loadFeaturesFromString(String reviewContents, int maxLength){\n        List<String> tokens = tokenizerFactory.create(reviewContents).getTokens();\n        List<String> tokensFiltered = new ArrayList<>();\n        for(String t : tokens ){\n            if(wordVectors.hasWord(t)) tokensFiltered.add(t);\n        }\n        int outputLength = Math.max(maxLength,tokensFiltered.size());\n        INDArray features = Nd4j.create(1, vectorSize, outputLength);\n\n        for(int j=0; j<tokens.size() && j<maxLength; j++ ){\n            String token = tokens.get(j);\n            INDArray vector = wordVectors.getWordVectorMatrix(token);\n            features.put(new INDArrayIndex[]{NDArrayIndex.point(0), \n                          NDArrayIndex.all(), NDArrayIndex.point(j)}, vector);\n        }\n        return features;\n    }\n```", "```py\n> Probabilities at last time step:\n p(positive): 0.003569001331925392\n p(negative): 0.9964309930801392\n The text express a negative sentiment, actually, it is Positive\n\np(positive): 0.003569058608263731\n p(negative): 0.9964308738708496\n The text express a negative sentiment, actually, it is Negative\n -------------------------------\n Probabilities at last time step:\n p(positive): 0.003569077467545867\n p(negative): 0.9964308738708496\n The text express a negative sentiment, actually, it is Negative\n\np(positive): 0.003569045104086399\n p(negative): 0.9964308738708496\n The text express a negative sentiment, actually, it is Positive\n -------------------------------\n Probabilities at last time step:\n p(positive): 0.003570008557289839\n p(negative): 0.996429979801178\n The text express a negative sentiment, actually, it is Positive\n\np(positive): 0.0035690285731106997\n p(negative): 0.9964309930801392\n The text express a negative sentiment, actually, it is Negative\n\n----- Sample evaluation completed! -----\n```", "```py\n<properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <java.version>1.8</java.version>\n        <nd4j.backend>nd4j-cuda-9.0-platform</nd4j.backend>\n        <nd4j.version>1.0.0-alpha</nd4j.version>\n        <dl4j.version>1.0.0-alpha</dl4j.version>\n        <datavec.version>1.0.0-alpha</datavec.version>\n        <arbiter.version>1.0.0-alpha</arbiter.version>\n        <logback.version>1.2.3</logback.version>\n </properties>\n```", "```py\n<dependency>\n         <groupId>org.nd4j</groupId>\n         <artifactId>nd4j-cuda-9.0-platform</artifactId>\n         <version>${nd4j.version}</version>\n</dependency>\n```", "```py\n17:03:55.317 [main] INFO org.nd4j.linalg.factory.Nd4jBackend - Loaded [JCublasBackend] backend\n 17:03:55.360 [main] WARN org.reflections.Reflections - given scan urls are empty. set urls in the configuration\n 17:04:06.410 [main] INFO org.nd4j.nativeblas.NativeOpsHolder - Number of threads used for NativeOps: 32\n 17:04:08.118 [main] DEBUG org.nd4j.jita.concurrency.CudaAffinityManager - Manually mapping thread [18] to device [0], out of [1] devices...\n 17:04:08.119 [main] DEBUG org.nd4j.jita.concurrency.CudaAffinityManager - Manually mapping thread [19] to device [0], out of [1] devices...\n 17:04:08.119 [main] DEBUG org.nd4j.jita.concurrency.CudaAffinityManager - Manually mapping thread [20] to device [0], out of [1] devices...\n 17:04:08.119 [main] DEBUG org.nd4j.jita.concurrency.CudaAffinityManager - Manually mapping thread [21] to device [0], out of [1] devices...\n 17:04:08.119 [main] DEBUG org.nd4j.jita.concurrency.CudaAffinityManager - Manually mapping thread [22] to device [0], out of [1] devices...\n 17:04:08.119 [main] DEBUG org.nd4j.jita.concurrency.CudaAffinityManager - Manually mapping thread [23] to device [0], out of [1] devices...\n 17:04:08.123 [main] INFO org.nd4j.nativeblas.Nd4jBlas - Number of threads used for BLAS: 0\n 17:04:08.127 [main] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Backend used: [CUDA]; OS: [Windows 10]\n 17:04:08.127 [main] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Cores: [8]; Memory: [7.0GB];\n 17:04:08.127 [main] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Blas vendor: [CUBLAS]\n 17:04:08.127 [main] INFO org.nd4j.linalg.jcublas.ops.executioner.CudaExecutioner - Device opName: [GeForce GTX 1050]; CC: [6.1]; Total/free memory: [4294967296]\n```", "```py\n<properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <java.version>1.8</java.version>\n        <nd4j.backend>nd4j-cuda-9.0-platform</nd4j.backend>\n        <nd4j.version>1.0.0-alpha</nd4j.version>\n        <dl4j.version>1.0.0-alpha</dl4j.version>\n        <datavec.version>1.0.0-alpha</datavec.version>\n        <arbiter.version>1.0.0-alpha</arbiter.version>\n        <dl4j.spark.version>1.0.0-alpha_spark_2</dl4j.spark.version>\n        <logback.version>1.2.3</logback.version>\n </properties>\n```", "```py\n<dependency>\n      <groupId>org.deeplearning4j</groupId>\n      <artifactId>dl4j-spark_2.11</artifactId>\n      <version>1.0.0-alpha_spark_2</version>\n</dependency>\n```", "```py\npublic static JavaSparkContext *spark*;\nstatic int *batchSizePerWorker* = 16;\n\npublic static JavaSparkContext getJavaSparkContext () {\n                    SparkConf sparkConf = new SparkConf();\n                    sparkConf.set(\"spark.locality.wait\", \"0\");\n                    sparkConf.setMaster(\"local[*]\").setAppName(\"DL4J Spark\");\n *spak* = new JavaSparkContext(sparkConf);\n return *spark*;\n}\n```", "```py\nList<DataSet> trainDataList = new ArrayList<>();\nwhile(train.hasNext()) {\n       trainDataList.add(train.next());\n    }\n```", "```py\nspark = createJavaSparkContext();\n```", "```py\nJavaRDD<DataSet> trainData = *spark*.parallelize(trainDataList);\n```", "```py\nTrainingMaster<?, ?> tm = (TrainingMaster<?, ?>) new ParameterAveragingTrainingMaster\n               .Builder(*batchSizePerWorker*)             \n               .averagingFrequency(5).workerPrefetchNumBatches(2)\n               .batchSizePerWorker(*batchSizePerWorker*).build();\n```", "```py\nSparkDl4jMultiLayer sparkNet = new SparkDl4jMultiLayer(*spark*, LSTMconf, tm);\n```", "```py\nsparkNet.setListeners(Collections.<IterationListener>*singletonList*(new ScoreIterationListener(1)));\nsparkNet.setListeners(new ScoreIterationListener(1));\n```", "```py\nfor (int i = 0; i < *numEpochs*; i++) {\n         sparkNet.fit(trainData);\n         System.*out*.println(\"Epoch \" + (i+1) + \" has been finished ...\");\n       }\n```", "```py\nMultiLayerNetwork outputNetwork = sparkNet.fit(trainData);\n\n//Save the model\nFile locationToSave = new File(*modelPath*);\n\nboolean saveUpdater = true;\nModelSerializer.*writeModel*(outputNetwork, locationToSave, saveUpdater);\n```", "```py\npublic static void extractZipFile(String inputPath, String outputPath) \n               throws IOException { if (inputPath == null || outputPath == null)\n return;\n final int bufferSize = 4096;\n if (!outputPath.endsWith(\"\" + File.*separatorChar*))\n                            outputPath = outputPath + File.*separatorChar*; \n try (ZipArchiveInputStream tais = new ZipArchiveInputStream(new \n                         GzipCompressorInputStream(\n                             new BufferedInputStream(new FileInputStream(inputPath))))) {\n                             ZipArchiveEntry entry;\n while ((entry = (ZipArchiveEntry) tais.getNextEntry()) != null) {\n if (entry.isDirectory()) {\n new File(outputPath + entry.getName()).mkdirs();\n                               } else {\n                                int count; \n                                byte data[] = newbyte[bufferSize];\n                                FileOutputStream fos = new FileOutputStream(outputPath + entry.getName());\n                                BufferedOutputStream dest = new BufferedOutputStream(fos, bufferSize);\n                                while ((count = tais.read(data, 0, bufferSize)) != -1) {\n                                       dest.write(data, 0, count);\n                                       }\n                            dest.close();\n                       }\n                 }\n           }\n}\n```", "```py\nNd4j.getMemoryManager().setAutoGcWindow(10000); // min 10s between calls\n```"]