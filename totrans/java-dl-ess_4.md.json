["```py\npublic static double ReLU(double x) {\n   if(x > 0) {\n       return x;\n   } else {\n       return 0.;\n   }\n}\n```", "```py\npublic static double dReLU(double y) {\n   if(y > 0) {\n       return 1.;\n   } else {\n       return 0.;\n   }\n}\n```", "```py\nif (activation == \"sigmoid\" || activation == null) {\n\n   this.activation = (double x) -> sigmoid(x);\n   this.dactivation = (double x) -> dsigmoid(x);\n\n} else if (activation == \"tanh\") {\n\n   this.activation = (double x) -> tanh(x);\n   this.dactivation = (double x) -> dtanh(x);\n\n} else if (activation == \"ReLU\") {\n\n   this.activation = (double x) -> ReLU(x);\n   this.dactivation = (double x) -> dReLU(x);\n\n} else {\n   throw new IllegalArgumentException(\"activation function not supported\");\n}\n```", "```py\nint[] hiddenLayerSizes = {100, 80};\ndouble pDropout = 0.5;\n```", "```py\npublic Dropout(int nIn, int[] hiddenLayerSizes, int nOut, Random rng, String activation) {\n\n   if (rng == null) rng = new Random(1234);\n\n   if (activation == null) activation = \"ReLU\";\n\n   this.nIn = nIn;\n   this.hiddenLayerSizes = hiddenLayerSizes;\n   this.nOut = nOut;\n   this.nLayers = hiddenLayerSizes.length;\n   this.hiddenLayers = new HiddenLayer[nLayers];\n   this.rng = rng;\n\n   // construct multi-layer\n   for (int i = 0; i < nLayers; i++) {\n       int nIn_;\n       if (i == 0) nIn_ = nIn;\n       else nIn_ = hiddenLayerSizes[i - 1];\n\n       // construct hidden layer\n       hiddenLayers[i] = new HiddenLayer(nIn_, hiddenLayerSizes[i], null, null, rng, activation);\n   }\n\n   // construct logistic layer\n   logisticLayer = new LogisticRegression(hiddenLayerSizes[nLayers - 1], nOut);\n}\n```", "```py\nList<double[][]> layerInputs = new ArrayList<>(nLayers+1);\nlayerInputs.add(X);\n```", "```py\nList<int[][]> dropoutMasks = new ArrayList<>(nLayers);\n```", "```py\n// forward hidden layers\nfor (int layer = 0; layer < nLayers; layer++) {\n\n   double[] x_;  // layer input\n   double[][] Z_ = new double[minibatchSize][hiddenLayerSizes[layer]];\n   int[][] mask_ = new int[minibatchSize][hiddenLayerSizes[layer]];\n\n   for (int n = 0; n < minibatchSize; n++) {\n\n       if (layer == 0) {\n           x_ = X[n];\n       } else {\n           x_ = Z[n];\n       }\n\n       Z_[n] = hiddenLayers[layer].forward(x_);\n       mask_[n] = dropout(Z_[n], pDrouput);  // apply dropout mask to units\n   }\n\n   Z = Z_;\n   layerInputs.add(Z.clone());\n\n   dropoutMasks.add(mask_);\n}\n```", "```py\npublic int[] dropout(double[] z, double p) {\n\n   int size = z.length;\n   int[] mask = new int[size];\n\n   for (int i = 0; i < size; i++) {\n       mask[i] = binomial(1, 1 - p, rng);\n       z[i] *= mask[i]; // apply mask\n   }\n\n   return mask;\n}\n```", "```py\n// forward & backward output layer\nD = logisticLayer.train(Z, T, minibatchSize, learningRate);\n\n// backward hidden layers\nfor (int layer = nLayers - 1; layer >= 0; layer--) {\n\n   double[][] Wprev_;\n\n   if (layer == nLayers - 1) {\n       Wprev_ = logisticLayer.W;\n   } else {\n       Wprev_ = hiddenLayers[layer+1].W;\n   }\n\n   // apply mask to delta as well\n   for (int n = 0; n < minibatchSize; n++) {\n       int[] mask_ = dropoutMasks.get(layer)[n];\n\n       for (int j = 0; j < D[n].length; j++) {\n           D[n][j] *= mask_[j];\n       }\n   }\n\n   D = hiddenLayers[layer].backward(layerInputs.get(layer), layerInputs.get(layer+1), D, Wprev_, minibatchSize, learningRate);\n}\n```", "```py\npublic void pretest(double pDropout) {\n\n   for (int layer = 0; layer < nLayers; layer++) {\n\n       int nIn_, nOut_;\n\n       if (layer == 0) {\n           nIn_ = nIn;\n       } else {\n           nIn_ = hiddenLayerSizes[layer];\n       }\n\n       if (layer == nLayers - 1) {\n           nOut_ = nOut;\n       } else {\n           nOut_ = hiddenLayerSizes[layer+1];\n       }\n\n       for (int j = 0; j < nOut_; j++) {\n           for (int i = 0; i < nIn_; i++) {\n               hiddenLayers[layer].W[j][i] *= 1 - pDropout;\n           }\n       }\n   }\n}\n```", "```py\npublic Integer[] predict(double[] x) {\n\n   double[] z = new double[0];\n\n   for (int layer = 0; layer < nLayers; layer++) {\n\n       double[] x_;\n\n       if (layer == 0) {\n           x_ = x;\n       } else {\n           x_ = z.clone();\n       }\n\n       z = hiddenLayers[layer].forward(x_);\n   }\n\n   return logisticLayer.predict(z);\n}\n```", "```py\nfinal int[] imageSize = {12, 12};\nfinal int channel = 1;\n```", "```py\nint[] nKernels = {10, 20};\nint[][] kernelSizes = { {3, 3}, {2, 2} };\nint[][] poolSizes = { {2, 2}, {2, 2} };\n```", "```py\nint nHidden = 20;\n```", "```py\n// construct convolution + pooling layers\nfor (int i = 0; i < nKernels.length; i++) {\n   int[] size_;\n   int channel_;\n\n   if (i == 0) {\n       size_ = new int[]{imageSize[0], imageSize[1]};\n       channel_ = channel;\n   } else {\n       size_ = new int[]{pooledSizes[i-1][0], pooledSizes[i-1][1]};\n       channel_ = nKernels[i-1];\n   }\n\n   convolvedSizes[i] = new int[]{size_[0] - kernelSizes[i][0] + 1, size_[1] - kernelSizes[i][1] + 1};\n   pooledSizes[i] = new int[]{convolvedSizes[i][0] / poolSizes[i][0], convolvedSizes[i][1] / poolSizes[i][0]};\n\n   convpoolLayers[i] = new ConvolutionPoolingLayer(size_, channel_, nKernels[i], kernelSizes[i], poolSizes[i], convolvedSizes[i], pooledSizes[i], rng, activation);\n}\n```", "```py\nif (W == null) {\n\n   W = new double[nKernel][channel][kernelSize[0]][kernelSize[1]];\n\n   double in_ = channel * kernelSize[0] * kernelSize[1];\n   double out_ = nKernel * kernelSize[0] * kernelSize[1] / (poolSize[0] * poolSize[1]);\n   double w_ = Math.sqrt(6\\. / (in_ + out_));\n\n   for (int k = 0; k < nKernel; k++) {\n       for (int c = 0; c < channel; c++) {\n           for (int s = 0; s < kernelSize[0]; s++) {\n               for (int t = 0; t < kernelSize[1]; t++) {\n                   W[k][c][s][t] = uniform(-w_, w_, rng);\n               }\n           }\n       }\n   }\n}\n\nif (b == null) b = new double[nKernel];\n```", "```py\n// build MLP\nflattenedSize = nKernels[nKernels.length-1] * pooledSizes[pooledSizes.length-1][0] * pooledSizes[pooledSizes.length-1][1];\n\n// construct hidden layer\nhiddenLayer = new HiddenLayer(flattenedSize, nHidden, null, null, rng, activation);\n\n// construct output layer\nlogisticLayer = new LogisticRegression(nHidden, nOut);\n```", "```py\n// cache pre-activated, activated, and downsampled inputs of each convolution + pooling layer for backpropagation\nList<double[][][][]> preActivated_X = new ArrayList<>(nKernels.length);\nList<double[][][][]> activated_X = new ArrayList<>(nKernels.length);\nList<double[][][][]> downsampled_X = new ArrayList<>(nKernels.length+1);  // +1 for input X\ndownsampled_X.add(X);\n\nfor (int i = 0; i < nKernels.length; i++) {\n   preActivated_X.add(new double[minibatchSize][nKernels[i]][convolvedSizes[i][0]][convolvedSizes[i][1]]);\n   activated_X.add(new double[minibatchSize][nKernels[i]][convolvedSizes[i][0]][convolvedSizes[i][1]]);\n   downsampled_X.add(new double[minibatchSize][nKernels[i]][convolvedSizes[i][0]][convolvedSizes[i][1]]);\n}\n```", "```py\n// forward convolution + pooling layers\ndouble[][][] z_ = X[n].clone();\nfor (int i = 0; i < nKernels.length; i++) {\n   z_ = convpoolLayers[i].forward(z_, preActivated_X.get(i)[n], activated_X.get(i)[n]);\n   downsampled_X.get(i+1)[n] = z_.clone();\n}\n```", "```py\npublic double[][][] forward(double[][][] x, double[][][] preActivated_X, double[][][] activated_X) {\n\n   double[][][] z = this.convolve(x, preActivated_X, activated_X);\n   return  this.downsample(z);\n```", "```py\npublic double[][][] convolve(double[][][] x, double[][][] preActivated_X, double[][][] activated_X) {\n\n   double[][][] y = new double[nKernel][convolvedSize[0]][convolvedSize[1]];\n\n   for (int k = 0; k < nKernel; k++) {\n       for (int i = 0; i < convolvedSize[0]; i++) {\n           for(int j = 0; j < convolvedSize[1]; j++) {\n\n               double convolved_ = 0.;\n\n               for (int c = 0; c < channel; c++) {\n                   for (int s = 0; s < kernelSize[0]; s++) {\n                       for (int t = 0; t < kernelSize[1]; t++) {\n                           convolved_ += W[k][c][s][t] * x[c][i+s][j+t];\n                       }\n                   }\n               }\n\n               // cache pre-activated inputs\n               preActivated_X[k][i][j] = convolved_ + b[k];\n               activated_X[k][i][j] = this.activation.apply(preActivated_X[k][i][j]);\n               y[k][i][j] = activated_X[k][i][j];\n           }\n       }\n   }\n\n   return y;\n}\n```", "```py\npublic double[][][] downsample(double[][][] x) {\n\n   double[][][] y = new double[nKernel][pooledSize[0]][pooledSize[1]];\n\n   for (int k = 0; k < nKernel; k++) {\n       for (int i = 0; i < pooledSize[0]; i++) {\n           for (int j = 0; j < pooledSize[1]; j++) {\n\n               double max_ = 0.;\n\n               for (int s = 0; s < poolSize[0]; s++) {\n                   for (int t = 0; t < poolSize[1]; t++) {\n\n                       if (s == 0 && t == 0) {\n                           max_ = x[k][poolSize[0]*i][poolSize[1]*j];\n                           continue;\n                       }\n                       if (max_ < x[k][poolSize[0]*i+s][poolSize[1]*j+t]) {\n                           max_ = x[k][poolSize[0]*i+s][poolSize[1]*j+t];\n                       }\n                   }\n               }\n\n               y[k][i][j] = max_;\n           }\n       }\n   }\n\n   return y;\n}\n```", "```py\n// flatten output to make it input for fully connected MLP\ndouble[] x_ = this.flatten(z_);\nflattened_X[n] = x_.clone();\n```", "```py\n// forward hidden layer\nZ[n] = hiddenLayer.forward(x_);\n```", "```py\n// forward & backward output layer\ndY = logisticLayer.train(Z, T, minibatchSize, learningRate);\n\n// backward hidden layer\ndZ = hiddenLayer.backward(flattened_X, Z, dY, logisticLayer.W, minibatchSize, learningRate);\n\n// backpropagate delta to input layer\nfor (int n = 0; n < minibatchSize; n++) {\n   for (int i = 0; i < flattenedSize; i++) {\n       for (int j = 0; j < nHidden; j++) {\n           dX_flatten[n][i] += hiddenLayer.W[j][i] * dZ[n][j];\n       }\n   }\n\n   dX[n] = unflatten(dX_flatten[n]);  // unflatten delta\n}\n\n// backward convolution + pooling layers\ndC = dX.clone();\nfor (int i = nKernels.length-1; i >= 0; i--) {\n   dC = convpoolLayers[i].backward(downsampled_X.get(i), preActivated_X.get(i), activated_X.get(i), downsampled_X.get(i+1), dC, minibatchSize, learningRate);\n}\n```", "```py\npublic double[][][][] backward(double[][][][] X, double[][][][] preActivated_X, double[][][][] activated_X, double[][][][] downsampled_X, double[][][][] dY, int minibatchSize, double learningRate) {\n\n   double[][][][] dZ = this.upsample(activated_X, downsampled_X, dY, minibatchSize);\n   return this.deconvolve(X, preActivated_X, dZ, minibatchSize, learningRate);\n\n}\n```", "```py\npublic double[][][][] upsample(double[][][][] X, double[][][][] Y, double[][][][] dY, int minibatchSize) {\n\n   double[][][][] dX = new double[minibatchSize][nKernel][convolvedSize[0]][convolvedSize[1]];\n\n   for (int n = 0; n < minibatchSize; n++) {\n\n       for (int k = 0; k < nKernel; k++) {\n           for (int i = 0; i < pooledSize[0]; i++) {\n               for (int j = 0; j < pooledSize[1]; j++) {\n\n                   for (int s = 0; s < poolSize[0]; s++) {\n                       for (int t = 0; t < poolSize[1]; t++) {\n\n                           double d_ = 0.;\n\n                           if (Y[n][k][i][j] == X[n][k][poolSize[0]*i+s][poolSize[1]*j+t]) {\n                               d_ = dY[n][k][i][j];\n                           }\n\n                           dX[n][k][poolSize[0]*i+s][poolSize[1]*j+t] = d_;\n                       }\n                   }\n               }\n           }\n       }\n   }\n\n   return dX;\n}\n```", "```py\n// calc gradients of W, b\nfor (int n = 0; n < minibatchSize; n++) {\n   for (int k = 0; k < nKernel; k++) {\n\n       for (int i = 0; i < convolvedSize[0]; i++) {\n           for (int j = 0; j < convolvedSize[1]; j++) {\n\n               double d_ = dY[n][k][i][j] * this.dactivation.apply(Y[n][k][i][j]);\n\n               grad_b[k] += d_;\n\n               for (int c = 0; c < channel; c++) {\n                   for (int s = 0; s < kernelSize[0]; s++) {\n                       for (int t = 0; t < kernelSize[1]; t++) {\n                           grad_W[k][c][s][t] += d_ * X[n][c][i+s][j+t];\n                       }\n                   }\n               }\n           }\n       }\n   }\n}\n```", "```py\n// update gradients\nfor (int k = 0; k < nKernel; k++) {\n   b[k] -= learningRate * grad_b[k] / minibatchSize;\n\n   for (int c = 0; c < channel; c++) {\n       for (int s = 0; s < kernelSize[0]; s++) {\n           for(int t = 0; t < kernelSize[1]; t++) {\n               W[k][c][s][t] -= learningRate * grad_W[k][c][s][t] / minibatchSize;\n           }\n       }\n   }\n}\n```", "```py\n// calc delta\nfor (int n = 0; n < minibatchSize; n++) {\n   for (int c = 0; c < channel; c++) {\n       for (int i = 0; i < imageSize[0]; i++) {\n           for (int j = 0; j < imageSize[1]; j++) {\n\n               for (int k = 0; k < nKernel; k++) {\n                   for (int s = 0; s < kernelSize[0]; s++) {\n                       for (int t = 0; t < kernelSize[1]; t++) {\n\n                           double d_ = 0.;\n\n                           if (i - (kernelSize[0] - 1) - s >= 0 && j - (kernelSize[1] - 1) - t >= 0) {\n                               d_ = dY[n][k][i-(kernelSize[0]-1)-s][j-(kernelSize[1]-1)-t] * this.dactivation.apply(Y[n][k][i- (kernelSize[0]-1)-s][j-(kernelSize[1]-1)-t]) * W[k][c][s][t];\n                           }\n\n                           dX[n][c][i][j] += d_;\n                       }\n                   }\n               }\n           }\n       }\n   }\n}\n```", "```py\npublic Integer[] predict(double[][][] x) {\n\n   List<double[][][]> preActivated = new ArrayList<>(nKernels.length);\n   List<double[][][]> activated = new ArrayList<>(nKernels.length);\n\n   for (int i = 0; i < nKernels.length; i++) {\n       preActivated.add(new double[nKernels[i]][convolvedSizes[i][0]][convolvedSizes[i][1]]);\n       activated.add(new double[nKernels[i]][convolvedSizes[i][0]][convolvedSizes[i][1]]);\n   }\n\n   // forward convolution + pooling layers\n   double[][][] z = x.clone();\n   for (int i = 0; i < nKernels.length; i++) {\n       z = convpoolLayers[i].forward(z, preActivated.get(i), activated.get(i));\n   }\n\n   // forward MLP\n   return logisticLayer.predict(hiddenLayer.forward(this.flatten(z)));\n}\n```"]