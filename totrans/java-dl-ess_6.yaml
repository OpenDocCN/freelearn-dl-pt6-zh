- en: Chapter 6. Approaches to Practical Applications – Recurrent Neural Networks
    and More
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, you learned quite a lot about deep learning. You should
    now understand the fundamentals of the concepts, theories, and implementations
    of deep neural networks. You also learned that you can experiment with deep learning
    algorithms on various data relatively easily by utilizing a deep learning library.
    The next step is to examine how deep learning can be applied to a broad range
    of other fields and how to utilize it for practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in this chapter, we'll first see how deep learning is actually applied.
    Here, you will see that the actual cases where deep learning is utilized are still
    very few. But why aren't there many cases even though it is such an innovative
    method? What is the problem? Later on, we'll think about the reasons. Furthermore,
    going forward we will also consider which fields we can apply deep learning to
    and will have the chance to apply deep learning and all the related areas of artificial
    intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics covered in this chapter include:'
  prefs: []
  type: TYPE_NORMAL
- en: Image recognition, natural language processing, and the neural networks models
    and algorithms related to them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difficulties of turning deep learning models into practical applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The possible fields where deep learning can be applied, and ideas on how to
    approach these fields
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll explore the potential of this big AI boom, which will lead to ideas and
    hints that you can utilize in deep learning for your research, business, and many
    sorts of activities.
  prefs: []
  type: TYPE_NORMAL
- en: Fields where deep learning is active
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We often hear that research for deep learning has always been ongoing and that's
    a fact. Many corporations, especially large tech companies such as Google, Facebook,
    Microsoft, and IBM, invest huge amounts of money into the research of deep learning,
    and we frequently hear news that some corporation has bought these research groups.
    However, as we look through, deep learning itself has various types of algorithms,
    and fields where these algorithms can be applied. Even so, it is a fact that is
    it not widely known which fields deep learning is utilized in or can be used in.
    Since the word "AI" is so broadly used, people can't properly understand which
    technology is used for which product. Hence, in this section, we will go through
    the fields where people have been trying to adopt deep learning actively for practical
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Image recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The field in which deep learning is most frequently incorporated is image recognition.
    It was Prof. Hinton and his team's invention that led to the term "deep learning."
    Their algorithm recorded the lowest error rates ever in an image recognition competition.
    The continuous research done to improve the algorithm led to even better results.
    Now, image recognition utilizing deep learning has gradually been adopted not
    only for studies, but also for practical applications and products. For example,
    Google utilizes deep learning to auto-generate thumbnails for YouTube or auto-tag
    and search photos in Google Photos. Like these popular products, deep learning
    is mainly applied to image tagging or categorizing and, for example, in the field
    of robotics, it is used for robots to specify things around them.
  prefs: []
  type: TYPE_NORMAL
- en: The reason why we can support these products and this industry is because deep
    learning is more suited to image processing, and this is because it can achieve
    higher precision rates than applications in any other field. Only because the
    precision and recall rate of image recognition is so high does it mean this industry
    has broad potential. An error rate of MNIST image classification is recorded at
    0.21 percent with a deep learning algorithm ([http://cs.nyu.edu/~wanli/dropc/](http://cs.nyu.edu/~wanli/dropc/)),
    and this rate can be no lower than the record for a human ([http://arxiv.org/pdf/0710.2231v1.pdf](http://arxiv.org/pdf/0710.2231v1.pdf)).
    In other words, if you narrow it down to just image recognition, it's nothing
    more than the fact that a machine may overcome a human. Why does only image recognition
    get such high precision while other fields need far more improvement in their
    methods?
  prefs: []
  type: TYPE_NORMAL
- en: One of the reasons is that the structure of feature extractions in deep learning
    is well suited for image data. In deep neural networks, many layers are stacked
    and features are extracted from training data step by step at each layer. Also,
    it can be said that image data is featured as a layered structure. When you look
    at images, you will unconsciously catch brief features first and then look into
    a more detailed feature. Therefore, the inherent property of deep learning feature
    extraction is similar to how an image is perceived and hence we can get an accurate
    realization of the features. Although image recognition with deep learning still
    needs more improvements, especially of how machines can understand images and
    their contents, obtaining high precision by just adopting deep learning to sample
    image data without preprocessing obviously means that deep learning and image
    data are a good match.
  prefs: []
  type: TYPE_NORMAL
- en: The other reason is that people have been working to improve algorithms slowly
    but steadily. For example, in deep learning algorithms, CNN, which can get the
    best precision for image recognition, has been improved every time it faces difficulties/tasks.
    Local receptive fields substituted with kernels of convolutional layers were introduced
    to avoid networks becoming too dense. Also, downsampling methods such as max-pooling
    were invented to avoid the overreaction of networks towards a gap of image location.
    This was originally generated from a trial and error process on how to recognize
    handwritten letters written in a certain frame such as a postal code. As such,
    there are many cases where a new approach is sought to adapt neural networks algorithms
    for practical applications. A complicated model, CNN is also built based on these
    accumulated yet steady improvements. While we don't need feature engineering with
    deep learning, we still need to consider an appropriate approach to solve specific
    problems, that is, we can't build omnipotent models, and this is known as the
    **No Free Lunch Theorem** (**NFLT**) for optimization.
  prefs: []
  type: TYPE_NORMAL
- en: In the image recognition field, the classification accuracy that can be achieved
    by deep learning is extremely high, and it is actually beginning to be used for
    practical applications. However, there should be more fields where deep learning
    can be applied. Images have a close connection to many industries. In the future,
    there will be many cases and many more industries that utilize deep learning.
    In this book, let's think about what industries we can apply image recognition
    to, considering the emergence of deep learning in the next sections.
  prefs: []
  type: TYPE_NORMAL
- en: Natural language processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second most active field, after image recognition, where the research of
    deep learning has progressed is **natural language processing** (**NLP**). The
    research in this field might become the most active going forward. With regard
    to image recognition, the prediction precision we could obtain almost reaches
    the ceiling, as it can perform even better classification than a human could.
    On the other hand, in NLP, it is true that the performance of a model gets a lot
    better thanks to deep learning, but it is also a fact that there are many tasks
    that still need to be solved.
  prefs: []
  type: TYPE_NORMAL
- en: For some products and practical applications, deep learning has already been
    applied. For example, NLP based on deep learning is applied to Google's voice
    search or voice recognition and Google translation. Also, IBM Watson, the cognitive
    computing system that understands and learns natural language and supports human
    decision-making, extracts keywords and entities from tons of documents, and has
    functions to label documents. And these functions are open to the public as the
    Watson API and anyone can utilize it without constraints.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding examples, NLP itself has a broad and varied
    range of types. In terms of fundamental techniques, we have the classification
    of sentence contents, the classification of words, and the specification of word
    meanings. Furthermore, languages such as Chinese or Japanese that don't leave
    a space between words require morphological analysis, which is also another technique
    available in NLP.
  prefs: []
  type: TYPE_NORMAL
- en: NLP contains a lot of things that need to be researched, therefore it needs
    to clarify what its purpose is, what its problems are, and how these problems
    can be solved. What model is the best to use and how to get good precision properly
    are topics that should be examined cautiously. As for image recognition, the CNN
    method was invented by solving tasks that were faced. Now, let's consider what
    approach we can think of and what the difficulties will be respectively for neural
    networks and NLP. Understanding past trial and error processes will be useful
    for research and applications going forward.
  prefs: []
  type: TYPE_NORMAL
- en: Feed-forward neural networks for NLP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The fundamental problem of NLP is "to predict the next word given a specific
    word or words". The problem is too simple, however; if you try to solve it with
    neural networks, then you will soon face several difficulties because documents
    or sentences as sample data using NLP have the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: The length of each sentence is not fixed but variable, and the number of words
    is astronomical
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There can be unforeseen problems such as misspelled words, acronyms, and so
    on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentences are sequential data, and so contain temporal information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why can these features pose a problem? Remember the model structure of general
    neural networks. For training and testing with neural networks, the number of
    neurons in each layer including the input layer needs to be fixed in advance and
    the networks need to be the same size for all the sample data. In the meantime,
    the length of the input data is not fixed and can vary a lot. This means that
    sample data cannot be applied to the model, at least as it is. Classification
    or generation by neural networks cannot be done without adding/amending something
    to this data.
  prefs: []
  type: TYPE_NORMAL
- en: We have to fix the length of input data, and one approach to handle this issue
    is a method that divides a sentence into a chunk of certain words from the beginning
    in order. This method is called **N-gram**. Here, *N* represents the size of each
    item, and an **N-gram** of size 1 is called a **unigram**, size 2 is a **bigram**,
    and size 3 is a **trigram**. When the size is larger, then it is simply called
    with the value of *N*, such as *four-gram*, *five-gram*, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at how N-gram works with NLP. The goal here is to calculate the
    probability of a word ![Feed-forward neural networks for NLP](img/B04779_06_14.jpg)
    given some history ![Feed-forward neural networks for NLP](img/B04779_06_15.jpg);![Feed-forward
    neural networks for NLP](img/B04779_06_16.jpg). We''ll represent a sequence of
    ![Feed-forward neural networks for NLP](img/B04779_06_17.jpg) words as ![Feed-forward
    neural networks for NLP](img/B04779_06_18.jpg). Then, the probability we want
    to compute is ![Feed-forward neural networks for NLP](img/B04779_06_19.jpg), and
    by applying the chain rule of probability to this term, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feed-forward neural networks for NLP](img/B04779_06_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It might look at first glance like these conditional probabilities help us,
    but actually they don''t because we have no way of calculating the exact probability
    of a word following a long sequence of preceding words, ![Feed-forward neural
    networks for NLP](img/B04779_06_21.jpg). Since the structure of a sentence is
    very flexible, we can''t simply utilize sample documents and a corpus to estimate
    the probability. This is where N-gram works. Actually, we have two approaches
    to solve this problem: the original N-gram model and the neural networks model
    based on N-gram. We''ll look at the first one to fully understand how the fields
    of NLP have developed before we dig into neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With N-gram, we don''t compute the probability of a word given its whole history,
    but approximate the history with the last *N* words. For example, the bigram model
    approximates the probability of a word just by the conditional probability of
    the preceding word, ![Feed-forward neural networks for NLP](img/B04779_06_21.jpg),
    and so follows the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feed-forward neural networks for NLP](img/B04779_06_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we can generalize and expand the equation for N-gram. In this case,
    the probability of a word can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feed-forward neural networks for NLP](img/B04779_06_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We get the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feed-forward neural networks for NLP](img/B04779_06_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Just bear in mind that these approximations with N-gram are based on the probabilistic
    model called the **Markov model**, where the probability of a word depends only
    on the previous word.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now what we need to do is estimate these N-gram probabilities, but how do we
    estimate them? One simple way of doing this is called the **maximum likelihood
    estimation** (**MLE**). This method estimates the probabilities by taking counts
    from a corpus and normalizing them. So when we think of a bigram as an example,
    we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feed-forward neural networks for NLP](img/B04779_06_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding formula, ![Feed-forward neural networks for NLP](img/B04779_06_26.jpg)
    denotes the counts of a word or a sequence of words. Since the denominator, that
    is, the sum of all bigram counts starting with a word, ![Feed-forward neural networks
    for NLP](img/B04779_06_27.jpg) is equal to the unigram count of ![Feed-forward
    neural networks for NLP](img/B04779_06_27.jpg), the preceding equation can be
    described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feed-forward neural networks for NLP](img/B04779_06_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Accordingly, we can generalize MLE for N-gram as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feed-forward neural networks for NLP](img/B04779_06_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Although this is a fundamental approach of NLP with N-gram, we now know how
    to compute N-gram probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast to this approach, the neural network models predict the conditional
    probability of a word ![Feed-forward neural networks for NLP](img/B04779_06_30.jpg)
    given a specific history, ![Feed-forward neural networks for NLP](img/B04779_06_31.jpg);
    ![Feed-forward neural networks for NLP](img/B04779_06_32.jpg). One of the models
    of NLP is called the **Neural ** **Network Language Model** (**NLMM**) ([http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)),
    and it can be illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feed-forward neural networks for NLP](img/B04779_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Feed-forward neural networks for NLP](img/B04779_06_17.jpg) is the
    size of the vocabulary, and each word in the vocabulary is an N-dimensional vector
    where only the index of the word is set to 1 and all the other indices to 0\.
    This method of representation is called *1-of-N coding*. The inputs of NLMM are
    the indices of the ![Feed-forward neural networks for NLP](img/B04779_06_33.jpg)
    previous words ![Feed-forward neural networks for NLP](img/B04779_06_34.jpg) (so
    they are *n-grams*). Since the size *N* is typically within the range of 5,000
    to 200,000, input vectors of NLMM are very sparse. Then, each word is mapped to
    the projection layer, for continuous space representation. This linear projection
    (activation) from a discrete to a continuous space is basically a look-up table
    with ![Feed-forward neural networks for NLP](img/B04779_06_35.jpg) entries, where
    ![Feed-forward neural networks for NLP](img/B04779_06_36.jpg) denotes the feature
    dimension. The projection matrix is shared for the different word positions in
    the context, and activates the word vectors to projection layer units ![Feed-forward
    neural networks for NLP](img/B04779_06_37.jpg) with ![Feed-forward neural networks
    for NLP](img/B04779_06_38.jpg). After the projection comes the hidden layer. Since
    the projection layer is in the continuous space, the model structure is just the
    same as the other neural networks from here. So, the activation can be represented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feed-forward neural networks for NLP](img/B04779_06_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Feed-forward neural networks for NLP](img/B04779_06_40.jpg) denotes
    the activation function, ![Feed-forward neural networks for NLP](img/B04779_06_41.jpg)
    the weights between the projection layer and the hidden layer, and ![Feed-forward
    neural networks for NLP](img/B04779_06_42.jpg) the biases of the hidden layer.
    Accordingly, we can get the output units as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feed-forward neural networks for NLP](img/B04779_06_43.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Feed-forward neural networks for NLP](img/B04779_06_44.jpg) denotes
    the weights between the hidden layer and the output layer, and ![Feed-forward
    neural networks for NLP](img/B04779_06_45.jpg) denotes the biases of the output
    layer. The probability of a word *i* given a specific history ![Feed-forward neural
    networks for NLP](img/B04779_06_31.jpg) can then be calculated using the softmax
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feed-forward neural networks for NLP](img/B04779_06_46.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, in NNLM, the model predicts the probability of all the words
    at the same time. Since the model is now described with the standard neural network,
    we can train the model using the standard backpropagation algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: NNLM is one approach of NLP using neural networks with N-gram. Though NNLM solves
    the problem of how to fix the number of inputs, the best *N* can only be found
    by trial and error, and it is the most difficult part of the whole model building
    process. In addition, we have to make sure that we don't put too much weight on
    the temporal information of the inputs here.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning for NLP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Neural networks with N-gram may work with certain cases, but contain some issues,
    such as what n-grams would return the best results, and do n-grams, the inputs
    of the model, still have a context? These are the problems not only of NLP, but
    of all the other fields that have time sequential data such as precipitation,
    stock prices, yearly crop of potatoes, movies, and so on. Since we have such a
    massive amount of this data in the real world, we can't ignore the potential issue.
    But then, how would it be possible to let neural networks be trained with time
    sequential data?
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One of the neural network models that is able to preserve the context of data
    within networks is **recurrent neural** **network** (**RNN**), the model that
    actively studies the evolution of deep learning algorithms. The following is a
    very simple graphical model of RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recurrent neural networks](img/B04779_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The difference between standard neural networks is that RNN has connections
    between hidden layers with respect to time. The input at time ![Recurrent neural
    networks](img/B04779_06_47.jpg) is activated in the hidden layer at time ![Recurrent
    neural networks](img/B04779_06_47.jpg), preserved in the hidden layer, and then
    propagated to the hidden layer at time ![Recurrent neural networks](img/B04779_06_48.jpg)
    with the input at time ![Recurrent neural networks](img/B04779_06_48.jpg). This
    enables the networks to contain the states of past data and reflect them. You
    might think that RNN is rather a dynamic model, but if you unfold the model at
    each time step, you can see that RNN is a static model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recurrent neural networks](img/B04779_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Since the model structure at each time step is the same as in general neural
    networks, you can train this model using the backpropagation algorithm. However,
    you need to consider time relevance when training, and there is a technique called
    **Backpropagation through Time** (**BPTT**) to handle this. In BPTT, the errors
    and gradients of the parameter are backpropagated to the layers of the past:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recurrent neural networks](img/B04779_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Thus, RNN can preserve contexts within the model. Theoretically, the network
    at each time step should consider the whole sequence up to then, but practically,
    time windows with a certain length are often applied to the model to make the
    calculation less complicated or to prevent the vanishing gradient problem and
    the exploding gradient problem. BPTT has enabled training among layers and this
    is why RNN is often considered to be one of the deep neural networks. We also
    have algorithms of deep RNN such as stacked RNN where hidden layers are stacked.
  prefs: []
  type: TYPE_NORMAL
- en: 'RNN has been adapted for NLP, and is actually one of the most successful models
    in this field. The original model optimized for NLP is called the **recurrent
    neural network language model** (**RNNLM**), introduced by Mikolov et al. ([http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)).
    The model architecture can be illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recurrent neural networks](img/B04779_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The network has three layers: an input layer ![Recurrent neural networks](img/B04779_06_49.jpg),
    a hidden layer ![Recurrent neural networks](img/B04779_06_50.jpg), and an output
    layer ![Recurrent neural networks](img/B04779_06_51.jpg). The hidden layer is
    also often called the context layer or the state layer. The value of each layer
    with respect to the time ![Recurrent neural networks](img/B04779_06_47.jpg) can
    be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recurrent neural networks](img/B04779_06_52.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Recurrent neural networks](img/B04779_06_53.jpg) denotes the sigmoid
    function, and ![Recurrent neural networks](img/B04779_06_54.jpg) the softmax function.
    Since the input layer contains the state layer at time ![Recurrent neural networks](img/B04779_06_55.jpg),
    it can reflect the whole context to the network. The model architecture implies
    that RNNLM can look up much broader contexts than feed-forward NNLM, in which
    the length of the context is constrained to *N* (-gram).
  prefs: []
  type: TYPE_NORMAL
- en: 'The whole time and the entire context should be considered while training RNN,
    but as mentioned previously, we often truncate the time length because BPTT requires
    a lot of calculations and often causes the gradient vanishing/exploding problem
    when learning long-term dependencies, hence the algorithm is often called **truncated
    BPTT**. If we unfold RNNLM with respect to time, the model can be illustrated
    as follows (in the figure, the unfolded time ![Recurrent neural networks](img/B04779_06_56.jpg)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recurrent neural networks](img/B04779_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here ![Recurrent neural networks](img/B04779_06_57.jpg) is the label vector
    of the output. Then, the error vector of the output can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recurrent neural networks](img/B04779_06_58.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We get the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recurrent neural networks](img/B04779_06_59.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here ![Recurrent neural networks](img/B04779_06_60.jpg) is the unfolding time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recurrent neural networks](img/B04779_06_61.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding image is the derivative of the activation function of the hidden
    layer. Since we use the sigmoid function here, we get the preceding equation.
    Then, we can get the error of the past as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recurrent neural networks](img/B04779_06_62.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'With these equations, we can now update the weight matrices of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recurrent neural networks](img/B04779_06_63.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Recurrent neural networks](img/B04779_06_64.jpg) is the learning rate.
    What is interesting in RNNLM is that each vector in the matrix shows the difference
    between words after training. This is because ![Recurrent neural networks](img/B04779_06_65.jpg)
    is the matrix that maps each word to a latent space, so after the training, mapped
    word vectors contain the meaning of the words. For example, the vector calculation
    of "king" – "man" + "woman" would return "queen". DL4J supports RNN, so you can
    easily implement this model with the library.
  prefs: []
  type: TYPE_NORMAL
- en: Long short term memory networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Training with the standard RNN requires the truncated BPTT. You might well doubt
    then that BPTT can really train the model enough to reflect the whole context,
    and this is very true. This is why a special kind of RNN, the **long short term
    memory** (**LSTM**) network, was introduced to solve the long-term dependency
    problem. LSTM is rather intimidating, but let's briefly explore the concept of
    LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin with, we have to think about how we can store and tell past information
    in the network. While the gradient exploding problem can be mitigated simply by
    setting a ceiling to the connection, the gradient vanishing problem still needs
    to be deeply considered. One possible approach is to introduce a unit that permanently
    preserves the value of its inputs and its gradient. So, when you look at a unit
    in the hidden layer of standard neural networks, it is simply described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Long short term memory networks](img/B04779_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'There''s nothing special here. Then, by adding a unit below to the network,
    the network can now memorize the past information within the neuron. The neuron
    added here has linear activation and its value is often set to 1\. This neuron,
    or cell, is called **constant error carousel** (**CEC**) because the error stays
    in the neuron like a carousel and won''t vanish. CEC works as a storage cell and
    stores past inputs. This solves the gradient vanishing problem, but raises another
    problem. Since all data propagated through is stocked in the neuron, it probably
    stores noise data as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Long short term memory networks](img/B04779_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This problem can be broken down into two problems: *input weight conflicts*
    and *output weight conflicts*. The key idea of input weight conflicts is to keep
    certain information within the network until it''s necessary; the neuron is to
    be activated only when the relevant information comes, but is not to be activated
    otherwise. Similarly, output weight conflicts can occur in all types of neural
    networks; the value of neurons is to be propagated only when necessary, and not
    to be propagated otherwise. We can''t solve these problems as long as the connection
    between neurons is represented with the weight of the network. Therefore, another
    method or technique of representation is required that controls the propagation
    of inputs and outputs. But how do we do this? The answer is putting units that
    act like "gates" before and behind the CEC, and these are called **input gate**
    and **output gate**, respectively. The graphical model of the gate can be described
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Long short term memory networks](img/B04779_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Ideally, the gate should return the discrete value of 0 or 1 corresponding to
    the input, 0 when the gate is closed and 1 when open, because it is a gate, but
    programmatically, the gate is set to return the value in the range of 0 to 1 so
    that it can be well trained with BPTT.
  prefs: []
  type: TYPE_NORMAL
- en: 'It may seem like we can now put and fetch exact information at an exact time,
    yet another problem still remains. With just two gates, the input gate and output
    gate, memories stored in the CEC can''t be refreshed easily in a few steps. Therefore,
    we need an additional gate that dynamically changes the value of the CEC. To do
    this, we add a **forget gate** to the architecture to control when the memory
    should be erased. The value preserved in the CEC is overridden with a new memory
    when the value of the gate takes a 0 or close to it. With these three gates, a
    unit can now memorize information or contexts of the past, and so it is called
    an **LSTM block** or an **LSTM** **memory block** because it is more of a block
    than a single neuron. The following is a figure that represents an LSTM block:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Long short term memory networks](img/B04779_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The standard LSTM structure was fully explained previously, but there''s a
    technique to get better performance from it, which we''ll explain now. Each gate
    receives connections from the input units and the outputs of all the units in
    LSTM, but there is no direct connection from the CEC. This means we can''t see
    the true hidden state of the network because the output of a block depends so
    much on the output gate; as long as the output gate is closed, none of the gates
    can access the CEC and it is devoid of essential information, which may debase
    the performance of LSTM. One simple yet effective solution is to add connections
    from the CEC to the gates in a block. These are called **peephole connections**,
    and act as standard weighted connections except that no errors are backpropagated
    from the gates through the peephole connections. The peephole connections let
    all gates assume the hidden state even when the output gate is closed. You''ve
    learned a lot of terms now, but as a result, the basic architecture of the whole
    connection can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Long short term memory networks](img/B04779_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For simplicity, a single LSTM block is described in the figure. You might be
    daunted because the preceding model is very intricate. However, when you look
    at the model step by step, you can understand how an LSTM network has figured
    out how to overcome difficulties in NLP. Given an input sequence ![Long short
    term memory networks](img/B04779_06_66.jpg), each network unit can be calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Long short term memory networks](img/B04779_06_67.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding formulas, ![Long short term memory networks](img/B04779_06_68.jpg)
    is the matrix of weights from the input gate to the input, ![Long short term memory
    networks](img/B04779_06_68.jpg) is the one from the forget gate to the input,
    and ![Long short term memory networks](img/B04779_06_69.jpg) is the one from the
    output gate to the input. ![Long short term memory networks](img/B04779_06_70.jpg)
    is the weight matrix from the cell to the input, ![Long short term memory networks](img/B04779_06_71.jpg)
    is the one from the cell to the LSTM output, and ![Long short term memory networks](img/B04779_06_72.jpg)
    is the one from the output to the LSTM output. ![Long short term memory networks](img/B04779_06_73.jpg),
    ![Long short term memory networks](img/B04779_06_74.jpg), and ![Long short term
    memory networks](img/B04779_06_75.jpg) are diagonal weight matrices for peephole
    connections. The ![Long short term memory networks](img/B04779_06_76.jpg) terms
    denote the bias vectors, ![Long short term memory networks](img/B04779_06_77.jpg)
    is the input gate bias vector, ![Long short term memory networks](img/B04779_06_78.jpg)
    is the forget gate bias vector, ![Long short term memory networks](img/B04779_06_79.jpg)
    is the output gate bias vector, ![Long short term memory networks](img/B04779_06_80.jpg)
    is the CEC cell bias vector, and ![Long short term memory networks](img/B04779_06_81.jpg)
    is the output bias vector. Here, ![Long short term memory networks](img/B04779_06_82.jpg)
    and ![Long short term memory networks](img/B04779_06_15.jpg) are activation functions
    of the cell input and cell output. ![Long short term memory networks](img/B04779_06_83.jpg)
    denotes the sigmoid function, and ![Long short term memory networks](img/B04779_06_84.jpg)
    the softmax function. ![Long short term memory networks](img/B04779_06_85.jpg)
    is the element-wise product of the vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'We won''t follow the further math equations in this book because they become
    too complicated just by applying BPTT, but you can try LSTM with DL4J as well
    as RNN. As CNN was developed within the field of image recognition, RNN and LSTM
    have been developed to resolve the issues of NLP that arise one by one. While
    both algorithms are just one approach to get a better performance using NLP and
    still need to be improved, since we are living beings that communicate using languages,
    the development of NLP will certainly lead to technological innovations. For applications
    of LSTM, you can reference *Sequence to Sequence Learning with Neural Networks*
    (Sutskever et al., [http://arxiv.org/pdf/1409.3215v3.pdf](http://arxiv.org/pdf/1409.3215v3.pdf)),
    and for more recent algorithms, you can reference *Grid Long Short-Term Memory*
    (Kalchbrenner et al., [http://arxiv.org/pdf/1507.01526v1.pdf](http://arxiv.org/pdf/1507.01526v1.pdf))
    and *Show, Attend and Tell: Neural Image Caption Generation with Visual Attention*
    (Xu et al., [http://arxiv.org/pdf/1502.03044v2.pdf](http://arxiv.org/pdf/1502.03044v2.pdf)).'
  prefs: []
  type: TYPE_NORMAL
- en: The difficulties of deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning has already got higher precision than humans in the image recognition
    field and has been applied to quite a lot of practical applications. Similarly,
    in the NLP field, many models have been researched. Then, how much deep learning
    is utilized in other fields? Surprisingly, there are still few fields where deep
    learning is successfully utilized. This is because deep learning is indeed innovative
    compared to past algorithms and definitely lets us take a big step towards materializing
    AI; however, it has some problems when used for practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: The first problem is that there are too many model parameters in deep learning
    algorithms. We didn't look in detail when you learned about the theory and implementation
    of algorithms, but actually deep neural networks have many hyper parameters that
    need to be decided compared to the past neural networks or other machine learning
    algorithms. This means we have to go through more trial and error to get high
    precision. Combinations of parameters that define a structure of neural networks,
    such as how many hidden layers are to be set or how many units each hidden layer
    should have, need lots of experiments. Also, the parameters for training and test
    configurations such as the learning rate need to be determined. Furthermore, peculiar
    parameters for each algorithm such as the corruption level in SDA and the size
    of kernels in CNN need additional trial and error. Thus, the great performance
    that deep learning provides is supported by steady parameter-tuning. However,
    people only look at one side of deep learning—that it can get great precision—
    and they tend to forget the hard process required to reach that point. Deep learning
    is not magic.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, deep learning often fails to train and classify data from simple
    problems. The shape of deep neural networks is so deep and complicated that the
    weights can''t be well optimized. In terms of optimization, data quantities are
    also important. This means that deep neural networks require a significant amount
    of time for each training. To sum up, deep learning shows its worth when:'
  prefs: []
  type: TYPE_NORMAL
- en: It solves complicated and hard problems when people have no idea what feature
    they can be classified as
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is sufficient training data to properly optimize deep neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compared to applications that constantly update a model using continuously updated
    data, once a model is built using a large-scale dataset that doesn't change drastically,
    applications that use the model universally are rather well suited for deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, when you look at business fields, you can say that there are more
    cases where the existing machine learning can get better results than using deep
    learning. For example, let's assume we would like to recommend appropriate products
    to users in an EC. In this EC, many users buy a lot of products daily, so purchase
    data is largely updated daily. In this case, do you use deep learning to get high-precision
    classification and recommendations to increase the conversion rates of users'
    purchases using this data? Probably not, because using the existing machine learning
    algorithms such as Naive Bayes, collaborative filtering, SVM, and so on, we can
    get sufficient precision from a practical perspective and can update the model
    and calculate quicker, which is usually more appreciated. This is why deep learning
    is not applied much in business fields. Of course, getting higher precision is
    better in any field, but in reality, higher precision and the necessary calculation
    time are in a trade-off relationship. Although deep learning is significant in
    the research field, it has many hurdles yet to clear considering practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: Besides, deep learning algorithms are not perfect, and they still need many
    improvements to their model itself. For example, RNN, as mentioned earlier, can
    only satisfy either how past information can be reflected to a network or how
    precision can be obtained, although it's contrived with techniques such as LSTM.
    Also, deep learning is still far from the true AI, although it's definitely a
    great technique compared to the past algorithms. Research on algorithms is progressing
    actively, but in the meantime, we need one more breakthrough to spread out and
    infiltrate deep learning into broader society. Maybe this is not just the problem
    of a model. Deep learning is suddenly booming because it is reinforced by huge
    developments in hardware and software. Deep learning is closely related to development
    of the surrounding technology.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, there are still many hurdles to clear before deep learning
    can be applied more practically in the real world, but this is not impossible
    to achieve. It isn't possible to suddenly invent AI to achieve technological singularity,
    but there are some fields and methods where deep learning can be applied right
    away. In the next section, we'll think about what kinds of industries deep learning
    can be utilized in. Hopefully, it will sow the seeds for new ideas in your business
    or research fields.
  prefs: []
  type: TYPE_NORMAL
- en: The approaches to maximizing deep learning possibilities and abilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several approaches to how we can apply deep learning to various industries.
    While it is true that an approach could be different depending on the task or
    purpose, we can briefly categorize the approaches in the following three ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Field-oriented approach**: This utilizes deep learning algorithms or models
    that are already thoroughly researched and can lead to great performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Breakdown-oriented approach**: This replaces the problems to be solved that
    deep learning can apparently be applied to with a different problem where deep
    learning can be well adopted'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output-oriented approach**: This explores new ways of how we express the
    output with deep learning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These approaches are all explained in detail in the following subsections. Each
    approach is divided into its suitable industries or areas where it is not suitable,
    but any of them could be a big hint for your activities going forward. There are
    still very few use cases of deep learning and bias against fields of use, but
    this means there should be many chances to create innovative and new things. Start-ups
    that utilize deep learning have been emerging recently and some of them have already
    achieved success to some extent. You can have a significant impact on the world
    depending on your ideas.
  prefs: []
  type: TYPE_NORMAL
- en: Field-oriented approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This approach doesn't require new techniques or algorithms. There are obviously
    fields that are well suited to the current deep learning techniques, and the concept
    here is to dive into these fields. As explained previously, since deep learning
    algorithms that have been practically studied and developed are mostly in image
    recognition and NLP, we'll explore some fields that can work in great harmony
    with them.
  prefs: []
  type: TYPE_NORMAL
- en: Medicine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Medical fields should be developed by deep learning. Tumors or cancers are detected
    on scanned images. This means nothing more than being able to utilize one of the
    strongest features of deep learning—the technique of image recognition. It is
    possible to dramatically increase precision using deep learning to help with the
    early detection of an illness and identifying the kind of illness. Since CNN can
    be applied to 3D images, 3D scanned images should be able to be analyzed relatively
    easily. By adopting deep learning more in the current medical field, deep learning
    should greatly contribute.
  prefs: []
  type: TYPE_NORMAL
- en: We can also say that deep learning can be significantly useful for the medical
    field in the future. The medical field has been under strict regulations; however,
    there is a movement progressing to ease the regulations in some countries, probably
    because of the recent development of IT and its potential. Therefore, there will
    be opportunities in business for the medical field and IT to have a synergistic
    effect. For example, if telemedicine is more infiltrated, there is the possibility
    that diagnosing or identifying a disease can be done not only by a scanned image,
    but also by an image shown in real time on a display. Also, if electronic charts
    become widespread, it would be easier to analyze medical data using deep learning.
    This is because medical records are compatible with deep learning as they are
    a dataset of texts and images. Then the symptoms of unknown diseases can be found.
  prefs: []
  type: TYPE_NORMAL
- en: Automobiles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can say that the surroundings of running cars are image sequences and text.
    Other cars and views are images and a road sign has text. This means we can also
    utilize deep learning techniques here, and it is possible to reduce the risk of
    accidents by improving driving assistance functions. It can be said that the ultimate
    type of driving assistance is self-driving cars, which is being tackled mainly
    by Google and Tesla. An example that is both famous and fascinating was when George
    Hotz, the first person to hack the iPhone, built a self-driving car in his garage.
    The appearance of the car was introduced in an article by Bloomberg Business ([http://www.bloomberg.com/features/2015-george-hotz-self-driving-car/](http://www.bloomberg.com/features/2015-george-hotz-self-driving-car/)),
    and the following image was included in the article:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Automobiles](img/B04779_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Self-driving cars have been already tested in the U.S., but since other countries
    have different traffic rules and road conditions, this idea requires further studying
    and development before self-driving cars are commonly used worldwide. The key
    to success in this field is in learning and recognizing surrounding cars, people,
    views, and traffic signs, and properly judging how to process them.
  prefs: []
  type: TYPE_NORMAL
- en: In the meantime, we don't have to just focus on utilizing deep learning techniques
    for the actual body of a car. Let's assume we could develop a smartphone app that
    has the same function as we just described, that is, recognizing and classifying
    surrounding images and text. Then, if you just set up the smartphone in your car,
    you could utilize it as a car-navigation app. In addition, for example, it could
    be used as a navigation app for blind people, providing them with good, reliable
    directions.
  prefs: []
  type: TYPE_NORMAL
- en: Advert technologies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Advert (ad) technologies could expand their coverage with deep learning. When
    we say ad technologies, this currently means recommendation or ad networks that
    optimize ad banners or products to be shown. On the other hand, when we say advertising,
    this doesn''t only mean banners or ad networks. There are various kinds of ads
    in the world depending on the type of media, such as TV ads, radio ads, newspaper
    ads, posters, flyers, and so on. We have also digital ad campaigns with YouTube,
    Vine, Facebook, Twitter, Snapchat, and so on. Advertising itself has changed its
    definition and content, but all ads have one thing in common: they consist of
    images and/or language. This means they are fields that deep learning is good
    at. Until now, we could only use user-behavior-based indicators, such as **page
    view** (**PV**), **click through rate** (**CTR**), and **conversion rate** (**CVR**),
    to estimate the effect of an ad, but if we apply deep learning technologies, we
    might be able to analyze the actual content of an ad and autogenerate ads going
    forward. Especially since movies and videos can only be analyzed as a result of
    image recognition and NLP, video recognition, not image recognition, will gather
    momentum besides ad technologies.'
  prefs: []
  type: TYPE_NORMAL
- en: Profession or practice
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Professions such as doctor, lawyer, patent attorney, and accountant are considered
    to be roles that deep learning can replace. For example, if NLP's precision and
    accuracy gets higher, any perusal that requires expertise can be left to a machine.
    As a machine can cover these time-consuming reading tasks, people can focus more
    on high-value tasks. In addition, if a machine classifies past judicial cases
    or medical cases on what disease caused what symptoms and so on, we would be able
    to build an app like Apple's Siri that answers simple questions that usually require
    professional knowledge. Then the machine could handle these professional cases
    to some extent if a doctor or a lawyer is too busy to help in a timely manner.
  prefs: []
  type: TYPE_NORMAL
- en: It's often said that AI takes away a human's job, but personally, this seems
    incorrect. Rather, a machine takes away menial work, which should support humans.
    A software engineer who works on AI programming can be described as having a professional
    job, but this work will also be changed in the future. For example, think about
    a car-related job, where the current work is building standard automobiles, but
    in the future, engineers will be in a position just like pit crews for Formula
    1 cars.
  prefs: []
  type: TYPE_NORMAL
- en: Sports
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning can certainly contribute to sports as well. In the study field
    known as sports science, it has become increasingly important to analyze and examine
    data from sports. As an example, you may know the book or movie *Moneyball*. In
    this film, they hugely increased the win percentage of the team by adopting a
    regression model in baseball. Watching sports itself is very exciting, but on
    the other hand, sport can be seen as a chunk of image sequences and number data.
    Since deep learning is good at identifying features that humans can't find, it
    will become easier to find out why certain players get good scores while others
    don't.
  prefs: []
  type: TYPE_NORMAL
- en: These fields we have mentioned are only a small part of the many fields where
    deep learning is capable of significantly contributing to development. We have
    looked into these fields from the perspective of whether a field has images or
    text, but of course deep learning should also show great performance for simple
    analysis with general number data. It should be possible to apply deep learning
    to various other fields, such as bioinformatics, finance, agriculture, chemistry,
    astronomy, economy, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Breakdown-oriented approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This approach might be similar to the approach considered in traditional machine
    learning algorithms. We already talked about how feature engineering is the key
    to improving precision in machine learning. Now we can say that this feature engineering
    can be divided into the following two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Engineering under the constraints of a machine learning model. The typical case
    is to make inputs discrete or continuous.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering to increase precision by machine learning. This tends to
    rely on the sense of a researcher.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a narrower meaning, feature engineering is considered as the second one,
    and this is the part that deep learning doesn't have to focus on, whereas the
    first one is definitely the important part, even for deep learning. For example,
    it's difficult to predict stock prices using deep learning. Stock prices are volatile
    and it's difficult to define inputs. Besides, how to apply an output value is
    also a difficult problem. Enabling deep learning to handle these inputs and outputs
    is also said to be feature engineering in the wider sense. If there is no limitation
    to the value of original data and/or data you would like to predict, it's difficult
    to insert these datasets into machine learning and deep learning algorithms, including
    neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: However, we can take a certain approach and apply a model to these previous
    problems by breaking down the inputs and/or outputs. In terms of NLP, as explained
    earlier, you might have thought, for example, that it would be impossible to put
    numberless words into features in the first place, but as you already know, we
    can train feed-forward neural networks with words by representing them with sparse
    vectors and combining N-grams into them. Of course, we can not only use neural
    networks, but also other machine learning algorithms such as SVM here. Thus, we
    can cultivate a new field where deep learning hasn't been applied by engineering
    to fit features well into deep learning models. In the meantime, when we focus
    on NLP, we can see that RNN and LSTM were developed to properly resolve the difficulties
    or tasks encountered in NLP. This can be considered as the opposite approach to
    feature engineering because in this case, the problem is solved by breaking down
    a model to fit into features.
  prefs: []
  type: TYPE_NORMAL
- en: Then, how do we do utilize engineering for stock prediction as we just mentioned?
    It's actually not difficult to think of inputs, that is, features. For example,
    if you predict stock prices daily, it's hard to calculate if you use daily stock
    prices as features, but if you use a rate of price change between a day and the
    day before, then it should be much easier to process as the price stays within
    a certain range and the gradients won't explode easily. Meanwhile, what is difficult
    is how to deal with outputs. Stock prices are of course continuous values, hence
    outputs can be various values. This means that in the neural network model where
    the number of units in the output layer is fixed, they can't handle this problem.
    What should we do here—should we give up?! No, wait a minute. Unfortunately, we
    can't predict a stock price itself, but there is an alternative prediction method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the problem is that we can classify stock prices to be predicted into
    infinite patterns. Then, can we make them into limited patterns? Yes, we can.
    Let''s forcibly make them. Think about the most extreme but easy to understand
    case: predicting whether tomorrow''s stock price, strictly speaking a close price,
    is up or down using the data from the stock price up to today. For this case,
    we can show it with a deep learning model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Breakdown-oriented approach](img/B04779_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding image, ![Breakdown-oriented approach](img/B04779_06_86.jpg)
    denotes the open price of a day, ![Breakdown-oriented approach](img/B04779_06_47.jpg);
    ![Breakdown-oriented approach](img/B04779_06_87.jpg) denotes the close price,
    ![Breakdown-oriented approach](img/B04779_06_88.jpg) is the high price, and ![Breakdown-oriented
    approach](img/B04779_06_89.jpg) is the actual price. The features used here are
    mere examples, and need to be fine-tuned when applied to real applications. The
    point here is that replacing the original task with this type of problem enables
    deep neural networks to theoretically classify data. Furthermore, if you classify
    the data by how much it will go up or down, you could make more detailed predictions.
    For example, you could classify data as shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Class | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Class 1 | Up more than 3 percent from the closing price |'
  prefs: []
  type: TYPE_TB
- en: '| Class 2 | Up more than 1~3 percent from the closing price |'
  prefs: []
  type: TYPE_TB
- en: '| Class 3 | Up more than 0~1 percent from the closing price |'
  prefs: []
  type: TYPE_TB
- en: '| Class 4 | Down more than 0~-1 percent from the closing price |'
  prefs: []
  type: TYPE_TB
- en: '| Class 5 | Down more than -1~-3 percent from the closing price |'
  prefs: []
  type: TYPE_TB
- en: '| Class 6 | Down more than -3 percent from the closing price |'
  prefs: []
  type: TYPE_TB
- en: Whether the prediction actually works, in other words whether the classification
    works, is unknown until we examine it, but the fluctuation of stock prices can
    be predicted in quite a narrow range by dividing the outputs into multiple classes.
    Once we can adopt the task into neural networks, then what we should do is just
    examine which model gets better results. In this example, we may apply RNN because
    the stock price is time sequential data. If we look at charts showing the price
    as image data, we can also use CNN to predict the future price.
  prefs: []
  type: TYPE_NORMAL
- en: 'So now we''ve thought about the approach by referring to examples, but to sum
    up in general, we can say that:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature engineering for models**: This is designing inputs or adjusting values
    to fit deep learning models, or enabling classification by setting a limitation
    for the outputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model engineering for features**: This is devising new neural network models
    or algorithms to solve problems in a focused field'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first one needs ideas for the part of designing inputs and outputs to fit
    to a model, whereas the second one needs to take a mathematical approach. Feature
    engineering might be easier to start if you are conscious of making an item prediction-limited.
  prefs: []
  type: TYPE_NORMAL
- en: Output-oriented approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The two previously mentioned approaches are to increase the percentage of correct
    answers for a certain field's task or problem using deep learning. Of course,
    it is essential and the part where deep learning proves its worth; however, increasing
    precision to the ultimate level may not be the only way of utilizing deep learning.
    Another approach is to devise the outputs using deep learning by slightly changing
    the point of view. Let's see what this means.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning is applauded as an innovative approach among researchers and technical
    experts of AI, but the world in general doesn't know much about its greatness
    yet. Rather, they pay attention to what a machine can't do. For example, people
    don't really focus on the image recognition capabilities of MNIST using CNN, which
    generates a lower error rate than humans, but they criticize that a machine can't
    recognize images perfectly. This is probably because people expect a lot when
    they hear and imagine AI. We might need to change this mindset. Let's consider
    DORAEMON, a Japanese national cartoon character who is also famous worldwide—a
    robot who has high intelligence and AI, but often makes silly mistakes. Do we
    criticize him? No, we just laugh it off or take it as a joke and don't get serious.
    Also, think about DUMMY / DUM-E, the robot arm in the movie *Iron Man*. It has
    AI as well, but makes silly mistakes. See, they make mistakes but we still like
    them.
  prefs: []
  type: TYPE_NORMAL
- en: In this way, it might be better to emphasize the point that machines make mistakes.
    Changing the expression part of a user interface could be the trigger for people
    to adopt AI rather than just studying an algorithm the most. Who knows? It's highly
    likely that you can gain the world's interest by thinking of ideas in creative
    fields, not from the perspective of precision. Deep Dream by Google is one good
    example. We can do more exciting things when art or design and deep learning collaborate.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you learned how to utilize deep learning algorithms for practical
    applications. The fields that are well studied are image recognition and NLP.
    While learning about the field of NLP, we looked through two new deep learning
    models: the RNN and LSTM networks, which can be trained with time sequential data.
    The training algorithm used in these models is BPTT. You also learned that there
    are three approaches to make the best of the deep learning ability: the field-oriented
    approach, the breakdown-oriented approach, and the output-oriented approach. Each
    approach has a different angle, and can maximize the possibility for deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: And …congratulations! You've just accomplished the learning part of deep learning
    with Java. Although there are still some models that have not been mentioned yet
    in this book, you can be sure there will be no problem in acquiring and utilizing
    them. The next chapter will introduce some libraries that are implemented with
    other programming languages, so just relax and take a look.
  prefs: []
  type: TYPE_NORMAL
