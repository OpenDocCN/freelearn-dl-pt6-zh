<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deep Neural Networks – Overview</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>In the past few years, we have seen remarkable progress in the field of AI (deep learning). Today, deep learning is the cornerstone of many advanced</span> <span>technological </span><span>applications, from self-driving cars to generating art and music. Scientists aim to help computers to not only understand speech but also speak in natural languages. Deep learning is a kind of machine learning method that is based on learning data representation as opposed to task-specific algorithms. Deep learning enables the computer to build complex concepts from simpler and smaller concepts. For example, a deep learning system recognizes the image of a person by combining lower label edges and corners and combines them into parts of the body in a hierarchical way. The day is not so far away when deep learning will be extended to applications that enable machines to think on their own.</span><br/></p>
<p><span>In this chapter, we will cover the following topics:</span></p>
<ul>
<li>Building blocks of a neural network</li>
<li>Introduction to TensorFlow</li>
<li>Introduction to Keras</li>
<li>Backpropagation</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building blocks of a neural network</h1>
                </header>
            
            <article>
                
<p>A neural network is made up of many artificial neurons. Is it a representation of the brain or is it a mathematical representation of some knowledge? Here, we will simply try to understand how a neural network is used in practice.<span> </span>A <strong>convolutional neural network</strong> (<strong>CNN</strong>) is a very special kind of multi-layer neural network. CNN is designed to recognize visual patterns directly from images with minimal processing. A graphical representation of this network is produced in the<span> </span>following image. <span>The field of neural networks was originally inspired by the goal of modeling biological neural systems, but since then it has branched in different directions and has become a matter of engineering and attaining good results in machine learning tasks.</span></p>
<p>An artificial neuron is a function that takes an input and produces an output. The number of neurons that are used depends on the task at hand. It could be as low as two or as many as several thousands. There are numerous ways of connecting artificial neurons together to create a CNN. One such topology that is commonly used is known as a<span> </span><strong>feed-forward network</strong>:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="208" src="assets/76320048-d832-4c03-b7ad-bdbdb1b4bbd8.png" width="349"/></div>
<p>Each neuron receives inputs from other neurons. The effect of each input line on the neuron is controlled by the weight. The weight can be positive or negative. The entire neural network learns to perform useful computations for recognizing objects by understanding the language. Now, we can connect those neurons into a network known as a feed-forward network. This means that the neurons in each layer feed their output forward to the next layer until we get a final output. This can be written as follows:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" height="13" src="assets/c628f996-d705-4a2b-ba50-c98351f1d30c.png" width="179"/></div>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" height="48" src="assets/3adae030-df62-40b2-ac2a-e99b0f921708.png" width="262"/></div>
<p>The preceding forward-propagating neuron can be implemented as follows:</p>
<pre>import numpy as np<br/>import math<br/><br/><br/>class Neuron(object):<br/>    def __init__(self):<br/>        self.weights = np.array([1.0, 2.0])<br/>        self.bias = 0.0<br/>    def forward(self, inputs):<br/>        """ Assuming that inputs and weights are 1-D numpy arrays and the bias is a number """<br/>        a_cell_sum = np.sum(inputs * self.weights) + self.bias<br/>        result = 1.0 / (1.0 + math.exp(-a_cell_sum)) # This is the sigmoid activation function<br/>        return result<br/>neuron = Neuron()<br/>output = neuron.forward(np.array([1,1]))<br/>print(output)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to TensorFlow</h1>
                </header>
            
            <article>
                
<p>TensorFlow is based on graph-based computation. Consider the following math expression, for example:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><em>c=(a+b)</em>, <em>d = b + 5</em>,</p>
<p class="mce-root CDPAlignCenter CDPAlign"> <em>e = c * d </em></p>
<p class="mce-root">In TensorFlow, this is represented as a computational graph, as shown here. This is powerful because computations are done in parallel:</p>
<div class="CDPAlignCenter CDPAlign"><img height="240" src="assets/2d33fa46-e26b-4c5c-b29a-b3318b25b39f.png" width="153"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing TensorFlow</h1>
                </header>
            
            <article>
                
<p>There are two easy ways to install TensorFlow:</p>
<ul>
<li>Using a virtual environment (recommended and described here)</li>
<li>With a Docker image</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">For macOS X/Linux variants</h1>
                </header>
            
            <article>
                
<p>The following code snippet creates a Python virtual environment and installs TensorFlow in that environment. You should have Anaconda installed before you run this code:</p>
<pre><span>#Creates a virtual environment named "tensorflow_env" assuming that python 3.7 version is already installed.</span><br/>conda create -n tensorflow_env python=3.7 <br/>#Activate points to the environment named "tensorflow" <span class="hljs-number"> </span>
<span class="hljs-built_in">source</span> activate tensorflow_env
conda install pandas matplotlib jupyter notebook scipy scikit-learn<br/>#installs latest tensorflow version into environment tensorflow_env
pip3 install tensorflow </pre>
<p>Please check out the latest updates on the official TensorFlow page, <a href="https://www.tensorflow.org/install/" target="_blank">https://www.tensorflow.org/install/</a>.</p>
<p><span>Try running the following code in your Python console to validate your installation. The console should print <kbd>Hello World!</kbd> if TensorFlow is installed and working:</span></p>
<pre><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-comment">#Creating TensorFlow object </span>
hello_constant = tf.constant(<span class="hljs-string">'Hello World!', name = 'hello_constant'</span>)
#Creating a session object for execution of the computational graph
<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:
    <span class="hljs-comment">#Implementing the tf.constant operation in the session</span>
    output = sess.run(hello_constant)
    print(output)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow basics</h1>
                </header>
            
            <article>
                
<p>In TensorFlow,<span> data isn't stored as integers, floats, strings, or other primitives. These values are encapsulated in an object called a <strong>tensor</strong>. It consists of a set of primitive values shaped into an array of any number of dimensions. The number of dimensions in a tensor is called its <strong>rank</strong>.<strong> </strong>In the preceding example, <kbd>hello_constant</kbd> is a constant string tensor with rank zero. A few more examples of constant tensors are as follows:</span></p>
<pre># A is an int32 tensor with rank = 0<br/>A = tf.constant(123) <br/># B is an int32 tensor with dimension of 1 ( rank = 1 ) <br/>B = tf.constant([123,456,789]) <br/># C is an int32 2- dimensional tensor <br/>C = tf.constant([ [123,456,789], [222,333,444] ])</pre>
<p>TensorFlow's core program is based on the idea of a computational graph. A computational graph is a directed graph consisting of the following two parts: </p>
<ul>
<li>Building a computational graph</li>
<li>Running a computational graph</li>
</ul>
<p>A computational graph executes within a <strong>session</strong>. A TensorFlow session is a runtime environment for the computational graph. It allocates the CPU or GPU and maintains the state of the TensorFlow runtime. The following code creates a session instance named <kbd>sess</kbd> using <kbd>tf.Session</kbd>. Then the <kbd>sess.run()</kbd> function evaluates the tensor and returns the results stored in the <kbd>output</kbd> variable. It finally prints as <kbd>Hello World!</kbd>:</p>
<pre>with tf.Session() as sess:<br/>    # Run the tf.constant operation in the session<br/>    output = sess.run(hello_constant)<br/>    print(output)</pre>
<p>Using TensorBoard, we can visualize the graph. To run TensorBoard, use the following command:</p>
<pre>tensorboard --logdir=path/to/log-directory</pre>
<p>Let's create a piece of simple addition code as follows. Create a constant integer <kbd>x</kbd> with value <kbd>5</kbd>, set the value of a new variable <kbd>y</kbd> after adding <kbd>5</kbd> to it, and print it:</p>
<pre class="mce-root">constant_x = tf.constant(5, name='constant_x')<br/>variable_y = tf.Variable(x + 5, name='variable_y')<br/>print (variable_y)</pre>
<p class="mce-root">The difference is that <kbd>variable_y</kbd> isn't given the current value of <kbd>x + 5</kbd> as it should in Python code. Instead, it is an equation; that means, when <kbd>variable_y</kbd> is computed, take the value of <kbd>x</kbd> at that point in time and add <kbd>5</kbd> to it. The computation of the value of <kbd>variable_y</kbd> is never actually performed in the preceding code. This piece of code actually belongs to the computational graph building section of a typical TensorFlow program. <span>After running this, you'll get something like <kbd>&lt;tensorflow.python.ops.variables.Variable object at 0x7f074bfd9ef0&gt;</kbd> and not the actual value of <kbd>variable_y</kbd> as <kbd>10</kbd>. To fix this, we have to execute the code section of the computational graph, which looks like this:<br/></span></p>
<pre>#initialize all variables<br/>init = tf.global_variables_initializer()<br/># All variables are now initialized<br/><br/>with tf.Session() as sess:<br/>    sess.run(init)<br/>    print(sess.run(variable_y))</pre>
<p>Here is the execution of some basic math functions, such as addition, subtraction, multiplication, and division with tensors. For more math functions, please refer to the documentation:</p>
<p><span>For TensorFlow math functions, go to <a href="https://www.tensorflow.org/versions/r0.12/api_docs/python/math_ops/basic_math_functions" target="_blank">https://www.tensorflow.org/versions/r0.12/api_docs/python/math_ops/basic_math_functions</a>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Basic math with TensorFlow</h1>
                </header>
            
            <article>
                
<p><span>The </span><kbd>tf.add()</kbd><span> function takes two numbers, two tensors, or one of each, and it returns their sum as a tensor:</span></p>
<pre class="mce-root">Addition<br/>x = tf.add(1, 2, name=None) # 3</pre>
<p class="mce-root">Here's an example with subtraction and multiplication:</p>
<pre class="mce-root">x = tf.subtract(1, 2,name=None) # -1<br/>y = tf.multiply(2, 5,name=None) # 10<br/> </pre>
<p>What if we want to use a non-constant? How to feed an input dataset to TensorFlow? For this, TensorFlow provides an API, <kbd>tf.placeholder()</kbd>, and uses <kbd>feed_dict</kbd>.</p>
<p>A <kbd>placeholder</kbd> is a variable that data is assigned to later in the <kbd>tf.session.run()</kbd> function. With the help of this, our operations can be created and we can build our computational graph without needing the data. Afterwards, this data is fed into the graph through these placeholders with the help of the <kbd>feed_dict</kbd> parameter in <kbd>tf.session.run()</kbd> to set the <kbd>placeholder</kbd> tensor. In the following example, the tensor <kbd>x</kbd> is set to the string <kbd>Hello World</kbd><span> before the session runs:</span></p>
<pre>x = tf.placeholder(tf.string)<br/><br/>with tf.Session() as sess:<br/>    output = sess.run(x, feed_dict={x: 'Hello World'})</pre>
<p>It's also possible to set more than one tensor using <kbd>feed_dict</kbd>, as follows:</p>
<pre>x = tf.placeholder(tf.string)<br/>y = tf.placeholder(tf.int32, None)<br/>z = tf.placeholder(tf.float32, None)<br/><br/>with tf.Session() as sess:<br/>    output = sess.run(x, feed_dict={x: 'Welcome to CNN', y: 123, z: 123.45}) </pre>
<p>Placeholders can also allow storage of arrays with the help of multiple dimensions. Please see the following example:</p>
<pre>import tensorflow as tf<br/><br/>x = tf.placeholder("float", [None, 3])<br/>y = x * 2<br/><br/>with tf.Session() as session:<br/>    input_data = [[1, 2, 3],<br/>                 [4, 5, 6],]<br/>    result = session.run(y, feed_dict={x: input_data})<br/>    print(result)</pre>
<div class="packt_tip">This will throw an error as <kbd>ValueError: invalid literal for...</kbd> in cases where the data passed to the <kbd>feed_dict</kbd> parameter doesn't match the tensor type and can't be cast into the tensor type.</div>
<p>The <kbd>tf.truncated_normal()</kbd> function returns a tensor with random values from a normal distribution. This is mostly used for weight initialization in a network:</p>
<pre>n_features = 5<br/>n_labels = 2<br/>weights = tf.truncated_normal((n_features, n_labels))<br/>with tf.Session() as sess:<br/>  print(sess.run(weights))</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Softmax in TensorFlow</h1>
                </header>
            
            <article>
                
<p>The softmax function converts its inputs, known as <strong>logit</strong> or <strong>logit scores</strong>, to be between 0 and 1, and also normalizes the outputs so that they all sum up to 1. In other words, the softmax <span>function turns your logits into probabilities.</span> Mathematically,<span> the </span>softmax function is defined as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" height="40" src="assets/f397aff3-9485-43c6-ae4f-7a33d9374b1e.png" width="136"/></div>
<p>In TensorFlow, the softmax function is implemented. It <span>takes logits and returns softmax activations that have the same type and shape as input logits, as shown in the following image:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="66" src="assets/53e7b660-22b9-4eec-b435-623d7458a621.png" width="159"/></div>
<p>The following code is used to implement this:</p>
<pre>logit_data = [2.0, 1.0, 0.1]<br/>logits = tf.placeholder(tf.float32)<br/>softmax = tf.nn.softmax(logits)<br/><br/>with tf.Session() as sess:<br/>    output = sess.run(softmax, feed_dict={logits: logit_data})<br/>    print( output )</pre>
<p>The way we represent labels mathematically is often called <strong>one-hot encoding</strong>. Each label is represented by a vector that has 1.0 for the correct label and 0.0 for everything else. This works well for most problem cases. However, when the problem has millions of labels, one-hot encoding is not efficient, since most of the vector elements are zeros. We measure the similarity distance between two probability vectors, known as <strong>cross-entropy</strong> and denoted by <strong>D</strong>.</p>
<div class="packt_tip">
<p>Cross-entropy is not symmetric. That means: <em>D(S,L) != D(L,S)</em></p>
</div>
<p><span>In machine learning, we define what it means for a model to be bad usually by a mathematical function. This function is called <strong>loss</strong>, <strong>cost</strong>, or <strong>objective</strong> function. One very common function used to determine the loss of a model is called the <strong>cross-entropy loss</strong>. This concept came from information theory (for more on this, please refer to Visual Information Theory at <a href="https://colah.github.io/posts/2015-09-Visual-Information/" target="_blank">https://colah.github.io/posts/2015-09-Visual-Information/</a>). Intuitively, the loss will be high if the model does a poor job of classifying on the training data, and it will be low otherwise, as shown here:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="92" src="assets/e1236125-87f5-4a42-8af8-84cdcd6920d1.png" width="188"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Cross-entropy loss function</div>
<p>In TensorFlow, we can write a cross-entropy function using <kbd>tf.reduce_sum()</kbd>; it takes an array of numbers and returns its sum as a tensor (see the following code block):</p>
<pre>x = tf.constant([[1,1,1], [1,1,1]])<br/>with tf.Session() as sess:<br/>    print(sess.run(tf.reduce_sum([1,2,3]))) #returns 6 <br/>    print(sess.run(tf.reduce_sum(x,0))) #sum along x axis, prints [2,2,2]</pre>
<p><span>But in practice, </span>while computing the softmax function, intermediate terms may be very large due to the exponentials. So, dividing large numbers can be numerically unstable. We should use TensorFlow's provided softmax and cross-entropy loss API. The following code snippet manually calculates cross-entropy loss and also prints the same using the TensorFlow API:</p>
<pre>import tensorflow as tf<br/><br/>softmax_data = [0.1,0.5,0.4]<br/>onehot_data = [0.0,1.0,0.0]<br/><br/>softmax = tf.placeholder(tf.float32)<br/>onehot_encoding = tf.placeholder(tf.float32)<br/><br/>cross_entropy = - tf.reduce_sum(tf.multiply(onehot_encoding,tf.log(softmax)))<br/><br/>cross_entropy_loss = tf.nn.softmax_cross_entropy_with_logits(logits=tf.log(softmax), labels=onehot_encoding)<br/><br/>with tf.Session() as session:<br/>    print(session.run(cross_entropy,feed_dict={softmax:softmax_data, onehot_encoding:onehot_data} ))<br/>    print(session.run(cross_entropy_loss,feed_dict={softmax:softmax_data, onehot_encoding:onehot_data} ))<br/><br/></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to the MNIST dataset </h1>
                </header>
            
            <article>
                
<p>Here we use <strong>MNIST</strong> (<strong>Modified National Institute of Standards and Technology</strong>), which consists of images of handwritten numbers and their labels. Since its release in 1999, this classic dataset is used for benchmarking classification algorithms. </p>
<p>The data files <kbd>train.csv</kbd> and <kbd>test.csv</kbd> consist of <span>hand-drawn digits, from 0 through 9 in the form of </span>gray-scale images. A digital image is a mathematical function of the form <em>f(x,y)=pixel</em> value. The images are two dimensional.</p>
<p>We can perform any mathematical function on the image. By computing the gradient on the image, we can measure how fast pixel values are changing and the direction in which they are changing. For image recognition, we convert the image into grayscale for simplicity and have one color channel. <strong>RGB</strong> representation of an image consists of three color channels, <strong>RED</strong>, <strong>BLUE</strong>, and <strong>GREEN</strong>. In the RGB color scheme, an image is a stack of three images RED, BLUE, and GREEN. In a grayscale color scheme, color is not important. Color images are computationally harder to analyze because they take more space in memory. Intensity, which is a measure of the lightness and darkness of an image, is very useful for recognizing objects. In some applications, for example, detecting lane lines in a self-driving car application, color is important because it has to distinguish yellow lanes and white lanes. A grayscale image does not provide enough information to distinguish between white and yellow lane lines.</p>
<p>Any grayscale image is interpreted by the computer as a matrix with one entry for each image pixel. Each image is 28 x 28 pixels in height and width, to give a sum of 784 pixels. Each pixel has a single pixel-value associated with it. This value indicates the lightness or darkness of that particular pixel. This pixel-value is an integer ranging from 0 to 255, where a value of zero means darkest and 255 is the whitest, and a gray pixel is between 0 and 255.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The simplest artificial neural network</h1>
                </header>
            
            <article>
                
<p>The following image represents a simple two-layer neural network:</p>
<div class="CDPAlignCenter CDPAlign"><img height="150" src="assets/246151fb-7893-448d-b9bb-7a87b387a24b.png" width="203"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Simple two-layer neural net</div>
<p>The first layer is the <strong>input layer</strong> and the last layer is the <strong>output layer</strong>. The middle layer is the <strong>hidden layer</strong>. If there is more than one hidden layer, then such a network is a deep neural network.</p>
<p>The input and output of each neuron in the hidden layer is connected to each neuron in the next layer. There can be any number of neurons in each layer depending on the problem. Let us consider an example. The simple example which you may already know is the popular hand written digit recognition that detects a number, say 5. This network will accept an image of 5 and will output 1 or 0. A 1 is to indicate the image in fact is a 5 and 0 otherwise. Once the network is created, it has to be trained. We can initialize with random weights and then feed input samples known as the <strong>training dataset</strong>. For each input sample, we check the output, compute the error rate and then adjust the weights so that whenever it sees 5 it outputs 1 and for everything else it outputs a zero. This type of training is called <strong>supervised learning</strong> and the method of adjusting the weights is called <strong>backpropagation</strong>. When constructing artificial neural network models, one of the primary considerations is how to choose activation functions for hidden and output layers. The three most commonly used activation functions are the sigmoid function, hyperbolic tangent function, and <strong>Rectified Linear Unit</strong><span> (</span><strong>ReLU</strong>). The beauty of the sigmoid function is that its derivative is<span> </span><span>evaluated at<em> z</em></span><span> and is</span> <span>simply <em>z</em> </span><span>multiplied by 1-minus <em>z</em></span><span>. That means:</span></p>
<p class="CDPAlignCenter CDPAlign"><span><em> dy/dx =σ(x)(1−σ(x))</em></span></p>
<p>This helps us to efficiently calculate gradients used in neural networks in a convenient manner. If the feed-forward activations of the logistic function for a given layer is kept in memory, the gradients for that particular layer can be evaluated with the help of simple multiplication and subtraction rather than implementing and re-evaluating the sigmoid function, since it requires extra exponentiation. <span>The following image shows us the </span>ReLU <span>activation function, which is zero when </span><em>x &lt; 0</em><span> and then linear with slope 1 when </span><em>x &gt; 0</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0b97ce94-3500-421b-ab0e-ac41ef330b0f.jpeg"/></div>
<p><span>The ReLU is a nonlinear function that computes the function </span><em><span class="MathJax"><span class="math"><span class="mrow"><span class="mi">f</span><span class="mo">(</span><span class="mi">x</span><span class="mo">)</span><span class="mo">=</span><span class="mo">max</span><span class="mo">(</span><span class="mn">0</span><span class="mo">, </span><span class="mi">x</span><span class="mo">)</span></span></span></span></em><span>. That means a ReLU function is 0 for negative inputs and <em>x</em> for all inputs <em>x &gt;0</em>. This means that the activation is thresholded at zero (see the preceding image on the left). TensorFlow implements the ReLU function in <kbd>tf.nn.relu()</kbd>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" height="47" src="assets/1eede7d4-c92f-446e-81a1-37361897b254.png" width="200"/></div>
<div class="packt_quote">Backpropagation, an abbreviation for "backward propagation of errors", is a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of a loss function with respect to all the weights in the network. The optimization method is fed with the gradient and uses it to get the weights updated to reduce the loss function.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a single-layer neural network with TensorFlow</h1>
                </header>
            
            <article>
                
<p>Let us build a single-layer neural net with TensorFlow step by step. In this example, we'll be using the MNIST dataset. This dataset is a set of 28 x 28 pixel grayscale images of hand written digits. This dataset consists of 55,000 training data, 10,000 test data, and 5,000 validation data. Every MNIST data point has two parts: an image of a handwritten digit and a corresponding label. The following code block loads data. <kbd>one_hot=True</kbd> means that the labels are one-hot encoded vectors instead of actual digits of the label. For example, if the label is <kbd>2</kbd>, you will see [0,0,1,0,0,0,0,0,0,0]. This allows us to directly use it in the output layer of the network:</p>
<pre>from tensorflow.examples.tutorials.mnist import input_data<br/>mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)</pre>
<p><span>Setting up placeholders and variables is done as follows:</span></p>
<pre><span class="c1"># All the pixels in the image (28 * 28 = 784)</span>
<span class="n">features_count</span> <span class="o">=</span> <span class="mi">784</span>
<span class="c1"># there are 10 digits i.e labels</span>
<span class="n">labels_count</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="n">features</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span><span class="n">features_count</span><span class="p">])</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels_count</span><span class="p">])</span>

<span class="c1">#Set the weights and biases tensors</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">((</span><span class="n">features_count</span><span class="p">,</span> <span class="n">labels_count</span><span class="p">)))</span>
<span class="n">biases</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">labels_count</span><span class="p">),</span><span class="n">name</span><span class="o">=</span><span class="s1">'biases'</span><span class="p">)</span></pre>
<p><span>Let's set up the optimizer in TensorFlow:</span></p>
<pre><span class="n">loss,<br/>optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>    </pre>
<p>Before we begin training, let's set up the variable initialization operation and an operation to measure the accuracy of our predictions, as follows:</p>
<pre><span class="c1"># Linear Function WX + b</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">),</span><span class="n">biases</span><span class="p">)</span>

<span class="n">prediction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

<span class="c1"># Cross entropy</span>
<span class="n">cross_entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">labels</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prediction</span><span class="p">),</span> <span class="n">reduction_indices</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Training loss</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">)</span>

<span class="c1"># Initializing all variables</span>
<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="c1"># Determining if the predictions are accurate</span>
<span class="n">is_correct_prediction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="c1"># Calculating prediction accuracy</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">is_correct_prediction</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span></pre>
<p>Now we can begin training the model, as shown in the following code snippet:</p>
<pre><span class="c1">#Beginning the session</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
   <span class="c1"># initializing all the variables</span>
   <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
   <span class="n">total_batch</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)</span>
   <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">avg_cost</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_batch</span><span class="p">):</span>
            <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">next_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">optimizer</span><span class="p">,</span><span class="n">loss</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">features</span><span class="p">:</span> <span class="n">batch_x</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">batch_y</span><span class="p">})</span>
            <span class="n">avg_cost</span> <span class="o">+=</span> <span class="n">c</span> <span class="o">/</span> <span class="n">total_batch</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Epoch:"</span><span class="p">,</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="s2">"cost ="</span><span class="p">,</span> <span class="s2">"</span><span class="si">{:.3f}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">avg_cost</span><span class="p">))</span>
   <span class="nb">print</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">features</span><span class="p">:</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">labels</span><span class="p">}))</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Keras deep learning library overview</h1>
                </header>
            
            <article>
                
<p>Keras is a high-level deep neural networks API in Python that runs on top of TensorFlow, CNTK, or Theano.</p>
<p><span>Here are some core concepts you need to know for working with Keras. TensorFlow is a deep learning library for numerical computation and machine intelligence. It is open source and uses data flow graphs for numerical computation. Mathematical operations are represented by nodes and multidimensional data arrays; that is, tensors are represented by graph edges.</span> <span>This framework is extremely technical and hence it is probably difficult for data analysts. Keras makes deep neural network coding simple. It also runs seamlessly on CPU and GPU machines.</span></p>
<p><span>A <strong>model</strong> is the<strong> </strong>core data structure of Keras</span>. The sequential model, which consists of a linear stack of layers, is the simplest type of model. <span>It provides common functions, such as <kbd>fit()</kbd>, <kbd>evaluate()</kbd>, and <kbd>compile()</kbd>.</span></p>
<p>You can create a sequential model with the help of the following lines of code:<strong><br/></strong></p>
<pre>from keras.models import Sequential

#<span class="hljs-operator"><span class="hljs-keyword">Creating</span> the <span class="hljs-keyword">Sequential</span> <span class="hljs-keyword">model</span>
<span class="hljs-keyword">model</span> = <span class="hljs-keyword">Sequential</span>()</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Layers in the Keras model</h1>
                </header>
            
            <article>
                
<p><span>A Keras layer is just like a neural network layer. There are fully connected layers, max pool layers, and activation layers. </span><span>A layer can be added to the model using the model's <kbd>add()</kbd> function. For example, a simple model can be represented by the following:</span></p>
<pre>from keras.models import Sequential
from keras.layers.core import Dense, Activation, Flatten

#<span class="hljs-operator"><span class="hljs-keyword">Creating</span> the <span class="hljs-keyword">Sequential</span> <span class="hljs-keyword">model</span>
<span class="hljs-keyword">model</span> = <span class="hljs-keyword">Sequential</span>()

#Layer 1 - <span class="hljs-keyword">Adding</span> a flatten layer
<span class="hljs-keyword">model</span>.<span class="hljs-keyword">add</span>(Flatten(input_shape=(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>)))

#Layer 2 - <span class="hljs-keyword">Adding</span> a fully connected layer
<span class="hljs-keyword">model</span>.<span class="hljs-keyword">add</span>(Dense(<span class="hljs-number">100</span>))

#Layer 3 - <span class="hljs-keyword">Adding</span> a ReLU activation layer
<span class="hljs-keyword">model</span>.<span class="hljs-keyword">add</span>(Activation(<span class="hljs-string">'relu'</span>))

#Layer 4- <span class="hljs-keyword">Adding</span> a fully connected layer
<span class="hljs-keyword">model</span>.<span class="hljs-keyword">add</span>(Dense(<span class="hljs-number">60</span>))

#Layer 5 - <span class="hljs-keyword">Adding</span> an ReLU activation layer
<span class="hljs-keyword">model</span>.<span class="hljs-keyword">add</span>(Activation(<span class="hljs-string">'relu'</span>))</span></pre>
<p><span>Keras will automatically infer the shape of all layers after the first layer. This means you only have to set the input dimensions for the first layer. The first layer from the preceding code snippet, <kbd>model.add(Flatten(input_shape=(32, 32, 3)))</kbd>, sets the input dimension to (32, 32, 3) and the output dimension to (3072=32 x 32 x 3). The second layer takes in the output of the first layer and sets the output dimensions to (100). This chain of passing the output to the next layer continues until the last layer, which is the output of the model.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Handwritten number recognition with Keras and MNIST</h1>
                </header>
            
            <article>
                
<p>A typical neural network for a digit recognizer may have 784 input pixels connected to 1,000 neurons in the hidden layer, which in turn connects to 10 output targets — one for each digit. Each layer is fully connected to the layer above. A graphical representation of this network is shown as follows, where <kbd>x</kbd> are the inputs, <kbd>h</kbd> are the hidden neurons, and <kbd>y</kbd> are the output class variables:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b24d6265-5067-4fd6-abfc-ccbacfa121a8.png"/></div>
<p>In this notebook, we will build a neural network that will recognize handwritten numbers from 0-9.</p>
<p>The type of neural network that we are building is used in a number of real-world applications, such as recognizing phone numbers and sorting postal mail by address. To build this network, we will use the<span> </span><strong>MNIST</strong><span> </span>dataset.</p>
<p>We will begin as shown in the following code by importing all the required modules, after which the data will be loaded, and then finally building the network:</p>
<pre><span class="c1"># Import Numpy, keras and MNIST data</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">keras.datasets</span> <span class="k">import</span> <span class="n">mnist</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="k">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers.core</span> <span class="k">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Activation</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="k">import</span> <span class="n">np_utils</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Retrieving training and test data</h1>
                </header>
            
            <article>
                
<p>The MNIST dataset already comprises both training and test data. There are 60,000 data points of training data and 10,000 points of test data. If you do not have the data file locally at the <kbd>'~/.keras/datasets/' +</kbd> path, it can be downloaded at this location.</p>
<p>Each MNIST data point has:</p>
<ul>
<li>An image of a handwritten digit</li>
<li>A corresponding label that is a number from 0-9 to help identify the image</li>
</ul>
<p>The images will be called, and will be the input to our neural network,<span> </span><strong>X</strong>; their corresponding labels<span> are </span><strong>y</strong>.</p>
<p>We want our labels as<span> </span>one-hot vectors. One-hot vectors are vectors of many zeros and one. It's easiest to see this in an example. The number 0 is represented as [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], and 4 is represented as [0, 0, 0, 0, 1, 0, 0, 0, 0, 0] as a one-hot vector.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Flattened data</h1>
                </header>
            
            <article>
                
<p>We will use flattened data in this example, or a representation of MNIST images in one dimension rather than two can also be used. Thus, each 28 x 28 pixels number image will be represented as a 784 pixel 1 dimensional array.</p>
<p>By flattening the data, information about the 2D structure of the image is thrown; however, our data is simplified. With the help of this, all our training data can be contained in one array of shape <span>(60,000, 784), wherein the first dimension represents the number of training images and the second depicts the number of pixels in each image. This kind of data is easy to analyze using a simple neural network, as follows:</span></p>
<pre><span class="c1"># Retrieving the training and test data</span>
<span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>


<span class="nb">print</span><span class="p">(</span><span class="s1">'X_train shape:'</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'X_test shape: '</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'y_train shape:'</span><span class="p">,</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'y_test shape: '</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visualizing the training data</h1>
                </header>
            
            <article>
                
<p>The following function will help you visualize the MNIST data. By passing in the index of a training example, the <kbd>show_digit</kbd><span> </span><span>function</span><span> </span><span>will display that training image along with its corresponding label in the title:</span></p>
<pre><span class="c1"># Visualize the data</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c1">#Displaying a training image by its index in the MNIST set</span>
<span class="k">def</span> <span class="nf">display_digit</span><span class="p">(</span><span class="n">index</span><span class="p">):</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Training data, index: </span><span class="si">%d</span><span class="s1">,  Label: </span><span class="si">%d</span><span class="s1">'</span> <span class="o">%</span> <span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">label</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray_r'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
<span class="c1"># Displaying the first (index 0) training image</span>
<span class="n">display_digit</span><span class="p">(0</span><span class="p">)</span></pre>
<pre><span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">60000</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">'float32'</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">'float32'</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">/=</span> <span class="mi">255</span>
<span class="n">X_test</span> <span class="o">/=</span> <span class="mi">255</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Train the matrix shape"</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Test the matrix shape"</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)<br/><br/></span></pre>
<pre><span class="c1">#One Hot encoding of labels.</span>
<span class="kn">from</span> <span class="nn">keras.utils.np_utils</span> <span class="k">import</span> <span class="n">to_categorical</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the network</h1>
                </header>
            
            <article>
                
<p>For this example, you'll define the following:</p>
<ul>
<li>The input layer, which you <span>should expect for each piece of MNIST data</span>, as it tells the network the number of inputs</li>
<li>Hidden layers, as they recognize patterns in data and also connect the input layer to the output layer</li>
<li>The output layer, as it defines how the network learns and gives a label as the output for a given image, as follows:</li>
</ul>
<pre><span class="c1"># Defining the neural network</span>
<span class="k">def</span> <span class="nf">build_model</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">784</span><span class="p">,)))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">'relu'</span><span class="p">))</span> <span class="c1"># An "activation" is just a non-linear function that is applied to the output</span>
 <span class="c1"># of the above layer. In this case, with a "rectified linear unit",</span>
 <span class="c1"># we perform clamping on all values below 0 to 0.</span>
                           
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>   <span class="c1">#With the help of Dropout helps we can protect the model from memorizing or "overfitting" the training data</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">'relu'</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">'softmax'</span><span class="p">))</span> <span class="c1"># This special "softmax" activation,</span>
    <span class="c1">#It also ensures that the output is a valid probability distribution,</span>
    <span class="c1">#Meaning that values obtained are all non-negative and sum up to 1.</span>
    <span class="k">return</span> <span class="n">model<br/></span></pre>
<pre><span class="c1">#Building the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span></pre>
<pre><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'rmsprop'</span><span class="p">,</span>
          <span class="n">loss</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span>
          <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the network</h1>
                </header>
            
            <article>
                
<p>Now that we've constructed the network, we feed it with data and train it, as follows:</p>
<pre><span class="c1"># Training</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">nb_epoch</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing</h1>
                </header>
            
            <article>
                
<p>After you're satisfied with the training output and accuracy, you can run the network on the<span> </span><strong>test dataset</strong><span> </span>to measure its performance!</p>
<div class="packt_tip">Keep in mind to perform this <span>only</span><span> </span><span>after you've completed the training and are satisfied with the results.</span></div>
<p>A good result will obtain<span> an accuracy </span><strong>higher than 95%</strong>. Some simple models have been known to achieve even up to 99.7% accuracy! We can test the model, as shown here:</p>
<pre><span class="c1"># Comparing the labels predicted by our model with the actual labels</span>

<span class="n">score</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="c1"># Printing the result</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Test score:'</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Test accuracy:'</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding backpropagation </h1>
                </header>
            
            <article>
                
<p>In this section, we will understand an intuition about backpropagation. This is a way of computing gradients using the chain rule. Understanding this process and its subtleties is critical for you to be able to understand and effectively develop, design, and debug neural networks.</p>
<p>In general, given a function <em>f(x)</em>, where <em>x</em> is a vector of inputs, we want to compute the gradient of <em>f</em> at <em>x</em> denoted by<span> </span><em>∇(f(x))</em>. This is because in the case of neural networks, the function <em>f</em> is basically a loss function (<em>L</em>) and the input <em>x</em> is the combination of weights and training data. The symbol<em> </em><span><em>∇</em>  is pronounced as <strong>nabla</strong>:</span></p>
<p class="CDPAlignCenter CDPAlign"><em>(xi, yi ) i = 1......N</em></p>
<p>Why do we take the gradient on weight parameters?</p>
<p>It is given that the training data is usually fixed and the parameters are variables that we have control over. We usually compute the gradient of the parameters so that we can use it for parameter updates. The gradient <span><em>∇f</em> is the vector of partial derivatives, that is:</span></p>
<p class="CDPAlignCenter CDPAlign"><em>∇f = [ df/dx, df/dy] = [y,x]</em></p>
<p>In a nutshell, backpropagation will consist of:</p>
<ul>
<li>Doing a feed-forward operation</li>
<li>Comparing the output of the model with the desired output</li>
<li>Calculating the error</li>
<li>Running the feedforward operation backwards (backpropagation) to spread the error to each of the weights</li>
<li>Using this to update the weights, and get a better model</li>
<li>Continuing this until we have a model that is good</li>
</ul>
<p>We will be building a neural network that recognizes digits from 0 to 9. This kind of network application is used for sorting postal mail by zip code, recognizing phone numbers and house numbers from images, extracting package quantities from image of the package and so on.</p>
<p>In most cases, backpropagation is implemented in a framework, such as TensorFlow. However, it is not always true that by simply adding an arbitrary number of hidden layers, backpropagation will magically work on the dataset. The fact is if the weight initialization is sloppy, these non linearity functions can saturate and stop learning. That means training loss will be flat and refuse to go down. This is known as the <strong>vanishing gradient problem</strong>. </p>
<p class="graf graf--p graf-after--pre">If your weight<span> </span>matrix<span> </span><em>W</em><span> </span>is initialized too large, the output of the matrix multiply too could probably have a very large range, which in turn will make all the outputs in the vector<em> z</em><span> </span>almost binary: either 1 or 0. However, if this is the case, then<span>, </span><em>z*(1-z)</em>, which is the local gradient of the sigmoid non-linearity, will become<span> </span><em>zero </em>(vanish)<span> </span><span>in both cases</span>,<strong> </strong>which will<strong> </strong><span>make the gradient for both</span><span> </span><em>x</em><span> </span><span>and</span><strong> </strong><em>W</em><span> also </span><span>zero. The rest of the backward pass will also come out all zero from this point onward on account of the multiplication in the chain rule.</span></p>
<p><span>Another nonlinear activation function is ReLU, which thresholds neurons at zero shown as follows. The forward and backward pass for a fully connected layer that uses ReLU would at the core include:</span></p>
<pre>z = np.maximum(0, np.dot(W, x)) #Representing forward pass<br/>dW = np.outer(z &gt; 0, x) #Representing backward pass: local gradient for W</pre>
<p class="graf graf--p graf-after--pre">If you observe this for a while, you'll see that should a neuron get clamped to zero in the forward pass (that is,<span> </span><em>z = 0</em>, it doesn't fire), then its weights will get a zero gradient. This can lead to what is called the <strong>dead ReLU</strong> problem. This means if a ReLU neuron is unfortunately initialized in such a way that it never fires, or if a neuron's weights ever get knocked off with a large update during training into this regime, in such cases this neuron will remain permanently dead. It is similar to permanent, irrecoverable brain damage. Sometimes, you can even forward the entire training set through a trained network and finally realize that a large fraction (about 40%) of your neurons were zero the entire time.</p>
<p>In<span> </span>calculus, the<span> </span>chain rule<span> </span>is<span> used </span>for computing the<span> </span>derivative<span> </span>of the<span> </span>composition<span> </span>of two or more<span> </span>functions. That is, if we have two functions as <em>f </em>and<span> </span><em>g</em>, then the chain rule represents the derivative of their composition<span> </span><em><span class="texhtml">f ∘ g.</span></em><span> T</span>he function that maps<span> </span><em>x</em><span> </span>to<span> </span><em>f(g(x))</em>) in terms of the derivatives of<span> </span><em>f</em><span> </span>and<span> </span><em>g</em><span> </span>and the<span> </span>product of functions<span> is expressed </span>as follows:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" height="17" src="assets/1505eff0-acc2-414b-a387-b63617368eb9.png" width="128"/></div>
<p>There is a more explicit way to represent this in terms of the variable. Let<span> </span><em><span class="texhtml">F = f ∘ g</span></em>, or equivalently,<span> </span><em><span class="texhtml">F(x) = f(g(x))</span></em><span> </span>for all<span> </span><em>x</em>. Then one can also write:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><em>F'(x)=f'(g(x))g'(x).</em></p>
<p>The chain rule can be written with the help of<span> </span>Leibniz's notation in the following way. If a variable<span> </span><em>z</em><span> </span>is dependent on a <span>variable</span><em> y</em>, which in turn is dependent on a variable<span> </span><em>x</em> (such that<span> </span><em>y</em><span> </span>and<span> </span><em>z</em><span> </span>are dependent variables), then<span> </span><em>z</em> depends on<span> </span><em>x</em><span> </span>as well <span>via the intermediate</span> <em>y</em>. The chain rule then states:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" height="36" src="assets/681874b8-f2ac-41f4-9fba-f70007ccd003.png" width="94"/></div>
<p><em>z = 1/(1 + np.exp(-np.dot(W, x)))</em> # forward pass<br/>
<em>dx = np.dot(W.T, z*(1-z))</em> # backward pass: local gradient for <em>x</em><br/>
<em>dW = np.outer(z*(1-z), x)</em> # backward pass: local gradient for <em>W</em></p>
<p>The forward pass on the left in the following figure calculates <em>z</em> as a function <em>f(x,y)</em> using the input variables <em>x</em> and <em>y</em>. The right side of the figures represents the backward pass. Receiving <em>dL/dz</em>, the gradient of the loss function with respect to <em>z</em>, the gradients of <em>x</em> and <em>y</em> on the loss function can be calculated by applying the chain rule, as shown in the following figure:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="176" src="assets/d1b049ab-5e05-49b1-9229-c01f1cb4e926.png" width="452"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we laid the foundation of neural networks and walked through the simplest artificial neural network. We learned how to build a single layer neural network using TensorFlow.</p>
<p>We studied the differences in the layers in the Keras model and demonstrated the famous handwritten number recognition with Keras and MNIST.</p>
<p>Finally, we understood what backpropagation is and used the MNIST dataset to build our network and train and test our data.</p>
<p>In the next chapter, we will introduce you to CNNs.</p>


            </article>

            
        </section>
    </body></html>