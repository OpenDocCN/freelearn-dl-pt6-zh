- en: Tips, Tricks, and the Road Ahead
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, we covered how to apply various deep learning networks to develop
    prediction and classification models. Several tips and tricks that we covered
    were unique to certain application areas and helped us arrive at better prediction
    or classification performance for the models that we developed.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will go over certain tips and tricks that will be very handy
    when you continue your journey of applying these methods to new data and different
    problems. We will cover four topics in total. Note that these approaches haven't
    been covered in the previous chapters, but we will make use of some of the examples
    from them to illustrate their use.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard for training performance visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing deep network models with LIME
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing model training with tfruns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Early stopping of network training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorBoard for training performance visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For visualizing deep network training performance, TensorBoard is a useful tool
    that is available as part of the TensorFlow package. We will rerun the deep network
    model that we used in [Chapter 2](c5c236d5-fc58-4d90-95b0-2b05b148b187.xhtml),
    *Deep Neural Networks for Multi-Class Classification*, where we used CTG data
    to develop a multi-class classification model for patients. For the code related
    to data processing, the model architecture, and compiling the model, you can refer
    to [Chapter 2](c5c236d5-fc58-4d90-95b0-2b05b148b187.xhtml), *Deep Neural Networks
    for Multi-Class Classification*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code for `model_one` from [Chapter 2](c5c236d5-fc58-4d90-95b0-2b05b148b187.xhtml), *Deep
    Neural Networks for Multi-Class Classification*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We have set a working directory, which will be the desktop where the results
    of training the model will be stored for visualization on TensorBoard.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model is fit using additional feature callbacks, where we use the `callback_tensorboard`
    function to store data in the `ctg/one` folder on the desktop for visualization
    later.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the `ctg` directory is automatically created at the time of fitting
    the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the `tensorboard` function is used for visualization using data stored
    in the `ctg/one` folder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot is of TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/168b0adc-e8e8-4b73-8638-39038396e058.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot shows the loss and accuracy plots for the training
    and validation data for 200 epochs. This was used for training the model. This
    visualization on TensorBoard is interactive in nature and provides the user with
    additional options so that they can explore and understand the model performance's
    during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen in all the chapters in this book that have illustrated the use
    of various deep learning methods, improving the performance of a classification
    or prediction model involves extensive experimentation. To help with such experimentation,
    one of the key benefits of using a TensorBoard is that it allows model performance
    to be compared very easily using interactive visualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'We ran three more models from [Chapter 2](c5c236d5-fc58-4d90-95b0-2b05b148b187.xhtml),
    *Deep Neural Networks for Multi-Class Classification*, and stored model training
    data within subfolders `two`, `three`, and `four` of the `ctg` folder. Run the
    following code for TensorBoard visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code creates TensorBoard visualizations for all four models.
    A screenshot of the resulting TensorBoard page is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6774e36-15d5-47be-acb2-9b3164f5ecee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding visualization shows the loss and accuracy values for the training
    and validation data for all four models. The following are some observations that
    we can make about this plot:'
  prefs: []
  type: TYPE_NORMAL
- en: The results for the four models that were run are presented in different colors
    to allow us to quickly identify them and make comparisons.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss and accuracy values based on the validation data show higher variability
    in the results compared to what can be observed by the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An option to download any plot or related data is also provided.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to visualize different models with different parameter values can
    be useful when we're making choices about the type of architecture to use for
    the deep network, the number of epochs, the batch size, and other model-related
    attributes that are of interest. It can also provide us with directions for further
    experimentation if needed and help us compare current and past results.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing deep network models with LIME
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the application examples that we've provided so far in this book, after we
    developed a classification or prediction deep network model, we carried out visualizations
    to view the overall performance of the models. These assessments are done using
    training and test data. The main idea behind such an assessment is to obtain an
    overall or global understanding of the model's performance. However, there are
    situations where we want to obtain a deeper understanding and also interpretations
    for a specific prediction. For example, we may be interested in understanding
    the main features or variables that have influenced a specific prediction in the
    test data. Such "local" interpretations are the focus of a package called **Local
    Interpretable Model-Agnostic Explanations**, or **LIME**. LIME can help provide
    deeper insights into each prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for carrying out visualization using LIME for the model we developed
    in Keras is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code, we use two functions to be able to use LIME
    with the Keras model. In the first function, we indicate that we will be working
    with a classification model. The second function obtains prediction probabilities.
    In this section, we will use `model_one` from [Chapter 2](c5c236d5-fc58-4d90-95b0-2b05b148b187.xhtml), *Deep
    Neural Networks for Multi-Class Classification*. Then, we'll use the `lime` function
    with the training data, the model (that is, `model_one`), and specify the binning
    of continuous variables as `FALSE`. The resulting explainer is used with the `explain`
    function, where we will specify the number of labels to use as one and specify
    the number of most important features to use for each case as four. We specify
    the kernel width as 0.5\. We can also see that the first three patients in the
    test data have the class labeled as 0, indicating that they belong to the normal
    patient category. Similarly, the 4th and 5th patients in the test data have been
    labeled as 2, indicating that they belong to the pathological patient category.
  prefs: []
  type: TYPE_NORMAL
- en: 'We obtained the following plot using `plot_features(explanation)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bbe08ddc-d37e-47b8-9e18-5da306e3e34c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding plot provides individual plots for the first five patients in
    the test data. Here are some of the observations that can be made from this plot:'
  prefs: []
  type: TYPE_NORMAL
- en: All five patients have been correctly classified.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first three patients have been classified as belonging to a class labeled
    as 0, representing a normal patient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The remaining two patients are classified as belonging to a class labeled as
    2, representing a pathological patient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The prediction probability for the first three cases is 0.97 or above and the
    prediction probability for the 4th and 5th patients is 0.72 and above.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This plot depicts the four most important features that have contributed to
    the specific classification of each patient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Features with blue bars support the model's conclusion, whereas features with
    red bars contradict the model's conclusion for each patient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Higher values for the X8, X10, and X20 variables seem to have a higher influence
    on a patient being classified as pathological.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Higher values for the X12 variable seems to influence a patient being classified
    as normal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following heatmap can be obtained using `plot_explanations(explanation)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/72a2ebcc-8dd6-4cb3-addb-f361fa0fcd95.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can make the following observations from the preceding heatmap:'
  prefs: []
  type: TYPE_NORMAL
- en: The heatmap makes comparing the different variables for each patient easier
    and thus helps with interpretation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It summarizes the results of the case, feature, and label combination and doesn't
    provide as much detail as the previous plot.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For class-X1, or patients labeled as normal (1, 2, and 3), all four features
    (X8, X10, X12, and X20) have very similar weights.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For class-X3, or patients labeled as pathological (4 and 5), once again, all
    four features (X8, X10, X13, and X20) have an approximately similar weight.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing model training with tfruns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we run a deep network model using Keras, we can make use of `tfruns` to
    visualize a loss and accuracy plot, as well as other model-related summaries.
    Although we can also obtain the plot and related summaries when required, the
    main advantage of using `tfruns` is that we can obtain them all in one place.
    We can make use of the following code to achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, the `R` file that''s being referenced contains the code
    to run `model_one` from [Chapter 2](c5c236d5-fc58-4d90-95b0-2b05b148b187.xhtml), *Deep
    Neural Networks for Multi-Class Classification*. The `mlp_ctg.R` file may be stored
    on the computer when we run the code. As soon as we have run the code, the following
    interactive screen is automatically presented:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b1ad8926-326d-482f-bc21-a0938e178876.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The page shown in the preceding screenshot provides the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An interactive plot of the loss and accuracy values for the training and validation
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model summary based on the model's architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information regarding the run, including the time it took to complete all epochs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A numeric summary in the form of accuracy and loss, based on the training and
    validation data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The samples that were used, the number of epochs, and the batch size that was
    specified
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Early stopping of network training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When training a network, we specify the number of epochs we need in advance,
    without knowing how many epochs will actually be needed. If we specify the number
    of epochs to be too few compared to what is actually required, we may have to
    train the network again by specifying more epochs. On the other hand, if we specify
    too many more epochs than what are actually needed, then this may lead to an overfitting
    situation and we may have to retrain the network by reducing the number of epochs.
    This trial and error approach can be very time-consuming for applications where
    each epoch takes a long time to complete. In such situations, we can make use
    of callbacks that can help stop the network training at a suitable time.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate this problem, let''s develop a classification model with the
    CTG data from [Chapter 2](c5c236d5-fc58-4d90-95b0-2b05b148b187.xhtml), *Deep Neural
    Networks for Multi-Class Classification*, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we have specified the number of epochs as 50\. Once
    the training process is completed, we can plot the loss and accuracy values for
    the training and validation data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c6c3241-00be-4b66-bf2c-429bd7d761f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding plot, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We can observe that the loss values for the validation data decrease initially
    for the first few epochs and then start to increase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The plot also shows that, after the first few epochs, the loss values for the
    training and validation data show divergence and tend to go in the opposite direction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we would like to stop the training process much earlier instead of waiting
    for all 50 epochs to be completed, then we can make use of the callback feature
    that's available in Keras.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code includes the callback feature within the `fit` function
    at the time of training the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, early stopping is included for callbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: The metric that we used for monitoring was validation loss values. Another metric
    that can be tried in this situation is validation accuracy since we are developing
    a classification model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have specified patience to be 10, which means that when there are no improvements
    for 10 epochs, the training process will be stopped automatically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The plot for the loss and accuracy are also useful in helping us decide on
    the appropriate values for patience. The following plot is for the loss and accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2bff25de-803a-486c-b48f-da342857f91d.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, this time, the training process didn't run for all 50 epochs
    and stopped as soon as there were no improvements in the loss values for 10 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developing classification and prediction models using deep learning networks
    involves extensive experimentation to arrive at models with high-quality performance.
    To help with this process, there are various methods that are very useful for
    visualizing and controlling network training. In this chapter, we went over four
    such useful methods. We saw that TensorBoard provides a tool that we can use to
    assess and compare model performance after training the network with different
    architectures and other changes in the model. The advantage of using TensorBoard
    lies in the fact that it brings all the necessary information together in one
    place in a user-friendly way. There are also situations where we want to understand
    how the main features or variables on a specific prediction are influenced when
    using a classification or prediction model. In such situations, we can visualize
    the impact that the main features will have using LIME.
  prefs: []
  type: TYPE_NORMAL
- en: Another useful tip that we illustrated in this chapter is visualization with
    the help of tfruns. When developing a deep network model, we come across various
    plots and summaries related to a specific model. Using tfruns, we can visualize
    all the information in one place with the help of an interactive screen. Another
    tip or trick that will be very useful in the journey ahead is the use of callbacks
    to automatically stop the training process when a suitable classification or prediction
    model has been developed. All the methods that were discussed in this chapter
    can be very useful for the journey ahead, especially when you're working on complex
    and challenging problems.
  prefs: []
  type: TYPE_NORMAL
