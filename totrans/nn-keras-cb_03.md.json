["```py\nimport pandas as pd\ndata = pd.read_csv('...') # Please add path to the file you downloaded\n```", "```py\ndata.describe()\n```", "```py\nvars = data.columns[1:]\nimport numpy as np\nfor var in vars:\n     data[var]= np.where(data[var].isnull(),data[var].median(),data[var])\n```", "```py\nfor var in vars:\n     x=data[var].quantile(0.95)\n     data[var+\"outlier_flag\"]=np.where(data[var]>x,1,0)\n     data[var]=np.where(data[var]>x,x,data[var])\n```", "```py\ndata['Debt_income_ratio_outlier']=np.where(data['Debt_incomeratio']>1,1,0)\ndata['Debt_income_ratio']=np.where(data['Debt_income_ratio']>1,1,data['Debt_income_ratio'])\n```", "```py\nfor var in vars:\n     data[var]= data[var]/data[var].max()\n```", "```py\nX = data.iloc[:,1:]\nY = data['Defaultin2yrs']\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state= 42)\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.utils import np_utils\nmodel = Sequential()\nmodel.add(Dense(1000, input_dim=X_train.shape[1], activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n```", "```py\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n```", "```py\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=1024, verbose=1)\n```", "```py\npred = model.predict(X_test)\n```", "```py\ntest_data = pd.DataFrame([y_test]).T\ntest_data['pred']=pred\ntest_data = test_data.reset_index(drop='index')\ntest_data = test_data.sort_values(by='pred',ascending=False)\nprint(test_data[:4500]['Defaultin2yrs'].sum())\n```", "```py\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=1024, verbose=1,class_weight = {0:1,1:100})\n```", "```py\npred = model.predict(X_test)\ntest_data = pd.DataFrame([y_test[:,1]]).T\ntest_data['pred']=pred[:,1]\ntest_data = test_data.reset_index(drop='index')\ntest_data = test_data.sort_values(by='pred',ascending=False)\ntest_data.columns = ['Defaultin2yrs','pred']\nprint(test_data[:4500]['Defaultin2yrs'].sum())\n```", "```py\nfrom keras.datasets import boston_housing\n(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n```", "```py\nimport numpy as np\ntrain_data2 = train_data/np.max(train_data,axis=0)\ntest_data2 = test_data/np.max(train_data,axis=0)\ntrain_targets = train_targets/np.max(train_targets)\ntest_targets = test_targets/np.max(train_targets)\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.utils import np_utils\nfrom keras.regularizers import l1\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=13, activation='relu', kernel_regularizer = l1(0.1)))\nmodel.add(Dense(1, activation='relu', kernel_regularizer = l1(0.1)))\nmodel.summary()\n```", "```py\nmodel.compile(loss='mean_absolute_error', optimizer='adam')\n```", "```py\nhistory = model.fit(train_data2, train_targets, validation_data=(test_data2, test_targets), epochs=100, batch_size=32, verbose=1)\n```", "```py\nnp.mean(np.abs(model.predict(test_data2) - test_targets))*50\n```", "```py\nimport keras.backend as K\ndef loss_function(y_true, y_pred):\n    return K.square(K.sqrt(y_pred)-K.sqrt(y_true))\n```", "```py\nmodel.compile(loss=loss_function, optimizer='adam')\n```", "```py\nhistory = model.fit(train_data2, train_targets, validation_data=(test_data2, test_targets), epochs=100, batch_size=32, verbose=1)\n```", "```py\nfrom keras.datasets import reuters\n(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)\n```", "```py\ntrain_data[0]\n```", "```py\nword_index = reuters.get_word_index()\n```", "```py\nimport numpy as np\ndef vectorize_sequences(sequences, dimension=10000):\n     results = np.zeros((len(sequences), dimension))\n     for i, sequence in enumerate(sequences):\n         results[i, sequence] = 1.\n     return results\n```", "```py\nx_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)\n```", "```py\nfrom keras.utils.np_utils import to_categorical\none_hot_train_labels = to_categorical(train_labels)\none_hot_test_labels = to_categorical(test_labels)\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nmodel = Sequential()\nmodel.add(Dense(64, activation='relu', input_shape=(10000,)))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(46, activation='softmax'))\nmodel.summary()\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n```", "```py\nhistory = model.fit(X_train, y_train,epochs=20,batch_size=512,validation_data=(X_test, y_test))\n```", "```py\nimport pandas as pd\ndata = pd.read_csv('/content/train.csv')\n```", "```py\nids = data['ID'].values\ndef extract_feature(file_name):\n    X, sample_rate = librosa.load(file_name)\n    stft = np.abs(librosa.stft(X))\n    mfccs = np.mean(librosa.feature.mfcc(y=X,sr=sample_rate, n_mfcc=40).T,axis=0)\n    return mfccs\n```", "```py\nx = []\ny = []\nfor i in range(len(ids)):     \n     try:\n         filename = '/content/Train/'+str(ids[i])+'.wav'\n         y.append(data[data['ID']==ids[i]]['Class'].values)\n         x.append(extract_feature(filename))\n     except:\n         continue\nx = np.array(x)\n```", "```py\ny2 = []\nfor i in range(len(y)):\n     y2.append(y[i][0])\ny3 = np.array(pd.get_dummies(y2))\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(1000, input_shape = (40,), activation = 'relu'))\nmodel.add(Dense(10,activation='sigmoid'))\nfrom keras.optimizers import Adam\nadam = Adam(lr=0.0001)\nmodel.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['acc'])\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y3, test_size=0.30,random_state=10)\nmodel.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose = 1)\n```", "```py\nimport pandas as pd\ndata2 = pd.read_csv('/content/stock_data.csv')\n```", "```py\nx= []\ny = []\nfor i in range(data2.shape[0]-5):\n x.append(data2.loc[i:(i+4)]['Close'].values)\n y.append(data2.loc[i+5]['Close'])\nimport numpy as np\nx = np.array(x)\ny = np.array(y)\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.30,random_state=10)\n```", "```py\n\nfrom keras.layers import Dense\nfrom keras.models import Sequential, Model\nmodel = Sequential()\nmodel.add(Dense(100, input_dim = 5, activation = 'relu'))\nmodel.add(Dense(1,activation='linear'))\nmodel.compile(optimizer='adam', loss='mean_squared_error')\n```", "```py\nmodel.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test), verbose = 1)\n```", "```py\nfrom bs4 import BeautifulSoup\nimport urllib, json\n\ndates = []\ntitles = []\nfor i in range(100):\n     try:\n         url = 'https://content.guardianapis.com/search?from-date=2010-01-01&section=business&page-size=200&order-by=newest&page='+str(i+1)+'&q=amazon&api-key=207b6047-a2a6-4dd2-813b-5cd006b780d7'\n         response = urllib.request.urlopen(url)\n         encoding = response.info().get_content_charset('utf8')\n         data = json.loads(response.read().decode(encoding))\n     for j in range(len(data['response']['results'])):\n         dates.append(data['response']['results'][j]['webPublicationDate'])\n         titles.append(data['response']['results'][j]['webTitle']) \n     except:\n         break\n```", "```py\nimport pandas as pd\ndata = pd.DataFrame(dates, titles)\ndata['date']=data['date'].str[:10]\ndata['date']=pd.to_datetime(data['date'], format = '%Y-%m-%d')\ndata = data.sort_values(by='date')\ndata_final = data.groupby('date').first().reset_index()\n```", "```py\ndata2['Date'] = pd.to_datetime(data2['Date'],format='%Y-%m-%d')\ndata3 = pd.merge(data2,data_final, left_on = 'Date', right_on = 'date', how='left')\n```", "```py\nimport nltk\nimport re\nnltk.download('stopwords')\nstop = nltk.corpus.stopwords.words('english')\ndef preprocess(text):\n     text = str(text)\n     text=text.lower()\n     text=re.sub('[^0-9a-zA-Z]+',' ',text)\n     words = text.split()\n     words2=[w for w in words if (w not in stop)]\n     words4=' '.join(words2)\n     return(words4)\ndata3['title'] = data3['title'].apply(preprocess)\n```", "```py\ndata3['title']=np.where(data3['title'].isnull(),'-','-'+data3['title'])\n```", "```py\ndocs = data3['title'].values\n\nfrom collections import Counter\ncounts = Counter()\nfor i,review in enumerate(docs):\n     counts.update(review.split())\nwords = sorted(counts, key=counts.get, reverse=True)\nvocab_size=len(words)\nword_to_int = {word: i for i, word in enumerate(words, 1)}\n```", "```py\nencoded_docs = []\nfor doc in docs:\n     encoded_docs.append([word_to_int[word] for word in doc.split()])\n\ndef vectorize_sequences(sequences, dimension=vocab_size):\n     results = np.zeros((len(sequences), dimension+1))\n     for i, sequence in enumerate(sequences):\n         results[i, sequence] = 1.\n     return results\nvectorized_docs = vectorize_sequences(encoded_docs)\n```", "```py\nx1 = np.array(x)\nx2 = np.array(vectorized_docs[5:])\ny = np.array(y)\n\nX1_train = x1[:2100,:]\nX2_train = x2[:2100, :]\ny_train = y[:2100]\nX1_test = x1[2100:,:]\nX2_test = x2[2100:,:]\ny_test = y[2100:]\n```", "```py\ninput1 = Input(shape=(2406,))\ninput1_hidden = (Dense(100, activation='relu'))(input1)\ninput1_output = (Dense(1, activation='tanh'))(input1_hidden)\n```", "```py\ninput2 = Input(shape=(5,))\ninput2_hidden = (Dense(100, activation='relu'))(input2)\ninput2_output = (Dense(1, activation='linear'))(input2_hidden)\n```", "```py\nfrom keras.layers import multiply\nout = multiply([model, model2])\n```", "```py\nmodel = Model([input1, input2], out)\nmodel.summary()\n```", "```py\nmodel.compile(optimizer='adam', loss='mean_squared_error')\nmodel.fit(x=[X2_train, X1_train], y=y_train, epochs=100,batch_size = 32, validation_data = ([X2_test, X1_test], y_test))\n```", "```py\nX_train = x[:2100,:,:]\ny_train = y[:2100]\nX_test = x[2100:,:,:]\ny_test = y[2100:]\n```", "```py\nweights = np.arange(X_train.shape[0]).reshape((X_train.shape[0]),1)/2100\n```", "```py\nimport keras.backend as K\nfrom functools import partial\n```", "```py\ndef custom_loss_4(y_true, y_pred, weights):\n     return K.square(K.abs(y_true - y_pred) * weights)\n```", "```py\ninput_layer = Input(shape=(5,1))\nweights_tensor = Input(shape=(1,))\n```", "```py\ninp1 = Dense(1000, activation='relu')(input_layer)\nout = Dense(1, activation='linear')(i3)\nmodel = Model([input_layer, weights_tensor], out)\n```", "```py\ncl4 = partial(custom_loss_4, weights=weights_tensor)\n```", "```py\ntest_weights = np.ones((156,1))\n```", "```py\nmodel = Model([input_layer, weights_tensor], out)\nmodel.compile(adam, cl4)\nmodel.fit(x=[X_train, weights], y=y_train, epochs=300,batch_size = 32, validation_data = ([X_test, test_weights], y_test))\n```"]