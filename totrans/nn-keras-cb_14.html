<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">End-to-End Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous chapters, we have learnt about analyzing sequential data (text) using the <strong>Recurrent Neural Network</strong> (<strong>RNN</strong>), and also about analyzing image data using the <strong>Convolutional Neural Network</strong> (<strong>CNN</strong>).</p>
<p class="mce-root">In this chapter, we will be learning about using the CNN + RNN combination to solve the following case studies:</p>
<ul>
<li>Handwritten-text recognition</li>
<li>Generating caption from image</li>
</ul>
<p>Additionally, we will also be learning about a new loss function called <strong>Connectionist Temporal Classification</strong> (<strong>CTC</strong>) loss while solving the handwritten-text-recognition problem.</p>
<p>Finally, we will be learning about beam search to come up with plausible alternatives to the generated text, while solving the caption generating from image problem.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>Consider a scenario where we are transcribing the image of a handwritten text. In this case, we would be dealing with image data and also sequential data (as the content in the image needs to be transcribed sequentially).</p>
<p>In traditional analysis, we would have hand-crafted the solution—for example: we might have slid a window across the image (where the window is of the average size of a character) so that the window would detect each character, and then output characters that it detects, with high confidence.</p>
<p>However, in this scenario, the size of the window or the number of windows we shall slide is hand crafted by us—which becomes a feature-engineering (feature generation) problem.</p>
<p>A more end-to-end approach shall be extracting the features obtained by passing the image through a CNN and then passing these features as inputs to various time steps of an RNN, so that we extract the output at various time steps.</p>
<p>Thus, we will be using a combination of CNN and RNN, and by approaching the problem this way, we do not have to build a hand-crafted feature at all and let the model figure the optimal parameters of CNN and RNN.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Connectionist temporal classification (CTC)</h1>
                </header>
            
            <article>
                
<p>One of the limitations to perform supervised learning on top of handwritten text recognition or in speech transcription is that, using a traditional approach, we would have to provide the label of which part of the image contain a certain character (in the case of hand-writing recognition) or which subsegment of the audio contains a certain phoneme (multiple phonemes combine to form a word utterance).</p>
<p>However, providing the ground truth for each character in image, or each phoneme in speech transcription, is prohibitively costly when building the dataset, where there are thousands of words or hundreds of hours of speech to transcribe.</p>
<p>CTC comes in handy to address the issue of not knowing the mapping of different parts of images to different characters. In this section, we will learn about how CTC loss functions.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Decoding CTC</h1>
                </header>
            
            <article>
                
<p>Let's say, we are transcribing an image that contains the text <strong>ab</strong>. The example can look like any of the following (with varying space between the characters <strong>a</strong> and <strong>b</strong>) and the output label (ground truth) is just the same <strong>ab</strong>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/cf409f12-0d59-4815-8c13-0abc072eb181.png" style="width:4.17em;height:8.33em;" width="102" height="202"/></p>
<p>In the next step, we shall divide these examples into multiple time steps, as follows (where each box represents a time step):</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1453 image-border" src="Images/77d67303-323b-495a-a8c4-8170d6abca28.png" style="width:7.08em;height:12.00em;" width="85" height="144"/></p>
<p>In the preceding example, we have a total of six time steps (each cell represents a time step). </p>
<p>We shall be predicting the output from each time step where the output of a time step is the softmax across the vocabulary.</p>
<p>Given that we are performing softmax, let's say the output of each time step for the first picture of <strong>ab</strong> is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1455 image-border" src="Images/1b3bac58-0c5a-48a2-a533-44f676518f0b.png" style="width:13.17em;height:8.25em;" width="312" height="195"/></p>
<p>Note that, the <strong>-</strong> in the preceding picture represents a blank space. Additionally, the output in the fourth and fifth time steps can be <strong>b</strong> if the features of the image are passed through a bidirectional LSTM (or GRU)—as the information in the next time step can also influence the output in a previous time step while performing a bidirectional analysis.</p>
<p>In the final step, we shall be squashing all the softmax outputs that have the same value in consecutive time steps.</p>
<p>The preceding results in our final output being: <strong>-a-b-</strong> for this example.</p>
<p>If, in case the ground truth is <strong>abb</strong>, we shall expect a <strong>-</strong> in between the two <strong>b</strong>s so that the consecutive <strong>b</strong>s do not get squashed into one.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Calculating the CTC loss value</h1>
                </header>
            
            <article>
                
<p>For the problem we were solving in the previous section, let's consider we have the following scenario, where the probability of having the character in a given time step is provided in the circle of the following diagram (note that, the probabilities add up to one in each time step from <strong>t0</strong> to <strong>t5</strong>):</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1101 image-border" src="Images/e6ae3e2b-0198-4204-93e2-c3099b144404.png" style="width:18.42em;height:9.92em;" width="710" height="381"/></p>
<p>However, to keep the calculation simple for us to understand, let's consider the scenario where the ground truth is <strong>a</strong> and not <strong>ab</strong> and also that the output has only three time steps and not six. The modified output across the three time steps looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1102 image-border" src="Images/f49d376c-e7ca-493c-9c01-8642c6ba3ff2.png" style="width:18.33em;height:14.67em;" width="710" height="569"/></p>
<p>We can obtain the ground truth of <strong>a</strong> if the softmax in each time step is any of the following scenarios:</p>
<table style="border-collapse: collapse;width: 101.353%" border="1">
<tbody>
<tr>
<td><strong>Output in each time step</strong></td>
<td><strong>Prob of character in time step 1</strong></td>
<td><strong><span>Prob of character in time step 2</span></strong></td>
<td><strong><span>Prob of character in time step 3</span></strong></td>
<td><strong>Probability of combination</strong></td>
<td><strong>Final probability</strong></td>
</tr>
<tr>
<td>- - a</td>
<td>0.8</td>
<td>0.1</td>
<td>0.1</td>
<td>0.8 x 0.1 x 0.1</td>
<td> 0.008</td>
</tr>
<tr>
<td>- a a</td>
<td>0.8</td>
<td>0.9</td>
<td>0.1</td>
<td><span>0.8 x 0.9 x 0.1</span></td>
<td>0.072</td>
</tr>
<tr>
<td>a a a</td>
<td>0.2</td>
<td>0.9</td>
<td>0.1</td>
<td><span>0.2 x 0.9 x 0.1</span></td>
<td>0.018</td>
</tr>
<tr>
<td>- a -</td>
<td>0.8</td>
<td>0.9</td>
<td>0.8</td>
<td><span>0.8 x 0.9 x 0.8</span></td>
<td>0.576</td>
</tr>
<tr>
<td>- a a</td>
<td>0.8</td>
<td>0.9</td>
<td>0.1</td>
<td><span>0.8 x 0.9 x 0.1</span></td>
<td>0.072</td>
</tr>
<tr>
<td>a - - </td>
<td>0.2</td>
<td>0.1</td>
<td>0.8</td>
<td><span>0.2 x 0.1 x 0.8</span></td>
<td>0.016</td>
</tr>
<tr>
<td>a a -</td>
<td>0.2</td>
<td>0.9</td>
<td>0.8</td>
<td><span>0.2 x 0.9 x 0.8</span></td>
<td>0.144</td>
</tr>
<tr>
<td colspan="5"><strong><span>Overall probability</span></strong></td>
<td><strong>0.906</strong></td>
</tr>
</tbody>
</table>
<p> </p>
<p>From the preceding results, we see that the overall probability of obtaining the ground truth <strong>a</strong> is 0.906.</p>
<p>CTC loss is the negative logarithm of the overall probability = <em>-log(0.906) = 0.04</em>.</p>
<p>Note that, as the combination of characters with the highest probability in each time step indicate the ground truth of <strong>a</strong>, the CTC loss is close to zero.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Handwritten-text recognition</h1>
                </header>
            
            <article>
                
<p>In this case study, we will be working toward transcribing the handwritten images so that we extract the text that is present in the pictures.</p>
<p>A sample of the handwriting looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/900d721e-6a43-4de5-9ced-850bc6332af3.png" style="width:14.67em;height:12.42em;" width="167" height="141"/><br/></p>
<p class="mce-root">Note that in the preceding diagram, the handwritten characters have varied length, the images are of different dimensions, the separation between the characters is varied, and the images are of different quality.</p>
<p>In this section, we will be learning about using CNN, RNN, and the CTC loss function together to transcribe the handwritten examples.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The strategy we will adopt to transcribe the handwritten examples is as follows:</p>
<ul>
<li>Download images that contain images of handwritten text:
<ul>
<li>Multiple datasets containing handwritten text images are provided in the code file associated with this case study in GitHub</li>
<li>Ensure that, along with the images, you have also taken the ground truth corresponding to the images</li>
</ul>
</li>
<li>Resize all images to be of the same size, let's say 32 x 128 in size</li>
<li>While resizing, we should also ensure that the aspect ratio of the picture is not distorted:
<ul>
<li>This is to ensure that images cannot look very blurred because the original image was changed to 32 x 128 in size</li>
</ul>
</li>
<li>We'll resize the images without distorting the aspect ratio, and then superimpose each of them on a different blank 32 x 128 image</li>
<li>Invert the colors of the images so that the background is in black and the handwritten content is in white </li>
<li>Scale the images so that their value is between zero and one</li>
<li>Pre-process the output (ground truth):
<ul>
<li>Extract all the unique characters in output</li>
<li>Assign an index for each character</li>
<li>Find the maximum length of output, and then ensure that the number of time steps for which we are predicting the content of time step is more than the maximum length of output</li>
<li>Ensure the same length of output for all outputs by padding the ground truths</li>
</ul>
</li>
<li>Pass the processed picture through a CNN so that we extract features that are of 32 x 256 in shape</li>
<li>Pass the extracted features from CNN through GRU unit that is bidirectional so that we encapsulate the information that is present in adjacent time steps</li>
<li>Each of the 256 features in 32 time steps is an input for the respective time step</li>
<li>Pass the output through a dense layer that has as many output values as the total number of unique characters in ground truth (the padded value (<strong>-</strong> in the example given in the introduction to the CTC loss section) shall also be one of the unique characters—where the padded value <strong>-</strong> represents either the space between characters, or the padding in the blank portion of the picture</li>
<li>Extract the softmax and its corresponding output character at each of the 32 time steps</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The preceding algorithm in code is performed as follows (the code file is available as <kbd>Handwritten_text_recognition.ipynb</kbd> in GitHub):</p>
<ol>
<li>Download and import the dataset. This dataset will contain the images of handwritten text and their corresponding ground truth (transcription).</li>
<li>Build a function that resizes pictures without distorting the aspect ratio and pad the rest of pictures so that all of them have the same shape:</li>
</ol>
<pre style="padding-left: 60px">def extract_img(img):<br/>     target = np.ones((32,128))*255<br/>     new_shape1 = 32/img.shape[0]<br/>     new_shape2 = 128/img.shape[1]<br/>     final_shape = min(new_shape1, new_shape2)<br/>     new_x = int(img.shape[0]*final_shape)<br/>     new_y = int(img.shape[1]*final_shape)<br/>     img2 = cv2.resize(img, (new_y,new_x ))<br/>     target[:new_x,:new_y] = img2[:,:,0]<br/>     target[new_x:,new_y:]=255<br/>     return 255-target</pre>
<p style="padding-left: 60px">In the preceding code, we are creating a blank picture (named <kbd>target</kbd>). In the next step, we have reshaped the picture to maintain its aspect ratio.</p>
<p style="padding-left: 60px">Finally, we have overwritten the rescaled picture on top of the blank one we created, and have returned the picture where the background is in black (255-target).</p>
<ol start="3">
<li>Read the pictures and store them in a list, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">filepath = '/content/words.txt'<br/>f = open(filepath)<br/>import cv2<br/>count = 0<br/>x = []<br/>y = []<br/>x_new = []<br/>chars = set()<br/>for line in f:<br/>     if not line or line[0]=='#':<br/>         continue<br/>     try:<br/>         lineSplit = line.strip().split(' ')<br/>         fileNameSplit = lineSplit[0].split('-')<br/>         img_path = '/content/'+fileNameSplit[0]+'/'+fileNameSplit[0] + '-' +              fileNameSplit[1]+'/'+lineSplit[0]+'.png'<br/>         img_word = lineSplit[-1]<br/>         img = cv2.imread(img_path)<br/>         img2 = extract_img(img)<br/>         x_new.append(img2)<br/>         x.append(img)<br/>         y.append(img_word)<br/>         count+=1<br/>     except:<br/>         continue</pre>
<p style="padding-left: 60px">In the preceding code, we are extracting each picture and also are modifying it per the function that we defined. The input and modified examples for different scenario:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1456 image-border" src="Images/cb7b3958-fbd3-4d1c-b6cb-40a4cc1ec9be.png" style="width:28.42em;height:15.33em;" width="382" height="206"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="4">
<li>Extract the unique characters in output, shown as follows:</li>
</ol>
<pre style="padding-left: 60px">import itertools<br/>list2d = y<br/>charList = list(set(list(itertools.chain(*list2d))))</pre>
<ol start="5">
<li>Create the output ground truth, as demonstrated in the following code:</li>
</ol>
<pre style="padding-left: 60px">num_images = 50000<br/><br/>import numpy as np<br/>y2 = []<br/>input_lengths = np.ones((num_images,1))*32<br/>label_lengths = np.zeros((num_images,1))<br/>for i in range(num_images):<br/>     val = list(map(lambda x: charList.index(x), y[i]))<br/>     while len(val)&lt;32:<br/>         val.append(79)<br/>     y2.append(val)<br/>     label_lengths[i] = len(y[i])<br/>     input_lengths[i] = 32</pre>
<p style="padding-left: 60px">In the preceding code, we are storing the index of each character in an output into a list. Additionally, if the output is less than 32 characters in size, we pad it with 79, which represents the blank value.</p>
<p style="padding-left: 60px">Finally, we are also storing the label length (in the ground truth) and also the input length (which is always 32 in size).</p>
<ol start="6">
<li>Convert the input and output into NumPy arrays, as follows:</li>
</ol>
<pre style="padding-left: 60px">x = np.asarray(x_new[:num_images])<br/>y2 = np.asarray(y2)<br/>x= x.reshape(x.shape[0], x.shape[1], x.shape[2],1)</pre>
<ol start="7">
<li>Define the objective, as shown here:</li>
</ol>
<pre style="padding-left: 60px">outputs = {'ctc': np.zeros([32])}</pre>
<p style="padding-left: 60px">We are initializing 32 zeros, as the batch size will be 32. For each value in batch size, we expect the loss value to be zero.</p>
<ol start="8">
<li>Define the CTC loss function as follows:</li>
</ol>
<pre style="padding-left: 60px">def ctc_loss(args):<br/>     y_pred, labels, input_length, label_length = args<br/>     return K.ctc_batch_cost(labels, y_pred, input_length, label_length)</pre>
<p style="padding-left: 60px">The preceding function takes the predicted values, ground truth (labels) and input, label lengths as input and calculates the CTC loss value.</p>
<ol start="9">
<li>Define the model, demonstrated as follows:</li>
</ol>
<pre style="padding-left: 60px">input_data = Input(name='the_input', shape = (32, 128,1), dtype='float32')<br/><br/>inner = Conv2D(32, (3,3), padding='same')(input_data)<br/>inner = Activation('relu')(inner)<br/>inner = MaxPooling2D(pool_size=(2,2),name='max1')(inner)<br/>inner = Conv2D(64, (3,3), padding='same')(inner)<br/>inner = Activation('relu')(inner)<br/>inner = MaxPooling2D(pool_size=(2,2),name='max2')(inner)<br/>inner = Conv2D(128, (3,3), padding='same')(input_data)<br/>inner = Activation('relu')(inner)<br/>inner = MaxPooling2D(pool_size=(2,2),name='max3')(inner)<br/>inner = Conv2D(128, (3,3), padding='same')(inner)<br/>inner = Activation('relu')(inner)<br/>inner = MaxPooling2D(pool_size=(2,2),name='max4')(inner)<br/>inner = Conv2D(256, (3,3), padding='same')(inner)<br/>inner = Activation('relu')(inner)<br/>inner = MaxPooling2D(pool_size=(4,2),name='max5')(inner)<br/>inner = Reshape(target_shape = ((32,256)), name='reshape')(inner)</pre>
<p style="padding-left: 60px" class="mce-root">In the preceding code, we are building the CNN that converts a picture with 32 x 128 shape into a picture of 32 x 256 in shape:</p>
<pre style="padding-left: 60px" class="mce-root">gru_1 = GRU(256, return_sequences = True, name = 'gru_1')(inner)<br/>gru_2 = GRU(256, return_sequences = True, go_backwards = True, name = 'gru_2')(inner)<br/>mix_1 = add([gru_1, gru_2])<br/>gru_3 = GRU(256, return_sequences = True, name = 'gru_3')(inner)<br/>gru_4 = GRU(256, return_sequences = True, go_backwards = True, name = 'gru_4')(inner)</pre>
<p style="padding-left: 60px">The architecture of model till the layers defined previously are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/ceeb523d-37f3-4c1d-9828-32dc1dddfe58.png" width="1178" height="481"/></p>
<p style="padding-left: 60px" class="mce-root">In the preceding code, we are passing the features obtained from CNN into a GRU. The architecture defined previously continues from the preceding graph shown is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/3703ea67-229b-4f73-bc63-a71be57a06d0.png" style="width:36.92em;height:9.83em;" width="727" height="193"/></p>
<p style="padding-left: 60px">In the following code, we are concatenating the output of two GRUs so that we take both bidirectional GRU and normal GRU-generated features into account:</p>
<pre style="padding-left: 60px" class="mce-root">merged = concatenate([gru_3, gru_4])</pre>
<p style="padding-left: 60px">The architecture after adding the preceding layer is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1463 image-border" src="Images/d3b78853-dfea-445c-8be8-538b17c2b466.png" style="width:36.08em;height:9.83em;" width="737" height="200"/></p>
<p style="padding-left: 60px">In the following code, we are passing the features of GRU output through a dense layer and applying softmax to get one of the possible 80 values as output:</p>
<pre style="padding-left: 60px" class="mce-root">dense = TimeDistributed(Dense(80))(merged)<br/>y_pred = TimeDistributed(Activation('softmax', name='softmax'))(dense)</pre>
<p style="padding-left: 60px">The architecture of the model continues as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1447 image-border" src="Images/1f69ad73-9c27-40b1-8720-e37833d6fce1.png" style="width:33.92em;height:9.58em;" width="684" height="193"/></p>
<ol start="10">
<li>Initialize the variables that are required for the CTC loss:</li>
</ol>
<pre style="padding-left: 60px">from keras.optimizers import Adam<br/>Optimizer = Adam()<br/>labels = Input(name = 'the_labels', shape=[32], dtype='float32')<br/>input_length = Input(name='input_length', shape=[1],dtype='int64')<br/>label_length = Input(name='label_length',shape=[1],dtype='int64')<br/>output = Lambda(ctc_loss, output_shape=(1,),name='ctc')([y_pred, labels, input_length, label_length])</pre>
<p style="padding-left: 60px">In the preceding code, we are mentioning that <kbd>y_pred</kbd> (predicted character values), actual labels, input length, and the label length are the inputs to the CTC loss function.</p>
<ol start="11">
<li>Build and compile the model as follows:</li>
</ol>
<pre style="padding-left: 60px">model = Model(inputs = [input_data, labels, input_length, label_length], outputs= output)<br/>model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = Optimizer)</pre>
<p style="padding-left: 60px">Note that there are multiple inputs that we are passing to our model. The CTC calculation is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/8ad595f7-6135-494d-8787-4979a3460b79.png" width="1913" height="182"/></p>
<ol start="12">
<li>Create the following vectors of inputs and outputs:</li>
</ol>
<pre style="padding-left: 60px">x2 = 1-np.array(x_new[:num_images])/255<br/>x2 = x2.reshape(x2.shape[0],x2.shape[1],x2.shape[2],1)<br/>y2 = np.array(y2[:num_images])<br/>input_lengths = input_lengths[:num_images]<br/>label_lengths = label_lengths[:num_images]</pre>
<ol start="13">
<li>Fit the model on multiple batches of pictures, demonstrated in the following code:</li>
</ol>
<pre style="padding-left: 60px">import random<br/><br/>for i in range(100):<br/>     samp=random.sample(range(x2.shape[0]-100),32)<br/>     x3=[x2[i] for i in samp]<br/>     x3 = np.array(x3)<br/>     y3 = [y2[i] for i in samp]<br/>     y3 = np.array(y3)<br/>     input_lengths2 = [input_lengths[i] for i in samp]<br/>     label_lengths2 = [label_lengths[i] for i in samp]<br/>     input_lengths2 = np.array(input_lengths2)<br/>     label_lengths2 = np.array(label_lengths2)<br/>     inputs = {<br/>     'the_input': x3,<br/>     'the_labels': y3,<br/>     'input_length': input_lengths2,<br/>     'label_length': label_lengths2,<br/>     }<br/>     outputs = {'ctc': np.zeros([32])}<br/>     model.fit(inputs, outputs,batch_size = 32, epochs=1, verbose =2)</pre>
<p style="padding-left: 60px">In the preceding code, we are sampling 32 pictures at a time, converting them into an array, and fitting the model to ensure that the CTC loss is zero.</p>
<p style="padding-left: 60px">Note that, we are excluding the last 100 pictures (in <kbd>x2</kbd>) from passing as input to model, so that we can test our model's accuracy on that data.</p>
<p style="padding-left: 60px">Furthermore, we are looping through the total dataset multiple times, as fetching all pictures into RAM and converting them into an array is very likely to crash the system, due to the huge memory requirement.</p>
<p style="padding-left: 60px">The training loss over increasing epochs is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1465 image-border" src="Images/65786b6a-f385-4f80-a721-9e7111c729f5.png" style="width:34.92em;height:24.75em;" width="382" height="271"/></p>
<ol start="14">
<li>Predict the output at each time for a test picture, using the following code:</li>
</ol>
<pre style="padding-left: 60px">model2 = Model(inputs = input_data, outputs = y_pred)<br/>pred = model2.predict(x2[-5].reshape(1,32,128,1))<br/><br/>pred2 = np.argmax(pred[0,:],axis=1)<br/>out = ""<br/>for i in pred2:<br/>  if(i==79):<br/>    continue<br/>  else:<br/>    out += charList[i]<br/>plt.imshow(x2[k].reshape(32,128))<br/>plt.title('Predicted word: '+out)<br/>plt.grid('off')</pre>
<p style="padding-left: 60px">In the preceding code, we are discarding the output if the predicted character at a time step is the character of 79.</p>
<p style="padding-left: 60px">A test examples and its corresponding predictions (in title) are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1615 image-border" src="Images/3841110b-83d4-4e17-a9ed-1a43606a7f33.png" style="width:36.17em;height:13.67em;" width="497" height="188"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Image caption generation</h1>
                </header>
            
            <article>
                
<p>In the previous case study, we learned about using CNN, RNN, and CTC loss together to transcribe the handwritten digits.</p>
<p>In this case study, we will learn about integrating CNN and RNN architectures to generate captions for a given picture.</p>
<p>Here is a sample of the picture:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1437 image-border" src="Images/d236b475-d8ec-405c-bc4a-712208d8b564.png" style="width:34.92em;height:46.42em;" width="677" height="900"/></p>
<ul>
<li class="CDPAlignLeft CDPAlign">A girl in red dress with Christmas tree in background</li>
<li>A girl is showing the Christmas tree</li>
<li>A girl is playing in the park</li>
<li>A girl is celebrating Christmas</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this section, let's list the strategy that we shall adopt to transcribe pictures:</p>
<ul>
<li>We'll be working toward generating captions from pictures by working on a dataset that has images as well as the captions associated with the images. Links of datasets that have images and their corresponding captions are provided in the corresponding notebook in GitHub.</li>
<li>We'll extract the VGG16 features of each picture.</li>
<li>We will also preprocess the captions text:
<ul>
<li>Convert all words to lower case</li>
<li>Remove punctuation</li>
<li>Add start and end tokens to each caption</li>
</ul>
</li>
</ul>
<ul>
<li>Keep only the pictures that are of a dog or a girl (we are performing this analysis only so that we train our model faster, as it takes ~5 hours to run this model, even on a GPU).</li>
<li>Assign an index to each unique word in the vocabulary of captions.</li>
<li>Pad all captions (where each word is represented by an index value) so that all captions are now of the same size.</li>
<li>To predict the first word, the model shall take the combination of the VGG16 features and the embedding of the start token.</li>
<li>Similarly, to predict the second word, the model will take the combination of the VGG16 features and the embedding combination of start token and the first word.</li>
<li>In a similar manner, we proceed to fetch all the predicted words.</li>
<li>We continue with the preceding steps until we predict the end token.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>We'll code up the strategy that we have defined previously, as follows <span>(the code file is available a</span>s <kbd>Image_captioning.ipynb</kbd> in GitH<span>ub)</span>:</p>
<ol>
<li>Download and import a dataset that contains images and their corresponding captions. The recommended datasets are provided in GitHub</li>
</ol>
<ol start="2">
<li>Import the relevant packages, as follows:</li>
</ol>
<pre style="padding-left: 60px">import glob<br/>from PIL import Image<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>import pickle<br/>from tqdm import tqdm<br/>import pandas as pd<br/>from keras.preprocessing import sequence<br/>from keras.models import Sequential<br/>from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, merge, Activation, Flatten<br/>from keras.optimizers import Adam, RMSprop<br/>from keras.layers.wrappers import Bidirectional<br/>from keras.applications.inception_v3 import InceptionV3<br/>from keras.preprocessing import image<br/>import nltk</pre>
<ol start="3">
<li>Load the caption dataset, shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">caption_file = '...'<br/>captions = open(caption_file, 'r').read().strip().split('\n')<br/>d = {}<br/>for i, row in enumerate(captions):<br/>     row = row.split('\t')<br/>     row[0] = row[0][:len(row[0])-2]<br/>     if row[0] in d:<br/>         d[row[0]].append(row[1])<br/>     else:<br/>         d[row[0]] = [row[1]]<br/>total_images = list(d.keys())</pre>
<ol start="4">
<li>Load the pictures and store the VGG16 features:</li>
</ol>
<pre style="padding-left: 60px">image_path = '...'<br/>from keras.applications.vgg16 import VGG16<br/>vgg16=VGG16(include_top=False, weights='imagenet', input_shape=(224,224,3))<br/><br/>import cv2<br/>x = []<br/>y = []<br/>x2 = []<br/>tot_images = ''<br/>for i in range(len(d.keys())):<br/>     for j in range(len(d[total_images[i]])):<br/>         img_path = image_path+total_images[i]<br/>         img = cv2.imread(img_path)<br/>         try:<br/>             img2 = cv2.resize(img, (224, 224))/255<br/>             img3 = vgg16.predict(img2.reshape(1,224,224,3))<br/>             x.append(img3)<br/>             y.append(d[total_images[i]][j])<br/>             tot_images = tot_images + ' '+total_images[i]<br/>         except:<br/>             continue</pre>
<ol start="5">
<li>Convert the VGG16 features into NumPy arrays:</li>
</ol>
<pre style="padding-left: 60px">x = np.array(x)<br/>x = x.reshape(x.shape[0],7,7,512)</pre>
<ol start="6">
<li>Build a function that removes the punctuation in captions, and also converts all words to lowercase:</li>
</ol>
<pre style="padding-left: 60px">def preprocess(text):<br/>     text=text.lower()<br/>     text=re.sub('[^0-9a-zA-Z]+',' ',text)<br/>     words = text.split()<br/>     words2 = words<br/>     words4=' '.join(words2)<br/>     return(words4)</pre>
<p style="padding-left: 60px">In the following code, we preprocess all the captions and also append the start and end tokens:</p>
<pre style="padding-left: 60px">caps = []<br/>for key, val in d.items():<br/>     if(key in img_path2):<br/>         for i in val:<br/>             i = preprocess(i)<br/>             caps.append('&lt;start&gt; ' + i + ' &lt;end&gt;')</pre>
<ol start="7">
<li>Append only the pictures that are of a child or a dog:</li>
</ol>
<pre style="padding-left: 60px">caps2 = []<br/>x2 = []<br/>img_path3 = []<br/>for i in range(len(caps)):<br/>     if (('girl') in caps[i]):<br/>         caps2.append(caps[i])<br/>         x2.append(x[i])<br/>         img_path2.append(img_path[i])<br/>     elif 'dog' in caps[i]:<br/>         caps2.append(caps[i])<br/>         x2.append(x[i])<br/>         img_path2.append(img_path[i])<br/>     else:<br/>         continue</pre>
<ol start="8">
<li>Extract all the unique words in captions, as follows:</li>
</ol>
<pre style="padding-left: 60px">words = [i.split() for i in caps2]<br/>unique = []<br/>for i in words:<br/>     unique.extend(i)<br/>unique = list(set(unique))<br/>vocab_size = len(unique)</pre>
<ol start="9">
<li>Assign indexes to words in the vocabulary, demonstrated in the following code:</li>
</ol>
<pre style="padding-left: 60px">word2idx = {val:(index+1) for index, val in enumerate(unique)}<br/>idx2word = {(index+1):val for index, val in enumerate(unique)}</pre>
<ol start="10">
<li>Identify the maximum length of a caption so that we pad all captions to be of the same length:</li>
</ol>
<pre style="padding-left: 60px">max_len = 0<br/>for c in caps:<br/>     c = c.split()<br/>     if len(c) &gt; max_len:<br/>         max_len = len(c)</pre>
<ol start="11">
<li>Pad all the captions to be of the same length, as follows:</li>
</ol>
<pre style="padding-left: 60px">n = np.zeros(vocab_size+1)<br/>y = []<br/>y2 = []<br/>for k in range(len(caps2)):<br/>     t= [word2idx[i] for i in caps2[k].split()]<br/>     y.append(len(t))<br/>     while(len(t)&lt;max_len):<br/>         t.append(word2idx['&lt;end&gt;'])<br/>     y2.append(t)</pre>
<ol start="12">
<li>Build the model that takes pictures as input and creates features from it:</li>
</ol>
<pre style="padding-left: 60px">from keras.layers import Input<br/>embedding_size = 300<br/>inp = Input(shape=(7,7,512))<br/>inp1 = Conv2D(512, (3,3), activation='relu')(inp)<br/>inp11 = MaxPooling2D(pool_size=(2, 2))(inp1)<br/>inp2 = Flatten()(inp11)<br/>img_emb = Dense(embedding_size, activation='relu') (inp2)<br/>img_emb2 = RepeatVector(max_len)(img_emb)</pre>
<ol start="13">
<li>Build a model that takes captions as input and creates features from it:</li>
</ol>
<pre style="padding-left: 60px">inp2 = Input(shape=(max_len,))<br/>cap_emb = Embedding((vocab_size+1), embedding_size, input_length=max_len) (inp2)<br/>cap_emb2 = LSTM(256, return_sequences=True)(cap_emb)<br/>cap_emb3 = TimeDistributed(Dense(300)) (cap_emb2)</pre>
<ol start="14">
<li>Concatenate the two models and come up with a softmax of probabilities across all the possible output words:</li>
</ol>
<pre style="padding-left: 60px">final1 = concatenate([img_emb2, cap_emb3])<br/>final2 = Bidirectional(LSTM(256, return_sequences=False))(final1)<br/>final3 = Dense(vocab_size+1)(final2)<br/>final4 = Activation('softmax')(final3)<br/><br/>final_model = Model([inp, inp2], final4)</pre>
<ol start="15">
<li>Compile the model, as follows:</li>
</ol>
<pre style="padding-left: 60px">from keras.optimizers import Adam<br/>adam = Adam(lr = 0.0001)<br/>final_model.compile(loss='categorical_crossentropy', optimizer = adam, metrics=['accuracy'])</pre>
<ol start="16">
<li>Fit the model, shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">for i in range(500):<br/>     x3 = []<br/>     x3_sent = []<br/>     y3 = []<br/>     shortlist_y = random.sample(range(len(y)-100),32)<br/>     for j in range(len(shortlist_y)):<br/>         for k in range(y[shortlist_y[j]]-1):<br/>             n = np.zeros(vocab_size+1) <br/>             x3.append(x2[shortlist_y[j]])<br/>             pad_sent = pad_sequences([y2[shortlist_y[j]][:(k+1)]], maxlen=max_len, padding='post')<br/>             x3_sent.append(pad_sent)<br/>             n[y2[shortlist_y[j]][(k+1)]] = 1<br/>             y3.append(n)<br/>             x3 = np.array(x3)<br/>             x3_sent =np.array(x3_sent)<br/>             x3_sent = x3_sent.reshape(x3_sent.shape[0], x3_sent.shape[2])<br/>             y3 = np.array(y3) <br/>             final_model.fit([x3/12, x3_sent], y3, batch_size = 32, epochs = 3, verbose = 1)</pre>
<p style="padding-left: 60px">In the preceding code, we are looping through all the pictures, 32 at a time. Additionally, we are creating the input dataset in such a way that the first <em>n</em><span> </span>number of output words in a caption are input along with the VGG16 features of a picture, and the corresponding output is the <em>n+1<sup>th</sup></em> word of the caption.</p>
<p style="padding-left: 60px">Furthermore, we are dividing the VGG16 features (<kbd>x3</kbd>) by 12, as we need to scale the input values between zero and one. </p>
<ol start="17">
<li>The output caption of a sample picture can be obtained as follows:</li>
</ol>
<pre style="padding-left: 60px">l=-25<br/>im_path = image_path+ img_path3[l]<br/>img1 = cv2.imread(im_path)<br/>plt.imshow(img1)</pre>
<p class="CDPAlignCenter CDPAlign"><img src="Images/4344dfc9-c830-4c95-abbe-a425403cb3d0.jpg" style="width:37.17em;height:23.67em;" width="492" height="314"/></p>
<p style="padding-left: 60px">The output is decoded as follows:</p>
<pre style="padding-left: 60px">p = np.zeros(max_len)<br/>p[0] = word2idx['&lt;start&gt;']<br/>for i in range(max_len-1):<br/>     pred= final_model.predict([x33[l].reshape(1,7,7,512)/12, p.reshape(1,max_len)])<br/>     pred2 = np.argmax(pred)<br/>     print(idx2word[pred2])<br/>     p[i+1] = pred2<br/>     if(idx2word[pred2]=='&lt;end&gt;'):<br/>         break</pre>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign">The output of the preceding code is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/6ef9c565-3bcb-4d8b-bc54-4776d13e1022.png" style="width:5.75em;height:13.00em;" width="70" height="158"/></p>
<p>Note that the generated caption correctly detected that the dog is black and is also jumping.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Generating captions, using beam search</h1>
                </header>
            
            <article>
                
<p>In the previous section on caption generation, we have decoded based on the word that has the highest probability in a given time step. In this section, we'll improve upon the predicted captions by using beam search.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Beam search works as follows:</p>
<ul>
<li>Extract the probability of various words in first time step (where VGG16 features of the picture and the start token are the input)</li>
<li>Instead of providing the most probable word as the output, we'll consider the top three probable words</li>
<li>We'll proceed to the next time step, where we extract the top three characters in this time step</li>
<li>We'll loop through the top three predictions in first time step, as an input to the prediction of second time step, and extract the top three predictions for each of the possible top three predictions in input:
<ul>
<li>Let's say that <em>a</em>, <em>b</em>, and <em>c</em> are the top three predictions in time-step one</li>
<li>We'll use <em>a</em> as input along with VGG16 features to predict the top three probable characters in time-step two, and similarly for <em>b</em> and <em>c</em></li>
<li>We have a total of nine combinations of outputs between the first time- step and the second time-step</li>
<li>Along with the combination, we'll also store the confidence of each prediction across all nine combinations:
<ul>
<li>For example: if the probability of <em>a</em> in time-step one is 0.4 and the probability of <em>x</em> in time step two is 0.5, then the probability of combination is 0.4 x 0.5 = 0.2</li>
</ul>
</li>
<li>We'll keep the top three combinations and discard the rest of the combinations</li>
</ul>
</li>
<li>We'll repeat the preceding step of shortlisting the top three combinations until we reach the end of the sentence</li>
</ul>
<p>The value of three is the beam length across which we are searching for the combination.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In this section, we'll code up the beam-search strategy that we discussed previously <span>(the code file is available as </span><kbd>Image_captioning.ipynb</kbd><span> in GitHub)</span>:</p>
<ol>
<li>Define a function that takes the VGG16 features of the picture as input, along with the sequence of words and their corresponding confidences from the previous time steps and return the top three predictions in the current time step:</li>
</ol>
<pre style="padding-left: 60px">beamsize = 3<br/>def get_top3(img, string_with_conf):<br/>     tokens, confidence = string_with_conf<br/>     p = np.zeros((1, max_len))<br/>     p[0, :len(tokens)] = np.array(tokens)<br/>     pred = final_model.predict([img.reshape(1,7,7,512)/12, p])<br/>     best_pred = list(np.argsort(pred)[0][-beamsize:])<br/>     best_confs = list(pred[0,best_pred])<br/>     top_best = [(tokens + list([best_pred[i]]), confidence*best_confs[i]) for i in range(beamsize)]<br/>     return top_best</pre>
<p style="padding-left: 60px">In the preceding step, we are separating the word IDs and their corresponding confidences provided in the <kbd>string_with_conf</kbd> parameter. Furthermore, we are storing the sequence of tokens in an array and using that to make a prediction.</p>
<p style="padding-left: 60px">In the next step, we are extracting the top three predictions in the next time step and storing it in <kbd>best_pred</kbd>.</p>
<p style="padding-left: 60px">Additionally, along with the best prediction of word IDs, we are also storing the confidence associated with each top three prediction in the current time step.</p>
<p style="padding-left: 60px">Finally, we are returning the three predictions of the second time step.</p>
<ol start="2">
<li>Loop through the range of the maximum possible length of sentence and extract the top three possible combinations of words across all time steps:</li>
</ol>
<pre style="padding-left: 60px">start_token = word2idx['&lt;start&gt;']<br/>best_strings = [([start_token], 1)]<br/>for i in range(max_len):<br/>     new_best_strings = []<br/>     for string in best_strings:<br/>         strings = get_top3(x33[l], string)<br/>         new_best_strings.extend(strings) <br/>         best_strings = sorted(new_best_strings, key=lambda x: x[1], reverse=True)[:beamsize]</pre>
<ol start="3">
<li>Loop through the preceding <kbd>best_strings</kbd> obtained to print the output:</li>
</ol>
<pre style="padding-left: 60px">for i in range(3):<br/>     string = best_strings[i][0]<br/>     print('============')<br/>     for j in string:<br/>         print(idx2word[j])<br/>         if(idx2word[j]=='&lt;end&gt;'):<br/>             break</pre>
<p style="padding-left: 60px">The output sentences for the same picture that we tested in the previous section are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/ec9b27db-9d6e-46a2-9716-f0b4e19d595b.png" style="width:8.50em;height:16.75em;" width="87" height="170"/>  <img src="Images/1448ba62-e00b-4a79-a24b-23d879f650c1.png" style="width:6.75em;height:16.67em;" width="70" height="174"/>  <img src="Images/764e7e00-72e6-45cc-ae51-1fe3818710f2.png" style="width:6.33em;height:16.75em;" width="64" height="169"/></p>
<p>Note that, in this specific case, the first and second sentences differed when it came to the words <kbd>jumping</kbd> and <kbd>playing</kbd>, and the third sentence happened to be the same as the first, as the probability of combination was much higher.</p>


            </article>

            
        </section>
    </div>



  </body></html>