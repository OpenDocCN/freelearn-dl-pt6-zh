- en: Getting Started with Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '深度学习入门  '
- en: In this chapter, we will explain some basic concepts of **Machine Learning**
    (**ML**) and **Deep Learning (DL)** that will be used in all subsequent chapters.
    We will start with a brief introduction to ML. Then we will move on to DL, which
    is one of the emerging branches of ML.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '在本章中，我们将解释一些基础的**机器学习**（**ML**）和**深度学习（DL）**概念，这些概念将在后续的所有章节中使用。我们将从简要介绍机器学习开始。接下来，我们将讲解深度学习，它是机器学习的一个新兴分支。  '
- en: We will briefly discuss some of the most well-known and widely used neural network
    architectures. Next, we will look at various features of deep learning frameworks
    and libraries. Then we will see how to prepare a programming environment, before
    moving on to coding with some open source, deep learning libraries such as **DeepLearning4J
    (DL4J)**.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将简要讨论一些最著名和广泛使用的神经网络架构。接下来，我们将了解深度学习框架和库的各种特性。然后，我们将学习如何准备编程环境，在此基础上使用一些开源深度学习库，如**DeepLearning4J
    (DL4J)**进行编程。  '
- en: 'Then we will solve a very famous ML problem: the Titanic survival prediction.
    For this, we will use an Apache Spark-based **Multilayer Perceptron** (**MLP**)
    classifier to solve this problem. Finally, we''ll see some frequently asked questions
    that will help us generalize our basic understanding of DL. Briefly, the following
    topics will be covered:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '然后我们将解决一个非常著名的机器学习问题：泰坦尼克号生存预测。为此，我们将使用基于 Apache Spark 的**多层感知器**（**MLP**）分类器来解决这个问题。最后，我们将看到一些常见问题解答，帮助我们将深度学习的基本理解推广到更广泛的应用。简而言之，以下主题将被覆盖：  '
- en: A soft introduction to ML
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '机器学习的简单介绍  '
- en: Artificial Neural Networks (ANNs)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工神经网络（ANNs）
- en: Deep neural network architectures
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '深度神经网络架构  '
- en: Deep learning frameworks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '深度学习框架  '
- en: Deep learning from disasters—Titanic survival prediction using MLP
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '从灾难中学习深度学习——使用 MLP 进行泰坦尼克号生存预测  '
- en: Frequently asked questions (FAQ)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '常见问题解答（FAQ）  '
- en: A soft introduction to ML
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '机器学习的简单介绍  '
- en: ML approaches are based on a set of statistical and mathematical algorithms
    in order to carry out tasks such as classification, regression analysis, concept
    learning, predictive modeling, clustering, and mining of useful patterns. Thus,
    with the use of ML, we aim at improving the learning experience such that it becomes
    automatic. Consequently, we may not need complete human interactions, or at least
    we can reduce the level of such interactions as much as possible.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '机器学习方法基于一组统计和数学算法，以执行诸如分类、回归分析、概念学习、预测建模、聚类和挖掘有用模式等任务。因此，通过使用机器学习，我们旨在改善学习体验，使其变得自动化。结果，我们可能不需要完全的人类互动，或者至少我们可以尽可能减少这种互动的程度。  '
- en: Working principles of ML algorithms
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '机器学习算法的工作原理  '
- en: 'We now refer to a famous definition of ML by Tom M. Mitchell (*Machine Learning,
    Tom Mitchell, McGraw Hill*), where he explained what learning really means from
    a computer science perspective:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '我们现在引用 Tom M. Mitchell 的经典机器学习定义（《机器学习，Tom Mitchell，McGraw Hill》），他从计算机科学的角度解释了学习真正意味着什么：  '
- en: '"A computer program is said to learn from experience E with respect to some
    class of tasks T and performance measure P, if its performance at tasks in T,
    as measured by P, improves with experience E."'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '“如果一个计算机程序在经验 E 的基础上，在某些任务类别 T 和性能度量 P 的衡量下，其在任务 T 上的表现通过经验 E 得到提升，那么我们就说该程序从经验中学习。”  '
- en: 'Based on this definition, we can conclude that a computer program or machine
    can do the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '根据这个定义，我们可以得出结论：计算机程序或机器可以执行以下任务：  '
- en: Learn from data and histories
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '从数据和历史中学习  '
- en: Improve with experience
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '通过经验提升  '
- en: Iteratively enhance a model that can be used to predict outcomes of questions
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '迭代优化一个可以用于预测问题结果的模型  '
- en: 'Since they are at the core of predictive analytics, almost every ML algorithm
    we use can be treated as an optimization problem. This is about finding parameters
    that minimize an objective function, for example, a weighted sum of two terms
    like a cost function and regularization. Typically, an objective function has
    two components:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '由于它们是预测分析的核心，几乎我们使用的每个机器学习算法都可以视为一个优化问题。这涉及到找到最小化目标函数的参数，例如，像成本函数和正则化这样的加权和。通常，一个目标函数有两个组成部分：  '
- en: A regularizer, which controls the complexity of the model
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个正则化器，用来控制模型的复杂性  '
- en: The loss, which measures the error of the model on the training data.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '损失，衡量模型在训练数据上的误差。  '
- en: On the other hand, the regularization parameter defines the trade-off between
    minimizing the training error and the model's complexity in an effort to avoid
    overfitting problems. Now, if both of these components are convex, then their
    sum is also convex; it is non-convex otherwise. More elaborately, when using an
    ML algorithm, the goal is to obtain the best hyperparameters of a function that
    return the minimum error when making predictions. Therefore, using a convex optimization
    technique, we can minimize the function until it converges towards the minimum
    error.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that a problem is convex, it is usually easier to analyze the asymptotic
    behavior of the algorithm, which shows how fast it converges as the model observes
    more and more training data. The challenge of ML is to allow training a model
    so that it can recognize complex patterns and make decisions not only in an automated
    way but also as intelligently as possible. The entire learning process requires
    input datasets that can be split (or are already provided) into three types, outlined
    as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '**A training set** is the knowledge base coming from historical or live data
    used to fit the parameters of the ML algorithm. During the training phase, the
    ML model utilizes the training set to find optimal weights of the network and
    reach the objective function by minimizing the training error. Here, the **back-prop
    rule** (or another more advanced optimizer with a proper updater; we''ll see this
    later on) is used to train the model, but all the hyperparameters are need to
    be set before the learning process starts**.**'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A validation set** is a set of examples used to tune the parameters of an
    ML model. It ensures that the model is trained well and generalizes towards avoiding
    overfitting. Some ML practitioners refer to it as a **development set** or **dev
    set** as well.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A test set** is used for evaluating the performance of the trained model
    on unseen data. This step is also referred to as **model inferencing**. After
    assessing the final model on the test set (that is, when we''re fully satisfied
    with the model''s performance), we do not have to tune the model any further but
    the trained model can be deployed in a production-ready environment.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A common practice is splitting the input data (after necessary pre-processing
    and feature engineering) into 60% for training, 10% for validation, and 20% for
    testing, but it really depends on use cases. Also, sometimes we need to perform
    up-sampling or down-sampling on the data based on the availability and quality
    of the datasets.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, the learning theory uses mathematical tools that derive from probability
    theory and information theory. Three learning paradigms will be briefly discussed:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram summarizes the three types of learning, along with the
    problems they address:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/417d3a88-3435-4383-a62a-f387d8a98dac.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: Types of learning and related problems
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: '**Supervised learning** is the simplest and most well-known automatic learning
    task. It is based on a number of pre-defined examples, in which the category to
    which each of the inputs should belong is already known. *Figure 2* shows a typical
    workflow of supervised learning.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督学习**是最简单且最著名的自动学习任务。它基于一组预定义的示例，其中每个输入所属的类别已经知道。*图2*展示了监督学习的典型工作流程。'
- en: 'An actor (for example, an ML practitioner, data scientist, data engineer, ML
    engineer, and so on) performs **Extraction Transformation Load** (**ETL**) and
    the necessary feature engineering (including feature extraction, selection, and
    so on) to get the appropriate data having features and labels. Then he does the
    following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一位参与者（例如，机器学习实践者、数据科学家、数据工程师、机器学习工程师等）执行**提取转换加载**（**ETL**）及必要的特征工程（包括特征提取、选择等），以获得具有特征和标签的适当数据。然后他执行以下操作：
- en: Splits the data into training, development, and test sets
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据拆分为训练集、开发集和测试集
- en: Uses the training set to train an ML model
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练集训练机器学习模型
- en: The validation set is used to validate the training against the overfitting
    problem and regularization
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证集用于验证训练是否过拟合以及正则化
- en: He then evaluates the model's performance on the test set (that is unseen data)
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，他在测试集上评估模型的表现（即未见过的数据）
- en: If the performance is not satisfactory, he can perform additional tuning to
    get the best model based on hyperparameter optimization
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果性能不令人满意，他可以进行额外的调优，以通过超参数优化获得最佳模型
- en: Finally, he deploys the best model in a production-ready environment
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，他将最佳模型部署到生产环境中
- en: '![](img/784a5592-403f-409c-8cbf-5cb2eba4deab.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/784a5592-403f-409c-8cbf-5cb2eba4deab.png)'
- en: Supervised learning in action
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习的实际应用
- en: In the overall life cycle, there might be many actors involved (for example,
    a data engineer, data scientist, or ML engineer) to perform each step independently
    or collaboratively.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个生命周期中，可能会有多个参与者参与（例如，数据工程师、数据科学家或机器学习工程师），他们独立或协作执行每个步骤。
- en: The supervised learning context includes **classification** and **regression**
    tasks; classification is used to predict which class a data point is part of (**discrete
    value**), while regression is used to predict **continuous values**. In other
    words, a classification task is used to predict the label of the class attribute,
    while a regression task is used to make a numeric prediction of the class attribute.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习的任务包括**分类**和**回归**；分类用于预测数据点属于哪个类别（**离散值**），而回归用于预测**连续值**。换句话说，分类任务用于预测类属性的标签，而回归任务则用于对类属性进行数值预测。
- en: In the context of supervised learning, **unbalanced data** refers to classification
    problems where we have unequal instances for different classes. For example, if
    we have a classification task for only two classes, **balanced data** would mean
    50% pre-classified examples for each of the classes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习的背景下，**不平衡数据**指的是分类问题，其中不同类别的实例数量不平衡。例如，如果我们有一个仅针对两个类别的分类任务，**平衡数据**意味着每个类别都有50%的预先分类示例。
- en: If the input dataset is a little unbalanced (for example, 60% data points for
    one class and 40% for the other class), the learning process will require for
    the input dataset to be split randomly into three sets, with 50% for the training
    set, 20% for the validation set, and the remaining 30% for the testing set.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入数据集稍微不平衡（例如，某一类占60%，另一类占40%），学习过程将要求将输入数据集随机拆分为三个子集，其中50%用于训练集，20%用于验证集，剩余30%用于测试集。
- en: Unsupervised learning
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: In **unsupervised learning**, an input set is supplied to the system during
    the training phase. In contrast with supervised learning, the input objects are
    not labeled with their class. For classification, we assumed that we are given
    a training dataset of correctly labeled data. Unfortunately, we do not always
    have that advantage when we collect data in the real world.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在**无监督学习**中，训练阶段将输入集提供给系统。与监督学习不同，输入对象没有被标记其类别。对于分类任务，我们假设给定一个正确标记的数据集。然而，在现实世界中收集数据时，我们不总是拥有这种优势。
- en: For example, let's say you have a large collection of totally legal, not pirated,
    MP3 files in a crowded and massive folder on your hard drive. In such a case,
    how could we possibly group songs together if we do not have direct access to
    their metadata? One possible approach could be to mix various ML techniques, but
    clustering is often the best solution.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, what if you can build a clustering predictive model that helps automatically
    group together similar songs and organize them into your favorite categories,
    such as *country*, *rap*, *rock*, and so on? In short, unsupervised learning algorithms
    are commonly used in clustering problems. The following diagram gives us an idea
    of a clustering technique applied to solve this kind of problem:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e2bb782-2a63-4dfd-a242-dc8b18df8dc7.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: Clustering techniques – an example of unsupervised learning
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Although the data points are not labeled, we can still do the necessary feature
    engineering and grouping of a set of objects in such a way that objects in the
    same group (called a **cluster**) are brought together. This is not easy for a
    human. Rather, a standard approach is to define a similarity measure between two
    objects and then look for any cluster of objects that are more similar to each
    other than they are to the objects in the other clusters. Once we've done the
    clustering of the data points (that is, MP3 files) and the validation is completed,
    we know the pattern of the data (that is, what type of MP3 files fall in which
    group).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Reinforcement learning** is an artificial intelligence approach that focuses
    on the learning of the system through its interactions with the environment. In
    reinforcement learning, the system''s parameters are adapted based on the feedback
    obtained from the environment, which in turn provides feedback on the decisions
    made by the system. The following diagram shows a person making decisions in order
    to arrive at their destination.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take an example of the route you take from home to work. In this case,
    you take the same route to work every day. However, out of the blue, one day you
    get curious and decide to try a different route with a view to finding the shortest
    path. This dilemma of trying out new routes or sticking to the best-known route
    is an example of **exploration versus exploitation**:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed24ebcf-4492-4302-9226-33c35d8f9972.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: An agent always tries to reach the destination
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: We can take a look at one more example in terms of a system modeling a chess
    player. In order to improve its performance, the system utilizes the result of
    its previous moves; such a system is said to be a system learning with reinforcement.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Putting ML tasks altogether
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have seen the basic working principles of ML algorithms. Then we have seen
    what the basic ML tasks are and how they formulate domain-specific problems. Now
    let''s take a look at how can we summarize ML tasks and some applications in the
    following diagram:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/899ceaf3-c710-4675-ae99-33c76cd6ac2f.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/899ceaf3-c710-4675-ae99-33c76cd6ac2f.png)'
- en: ML tasks and some use cases from different application domains
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 来自不同应用领域的机器学习任务及一些应用案例
- en: However, the preceding figure lists only a few use cases and applications using
    different ML tasks. In practice, ML is used in numerous use cases and applications.
    We will try to cover a few of those throughout this book.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，前面的图表只列出了使用不同机器学习任务的一些应用案例。在实践中，机器学习在许多应用场景中都有广泛应用。我们将在本书中尽量涵盖其中的一些案例。
- en: Delving into deep learning
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入了解深度学习
- en: Simple ML methods that were used in normal-size data analysis are not effective
    anymore and should be substituted by more robust ML methods. Although classical
    ML techniques allow researchers to identify groups or clusters of related variables,
    the accuracy and effectiveness of these methods diminish with large and high-dimensional
    datasets.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以往在常规数据分析中使用的简单机器学习方法已经不再有效，应该被更强大的机器学习方法所替代。尽管传统的机器学习技术允许研究人员识别相关变量的组或聚类，但随着大规模和高维数据集的增加，这些方法的准确性和有效性逐渐降低。
- en: Here comes deep learning, which is one of the most important developments in
    artificial intelligence in the last few years. Deep learning is a branch of ML
    based on a set of algorithms that attempt to model high-level abstractions in
    data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里出现了深度学习，它是近年来人工智能领域最重要的进展之一。深度学习是机器学习的一个分支，基于一套算法，旨在尝试对数据中的高级抽象进行建模。
- en: How did DL take ML into next level?
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习是如何将机器学习提升到一个新水平的？
- en: 'In short, deep learning algorithms are mostly a set of ANNs that can make better
    representations of large-scale datasets, in order to build models that learn these
    representations very extensively. Nowadays it''s not limited to ANNs, but there
    have been really many theoretical advances and software and hardware improvements
    that were necessary for us to get to this day. In this regard, Ian Goodfellow
    et al. (Deep Learning, MIT Press, 2016) defined deep learning as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，深度学习算法大多是一些人工神经网络（ANN），它们能够更好地表示大规模数据集，从而建立能够深入学习这些表示的模型。如今，这不仅仅局限于人工神经网络，实际上，理论上的进展以及软件和硬件的改进都是我们能够走到今天这一步的必要条件。在这方面，Ian
    Goodfellow 等人（《深度学习》，MIT出版社，2016年）将深度学习定义如下：
- en: '"Deep learning is a particular kind of machine learning that achieves great
    power and flexibility by learning to represent the world as a nested hierarchy
    of concepts, with each concept defined in relation to simpler concepts, and more
    abstract representations computed in terms of less abstract ones."'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: “深度学习是一种特定类型的机器学习，通过学习将世界表示为一个嵌套的概念层级结构，在这个层级中，每个概念都是相对于更简单的概念来定义的，更抽象的表示是通过较不抽象的表示来计算的，从而实现了巨大的能力和灵活性。”
- en: 'Let''s take an example; suppose we want to develop a predictive analytics model,
    such as an animal recognizer, where our system has to resolve two problems:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举个例子；假设我们想开发一个预测分析模型，比如一个动物识别器，在这种情况下，我们的系统需要解决两个问题：
- en: To classify whether an image represents a cat or a dog
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于判断一张图片是猫还是狗
- en: To cluster images of dogs and cats.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于将狗和猫的图片进行聚类。
- en: If we solve the first problem using a typical ML method, we must define the
    facial features (ears, eyes, whiskers, and so on) and write a method to identify
    which features (typically nonlinear) are more important when classifying a particular
    animal.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用典型的机器学习方法来解决第一个问题，我们必须定义面部特征（如耳朵、眼睛、胡须等），并编写一个方法来识别在分类特定动物时哪些特征（通常是非线性的）更为重要。
- en: However, at the same time, we cannot address the second problem because classical
    ML algorithms for clustering images (such as **k-means**) cannot handle nonlinear
    features. Deep learning algorithms will take these two problems one step further
    and the most important features will be extracted automatically after determining
    which features are the most important for classification or clustering.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与此同时，我们无法解决第二个问题，因为用于图像聚类的传统机器学习算法（例如**k-means**）无法处理非线性特征。深度学习算法将把这两个问题提升一个层次，最重要的特征将在确定哪些特征对分类或聚类最为重要后自动提取。
- en: 'In contrast, when using a classical ML algorithm, we would have to provide
    the features manually. In summary, the deep learning workflow would be as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，当使用传统的机器学习算法时，我们必须手动提供这些特征。总结来说，深度学习的工作流程如下：
- en: A deep learning algorithm would first identify the edges that are most relevant
    when clustering cats or dogs. It would then try to find various combinations of
    shapes and edges hierarchically. This step is called ETL.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习算法首先会识别在聚类猫或狗时最相关的边缘。接着，它会尝试以层级方式找到各种形状和边缘的组合。这个步骤叫做ETL（提取、转换、加载）。
- en: After several iterations, hierarchical identification of complex concepts and
    features is carried out. Then, based on the identified features, the DL algorithm
    automatically decides which of these features are most significant (statistically)
    to classify the animal. This step is feature extraction.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经过多次迭代后，复杂概念和特征的层级识别被执行。然后，基于已识别的特征，深度学习算法自动决定哪些特征在统计上对分类动物最为重要。这个步骤叫做特征提取。
- en: Finally, it takes out the label column and performs unsupervised training using
    **AutoEncoders** (**AEs**) to extract the latent features to be redistributed
    to k-means for clustering.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，算法去掉标签列，使用**自编码器**（**AEs**）进行无监督训练，以提取潜在特征，再将这些特征分配给k-means进行聚类。
- en: Then the clustering assignment hardening loss (CAH loss) and reconstruction
    loss are jointly optimized towards optimal clustering assignment. Deep Embedding
    Clustering (see more at [https://arxiv.org/pdf/1511.06335.pdf](https://arxiv.org/pdf/1511.06335.pdf))
    is an example of such an approach. We will discuss deep learning-based clustering
    approaches in [Chapter 11](b458baf5-6590-4e69-84d4-8c02a898dbca.xhtml), *Discussion,
    Current Trends, and Outlook*.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，聚类分配硬化损失（CAH损失）和重建损失被联合优化，以实现最优的聚类分配。深度嵌入聚类（详见[https://arxiv.org/pdf/1511.06335.pdf](https://arxiv.org/pdf/1511.06335.pdf)）就是这种方法的一个例子。我们将在[第11章](b458baf5-6590-4e69-84d4-8c02a898dbca.xhtml)中讨论基于深度学习的聚类方法，*讨论、当前趋势与展望*。
- en: Up to this point, we have seen that deep learning systems are able to recognize
    what an image represents. A computer does not see an image as we see it because
    it only knows the position of each pixel and its color. Using deep learning techniques,
    the image is divided into various layers of analysis.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们看到深度学习系统能够识别图像代表的是什么。计算机看图像的方式与我们不同，因为它只知道每个像素的位置和颜色。通过深度学习技术，图像被分解成多个分析层次。
- en: 'At a lower level, the software analyzes, for example, a grid of a few pixels
    with the task of detecting a type of color or various nuances. If it finds something,
    it informs the next level, which at this point checks whether or not that given
    color belongs to a larger form, such as a line. The process continues to the upper
    levels until you understand what is shown in the image. The following diagram
    shows what we have discussed in the case of an image classification system:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在较低的层次上，软件分析例如一小块像素网格，任务是检测某种颜色或其不同的色调。如果它发现了什么，它会通知下一层，下一层会检查该颜色是否属于更大的形态，比如一条线。这个过程会一直持续到更高层次，直到你理解图像展示的内容。下图展示了我们在图像分类系统中讨论的内容：
- en: '![](img/9ee85d48-6b19-4c74-bce0-196adfd06480.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ee85d48-6b19-4c74-bce0-196adfd06480.png)'
- en: A deep learning system at work on a dog versus cat classification problem
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理狗与猫分类问题时的深度学习系统工作原理
- en: 'More precisely, the preceding image classifier can be built layer by layer,
    as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 更准确地说，前述的图像分类器可以逐层构建，如下所示：
- en: '**Layer 1**: The algorithm starts identifying the dark and light pixels from
    the raw images'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第一层**：算法开始识别原始图像中的暗像素和亮像素。'
- en: '**Layer 2**: The algorithm then identifies edges and shapes'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第二层**：算法接着识别边缘和形状。'
- en: '**Layer 3**: It then learns more complex shapes and objects'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第三层**：接下来，算法识别更复杂的形状和物体。'
- en: '**Layer 4**: The algorithm then learns which objects define a human face'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第四层**：算法接着学习哪些物体定义了人脸。'
- en: Although this is a very simple classifier, software capable of doing these types
    of things is now widespread and is found in systems for recognizing faces, or
    in those for searching by an image on Google, for example. These pieces of software
    are based on deep learning algorithms.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这只是一个非常简单的分类器，但能够进行这些操作的软件如今已经非常普及，广泛应用于面部识别系统，或例如在Google中通过图像进行搜索的系统。这些软件是基于深度学习算法的。
- en: On the contrary, by using a linear ML algorithm, we cannot build such applications
    since these algorithms are incapable of handling nonlinear image features. Also,
    using ML approaches, we typically handle a few hyperparameters only. However,
    when neural networks are brought to the party, things become too complex. In each
    layer, there are millions or even billions of hyperparameters to tune, so much
    that the cost function becomes non-convex.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Another reason is that activation functions used in hidden layers are nonlinear,
    so the cost is non-convex. We will discuss this phenomenon in more detail in later
    chapters but let's take a quick look at ANNs.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Artificial Neural Networks
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ANNs work on the concept of deep learning. They represent the human nervous
    system in how the nervous system consists of a number of neurons that communicate
    with each other using axons.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Biological neurons
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The working principles of ANNs are inspired by how a human brain works, depicted
    in *Figure 7*. The receptors receive the stimuli either internally or from the
    external world; then they pass the information into the biological *neurons* for
    further processing. There are a number of dendrites, in addition to another long
    extension called the **axon**.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'Towards its extremity, there are minuscule structures called **synaptic terminals,**
    used to connect one neuron to the dendrites of other neurons. Biological neurons
    receive short electrical impulses called **signals** from other neurons, and in
    response, they trigger their own signals:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3974bdf0-c8d9-4568-a829-fec048f9598c.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
- en: Working principle of biological neurons
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: We can thus summarize that the neuron comprises a cell body (also known as the
    soma), one or more **dendrites** for receiving signals from other neurons, and
    an **axon** for carrying out the signals generated by the neurons.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: A neuron is in an active state when it is sending signals to other neurons.
    However, when it is receiving signals from other neurons, it is in an inactive
    state. In an idle state, a neuron accumulates all the signals received before
    reaching a certain activation threshold. This whole thing motivated researchers
    to introduce an ANN.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: A brief history of ANNs
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Inspired by the working principles of biological neurons, Warren McCulloch and
    Walter Pitts proposed the first artificial neuron model in 1943 in terms of a
    computational model of nervous activity. This simple model of a biological neuron,
    also known as an **artificial neuron (AN),** has one or more binary (on/off) inputs
    and one output only.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'An AN simply activates its output when more than a certain number of its inputs
    are active. For example, here we see a few ANNs that perform various logical operations.
    In this example, we assume that a neuron is activated only when at least two of
    its inputs are active:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e2b1c75-5e3e-4236-8a64-38389a64f4cf.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: ANNs performing simple logical computations
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: The example sounds too trivial, but even with such a simplified model, it is
    possible to build a network of ANs. Nevertheless, these networks can be combined
    to compute complex logical expressions too. This simplified model inspired John
    von Neumann, Marvin Minsky, Frank Rosenblatt, and many others to come up with
    another model called a **perceptron** back in 1957.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'The perceptron is one of the simplest ANN architectures we''ve seen in the
    last 60 years. It is based on a slightly different AN called a **Linear Threshold
    Unit** (**LTU**). The only difference is that the inputs and outputs are now numbers
    instead of binary on/off values. Each input connection is associated with a weight.
    The LTU computes a weighted sum of its inputs, then applies a step function (which
    resembles the action of an activation function) to that sum, and outputs the result:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75a216da-3354-4b5f-bd0a-023f44330222.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: The left-side figure represents an LTU and the right-side figure shows a perceptron
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: One of the downsides of a perceptron is that its decision boundary is linear.
    Therefore, they are incapable of learning complex patterns. They are also incapable
    of solving some simple problems like **Exclusive OR** (**XOR**). However, later
    on, the limitations of perceptrons were somewhat eliminated by stacking multiple
    perceptrons, called MLP.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: How does an ANN learn?
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Based on the concept of biological neurons, the term and the idea of ANs arose.
    Similarly to biological neurons, the artificial neuron consists of the following:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: One or more incoming connections that aggregate signals from neurons
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One or more output connections for carrying the signal to the other neurons
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An **activation function**, which determines the numerical value of the output
    signal
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The learning process of a neural network is configured as an *iterative process*
    of *optimization* of the *weights* (see more in the next section). The weights
    are updated in each epoch. Once the training starts, the aim is to generate predictions
    by minimizing the loss function. The performance of the network is then evaluated
    on the test set.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Now we know the simple concept of an artificial neuron. However, generating
    only some artificial signals is not enough to learn a complex task. Albeit, a
    commonly used supervised learning algorithm is the backpropagation algorithm,
    which is very commonly used to train a complex ANN.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: ANNs and the backpropagation algorithm
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The backpropagation algorithm aims to minimize the error between the current
    and the desired output. Since the network is feedforward, the activation flow
    always proceeds forward from the input units to the output units.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'The gradient of the cost function is backpropagated and the network weights
    get updated; the overall method can be applied to any number of hidden layers
    recursively. In such a method, the incorporation between two phases is important.
    In short, the basic steps of the training procedure are as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the network with some random (or more advanced XAVIER) weights
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For all training cases, follow the steps of forward and backward passes as outlined
    next
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Forward and backward passes
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the forward pass, a number of operations are performed to obtain some predictions
    or scores. In such an operation, a graph is created, connecting all dependent
    operations in a top-to-bottom fashion. Then the network's error is computed, which
    is the difference between the predicted output and the actual output.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the backward pass is involved mainly with mathematical operations,
    such as creating derivatives for all differential operations (that is auto-differentiation
    methods), top to bottom (for example, measuring the loss function to update the
    network weights), for all the operations in the graph, and then using them in
    chain rule.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'In this pass, for all layers starting with the output layer back to the input
    layer, it shows the network layer''s output with the correct input (error function).
    Then it adapts the weights in the current layer to minimize the error function.
    This is backpropagation''s optimization step. By the way, there are two types
    of auto-differentiation methods:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '**Reverse mode**: Derivation of a single output with respect to all inputs'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Forward mode**: Derivation of all outputs with respect to one input'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The backpropagation algorithm processes the information in such a way that the
    network decreases the global error during the learning iterations; however, this
    does not guarantee that the global minimum is reached. The presence of hidden
    units and the nonlinearity of the output function mean that the behavior of the
    error is very complex and has many local minimas.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: This backpropagation step is typically performed thousands or millions of times,
    using many training batches, until the model parameters converge to values that
    minimize the cost function. The training process ends when the error on the validation
    set begins to increase, because this could mark the beginning of a phase overfitting.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Weights and biases
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Besides the state of a neuron, synaptic weight is considered, which influences
    the connection within the network. Each weight has a numerical value indicated
    by *W[ij]*, which is the synaptic weight connecting neuron *i* to neuron *j*.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '**Synaptic weight**: This concept evolved from biology and refers to the strength
    or amplitude of a connection between two nodes, corresponding in biology to the
    amount of influence the firing of one neuron has on another.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'For each neuron (also known as, unit) *i*, an input vector can be defined by
    *x[i]*= (*x[1]*, *x[2]*,...*x[n]*) and a weight vector can be defined by *w[i]*=
    (*w[i1]*, *w[i2]*,...*w[in]*). Now, depending on the position of a neuron, the
    weights and the output function determine the behavior of an individual neuron.
    Then during forward propagation, each unit in the hidden layer gets the following
    signal:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e468c385-a1c8-4d70-a6c5-6ecf83d1cdb4.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
- en: 'Nevertheless, among the weights, there is also a special type of weight called
    *bias* unit *b.* Technically, bias units aren''t connected to any previous layer,
    so they don''t have true activity. But still, the bias *b* value allows the neural
    network to shift the activation function to the left or right. Now, taking the
    bias unit into consideration, the modified network output can be formulated as
    follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ebd48a2a-9c42-4031-8d49-48b9a900f105.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: 'The preceding equation signifies that each hidden unit gets the sum of inputs
    multiplied by the corresponding weight—summing junction. Then the resultant in
    the summing junction is passed through the activation function, which squashes
    the output as depicted in the following figure:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/64811b83-4195-4c85-97db-d7a5c3cda41d.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: Artificial neuron model
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, a tricky question: how do we initialize the weights? Well, if we initialize
    all weights to the same value (for example, 0 or 1), each hidden neuron will get
    exactly the same signal. Let''s try to break it down:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: If all weights are initialized to 1, then each unit gets a signal equal to the
    sum of the inputs
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If all weights are 0, which is even worse, every neuron in a hidden layer will
    get zero signal
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For network weight initialization, Xavier initialization is nowadays used widely.
    It is similar to random initialization but often turns out to work much better
    since it can automatically determine the scale of initialization based on the
    number of input and output neurons.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'Interested readers should refer to this publication for detailed info: Xavier
    Glorot and Yoshua Bengio, *Understanding the difficulty of training deep feedforward
    neural networks*: proceedings of the 13^(th) international conference on **Artificial
    Intelligence and Statistics** (**AISTATS**) 2010, Chia Laguna Resort, Sardinia,
    Italy; Volume 9 of JMLR: W&CP.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: You may be wondering whether you can get rid of random initialization while
    training a regular DNN (for example, MLP or DBN). Well, recently, some researchers
    have been talking about random orthogonal matrix initializations that perform
    better than just any random initialization for training DNNs.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to initializing the biases, we can initialize them to be zero.
    But setting the biases to a small constant value such as 0.01 for all biases ensures
    that all **Rectified Linear Unit** (**ReLU**) units can propagate some gradient.
    However, it neither performs well nor shows consistent improvement. Therefore,
    sticking with zero is recommended.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Weight optimization
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before the training starts, the network parameters are set randomly. Then to
    optimize the network weights, an iterative algorithm called **Gradient Descent**
    (**GD**) is used. Using GD optimization, our network computes the cost gradient
    based on the training set. Then, through an iterative process, the gradient *G*
    of the error function *E* is computed*.*
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'In following graph, gradient **G** of error function ***E*** provides the direction
    in which the error function with current values has the steeper slope. Since the
    ultimate target is to reduce the network error, GD makes small steps in the opposite
    direction *-***G**. This iterative process is executed a number of times, so the
    error *E* would move down towards the global minima*.* This way, the ultimate
    target is to reach a point where **G = 0***,* where no further optimization is
    possible:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd401445-a7a4-4b48-87da-94f13c8d52d1.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
- en: Searching for the minimum for the error function E; we move in the direction
    in which the gradient G of E is minimal
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: The downside is that it takes too long to converge, which makes it impossible
    to meet the demand of handling large-scale training data. Therefore, a faster
    GD called **Stochastic Gradient Descent** (**SDG**) is proposed, which is also
    a widely used optimizer in DNN training. In SGD, we use only one training sample
    per iteration from the training set to update the network parameters.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: I'm not saying SGD is the only available optimization algorithm, but there are
    so many advanced optimizers available nowadays, for example, Adam, RMSProp, ADAGrad,
    Momentum, and so on. More or less, most of them are either direct or indirect
    optimized versions of SGD.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: By the way, the term **stochastic** comes from the fact that the gradient based
    on a single training sample per iteration is a stochastic approximation of the
    true cost gradient.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To allow a neural network to learn complex decision boundaries, we apply a
    non-linear activation function to some of its layers. Commonly used functions
    include Tanh, ReLU, softmax, and variants of these. More technically, each neuron
    receives as input signal the weighted sum of the synaptic weights and the activation
    values of the neurons connected. One of the most widely used functions for this
    purpose is the so-called **sigmoid function**. It is a special case of the logistic
    function, which is defined by the following formula:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/60003fbe-ed08-424d-9478-582a826551bd.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: The domain of this function includes all real numbers, and the co-domain is
    (*0, 1*). This means that any value obtained as an output from a neuron (as per
    the calculation of its activation state), will always be between zero and one.
    The sigmoid function, as represented in the following diagram, provides an interpretation
    of the saturation rate of a neuron, from not being active (*= 0*) to complete
    saturation, which occurs at a predetermined maximum value (*= 1*).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, a hyperbolic tangent, or **tanh**, is another form of the
    activation function. Tanh squashes a real-valued number to the range *[-1, 1]*.
    In particular, mathematically, tanh activation function can be expressed as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e72b887a-0d4b-4045-a62c-2db8cd4f3c4e.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: 'The preceding equation can be represented in the following figure:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a367bab-2c52-4abd-ad34-337accfb4b1a.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: Sigmoid versus tanh activation function
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: In general, in the last level of an **feedforward neural network** (**FFNN**),
    the softmax function is applied as the decision boundary. This is a common case,
    especially when solving a classification problem. In probability theory, the output
    of the softmax function is squashed as the probability distribution over *K* different
    possible outcomes. Nevertheless, the softmax function is used in various multiclass
    classification methods, such that the network's output is distributed across classes
    (that is, probability distribution over the classes) having a dynamic range between
    *-1* and *1* or *0* and *1*.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: For a regression problem, we do not need to use any activation function since
    the network generates continuous values—probabilities. However, I've seen people
    using the IDENTITY activation function for regression problems nowadays. We'll
    see this in later chapters.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: To conclude, choosing proper activation functions and network weights initialization
    are two problems that make a network perform at its best and help to obtain good
    training. We'll discuss more in upcoming chapters; we will see where to use which
    activation function.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Neural network architectures
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are various types of architectures in neural networks. We can categorize
    DL architectures into four groups: **Deep Neural Networks** (**DNNs**), **Convolutional
    Neural Networks** (**CNNs**), **Recurrent Neural Networks** (**RNNs**), and **Emergent
    Architectures** (**EAs**).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, based on these architectures, researchers come up with so many variants
    of these for domain-specific use cases and research problems. The following sections
    of this chapter will give a brief introduction to these architectures. More detailed
    analysis, with examples of applications, will be the subject of later chapters
    of this book.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Deep neural networks
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DNNs are neural networks having complex and deeper architecture with a large
    number of neurons in each layer, and there are many connections. The computation
    in each layer transforms the representations in the subsequent layers into slightly
    more abstract representations. However, we will use the term DNN to refer specifically
    to the MLP, the **Stacked Auto-Encoder** (**SAE**), and **Deep Belief Networks**
    (**DBNs**).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'SAEs and DBNs use AEs and **Restricted Boltzmann Machines** (**RBMs**) as building
    blocks of the architectures. The main difference between these and MLPs is that
    training is executed in two phases: unsupervised pre-training and supervised fine-tuning.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0a9b905-3a15-43a7-8160-3a0c2a2f3d2a.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: SAE and DBN using AE and RBM respectively
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: In unsupervised pre-training, shown in the preceding diagram, the layers are
    stacked sequentially and trained in a layer-wise manner, like an AE or RBM using
    unlabeled data. Afterwards, in supervised fine-tuning, an output classifier layer
    is stacked and the complete neural network is optimized by retraining with labeled
    data.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Multilayer Perceptron
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As discussed earlier, a single perceptron is even incapable of approximating
    an XOR function. To overcome this limitation, multiple perceptrons are stacked
    together as MLPs, where layers are connected as a directed graph. This way, the
    signal propagates one way, from input layer to hidden layers to output layer,
    as shown in the following diagram:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/25203e1f-35e2-409d-abb0-1059fbc94385.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
- en: An MLP architecture having an input layer, two hidden layers, and an output
    layer
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'Fundamentally, an MLP is one the most simple FFNNs having at least three layers:
    an input layer, a hidden layer, and an output layer. An MLP was first trained
    with a backpropogation algorithm in the 1980s.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Deep belief networks
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To overcome the overfitting problem in MLPs, the DBN was proposed by Hinton
    et al. It uses a greedy, layer-by-layer, pre-training algorithm to initialize
    the network weights through probabilistic generative models.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'DBNs are composed of a visible layer and multiple layers—**hidden units**.
    The top two layers have undirected, symmetric connections in between and form
    an associative memory, whereas lower layers receive top-down, directed connections
    from the preceding layer. The building blocks of a DBN are RBMs, as you can see
    in the following figure, where several RBMs are *stacked* one after another to
    form DBNs:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/67d0cd1e-9840-481f-b2a0-749cd099c0bf.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: A DBN configured for semi-supervised learning
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'A single RBM consists of two layers. The first layer is composed of visible
    neurons, and the second layer consists of hidden neurons. *Figure 16* shows the
    structure of a simple RBM, where the neurons are arranged according to a symmetrical
    bipartite graph:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c7b7adc-9112-4c9e-872e-20a69f6cde71.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: RBM architecture
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: In DBNs, an RBM is trained first with input data, called unsupervised pre-training,
    and the hidden layer represents the features learned using a greedy learning approach
    called supervised fine-tuning. Despite numerous successes, DBNs are being replaced
    by AEs.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An AE is a network with three or more layers, where the input layer and the
    output layer have the same number of neurons, and those intermediate (hidden layers)
    have a lower number of neurons. The network is trained to reproduce in the output,
    for each piece of input data, the same pattern of activity as in the input.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'Useful applications of AEs are data denoising and dimensionality reduction
    for data visualization. The following diagram shows how an AE typically works.
    It reconstructs the received input through two phases: an encoding phase, which
    corresponds to a dimensional reduction for the original input, and a decoding
    phase, which is capable of reconstructing the original input from the encoded
    (compressed) representation:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/281f46f7-6d2f-4ad6-9fa3-399aef7aa193.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: Encoding and decoding phases of an AE
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural networks
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNNs have achieved much and wide adoption in computer vision (for example, image
    recognition). In CNN networks, the connection scheme that defines the convolutional
    layer (conv) is significantly different compared to an MLP or DBN.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'Importantly, a DNN has no prior knowledge of how the pixels are organized;
    it does not know that nearby pixels are close. A CNN''s architecture embeds this
    prior knowledge. Lower layers typically identify features in small areas of the
    image, while higher layers combine lower-level features into larger features.
    This works well with most natural images, giving CNNs a decisive head start over
    DNNs:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2227d18b-1d82-4a75-9678-694029831dd2.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: A regular DNN versus a CNN
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Take a close look at the preceding diagram; on the left is a regular three-layer
    neural network, and on the right, a CNN arranges its neurons in three dimensions
    (width, height, and depth). In a CNN architecture, a few convolutional layers
    are connected in a cascade style, where each layer is followed by a ReLUlayer,
    then a pooling layer, then a few more convolutional layers (+ReLU), then another
    pooling layer, and so on.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'The output from each conv layer is a set of objects called feature maps that
    are generated by a single kernel filter. Then the feature maps can be used to
    define a new input to the next layer. Each neuron in a CNN network produces an
    output followed by an activation threshold, which is proportional to the input
    and not bound. This type of layer is called a convolutional layer. The following
    diagram is a schematic of the architecture of a CNN used for facial recognition:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/209878a0-02ba-4912-80d8-d8662a4083b4.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: A schematic architecture of a CNN used for facial recognition
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural networks
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A **recurrent neural network** **(RNN)** is a class of **artificial neural
    network** (**ANN**) where connections between units form a directed cycle. RNN
    architecture was originally conceived by Hochreiter and Schmidhuber in 1997\.
    RNN architectures have standard MLPs plus added loops (as shown in the following
    diagram), so they can exploit the powerful nonlinear mapping capabilities of the
    MLP; and they have some form of memory:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78c14ba6-ccda-4db1-92b6-cee4f27af9ab.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
- en: RNN architecture
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: The preceding image shows a a very basic RNN having an input layer, 2 recurrent
    layers and an output layer. However, this basic RNN suffers from gradient vanishing
    and exploding problem and cannot model the long-term depedencies. Therefore, more
    advanced architectures are designed to utilize sequential information of input
    data with cyclic connections among building blocks such as perceptrons. These
    architectures include **Long-Short-Term Memory** (**LSTM**), **Gated Recurrent
    Units** (**GRUs**), **Bidirectional-LSTM** and other variants.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'Consequently, LSTM and GR can overcome the drawbacks of regular RNNs: gradient
    vanishing/exploding problem and the long-short term dependency. We will look at
    these architectures in chapter 2.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Emergent architectures
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many other emergent DL architectures have been suggested, such as **Deep SpatioTemporal
    Neural Networks** (**DST-NNs**), **Multi-Dimensional Recurrent Neural Networks**
    (**MD-RNNs**), and **Convolutional AutoEncoders** (**CAEs**).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, there are a few more emerging networks, such as **CapsNets** (which
    is an improved version of a CNN, designed to remove the drawbacks of regular CNNs),
    RNN for image recognition, and **Generative Adversarial Networks** (**GANs**)
    for simple image generation. Apart from these, factorization machines for personalization
    and deep reinforcement learning are also being used widely.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Residual neural networks
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since there are sometimes millions of billions of hyperparameters and other
    practical aspects, it's really difficult to train deeper neural networks. To overcome
    this limitation, Kaiming He et al. (see [https://arxiv.org/abs/1512.03385v1](https://arxiv.org/abs/1512.03385v1))
    proposed a residual learning framework to ease the training of networks that are
    substantially deeper than those used previously.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: They also explicitly reformulated the layers as learning residual functions
    with reference to the layer inputs, instead of learning unreferenced functions.
    This way, these residual networks are easier to optimize and can gain accuracy
    from considerably increased depth.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: The downside is that building a network by simply stacking residual blocks inevitably
    limits its optimization ability. To overcome this limitation, Ke Zhang et al.
    also proposed using a Multilevel Residual Network ([https://arxiv.org/abs/1608.02908](https://arxiv.org/abs/1608.02908)).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial networks
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GANs are deep neural net architectures that consist of two networks pitted against
    each other (hence the name "adversarial"). Ian Goodfellow et al. introduced GANs
    in a paper (see more at [https://arxiv.org/abs/1406.2661v1](https://arxiv.org/abs/1406.2661v1)).
    In GANs, the two main components are the **generator** **and discriminator**.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2cf8b4f1-7163-4af1-aa4b-6066329d554a.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: Working principle of Generative Adversarial Networks (GANs)
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: The Generator will try to generate data samples out of a specific probability
    distribution, which is very similar to the actual object. The discriminator will
    judge whether its input is coming from the original training set or from the generator
    part.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Capsule networks
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNNs perform well at classifying images. However, if the images have rotation,
    tilt, or any other different orientation, then CNNs show relatively very poor
    performance. Even the pooling operation in CNNs cannot much help against the positional
    invariance.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: This issue in CNNs has led us to the recent advancement of CapsNet through the
    paper titled *Dynamic Routing Between Capsules*(see more at [https://arxiv.org/abs/1710.09829](https://arxiv.org/abs/1710.09829))
    by Geoffrey Hinton et al.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Unlike a regular DNN, where we keep on adding layers, in CapsNets, the idea
    is to add more layers inside a single layer. This way, a CapsNet is a nested set
    of neural layers. We'll discuss more in [Chapter 11](b458baf5-6590-4e69-84d4-8c02a898dbca.xhtml),
    *Discussion, Current Trends, and Outlook*.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: DL frameworks and cloud platforms
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll present some of the most popular deep learning frameworks.
    Then we will discuss some cloud based platforms where you can deploy/run your
    DL applications. In short, almost all of the libraries provide the possibility
    of using a graphics processor to speed up the learning process, are released under
    an open license, and are the result of university research groups.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning frameworks
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**TensorFlow** is mathematical software, and an open source software library
    for machine intelligence. The Google Brain team developed it in 2011 and open-sourced
    it in 2015\. The main features offered by the latest release of TensorFlow (v1.8
    during the writing of this book) are faster computing, flexibility, portability,
    easy debugging, a unified API, transparent use of GPU computing, easy use, and
    extensibility. Once you have constructed your neural network model, after the
    necessary feature engineering, you can simply perform the training interactively
    using plotting or TensorBoard.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '**Keras** is a deep learning library that sits atop TensorFlow and Theano,
    providing an intuitive API inspired by Torch. It is perhaps the best Python API
    in existence. DeepLearning4J relies on Keras as its Python API and imports models
    from Keras and through Keras from Theano and TensorFlow.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '**Theano** is also a deep learning framework written in Python. It allows using
    GPU, which is 24x faster than a single CPU. Defining, optimizing, and evaluating
    complex mathematical expressions is very straightforward in Theano.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '**Neon** is a Python-based deep learning framework developed by Nirvana. Neon
    has a syntax similar to Theano''s high-level framework (for example, Keras). Currently,
    Neon is considered the fastest tool for GPU-based implementation, especially for
    CNNs. But its CPU-based implementation is relatively worse than most other libraries.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '**PyTorch** is a vast ecosystem for ML that offers a large number of algorithms
    and functions, including for DL and for processing various types of multimedia
    data, with a particular focus on parallel computing. Torch is a highly portable
    framework supported on various platforms, including Windows, macOS, Linux, and
    Android.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '**Caffe**, developed primarily by **Berkeley Vision and Learning Center** (**BVLC**),
    is a framework designed to stand out because of its expression, speed, and modularity.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '**MXNet** *(*[http://mxnet.io/](http://mxnet.io/)) is a deep learning framework
    that supports many languages, such as R, Python, C++, and Julia. This is helpful
    because if you know any of these languages, you will not need to step out of your
    comfort zone at all to train your deep learning models. Its backend is written
    in C++ and CUDA and it is able to manage its own memory in a way similar to Theano.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: The **Microsoft Cognitive Toolkit** (**CNTK**) is a unified deep learning toolkit
    from Microsoft Research that makes it easy to train and combine popular model
    types across multiple GPUs and servers. CNTK implements highly efficient CNN and
    RNN training for speech, image, and text data. It supports cuDNN v5.1 for GPU
    acceleration.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: DeepLearning4J is one of the first commercial-grade, open source, distributed
    deep learning libraries written for Java and Scala. This also provides integrated
    support for Hadoop and Spark. DeepLearning4 is designed to be used in business
    environments on distributed GPUs and CPUs.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: DeepLearning4J aims to be cutting-edge and plug-and-play, with more convention
    than configuration, which allows for fast prototyping for non-researchers. The
    following libraries can be integrated with DeepLearning4 and will make your JVM
    experience easier whether you are developing your ML application in Java or Scala.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: ND4J is just like NumPy for JVM. It comes with some basic operations of linear
    algebra such as matrix creation, addition, and multiplication. ND4S, on the other
    hand, is a scientific computing library for linear algebra and matrix manipulation.
    It supports n-dimensional arrays for JVM-based languages.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'To conclude, the following figure shows the last 1 year''s Google trends concerning
    the popularity of different DL frameworks:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea45c7d9-c0ff-44c7-8887-fec8cbdfaa15.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: The trends of different DL frameworks. TensorFlow and Keras are most dominating.
    Theano is losing its popularity. On the other hand, DeepLearning4J is emerging
    for JVM.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Cloud-based platforms for DL
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from the preceding libraries, there have been some recent initiatives
    for deep learning on the cloud. The idea is to bring deep learning capabilities
    to big data with millions of billions of data points and high-dimensional data.
    For example, **Amazon Web Services** (**AWS**), Microsoft Azure, Google Cloud
    Platform, and **NVIDIA GPU Cloud** (**NGC**) all offer machine and deep learning
    services that are native to their public clouds.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: In October 2017, AWS released deep learning **Amazon Machine Images** (**AMIs**)
    for Amazon **Elastic Compute Cloud** (**EC2**) P3 instances. These AMIs come pre-installed
    with deep learning frameworks, such as TensorFlow, Gluon, and Apache MXNet, that
    are optimized for the NVIDIA Volta V100 GPUs within Amazon EC2 P3 instances.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: The Microsoft Cognitive Toolkit is Azure's open source, deep learning service.
    Similar to AWS's offering, it focuses on tools that can help developers build
    and deploy deep learning applications.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, NGC empowers AI scientists and researchers with GPU-accelerated
    containers (see [https://www.nvidia.com/en-us/data-center/gpu-cloud-computing/](https://www.nvidia.com/en-us/data-center/gpu-cloud-computing/)).
    NGC features containerized deep learning frameworks such as TensorFlow, PyTorch,
    MXNet, and more that are tuned, tested, and certified by NVIDIA to run on the
    latest NVIDIA GPUs.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a minimum of knowledge about available DL libraries, frameworks,
    and cloud-based platforms for running and deploying our DL applications, we can
    dive into coding. First, we will start by solving the famous Titanic survival
    prediction problem. However, we won't use the previously listed frameworks; we
    will be using the Apache Spark ML library. Since we will be using Spark along
    with other DL libraries, knowing a little bit of Spark would help us grasp things
    in the upcoming chapters.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning from a disaster – Titanic survival prediction
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to solve the famous Titanic survival prediction
    problem available on Kaggle (see [https://www.kaggle.com/c/titanic/data](https://www.kaggle.com/c/titanic/data)).
    The task is to complete the analysis of what sorts of people are likely to survive
    using an ML algorithm.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Problem description
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before diving into the coding, let''s see a short description of the problem.
    This paragraph is directly quoted from the Kaggle Titanic survival prediction
    page:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '"The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.
    On April 15, 1912, during her maiden voyage, the Titanic sank after colliding
    with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational
    tragedy shocked the international community and led to better safety regulations
    for ships. One of the reasons that the shipwreck led to such loss of life was
    that there were not enough lifeboats for the passengers and crew. Although there
    was some element of luck involved in surviving the sinking, some groups of people
    were more likely to survive than others, such as women, children, and the upper
    class. In this challenge, we ask you to complete the analysis of what sorts of
    people were likely to survive. In particular, we ask you to apply the tools of
    machine learning to predict which passengers survived the tragedy."'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, before going even deeper, we need to know about the data of the passengers
    traveling on the Titanic during the disaster so that we can develop a predictive
    model that can be used for survival analysis. The dataset can be downloaded from
    [https://github.com/rezacsedu/TitanicSurvivalPredictionDataset](https://github.com/rezacsedu/TitanicSurvivalPredictionDataset).
    There are two `.csv` files:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '**The training set** (`train.csv`): Can be used to build your ML models. This
    file also includes labels as the *ground truth* for each passenger for the training
    set.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The test set** (`test.csv`): Can be used to see how well your model performs
    on unseen data. However, for the test set, we do not provide the ground truth
    for each passenger.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In short, for each passenger in the test set, we have to use the trained model
    to predict whether they''ll survive the sinking of the Titanic. *Table 1* shows
    the metadata of the training set:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '| **Variable** | **Definition** |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
- en: '| `survival` | Two labels:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '*0 = No*'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*1 = Yes*'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '| `pclass` | This is a proxy for the **Socioeconomic Status** (**SES**) of
    a passenger and is categorized as upper, middle, and lower. In particular, *1
    = 1^(st)*, *2 = 2^(nd)*, *3 = 3^(rd).* |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
- en: '| `sex` | Male or female. |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
- en: '| `Age` | Age in years. |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
- en: '| `sibsp` | This signifies family relations as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '*Sibling = brother, sister, stepbrother, stepsister*'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Spouse = husband, wife (mistresses and fiancés were ignored)*'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '| `parch` | In the dataset, family relations are defined as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '*Parent = mother, father*'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Child = daughter, son, stepdaughter, stepson*'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some children traveled only with a nanny, therefore *parch=0* for them. |
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '| `ticket` | Ticket number. |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
- en: '| `fare` | Passenger ticket fare. |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
- en: '| cabin | Cabin number. |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
- en: '| `embarked` | Three ports:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '*C = Cherbourg*'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Q = Queenstown*'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*S = Southampton*'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the question would be: using this labeled data, can we draw some straightforward
    conclusions? Say that being a woman, being in first class, and being a child were
    all factors that could boost a passenger''s chances of survival during this disaster.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: To solve this problem, we can start from the basic MLP, which is one of the
    oldest deep learning algorithms. For this, we use the Spark-based `MultilayerPerceptronClassifier`.
    At this point, you might be wondering why I am talking about Spark since it is
    not a DL library. However, Spark has an MLP implementation, which would be enough
    to serve our objective.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Then from the next chapter, we'll gradually start using more robust DNN by using
    DeepLearning4J, a JVM-based framework for developing deep learning applications.
    So let's see how to configure our Spark environment.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the programming environment
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I am assuming that Java is already installed on your machine and the `JAVA_HOME`
    is set too. Also, I''m assuming that your IDE has the Maven plugin installed.
    If so, then just create a Maven project and add the project properties as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the preceding tag, I specified Spark (that is, 2.3.0), but you can adjust
    it. Then add the following dependencies in the `pom.xml` file:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then if everything goes smoothly, all the JAR files will be downloaded in the
    project home as Maven dependencies. Alright! Then we can start writing the code.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering and input dataset preparation
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this sub-section, we will see some basic feature engineering and dataset
    preparation that can be fed into the MLP classifier. So let''s start by creating
    `SparkSession`, which is the gateway to access Spark:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then let''s read the training set and see a glimpse of it:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A snapshot of the dataset can be seen as follows:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ac60d70-682f-4073-8ad9-5519b7be3bd8.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
- en: A snapshot of the Titanic survival dataset
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Now we can see that the training set has both categorical as well as numerical
    features. In addition, some features are not important, such as `PassengerID`,
    `Ticket`, and so on. The same also applies to the `Name` feature unless we manually
    create some features based on the title. However, let's keep it simple. Nevertheless,
    some columns contain null values. Therefore, lots of consideration and cleaning
    are required.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: I ignore the `PassengerId`, `Name`, and `Ticket` columns. Apart from these,
    the `Sex` column is categorical, so I've encoded the passengers based on `male`
    and `female`. Then the `Embarked` column is encoded too. We can encode `S` as
    `0`, `C` as `1`, and `Q` as `2`.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'For this also, we can write user-defined-functions (also known as UDFs) called
    `normSex` and `normEmbarked` for `Sex` and `Embarked`, respectively. Let''s see
    their signatures:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Therefore, this UDF takes a `String` type and encodes as an integer. Now the
    `normSex` UDF also works similarly:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'So we can now select only useful columns but for the `Sex` and `Embarked` columns
    with the aforementioned UDFs:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](img/100ec195-8342-422b-9db2-22e4838df190.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
- en: Now we have been able to convert a categorical column into a numeric; however,
    as we can see, there are still null values. Therefore, what can we do? We can
    either drop the `null` values altogether or apply some `null` imputing techniques
    with the mean value of those particular columns. I believe the second approach
    is better.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, again for this null imputation, we can write UDFs too. However, for that
    we need to know some statistics about those numerical columns. Unfortunately,
    we cannot perform the summary statistics on DataFrame. Therefore, we have to convert
    the DataFrame into `JavaRDD<Vector>`. Well, we also ignore the `null` entries
    for calculating this:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now let''s compute the multivariate statistical `summary`. The `summary` statistical
    will be further used to calculate the `meanAge` and `meanFare` for the corresponding
    missing entries for these two features:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now let''s create two more UDFs for the null imputation on the `Age` and `Fare`
    columns:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Therefore, we have defined a UDF, which fills in the `meanFare` values if the
    data has no entry. Now let''s create another UDF for the `Age` column:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we need to register the UDFs as follows:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Therefore, let''s apply the preceding UDFs for `null` imputation:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](img/e0d3af75-42f6-4d1b-9360-25daec84fbf3.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
- en: 'Great! We now can see that the `null` values are replaced with the mean value
    for the `Age` and `Fare` columns. However, still the numeric values are not scaled.
    Therefore, it would be a better idea to scale them. However, for that, we need
    to compute the mean and variance and then store them as a model to be used for
    later scaling:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then we need an encoder for the numeric values (that is, `Integer`; either
    `BINARY` or `Double`):'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then we can create a `VectorPair` consisting of the label (that is, `Survived`)
    and the features. Here the encoding is, basically, creating a scaled feature vector:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the preceding code block, the `getScaledVector()` method does perform the
    scaling operation. The signature of this method can be seen as follows:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Since we planned to use a Spark ML-based classifier (that is, an MLP implementation),
    we need to convert this RDD of the vector to an ML vector:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, let''s see how the resulting DataFrame looks:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](img/0fe08cf5-78c7-4260-bb72-51b3ac6fba75.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
- en: 'Up to this point, we have been able to prepare our features. Still, this is
    an MLlib-based vector, so we need to further convert this into an ML vector:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Fantastic! Now were'' almost done preparing a training set that can be consumed
    by the MLP classifier. Since we also need to evaluate the model''s performance,
    we can randomly split the training data for the training and test sets. Let''s
    allocate 80% for training and 20% for testing. These will be used to train the
    model and evaluate the model, respectively:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Alright. Now that we have the training set, we can perform training on an MLP
    model.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Training MLP classifier
  id: totrans-345
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Spark, an MLP is a classifier that consists of multiple layers. Each layer
    is fully connected to the next layer in the network. Nodes in the input layer
    represent the input data, whereas other nodes map inputs to outputs by a linear
    combination of the inputs with the node’s weights and biases and by applying an
    activation function.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: Interested readers can take a look at [https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier](https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier).
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: So let's create the layers for the MLP classifier. For this example, let's make
    a shallow network considering the fact that our dataset is not that highly dimensional.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume that only 18 neurons in the first hidden layer and `8` neurons
    in the second hidden layer would be sufficient. Note that the input layer has
    `10` inputs, so we set `10` neurons and `2` neurons in the output layers since
    our MLP will predict only `2` classes. One thing is very important—the number
    of inputs has to be equal to the size of the feature vectors and the number of
    outputs has to be equal to the total number of labels:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then we instantiate the model with the trainer and set its parameters:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: So, as you can understand, the preceding `MultilayerPerceptronClassifier()`
    is the classifier trainer based on the MLP. Each layer has a sigmoid activation
    function except the output layer, which has the softmax activation. Note that
    Spark-based MLP implementation supports only minibatch GD and LBFGS optimizers.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: In short, we cannot use other activation functions such as ReLU or tanh in the
    hidden layers. Apart from this, other advanced optimizers are also not supported,
    nor are batch normalization and so on. This is a serious constraint of this implementation.
    In the next chapter, we will try to overcome this with DL4J.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: We have also set the convergence tolerance of iterations as a very small value
    so that it will lead to higher accuracy with the cost of more iterations. We set
    the block size for stacking input data in matrices to speed up the computation.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: If the size of the training set is large, then the data is stacked within partitions.
    If the block size is more than the remaining data in a partition, then it is adjusted
    to the size of this data. The recommended size is between 10 and 1,000, but the
    default block size is 128.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we plan to iterate the training 1,000 times. So let''s start training
    the model using the training set:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Evaluating the MLP classifier
  id: totrans-359
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When the training is completed, we compute the prediction on the test set to
    evaluate the robustness of the model:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, how about seeing some sample predictions? Let''s observe both the true
    labels and the predicted labels:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](img/e47955e9-f0ad-4335-a487-19aaa4c43a47.png)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
- en: 'We can see that some predictions are correct but some of them are wrong too.
    Nevertheless, in this way, it is difficult to guess the performance. Therefore,
    we can compute performance metrics such as precision, recall, and f1 measure:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now let''s compute the classification''s `accuracy`, `precision`, `recall`,
    `f1` measure, and error on test data:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Well done! We have been able to achieve a fair accuracy rate, that is, 78%.
    Still we can improve the with additional feature engineering. More tips will be
    given in the next section! Now, before concluding this chapter, let''s try to
    utilize the trained model to get the prediction on the test set. First, we read
    the test set and create the DataFrame:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Nevertheless, even if you see the test set, it has some null values. So let''s
    do null imputation on the `Age` and `Fare` columns. If you don''t prefer using
    UDF, you can create a MAP where you include your imputing plan:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then again, we create an RDD of `vectorPair` consisting of features and labels
    (target column):'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then we create a Spark DataFrame:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, let''s convert the MLib vectors to ML based vectors:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, let''s perform the model inferencing, that is, create a prediction for
    the `PassengerId` column and show the sample `prediction`:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![](img/dadcc05d-8734-4479-85d1-7adeca1c1ac9.png)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
- en: 'Finally, let''s write the result in a CSV file:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Frequently asked questions (FAQs)
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have solved the Titanic survival prediction problem with an acceptable
    level of accuracy, there are other practical aspects of this problem and of overall
    deep learning phenomena that need to be considered too. In this section, we will
    see some frequently asked questions that might be already in your mind. Answers
    to these questions can be found in *Appendix A*.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: 'Draw an ANN using the original artificial neurons that compute the XOR operation:
    *A*⊕ *B*. Describe this problem formally as a classification problem. Why can''t
    simple neurons solve this problem? How does an MLP solve this problem by stacking
    multiple perceptrons?'
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have briefly seen the history of ANNs. What are the most significant milestones
    in the era of deep learning? Can we explain the timeline in a single figure?
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can I use another deep learning framework for solving this Titanic survival
    prediction problem more flexibly?
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can I use `Name` as a feature to be used in the MLP in the code?
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I understand the number of neurons in the input and output layers. But how many
    neurons should I set for the hidden layers?
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can't we improve the predictive accuracy by the cross-validation and grid search
    technique?
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-393
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced some fundamental themes of DL. We started our
    journey with a basic but comprehensive introduction to ML. Then we gradually moved
    on to DL and different neural architectures. Then we got a brief overview of the
    most important DL frameworks. Finally, we saw some frequently asked questions
    related to deep learning and the Titanic survival prediction problem.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll begin our journey into DL by solving the Titanic
    survival prediction problem using MLP. Then'll we start developing an end-to-end
    project for cancer type classification using a recurrent LSTM network. A very-high-dimensional
    gene expression dataset will be used for training and evaluating the model.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: Answers to FAQs
  id: totrans-396
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Answer to question 1: There are many ways to solve this problem:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '*A* ⊕ *B= (A ∨ ¬ B)∨ (¬ A ∧ B)*'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*A* ⊕ *B = (A ∨ B) ∧ ¬(A ∨ B)*'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*A* ⊕ *B = (A ∨ B) ∧ (¬ A ∨ ∧ B)*, and so on'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If we go with the first approach, the resulting ANNs would look like this:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/499ef0aa-0177-40a6-8bf8-8aa808cfef1a.png)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
- en: 'Now from computer science literature, we know that only two input combinations
    and one output are associated with the XOR operation. With inputs (0, 0) or (1,
    1) the network outputs 0; and with inputs (0, 1) or (1, 0), it outputs 1\. So
    we can formally represent the preceding truth table as follows:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '| **X0** | **X1** | **Y** |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 1 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 1 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 0 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
- en: 'Here, each pattern is classified into one of two classes that can be separated
    by a single line *L*. They are known as linearly separable patterns, as represented
    here:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19d184b5-45b8-431d-baa0-a389b9f93625.png)'
  id: totrans-410
  prefs: []
  type: TYPE_IMG
- en: 'Answer to question 2: The most significant progress in ANN and DL can be described
    in the following timeline. We have already seen how artificial neurons and perceptrons
    provided the base in 1943s and 1958s respectively. Then, XOR was formulated as
    a linearly non-separable problem in 1969 by Minsky et al. But later in 1974, Werbos
    et al. demonstrated the backpropagation algorithm for training the perceptron
    in 1974.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: However, the most significant advancement happened in the 1980s, when John Hopfield
    et al. proposed the Hopfield Network in 1982\. Then, Hinton, one of the godfathers
    of neural networks and deep learning, and his team proposed the Boltzmann machine
    in 1985\. However, probably one of the most significant advances happened in 1986,
    when Hinton et al. successfully trained the MLP, and Jordan et. al. proposed RNNs.
    In the same year, Smolensky et al. also proposed an improved version of the RBM.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: In the 1990s, the most significant year was 1997\. Lecun et al. proposed LeNet
    in 1990, and Jordan et al. proposed RNN in 1997\. In the same year, Schuster et
    al. proposed an improved version of LSTM and an improved version of the original
    RNN, called **bidirectional RNN**.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: Despite significant advances in computing, from 1997 to 2005, we hadn't experienced
    much advancement, until Hinton struck again in 2006\. He and his team proposed
    a DBN by stacking multiple RBMs. Then in 2012, again Hinton invented dropout,
    which significantly improved regularization and overfitting in a DNN.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: After that, Ian Goodfellow et al. introduced GANs, a significant milestone in
    image recognition. In 2017, Hinton proposed CapsNets to overcome the limitations
    of regular CNNs—so far one of the most significant milestones.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer to question 3**: Yes, you can use other deep learning frameworks described
    in the *Deep learning frameworks* section. However, since this book is about using
    Java for deep learning, I would suggest going for DeepLearning4J. We will see
    how flexibly we can create networks by stacking input, hidden, and output layers
    using DeepLearning4J in the next chapter.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer to question 4**: Yes, you can, since the passenger''s name containing
    a different title (for example, Mr., Mrs., Miss, Master, and so on) could be significant
    too. For example, we can imagine that being a woman (that is, Mrs.) and being
    a junior (for example, Master.) could give a higher chance of survival.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: 'Even, after watching the famous movie Titanic (1997), we can imagine that being
    in a relationship, a girl might have a good chance of survival since his boyfriend
    would try to save her! Anyway, this is just for imagination, so do not take it
    seriously. Now, we can write a user-defined function to encode this using Apache
    Spark. Let''s take a look at the following UDF in Java:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Next, we can register the UDF. Then I had to register the preceding UDF as
    follows:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The resulting column would look like this:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2b16249-8d6f-45ec-9709-5f2b1f17060c.png)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
- en: '**Answer to question 5:** For many problems, you can start with just one or
    two hidden layers. This setting will work just fine using two hidden layers with
    the same total number of neurons (continue reading to get an idea about a number
    of neurons) in roughly the same amount of training time. Now let''s see some naïve
    estimation about setting the number of hidden layers:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '**0**: Only capable of representing linear separable functions'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1**: Can approximate any function that contains a continuous mapping from
    one finite space to another'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2**: Can represent an arbitrary decision boundary to arbitrary accuracy'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, for a more complex problem, you can gradually ramp up the number of
    hidden layers, until you start overfitting the training set. Nevertheless, you
    can try increasing the number of neurons gradually until the network starts overfitting.
    This means the upper bound on the number of hidden neurons that will not result
    in overfitting is:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b39b8c6-77fd-459a-83c9-27b442708c37.png)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '*N[i]* = number of input neurons'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*N[o]* = number of output neurons'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*N[s]* = number of samples in training dataset'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*α* = an arbitrary scaling factor, usually *2-10*'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the preceding equation does not come from any research but from my
    personal working experience.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer to question 6:** Of course, we can. We can cross-validate the training
    and create a grid search technique for finding the best hyperparameters. Let''s
    give it a try.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have the layers defined. Unfortunately, we cannot cross-validate
    layers. Probably, it''s either a bug or made intentionally by the Spark guys.
    So we stick to a single layering:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Then we create the trainer and set only the layer and seed parameters:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We search through the MLP''s different hyperparameters for the best model:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We then set up the cross-validator and perform 10-fold cross-validation:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Then we perform training using the cross-validated model:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Finally, we evaluate the cross-validated model on the test set, as follows:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now we can compute and show the performance metrics, similar to our previous
    example:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
