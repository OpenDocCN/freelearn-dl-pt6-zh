["```py\n./trtexec --onnx=/path/to/alexnet.onnx --output=prob --saveEngine=alexnet.plan\n```", "```py\n./trtexec --output=prob --loadEngine=alexnet.plan\n```", "```py\n$ sudo ./install_GUI.sh\n$ sudo ./install.sh\n```", "```py\n$ cd /opt/intel/openvino/install_dependencies\n$ chmod +x install_openvino_dependencies.sh\n$ sudo -E ./install_openvino_dependencies.sh\n```", "```py\n$ source /opt/intel/openvino/bin/setupvars.sh\n```", "```py\n$ cd /opt/intel/openvino/deployment_tools/model_optimizer/install_prerequisites\n$ sudo ./install_prerequisites.sh\n```", "```py\n$ cd /opt/intel/openvino/deployment_tools/demo\n$ ./demo_squeezenet_download_convert_run.sh\n```", "```py\n$ cd /opt/intel/openvino/deployment_tools/model_optimizer\n$ python3 mo.py --input_model /path/to/alexnet.onnx\nModel Optimizer arguments:\nCommon parameters:\n - Path to the Input Model:      /path/to/alexnet.onnx\n - Path for generated IR:        /opt/intel/openvino_2019.1.094/deployment_tools/model_optimizer/.\n - IR output name:       alexnet\n - Log level:    ERROR\n - Batch:        Not specified, inherited from the model\n - Input layers:         Not specified, inherited from the model\n - Output layers:        Not specified, inherited from the model\n - Input shapes:         Not specified, inherited from the model\n - Mean values:          Not specified\n - Scale values:         Not specified\n - Scale factor:         Not specified\n - Precision of IR:      FP32\n - Enable fusing:        True\n - Enable grouped convolutions fusing:   True\n - Move mean values to preprocess section:       False\n - Reverse input channels:       False\nONNX specific parameters:\nModel Optimizer version:        2019.1.0-341-gc9b66a2\n\n[ SUCCESS ] Generated IR model.\n[ SUCCESS ] XML file: /opt/intel/openvino_2019.1.094/deployment_tools/model_optimizer/./alexnet.xml\n[ SUCCESS ] BIN file: /opt/intel/openvino_2019.1.094/deployment_tools/model_optimizer/./alexnet.bin\n[ SUCCESS ] Total execution time: 6.48 seconds.\n```", "```py\n$ cd /opt/intel/openvino/deployment_tools/inference_engine/samples\n$ ./build_samples.sh\n```", "```py\n$ export LD_LIBRARY_PATH=/opt/intel/openvino/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino/deployment_tools/inference_engine/lib:.:$LD_LIBRARY_PATH\n```", "```py\n$ ./hello_classification /path/to/alexnet.xml /path/to/sunflower.jpg\n\nTop 10 results:\nImage /path/to/sunflower.jpg\nclassid probability\n------- -----------\n985     0.9568181\n309     0.0330606\n328     0.0035365\n946     0.0012036\n308     0.0007907\n310     0.0005121\n723     0.0004805\n108     0.0004062\n950     0.0003640\n947     0.0003286\n```"]