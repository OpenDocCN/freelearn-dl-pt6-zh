<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Exploring Java Deep Learning Libraries &#x2013; DL4J, ND4J, and More"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Exploring Java Deep Learning Libraries – DL4J, ND4J, and More</h1></div></div></div><p>In the previous chapters, you learned the core theories of deep learning algorithms and implemented them from scratch. While we can now say that implementations of deep learning are not so difficult, we can't deny the fact that it still takes some time to implement models. To mitigate this situation, you'll learn how to write code with the Java library of deep learning in this chapter so that we can focus more on the critical part of data analysis rather than the trivial part.</p><p>The topics you'll learn about in this chapter are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">An introduction to the deep learning library of Java</li><li class="listitem" style="list-style-type: disc">Example code and how to write your own code with the library</li><li class="listitem" style="list-style-type: disc">Some additional ways to optimize the model to get a higher precision rate</li></ul></div><div class="section" title="Implementing from scratch versus a library/framework"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec26"/>Implementing from scratch versus a library/framework</h1></div></div></div><p>We <a id="id336" class="indexterm"/>implemented the machine learning <a id="id337" class="indexterm"/>algorithms of neural networks in <a class="link" href="ch02.html" title="Chapter 2. Algorithms for Machine Learning – Preparing for Deep Learning">Chapter 2</a>, <span class="emphasis"><em>Algorithms for Machine Learning – Preparing for Deep Learning</em></span>, and many deep learning algorithms from scratch in <a class="link" href="ch03.html" title="Chapter 3. Deep Belief Nets and Stacked Denoising Autoencoders">Chapter 3</a>, <span class="emphasis"><em>Deep Belief Nets and Stacked Denoising Autoencoders</em></span> and <a class="link" href="ch04.html" title="Chapter 4. Dropout and Convolutional Neural Networks">Chapter 4</a>, <span class="emphasis"><em>Dropout and Convolutional Neural Networks</em></span>. Of course, we can apply our own code to practical applications with some customizations, but we have to be careful when we want to utilize them because we can't deny the possibility that they might cause several problems in the future. What could they be? Here are the possible situations:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The code we wrote has some missing parameters for better optimization because we implemented just the essence of the algorithms for simplicity and so you better understand the concepts. While you can still train and optimize the model with them, you could get higher precision rates by adding another parameter of your own implementation.</li><li class="listitem" style="list-style-type: disc">As mentioned in the previous chapter, there are still many useful deep learning algorithms not explained in this book. While you now have the core components of the deep learning algorithms, you might need to implement additional classes or methods to get the desired results in your fields and applications.</li><li class="listitem" style="list-style-type: disc">The assumed time consumption will be very critical to the application, especially when you think of analyzing huge amounts of data. It is true that Java has a better performance in terms of speed compared to other popular languages such as Python and R, but you may still need to consider the time cost. One plausible approach to solve the problem is using GPU instead of CPU, but this requires complex implementations to adjust the code for GPU computing.</li></ul></div><p>These are the main causal issues, and you might also need to take into consideration that we don't handle exceptions in the code.</p><p>This does not <a id="id338" class="indexterm"/>mean that implementing from scratch would have fatal errors. The code we wrote can be used substantially as an application for certain scaled data; however, you need to take into consideration that you require further coding for the fundamental parts you have implemented if you use large-scale data mining, where, generally, deep learning is required. This means you need to bear in mind that implementation from scratch has more flexibility as you can change the code if required, but at the same time it has a negative side in that the algorithm's tuning and maintenance also has to be done independently.</p><p>So, how can we solve the problems just mentioned? This is where a library (or framework) comes in. Thanks to active research into deep learning globally, there are many libraries developed and published using various programming languages all over the world. Of course, each library has its respective features but the features that every library commonly has can be summarized as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A model's training can be done just by defining a layer structure of deep learning. You can focus on parameter setting and tuning, and you don't need to think about the algorithms.</li><li class="listitem" style="list-style-type: disc">Most of the libraries are open to the public as open source projects and are actively updated daily. Therefore, if there are bugs, there's a high possibility that these bugs will be fixed quickly (and, of course, committing to a project by fixing it yourself should be welcomed).</li><li class="listitem" style="list-style-type: disc">It's easy to switch between running the program on CPU or on GPU. As a library supplements the cumbersome coding element of GPU computing, you can just focus on the implementation without considering CPU or GPU, if a machine supports GPU.</li></ul></div><p>Long story short, you can leave out all the parts that could be brutal when you implement to a library from scratch. Thanks to this, you can take more time on the essential data mining section, hence if you want to utilize practical applications, there's a high possibility that you can perform data analysis more efficiently using a library.</p><p>However, depending too much on a library isn't good. Using a library is convenient, but on the flip side, it has some demerits, as listed here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Since you <a id="id339" class="indexterm"/>can build various <a id="id340" class="indexterm"/>deep learning models easily, you can implement without having a concrete understanding of what theory the model is supported by. This might not be a problem if we only consider implementations related to a specific model, but there will be a risk you can't deal with when you want to combine other methods or consider other methods when applying the model.</li><li class="listitem" style="list-style-type: disc">You can't use algorithms not supported by a library, hence there might be a case where you can't choose a model you would like to use. This can be solved by a version upgrade, but on the other hand, there's a possibility that some part of a past implementation might be deprecated due to a change of specification by the upgrade. Moreover, we can't deny the possibility that the development of a library is suddenly terminated or utilization turns out to be chargeable due to a sudden change in its license. In these cases, there's a risk that the code you have developed up to this point cannot be used.</li><li class="listitem" style="list-style-type: disc">The precision rate you can get from experimentation depends on how a library is implemented. For example, if we conduct an experiment with the same neural network model in two different libraries, the results we obtain can be hugely changed. This is because neural network algorithms include a stochastic operation, and the calculation accuracy of a machine is limited, that is, calculated values during the process could have fluctuations based on the method of implementation.</li></ul></div><p>Because you well understand the fundamental concepts and theories of deep learning algorithms thanks to the previous chapters, we don't need to worry about the first point. However, we need to be careful about the remaining two points. From the next section on, implementation <a id="id341" class="indexterm"/>using a library is <a id="id342" class="indexterm"/>introduced and we'll be more conscious of the merits and demerits we just discussed.</p></div></div>
<div class="section" title="Introducing DL4J and ND4J"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec27"/>Introducing DL4J and ND4J</h1></div></div></div><p>A lot of the <a id="id343" class="indexterm"/>libraries of deep learning have been developed all over the world. In November 2015, <span class="strong"><strong>TensorFlow</strong></span> (<a class="ulink" href="http://www.tensorflow.org/">http://www.tensorflow.org/</a>), a machine learning/deep learning library developed by Google, became <a id="id344" class="indexterm"/>open to the public and attracted great attention.</p><p>When we look at the programming language with which libraries are being developed, most of them open to the public are developed by Python or use the Python API. TensorFlow is developed with C++ on the backend but it's also possible to write code with Python. This book focuses on Java to learn deep learning, hence the libraries developed by other languages will be briefly introduced in <a class="link" href="ch07.html" title="Chapter 7. Other Important Deep Learning Libraries">Chapter 7</a>, <span class="emphasis"><em>Other Important Deep Learning Libraries</em></span>.</p><p>So, what Java-based libraries do we have? Actually, there are a few cases that are actively developed (perhaps there are also some projects not open to public). However, there is only one library we can use practically: <span class="strong"><strong>Deeplearning4j</strong></span> (<span class="strong"><strong>DL4J</strong></span>). The <a id="id345" class="indexterm"/>official project page URL is <a class="ulink" href="http://deeplearning4j.org/">http://deeplearning4j.org/</a>. This library is also open source and the source code is all published on GitHub. The URL is <a class="ulink" href="https://github.com/deeplearning4j/deeplearning4j">https://github.com/deeplearning4j/deeplearning4j</a>. This library was developed by <a id="id346" class="indexterm"/>Skymind (<a class="ulink" href="http://www.skymind.io/">http://www.skymind.io/</a>). What kind of library is this? If you look at the project page, it's introduced as follows:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"Deeplearning4j is the first commercial-grade, open-source, distributed deep-learning library written for Java and Scala. Integrated with Hadoop and Spark, DL4J is designed to be used in business environments, rather than as a research tool. Skymind is its commercial support arm.</em></span></p><p><span class="emphasis"><em>Deeplearning4j aims to be cutting-edge plug and play, more convention than configuration, which allows for fast prototyping for non-researchers. DL4J is customizable at scale. Released under the Apache 2.0 license, all derivatives of DL4J belong to their authors."</em></span></p></blockquote></div><p>When you read this, you will see that the biggest feature of DL4J is that it was designed on the premise of being integrated with Hadoop. This indicates that DL4J suits the processing of large-scale data and is more scalable than other libraries. Moreover, DL4J supports GPU computing, so it's possible to process data even faster.</p><p>Also, DL4J uses a library called <span class="strong"><strong>N-Dimensional Arrays for Java</strong></span> (<span class="strong"><strong>ND4J</strong></span>) internally. The project page is <a class="ulink" href="http://nd4j.org/">http://nd4j.org/</a>. The <a id="id347" class="indexterm"/>same as DL4J, this library is also published on GitHub as an open source project: <a class="ulink" href="https://github.com/deeplearning4j/nd4j">https://github.com/deeplearning4j/nd4j</a>. The developer of the library is the same as DL4J, Skymind. As you can see from the name of the library, this is a scientific computing library that enables us to handle versatile <span class="emphasis"><em>n</em></span>-dimensional array objects. If you are a Python developer, it might be easier for you to understand this if you imagine NumPy, as ND4J is a library inspired by NumPy. ND4J also supports GPU computing and the reason why DL4J is able to do GPU integration is because it uses ND4J internally.</p><p>What good can come from <a id="id348" class="indexterm"/>working with them on GPUs? Let's briefly look at this point. The biggest difference between CPU and GPU is the difference in the number of cores. GPU is, as represented in its name, a graphical processing unit, originally an integrated circuit for image processing. This is why GPU is well optimized to handle the same commands simultaneously. Parallel processing is its forte. On the other hand, as CPU needs to process various commands, these tasks are basically made to be processed in order. Compared to CPU, GPU is good at processing huge numbers of simple tasks, therefore calculations such as training iterations of deep learning is its field of expertise.</p><p>Both ND4J and DL4J are very useful for research and data mining with deep learning. From the next section on, we'll see how these are used for deep learning in simple examples. You can easily understand the contents because you should already understand the core theories of deep learning by now. Hopefully, you can make use of this for your fields of study or business.</p></div>
<div class="section" title="Implementations with ND4J"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec28"/>Implementations with ND4J</h1></div></div></div><p>As there are many cases <a id="id349" class="indexterm"/>where ND4J alone can be used conveniently, let's briefly grasp how to use ND4J before looking into the explanation of DL4J. If you would like to use ND4J alone, once you create a new Maven project, then you can use ND4J by adding the following code to <code class="literal">pom.xml</code>:</p><div class="informalexample"><pre class="programlisting">&lt;properties&gt;
   &lt;nd4j.version&gt;0.4-rc3.6&lt;/nd4j.version&gt;
&lt;/properties&gt;

&lt;dependencies&gt;
   &lt;dependency&gt;
       &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
       &lt;artifactId&gt;nd4j-jblas&lt;/artifactId&gt;
       &lt;version&gt;${nd4j.version}&lt;/version&gt;
   &lt;/dependency&gt;
   &lt;dependency&gt;
       &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
       &lt;artifactId&gt;nd4j-perf&lt;/artifactId&gt;
       &lt;version&gt;${nd4j.version}&lt;/version&gt;
   &lt;/dependency&gt;
&lt;/dependencies&gt;</pre></div><p>Here, <code class="literal">&lt;nd4j.version&gt;</code> describes the latest version of ND4J, but please check whether it is updated when you actually implement the code. Also, switching from CPU to GPU is easy while working with <a id="id350" class="indexterm"/>ND4J. If you have CUDA installed with version 7.0, then what you do is just define <code class="literal">artifactId</code> as follows:</p><div class="informalexample"><pre class="programlisting">&lt;dependency&gt;
   &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
   &lt;artifactId&gt;nd4j-jcublas-7.0&lt;/artifactId&gt;
   &lt;version&gt;${nd4j.version}&lt;/version&gt;
&lt;/dependency&gt;</pre></div><p>You can replace the version of <code class="literal">&lt;artifactId&gt;</code> depending on your configuration.</p><p>Let's look at a simple example of what calculations are possible with ND4J. The type we utilize with ND4J is <code class="literal">INDArray</code>, that is, an extended type of <code class="literal">Array</code>. We begin by importing the following dependencies:</p><div class="informalexample"><pre class="programlisting">import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;</pre></div><p>Then, we define <code class="literal">INDArray</code> as follows:</p><div class="informalexample"><pre class="programlisting">INDArray x = Nd4j.create(new double[]{1, 2, 3, 4, 5, 6}, new int[]{3, 2});
System.out.println(x);</pre></div><p>
<code class="literal">Nd4j.create</code> takes two arguments. The former defines the actual values within <code class="literal">INDArray</code>, and the latter defines the shape of the vector (matrix). By running this code, you get the following result:</p><div class="informalexample"><pre class="programlisting">[[1.00,2.00]
 [3.00,4.00]
 [5.00,6.00]]</pre></div><p>Since <code class="literal">INDArray</code> can output its values with <code class="literal">System.out.print</code>, it's easy to debug. Calculation with scalar can also be done with ease. Add 1 to <code class="literal">x</code> as shown here:</p><div class="informalexample"><pre class="programlisting">x.add(1);
</pre></div><p>Then, you will get the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[[2.00,3.00]</strong></span>
<span class="strong"><strong> [4.00,5.00]</strong></span>
<span class="strong"><strong> [6.00,7.00]]</strong></span>
</pre></div><p>Also, the calculation within <a id="id351" class="indexterm"/>
<code class="literal">INDArray</code> can be done easily, as shown in the following example:</p><div class="informalexample"><pre class="programlisting">INDArray y = Nd4j.create(new double[]{6, 5, 4, 3, 2, 1}, new int[]{3, 2});</pre></div><p>Then, basic arithmetic operations can be represented as follows:</p><div class="informalexample"><pre class="programlisting">x.add(y)
x.sub(y)
x.mul(y)
x.div(y)</pre></div><p>These will return the following result:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[[7.00,7.00]</strong></span>
<span class="strong"><strong> [7.00,7.00]</strong></span>
<span class="strong"><strong> [7.00,7.00]]</strong></span>
<span class="strong"><strong>[[-5.00,-3.00]</strong></span>
<span class="strong"><strong> [-1.00,1.00]</strong></span>
<span class="strong"><strong> [3.00,5.00]]</strong></span>
<span class="strong"><strong>[[6.00,10.00]</strong></span>
<span class="strong"><strong> [12.00,12.00]</strong></span>
<span class="strong"><strong> [10.00,6.00]]</strong></span>
<span class="strong"><strong>[[0.17,0.40]</strong></span>
<span class="strong"><strong> [0.75,1.33]</strong></span>
<span class="strong"><strong> [2.50,6.00]]</strong></span>
</pre></div><p>Also, ND4J has destructive arithmetic operators. When you write the <code class="literal">x.addi(y)</code> command, <code class="literal">x</code> changes its own values so that <code class="literal">System.out.println(x);</code> will return the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[[7.00,7.00]</strong></span>
<span class="strong"><strong> [7.00,7.00]</strong></span>
<span class="strong"><strong> [7.00,7.00]]</strong></span>
</pre></div><p>Likewise, <code class="literal">subi</code>, <code class="literal">muli</code>, and <code class="literal">divi</code> are also destructive operators. There are also many other methods that can conveniently perform calculations between vectors or matrices. For more information, you can <a id="id352" class="indexterm"/>refer to <a class="ulink" href="http://nd4j.org/documentation.html">http://nd4j.org/documentation.html</a>, <a class="ulink" href="http://nd4j.org/doc/">http://nd4j.org/doc/</a> and <code class="literal">http://nd4j.org/apidocs/</code>.</p><p>Let's look at one more example to see how machine learning algorithms can be written with ND4J. We'll implement the easiest example, perceptrons, based on the source code written in <a class="link" href="ch02.html" title="Chapter 2. Algorithms for Machine Learning – Preparing for Deep Learning">Chapter 2</a>, <span class="emphasis"><em>Algorithms for Machine Learning – Preparing for Deep Learning</em></span>. We set the package name <code class="literal">DLWJ.examples.ND4J</code> and the file (class) name <code class="literal">Perceptrons.java</code>.</p><p>First, let's add these two <a id="id353" class="indexterm"/>lines to import from ND4J:</p><div class="informalexample"><pre class="programlisting">import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;</pre></div><p>The model has two parameters: <code class="literal">num</code> of the input layer and the weight. The former doesn't change from the previous code; however, the latter isn't <code class="literal">Array</code> but <code class="literal">INDArray</code>:</p><div class="informalexample"><pre class="programlisting">public int nIn;       // dimensions of input data
public INDArray w;</pre></div><p>You can see from the constructor that since the weight of the perceptrons is represented as a vector, the number of rows is set to the number of units in the input layer and the number of columns to 1. This definition is written here:</p><div class="informalexample"><pre class="programlisting">public Perceptrons(int nIn) {

   this.nIn = nIn;
   w = Nd4j.create(new double[nIn], new int[]{nIn, 1});

}</pre></div><p>Then, because we define the model parameter as <code class="literal">INDArray</code>, we also define the demo data, training data, and test data as <code class="literal">INDArray</code>. You can see these definitions at the beginning of the main method:</p><div class="informalexample"><pre class="programlisting">INDArray train_X = Nd4j.create(new double[train_N * nIn], new int[]{train_N, nIn});  // input data for training
INDArray train_T = Nd4j.create(new double[train_N], new int[]{train_N, 1});          // output data (label) for training

INDArray test_X = Nd4j.create(new double[test_N * nIn], new int[]{test_N, nIn});  // input data for test
INDArray test_T = Nd4j.create(new double[test_N], new int[]{test_N, 1});          // label of inputs
INDArray predicted_T = Nd4j.create(new double[test_N], new int[]{test_N, 1});     // output data predicted by the model</pre></div><p>When we substitute a <a id="id354" class="indexterm"/>value into <code class="literal">INDArray</code>, we use <code class="literal">put</code>. Please be careful that any value we can set with <code class="literal">put</code> is only the values of the <code class="literal">scalar</code> type:</p><div class="informalexample"><pre class="programlisting">train_X.put(i, 0, Nd4j.scalar(g1.random()));
train_X.put(i, 1, Nd4j.scalar(g2.random()));
train_T.put(i, Nd4j.scalar(1));</pre></div><p>The flow from a model building and training is the same as the previous code:</p><div class="informalexample"><pre class="programlisting">// construct perceptrons
Perceptrons classifier = new Perceptrons(nIn);

// train models
while (true) {
   int classified_ = 0;

   for (int i=0; i &lt; train_N; i++) {
       classified_ += classifier.train(train_X.getRow(i), train_T.getRow(i), learningRate);
   }

   if (classified_ == train_N) break;  // when all data classified correctly

   epoch++;
   if (epoch &gt; epochs) break;
}</pre></div><p>Each piece of training data is given to the <code class="literal">train</code> method by <code class="literal">getRow()</code>. First, let's see the entire content of the <code class="literal">train</code> method:</p><div class="informalexample"><pre class="programlisting">public int train(INDArray x, INDArray t, double learningRate) {

   int classified = 0;

   // check if the data is classified correctly
   double c = x.mmul(w).getDouble(0) * t.getDouble(0);

   // apply steepest descent method if the data is wrongly classified
   if (c &gt; 0) {
       classified = 1;
   } else {
       w.addi(x.transpose().mul(t).mul(learningRate));
   }

   return classified;
}</pre></div><p>We first focus our attention on the following code:</p><div class="informalexample"><pre class="programlisting">   // check if the data is classified correctly
   double c = x.mmul(w).getDouble(0) * t.getDouble(0);</pre></div><p>This is the part that checks <a id="id355" class="indexterm"/>whether the data is classified correctly by perceptions, as shown in the following equation:</p><div class="mediaobject"><img src="graphics/B04779_05_06.jpg" alt="Implementations with ND4J"/></div><p>You can see from the code that <code class="literal">.mmul()</code> is for the multiplication between vectors or matrices. We wrote this part of the calculation in <a class="link" href="ch02.html" title="Chapter 2. Algorithms for Machine Learning – Preparing for Deep Learning">Chapter 2</a>, <span class="emphasis"><em>Algorithms for Machine Learning – Preparing for Deep Learning</em></span>, as follows:</p><div class="informalexample"><pre class="programlisting">   double c = 0.;

   // check if the data is classified correctly
   for (int i = 0; i &lt; nIn; i++) {
       c += w[i] * x[i] * t;
   }</pre></div><p>By comparing both codes, you can see that multiplication between vectors or matrices can be written easily with <code class="literal">INDArray</code>, and so you can implement the algorithm intuitively just by following the equations.</p><p>The equation to update the model parameters is as follows:</p><div class="informalexample"><pre class="programlisting">       w.addi(x.transpose().mul(t).mul(learningRate));</pre></div><p>Here, again, you can implement the code like you write a math equation. The equation is represented as follows:</p><div class="mediaobject"><img src="graphics/B04779_05_13.jpg" alt="Implementations with ND4J"/></div><p>The last time we implemented this part, we wrote it with a <code class="literal">for</code> loop:</p><div class="informalexample"><pre class="programlisting">for (int i = 0; i &lt; nIn; i++) {
   w[i] += learningRate * x[i] * t;
}</pre></div><p>Furthermore, the prediction after the training is also the standard forward activation, shown as the following equation:</p><div class="mediaobject"><img src="graphics/B04779_05_15.jpg" alt="Implementations with ND4J"/></div><p>Here:</p><div class="mediaobject"><img src="graphics/B04779_05_16.jpg" alt="Implementations with ND4J"/></div><p>We can simply define the <code class="literal">predict</code> method with just a single line inside, as follows:</p><div class="informalexample"><pre class="programlisting">public int predict(INDArray x) {

   return step(x.mmul(w).getDouble(0));
}</pre></div><p>When you run the program, you can see its precision and accuracy, and the recall is the same as we get with the previous code.</p><p>Thus, it'll greatly help that you implement the algorithms analogous to mathematical equations. We only implement perceptrons here, but please try other algorithms by yourself.</p></div>
<div class="section" title="Implementations with DL4J"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec29"/>Implementations with DL4J</h1></div></div></div><p>ND4J is the library that <a id="id356" class="indexterm"/>helps you to implement deep learning easily and conveniently. However, you have to implement the algorithms yourself, which is not too different from its implementation in the previous chapters. In other words, ND4J is just a library that makes calculating numerical values easier and is not a library that is optimized for deep learning algorithms. One library that makes deep learning easier to handle is <a id="id357" class="indexterm"/>DL4J. Fortunately, as for DL4J, some example code with typical methods is published on GitHub (<a class="ulink" href="https://github.com/deeplearning4j/dl4j-0.4-examples">https://github.com/deeplearning4j/dl4j-0.4-examples</a>). These examples are used on the premise that you are using DL4J's version 0.4-*. When you actually clone this repository, please check the latest version again. In this section, we'll extract the fundamental part from these <a id="id358" class="indexterm"/>sample programs and take a look at it. We'll reference the forked repository on <a class="ulink" href="https://github.com/yusugomori/dl4j-0.4-examples">https://github.com/yusugomori/dl4j-0.4-examples</a> as a screenshot in this section.</p><div class="section" title="Setup"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec28"/>Setup</h2></div></div></div><p>Let's first set up the <a id="id359" class="indexterm"/>environments from our cloned repository. If you're using IntelliJ, you can import the project from <span class="strong"><strong>File</strong></span> | <span class="strong"><strong>New</strong></span> | <span class="strong"><strong>Project</strong></span> from existing sources and select the path of the repository. Then, choose <span class="strong"><strong>Import project from external model</strong></span> and select <span class="strong"><strong>Maven</strong></span> as follows:</p><div class="mediaobject"><img src="graphics/B04779_05_01.jpg" alt="Setup"/></div><p>You don't have to do anything special for the other steps except click <span class="strong"><strong>Next</strong></span>. Please be careful that the supported versions of <a id="id360" class="indexterm"/>JDK are 1.7 or above. This may not be a problem because we needed version 1.8 or above in the previous chapters. Once you have set it up without a problem, you can confirm the structure of the directories as follows:</p><div class="mediaobject"><img src="graphics/B04779_05_02.jpg" alt="Setup"/></div><p>Once you have set up the <a id="id361" class="indexterm"/>project, let's first look at <code class="literal">pom.xml</code>. You can see that the description of the packages related to DL4J is written as:</p><div class="informalexample"><pre class="programlisting">&lt;dependency&gt;
   &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
   &lt;artifactId&gt;deeplearning4j-nlp&lt;/artifactId&gt;
   &lt;version&gt;${dl4j.version}&lt;/version&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
   &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;
   &lt;artifactId&gt;deeplearning4j-core&lt;/artifactId&gt;
   &lt;version&gt;${dl4j.version}&lt;/version&gt;
&lt;/dependency&gt;</pre></div><p>Also, you can see from the following lines that DL4J depends on ND4J:</p><div class="informalexample"><pre class="programlisting">&lt;dependency&gt;
   &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
   &lt;artifactId&gt;nd4j-x86&lt;/artifactId&gt;
   &lt;version&gt;${nd4j.version}&lt;/version&gt;
&lt;/dependency&gt;</pre></div><p>If you would like to run a program on GPU, what you have to do is just change this written section. As mentioned in the previous section, this can be written as follows if you have CUDA installed:</p><div class="informalexample"><pre class="programlisting">&lt;dependency&gt;
   &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
   &lt;artifactId&gt;nd4j-jcublas-XXX&lt;/artifactId&gt;
   &lt;version&gt;${nd4j.version}&lt;/version&gt;
&lt;/dependency&gt;</pre></div><p>Here, <code class="literal">XXX</code> is the version of <a id="id362" class="indexterm"/>CUDA and depends on your machine's preference. It's great to adopt GPU computing only using this. We don't have to do anything special and we can focus on implementations of deep learning.</p><p>The other characteristic library that DL4J develops and uses is <span class="strong"><strong>Canova</strong></span>. The part that corresponds to <code class="literal">pom.xml</code> is as follows:</p><div class="informalexample"><pre class="programlisting">&lt;dependency&gt;
   &lt;artifactId&gt;canova-nd4j-image&lt;/artifactId&gt;
   &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
   &lt;version&gt;${canova.version}&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
   &lt;artifactId&gt;canova-nd4j-codec&lt;/artifactId&gt;
   &lt;groupId&gt;org.nd4j&lt;/groupId&gt;
   &lt;version&gt;${canova.version}&lt;/version&gt;
&lt;/dependency&gt;</pre></div><p>Canova is also, of course, an open source library and its source code can be seen on GitHub at <a class="ulink" href="https://github.com/deeplearning4j/Canova">https://github.com/deeplearning4j/Canova</a>. As explained on that page, Canova is the library used to vectorize raw data into usable vector formats across the machine learning tools. This also helps us focus on the more important part of data mining because data formatting is indispensable in whatever research or experiment we're performing.</p></div><div class="section" title="Build"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec29"/>Build</h2></div></div></div><p>Let's look at the source code <a id="id363" class="indexterm"/>in the examples and see how to build a deep learning model. During the process, the terms of deep learning that you haven't yet learned are also briefly explained. The examples are implemented with various models such as MLP, DBN, and CNN, but there is one problem here. As you can see when looking at <code class="literal">README.md</code>, some methods don't generate good precision. This is because, as explained in the previous section, the calculation precision a machine has is limited and fluctuation occurring with calculated values during the process depends completely on the difference of implementation. Hence, practically, learning can't be done properly, although theoretically it should be done well. You can get better results by, for example, changing the seed values or <a id="id364" class="indexterm"/>adjusting the parameters, but as we would like to focus on how to use a library, we'll use a model that gets higher precision as an example.</p><div class="section" title="DBNIrisExample.java"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec01"/>DBNIrisExample.java</h3></div></div></div><p>Let's first look at <a id="id365" class="indexterm"/>
<code class="literal">DBNIrisExample.java</code> in the package of <code class="literal">deepbelief</code>. Iris, contained in the filename, is one of the benchmark datasets often used when measuring the precision or accuracy of a machine learning method. The dataset contains 150 pieces of data out of 3 classes of 50 instances each, and each class refers to a type of Iris plant. The number of inputs is 4 and the number of outputs is therefore 3. One class is linearly separable from the other two; the latter are not linearly separable from each other.</p><p>The implementation begins by setting up the configuration. Here are the variables that need setting:</p><div class="informalexample"><pre class="programlisting">final int numRows = 4;
final int numColumns = 1;
int outputNum = 3;
int numSamples = 150;
int batchSize = 150;
int iterations = 5;
int splitTrainNum = (int) (batchSize * .8);
int seed = 123;
int listenerFreq = 1;</pre></div><p>In DL4J, input data can be up to two-dimensional data, hence you need to assign the number of rows and columns of the data. As Iris is one-dimensional data, <code class="literal">numColumns</code> is set as <code class="literal">1</code>. Here <code class="literal">numSamples</code> is the total data and <code class="literal">batchSize</code> is the amount of data in each mini-batch. Since the total data is 150 and it is relatively small, <code class="literal">batchSize</code> is set at the same number. This means that learning is done without splitting the data into mini-batches. <code class="literal">splitTrainNum</code> is the variable that decides the allocation between the training data and test data. Here, 80% of all the dataset is training data and 20% is the test data. In the previous section, <code class="literal">listenerFreq</code> decides how often we see loss function's value for logging is seen in the process. This value is set to 1 here, which means the value is logged after each epoch.</p><p>Subsequently, we need to fetch the dataset. In DL4J, a class that can easily fetch data with respect to a typical dataset, such as Iris, MINST, and LFW, is prepared. Therefore, you can just write the following line if you would like to fetch the Iris dataset:</p><div class="informalexample"><pre class="programlisting">DataSetIterator iter = new IrisDataSetIterator(batchSize, numSamples);</pre></div><p>The following two lines <a id="id366" class="indexterm"/>are to format data:</p><div class="informalexample"><pre class="programlisting">DataSet next = iter.next();
next.normalizeZeroMeanZeroUnitVariance();</pre></div><p>This code splits the data into training data and test data and stores them respectively:</p><div class="informalexample"><pre class="programlisting">SplitTestAndTrain testAndTrain = next.splitTestAndTrain(splitTrainNum, new Random(seed));
DataSet train = testAndTrain.getTrain();
DataSet test = testAndTrain.getTest();</pre></div><p>As you can see, it makes data handling easier by treating all the data DL4J prepares with the <code class="literal">DataSet</code> class.</p><p>Now, let's actually build a model. The basic structure is as follows:</p><div class="informalexample"><pre class="programlisting">MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder().layer().layer() … .layer().build();
MultiLayerNetwork model = new MultiLayerNetwork(conf);
model.init();</pre></div><p>The code begins by defining the model configuration and then builds and initializes the actual model with the definition. Let's take a look at the configuration details. At the beginning, the whole network is set up:</p><div class="informalexample"><pre class="programlisting">MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
   .seed(seed)
   .iterations(iterations)
   .learningRate(1e-6f)
   .optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT)
   .l1(1e-1).regularization(true).l2(2e-4)
   .useDropConnect(true)
   .list(2)</pre></div><p>The configuration setup is self-explanatory. However, since you haven't learned about regularization before now, let's briefly check it out.</p><p>Regularization prevents the neural networks model from overfitting and makes the model more generalized. To achieve this, the evaluation function <span class="inlinemediaobject"><img src="graphics/B04779_05_07.jpg" alt="DBNIrisExample.java"/></span> is rewritten with the penalty term as follows:</p><div class="mediaobject"><img src="graphics/B04779_05_08.jpg" alt="DBNIrisExample.java"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_05_09.jpg" alt="DBNIrisExample.java"/></span> denotes the vector <a id="id367" class="indexterm"/>norm. The regularization is called L1 regularization when <span class="inlinemediaobject"><img src="graphics/B04779_05_10.jpg" alt="DBNIrisExample.java"/></span> and L2 regularization when <span class="inlinemediaobject"><img src="graphics/B04779_05_11.jpg" alt="DBNIrisExample.java"/></span>. The norm is called L1 norm and L2 norm, respectively. That's why we have <code class="literal">.l1()</code> and <code class="literal">.l2()</code> in the code. <span class="inlinemediaobject"><img src="graphics/B04779_05_12.jpg" alt="DBNIrisExample.java"/></span> is the hyper parameter. These regularization terms make the model more sparse. L2 regularization is also called weight decay and is used to prevent the vanishing gradient problem.</p><p>The <code class="literal">.useDropConnect()</code> command is used to enable dropout and <code class="literal">.list()</code> to define the number of layers, excluding the input layer.</p><p>When you set up a whole model, then the next step is to configure each layer. In this sample code, the model is not defined as deep neural networks. One single RBM layer is defined as a hidden layer:</p><div class="informalexample"><pre class="programlisting">.layer(0, new RBM.Builder(RBM.HiddenUnit.RECTIFIED, RBM.VisibleUnit.GAUSSIAN)
 .nIn(numRows * numColumns)
 .nOut(3)
 .weightInit(WeightInit.XAVIER)
 .k(1)
 .activation("relu")
 .lossFunction(LossFunctions.LossFunction.RMSE_XENT)
 .updater(Updater.ADAGRAD)
 .dropOut(0.5)
 .build()
)</pre></div><p>Here, the value of <code class="literal">0</code> in the first line is the layer's index and <code class="literal">.k()</code> is for contrastive divergence. Since Iris' data is of float values, we can't use binary RBM. That's why we have <code class="literal">RBM.VisibleUnit.GAUSSIAN</code> here, enabling the model to handle continuous values. Also, as for the definition of this layer, what should be especially mentioned is the role of <code class="literal">Updater.ADAGRAD</code>. This is used to optimize the learning rate. For now, we go on to the model structure, and a <a id="id368" class="indexterm"/>detailed explanation of the optimizer will be introduced at the end of this chapter.</p><p>The subsequent output layer is very simple and self-explanatory:</p><div class="informalexample"><pre class="programlisting"> .layer(1, new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT)
   .nIn(3)
   .nOut(outputNum)
   .activation("softmax")
   .build()
)</pre></div><p>Thus, the neural networks have been built with three layers : input layer, hidden layer, and output layer. The graphical model of this example can be illustrated as follows:</p><div class="mediaobject"><img src="graphics/B04779_05_03.jpg" alt="DBNIrisExample.java"/></div><p>After the model building, we need to train the networks. Here, again, the code is super simple:</p><div class="informalexample"><pre class="programlisting">model.setListeners(Arrays.asList((IterationListener) new ScoreIterationListener(listenerFreq)));
model.fit(train);</pre></div><p>Because the first line is to log the process, what we need to do to train the model is just to write <code class="literal">model.fit()</code>.</p><p>Testing or evaluating the model is also easy with DL4J. First, the variables for evaluation are set up as follows:</p><div class="informalexample"><pre class="programlisting">Evaluation eval = new Evaluation(outputNum);
INDArray output = model.output(test.getFeatureMatrix());</pre></div><p>Then, we can get the values <a id="id369" class="indexterm"/>of the feature matrix using:</p><div class="informalexample"><pre class="programlisting">eval.eval(test.getLabels(), output);
log.info(eval.stats());</pre></div><p>By running the code, we will have the result as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>==========================Scores=====================================</strong></span>
<span class="strong"><strong> Accuracy:  0.7667</strong></span>
<span class="strong"><strong> Precision: 1</strong></span>
<span class="strong"><strong> Recall:    0.7667</strong></span>
<span class="strong"><strong> F1 Score:  0.8679245283018869</strong></span>
<span class="strong"><strong>=====================================================================</strong></span>
</pre></div><p>
<code class="literal">F1 Score</code>, also called <code class="literal">F-Score</code> or <code class="literal">F-measure</code>, is the harmonic means of precision and recall, and is represented as follows:</p><div class="mediaobject"><img src="graphics/B04779_05_14.jpg" alt="DBNIrisExample.java"/></div><p>This value is often calculated to measure the model's performance as well. Also, as written in the example, you can see the actual values and predicted values by writing the following:</p><div class="informalexample"><pre class="programlisting">for (int i = 0; i &lt; output.rows(); i++) {
   String actual = test.getLabels().getRow(i).toString().trim();
   String predicted = output.getRow(i).toString().trim();
   log.info("actual " + actual + " vs predicted " + predicted);
}</pre></div><p>That's it for the whole training and test process. The neural networks in the preceding code are not deep, but you <a id="id370" class="indexterm"/>can easily build deep neural networks just by changing the configuration as follows:</p><div class="informalexample"><pre class="programlisting">MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
       .seed(seed)
       .iterations(iterations)
       .learningRate(1e-6f)
       .optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT)
       .l1(1e-1).regularization(true).l2(2e-4)
       .useDropConnect(true)
       .list(3)
       .layer(0, new RBM.Builder(RBM.HiddenUnit.RECTIFIED, RBM.VisibleUnit.GAUSSIAN)
                       .nIn(numRows * numColumns)
                       .nOut(4)
                       .weightInit(WeightInit.XAVIER)
                       .k(1)
                       .activation("relu")
                       .lossFunction(LossFunctions.LossFunction.RMSE_XENT)
                       .updater(Updater.ADAGRAD)
                       .dropOut(0.5)
                       .build()
       )
       .layer(1, new RBM.Builder(RBM.HiddenUnit.RECTIFIED, RBM.VisibleUnit.GAUSSIAN)
                       .nIn(4)
                       .nOut(3)
                       .weightInit(WeightInit.XAVIER)
                       .k(1)
                       .activation("relu")
                       .lossFunction(LossFunctions.LossFunction.RMSE_XENT)
                       .updater(Updater.ADAGRAD)
                       .dropOut(0.5)
                       .build()
       )
       .layer(2, new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT)
                       .nIn(3)
                       .nOut(outputNum)
                       .activation("softmax")
                       .build()
       )
       .build();</pre></div><p>As you can see, building deep neural networks requires just simple implementations with DL4J. Once you set up the model, what you need to do is adjust the parameters. For example, increasing the <a id="id371" class="indexterm"/>iterations value or changing the seed value would return a better result.</p></div><div class="section" title="CSVExample.java"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec02"/>CSVExample.java</h3></div></div></div><p>In the previous example, we <a id="id372" class="indexterm"/>train the model with the dataset used as a benchmark indicator. When you would like to train and test the model with your own prepared data, you can easily import it from CSV. Let's look at <code class="literal">CSVExample.java</code> in the CSV package. The first step is to initialize the CSV reader as follows:</p><div class="informalexample"><pre class="programlisting">RecordReader recordReader = new CSVRecordReader(0,",");</pre></div><p>In DL4J, a class called <code class="literal">CSVRecordReader</code> is prepared and you can easily import data from a CSV file. The value of the first argument in the <code class="literal">CSVRecordReader</code> class represents how many lines should be skipped in the file. This is convenient when the file contains header rows. The second argument is the delimiter. To actually read a file and import data, the code can be written as follows:</p><div class="informalexample"><pre class="programlisting">recordReader.initialize(new FileSplit(new ClassPathResource("iris.txt").getFile()));</pre></div><p>With this code, the file in <code class="literal">resources/iris.txt</code> will be imported to the model. The values in the file here are the same as ones as in the Iris dataset. To use this initialized data for model training, we define the iterator as follows:</p><div class="informalexample"><pre class="programlisting">DataSetIterator iterator = new RecordReaderDataSetIterator(recordReader,4,3);
DataSet next = iterator.next();</pre></div><p>In the previous example, we used the <code class="literal">IrisDataSetIterator</code> class, but here the <code class="literal">RecordReaderDataSetIterator</code> class is used because we use our own prepared data. The values <code class="literal">4</code> and <code class="literal">3</code> are the number of features and labels, respectively.</p><p>Building and training a model can be done in almost the same way as the process explained in the previous example. In this example, we build deep neural networks of two hidden layers with the <a id="id373" class="indexterm"/>dropout and the rectifier, that is, we have an input layer - hidden layer - hidden layer - output layer, as follows:</p><div class="informalexample"><pre class="programlisting">MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
       .seed(seed)
       .iterations(iterations)
       .constrainGradientToUnitNorm(true).useDropConnect(true)
       .learningRate(1e-1)
       .l1(0.3).regularization(true).l2(1e-3)
       .constrainGradientToUnitNorm(true)
       .list(3)
       .layer(0, new DenseLayer.Builder().nIn(numInputs).nOut(3)
               .activation("relu").dropOut(0.5)
               .weightInit(WeightInit.XAVIER)
               .build())
       .layer(1, new DenseLayer.Builder().nIn(3).nOut(2)
               .activation("relu")
               .weightInit(WeightInit.XAVIER)
               .build())
       .layer(2, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
               .weightInit(WeightInit.XAVIER)
               .activation("softmax")
               .nIn(2).nOut(outputNum).build())
       .backprop(true).pretrain(false)
       .build();</pre></div><p>We can run the model using the following lines of code:</p><div class="informalexample"><pre class="programlisting">MultiLayerNetwork model = new MultiLayerNetwork(conf);
model.init();</pre></div><p>The graphical model is as follows:</p><div class="mediaobject"><img src="graphics/B04779_05_04.jpg" alt="CSVExample.java"/></div><p>This time, however, the way to code for training is slightly different from the previous example. Before, we split the data into training data and test data using the following:</p><div class="informalexample"><pre class="programlisting">SplitTestAndTrain testAndTrain = next.splitTestAndTrain(splitTrainNum, new Random(seed));</pre></div><p>This shows that we shuffle <a id="id374" class="indexterm"/>the data within the <code class="literal">.splitTestAndTrain()</code> method. In this example, we set up training data with the following code:</p><div class="informalexample"><pre class="programlisting">next.shuffle();
SplitTestAndTrain testAndTrain = next.splitTestAndTrain(0.6);</pre></div><p>As you can see, here the data is first shuffled and then split into training data and test data. Be careful that the types of the arguments in <code class="literal">.splitTestAndTrain()</code> are different from each other. This will be beneficial because we don't have to count the exact amount of data or training data. The actual training is done using:</p><div class="informalexample"><pre class="programlisting">model.fit(testAndTrain.getTrain());</pre></div><p>The way to evaluate the model is just the same as the previous example:</p><div class="informalexample"><pre class="programlisting">Evaluation eval = new Evaluation(3);
DataSet test = testAndTrain.getTest();
INDArray output = model.output(test.getFeatureMatrix());
eval.eval(test.getLabels(), output);
log.info(eval.stats());</pre></div><p>With the preceding code, we get the following result:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>==========================Scores=====================================</strong></span>
<span class="strong"><strong> Accuracy:  1</strong></span>
<span class="strong"><strong> Precision: 1</strong></span>
<span class="strong"><strong> Recall:    1</strong></span>
<span class="strong"><strong> F1 Score:  1.0</strong></span>
<span class="strong"><strong>=====================================================================</strong></span>
</pre></div><p>In addition to the dataset of <a id="id375" class="indexterm"/>a benchmark indicator, you can now analyze whatever data you have.</p><p>To make the model even deeper, you just need to add another layer as follows:</p><div class="informalexample"><pre class="programlisting">MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
       .seed(seed)
       .iterations(iterations)
       .constrainGradientToUnitNorm(true).useDropConnect(true)
       .learningRate(0.01)
       .l1(0.0).regularization(true).l2(1e-3)
       .constrainGradientToUnitNorm(true)
       .list(4)
       .layer(0, new DenseLayer.Builder().nIn(numInputs).nOut(4)
               .activation("relu").dropOut(0.5)
               .weightInit(WeightInit.XAVIER)
               .build())
       .layer(1, new DenseLayer.Builder().nIn(4).nOut(4)
               .activation("relu").dropOut(0.5)
               .weightInit(WeightInit.XAVIER)
               .build())
       .layer(2, new DenseLayer.Builder().nIn(4).nOut(4)
               .activation("relu").dropOut(0.5)
               .weightInit(WeightInit.XAVIER)
               .build())
       .layer(3, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
               .weightInit(WeightInit.XAVIER)
               .activation("softmax")
               .nIn(4).nOut(outputNum).build())
       .backprop(true).pretrain(false)
       .build();</pre></div></div></div><div class="section" title="CNNMnistExample.java/LenetMnistExample.java"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec30"/>CNNMnistExample.java/LenetMnistExample.java</h2></div></div></div><p>CNN is rather <a id="id376" class="indexterm"/>complicated compared to other models because of its structure, but we don't need to worry about these complications because we can easily implement CNN with DL4J. Let's take a look at <code class="literal">CNNMnistExample.java</code> in the package of convolution. In this example, we train the model with the MNIST dataset (<a class="ulink" href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>), one of the most famous benchmark indicators. As mentioned in <a class="link" href="ch01.html" title="Chapter 1. Deep Learning Overview">Chapter 1</a>, <span class="emphasis"><em>Deep Learning Overview</em></span>, this dataset contains 70,000 handwritten numbers data from 0 to 9, with both a height and width of 28 pixels each.</p><p>First, we define the values necessary for the model:</p><div class="informalexample"><pre class="programlisting">int numRows = 28;
int numColumns = 28;
int nChannels = 1;
int outputNum = 10;
int numSamples = 2000;
int batchSize = 500;
int iterations = 10;
int splitTrainNum = (int) (batchSize*.8);
int seed = 123;
int listenerFreq = iterations/5;</pre></div><p>Since images in MNIST are all grayscale data, the number of channels is set to <code class="literal">1</code>. In this example, we use <code class="literal">2,000</code> data of <code class="literal">70,000</code> and split it into training data and test data. The size of the mini-batch is <code class="literal">500</code> here, so the training data is divided into 4 mini-batches. Furthermore, the data in each mini-batch is split into training data and test data, and each piece of test data is stored in <code class="literal">ArrayList</code>:</p><div class="informalexample"><pre class="programlisting">List&lt;INDArray&gt; testInput = new ArrayList&lt;&gt;();
List&lt;INDArray&gt; testLabels = new ArrayList&lt;&gt;();</pre></div><p>We didn't have to set <code class="literal">ArrayList</code> in the previous examples because we had just one batch. For the <code class="literal">MnistDataSetIterator</code> class, we can set the MNIST data just by using:</p><div class="informalexample"><pre class="programlisting">DataSetIterator mnistIter = new MnistDataSetIterator(batchSize,numSamples, true);</pre></div><p>Then, we build the model with a convolutional layer and subsampling layer. Here, we have one convolutional layer and one max-pooling layer, directly followed by an output layer. The structure of the configurations for CNN is slightly different from the other algorithms:</p><div class="informalexample"><pre class="programlisting">MultiLayerConfiguration.Builder builder = new NeuralNetConfiguration.Builder().layer().layer(). … .layer()
new ConvolutionLayerSetup(builder,numRows,numColumns,nChannels);
MultiLayerConfiguration conf = builder.build();
MultiLayerNetwork model = new MultiLayerNetwork(conf);
model.init();</pre></div><p>The difference is that we can't build a model directly from the configuration because we need to tell the <a id="id377" class="indexterm"/>builder to set up a convolutional layer using <code class="literal">ConvolutionLayerSetup()</code> in advance. Each <code class="literal">.layer()</code> requires just the same method of coding. The convolutional layer is defined as:</p><div class="informalexample"><pre class="programlisting">.layer(0, new ConvolutionLayer.Builder(10, 10)
       .stride(2,2)
       .nIn(nChannels)
       .nOut(6)
       .weightInit(WeightInit.XAVIER)
       .activation("relu")
       .build())</pre></div><p>Here, the value of <code class="literal">10</code> in <code class="literal">ConvolutionLayer.Builder()</code> is the size of the kernels, and the value of <code class="literal">6</code> in <code class="literal">.nOut()</code> is the number of kernels. Also, <code class="literal">.stride()</code> defines the size of the strides of the kernels. The code we implemented from scratch in <a class="link" href="ch04.html" title="Chapter 4. Dropout and Convolutional Neural Networks">Chapter 4</a>, <span class="emphasis"><em>Dropout and Convolutional Neural Networks</em></span> has a functionality equivalent only to <code class="literal">.stride(1, 1)</code>. The larger the number is, the less time it takes because it decreases the number of calculations necessary for convolutions, but we have to be careful at the same time that it might also decrease the model's precision. Anyway, we can implement convolutions with more flexibility now.</p><p>The <code class="literal">subsampling</code> layer is described as:</p><div class="informalexample"><pre class="programlisting">.layer(1, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX, new int[] {2,2})
       .build())</pre></div><p>Here, <code class="literal">{2, 2}</code> is the size of the pooling windows. You may have noticed that we don't have to set the size of the inputs for each layer, including the output layer. These values are automatically set once you set up the model.</p><p>The output layer can be written just the same as in the other models:</p><div class="informalexample"><pre class="programlisting">.layer(2, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
       .nOut(outputNum)
       .weightInit(WeightInit.XAVIER)
       .activation("softmax")
       .build())</pre></div><p>The graphical model of this example is as follows:</p><div class="mediaobject"><img src="graphics/B04779_05_05.jpg" alt="CNNMnistExample.java/LenetMnistExample.java"/></div><p>After the building comes the training. Since we have multiple mini-batches, we need to iterate training <a id="id378" class="indexterm"/>through all the batches. This can be achieved easily using <code class="literal">.hasNext()</code> on <code class="literal">DataSetIterator</code> and <code class="literal">mnistIter</code> in this case. The whole training process can be written as follows:</p><div class="informalexample"><pre class="programlisting">model.setListeners(Arrays.asList((IterationListener) new ScoreIterationListener(listenerFreq)));
while(mnistIter.hasNext()) {
   mnist = mnistIter.next();
   trainTest = mnist.splitTestAndTrain(splitTrainNum, new Random(seed));
   trainInput = trainTest.getTrain();
   testInput.add(trainTest.getTest().getFeatureMatrix());
   testLabels.add(trainTest.getTest().getLabels());
   model.fit(trainInput);
}</pre></div><p>Here, the test data and test labels are stocked for further use.</p><p>During the test, again, we need to iterate the evaluation process of the test data because we have more than one mini-batch:</p><div class="informalexample"><pre class="programlisting">for(int i = 0; i &lt; testInput.size(); i++) {
   INDArray output = model.output(testInput.get(i));
   eval.eval(testLabels.get(i), output);
}</pre></div><p>Then, we use the same as in the other examples:</p><div class="informalexample"><pre class="programlisting">log.info(eval.stats());</pre></div><p>This will return the <a id="id379" class="indexterm"/>result as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>==========================Scores=====================================</strong></span>
<span class="strong"><strong> Accuracy:  0.832</strong></span>
<span class="strong"><strong> Precision: 0.8783</strong></span>
<span class="strong"><strong> Recall:    0.8334</strong></span>
<span class="strong"><strong> F1 Score:  0.8552464933704985</strong></span>
<span class="strong"><strong>=====================================================================</strong></span>
</pre></div><p>The example just given is the model with one convolutional layer and one subsampling layer, but you have deep convolutional neural networks with <code class="literal">LenetMnistExample.java</code>. In this example, there are two convolutional layers and subsampling layers, followed by fully connected multi-layer perceptrons:</p><div class="informalexample"><pre class="programlisting">MultiLayerConfiguration.Builder builder = new NeuralNetConfiguration.Builder()
       .seed(seed)
       .batchSize(batchSize)
       .iterations(iterations)
       .regularization(true).l2(0.0005)
       .learningRate(0.01)
       .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
       .updater(Updater.NESTEROVS).momentum(0.9)
       .list(6)
       .layer(0, new ConvolutionLayer.Builder(5, 5)
               .nIn(nChannels)
               .stride(1, 1)
               .nOut(20).dropOut(0.5)
               .weightInit(WeightInit.XAVIER)
               .activation("relu")
               .build())
       .layer(1, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX, new int[]{2, 2})
               .build())
       .layer(2, new ConvolutionLayer.Builder(5, 5)
               .nIn(20)
               .nOut(50)
               .stride(2,2)
               .weightInit(WeightInit.XAVIER)
               .activation("relu")
               .build())
       .layer(3, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX, new int[]{2, 2})
               .build())
       .layer(4, new DenseLayer.Builder().activation("tanh")
               .nOut(500).build())
       .layer(5, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
               .nOut(outputNum)
               .weightInit(WeightInit.XAVIER)
               .activation("softmax")
               .build())
       .backprop(true).pretrain(false);
new ConvolutionLayerSetup(builder,28,28,1);</pre></div><p>As you can see in <a id="id380" class="indexterm"/>the first convolutional layer, dropout can easily be applied to CNN with DL4J.</p><p>With this model, we get the following result:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>==========================Scores=====================================</strong></span>
<span class="strong"><strong> Accuracy:  0.8656</strong></span>
<span class="strong"><strong> Precision: 0.8827</strong></span>
<span class="strong"><strong> Recall:    0.8645</strong></span>
<span class="strong"><strong> F1 Score:  0.873490476878917</strong></span>
<span class="strong"><strong>=====================================================================</strong></span>
</pre></div><p>You can see from the MNIST dataset page (<a class="ulink" href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>) that the state-of-the-art result is much better than the one above. Here, again, you would realize how <a id="id381" class="indexterm"/>important the combination of parameters, activation functions, and optimization algorithms are.</p></div><div class="section" title="Learning rate optimization"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec31"/>Learning rate optimization</h2></div></div></div><p>We have learned <a id="id382" class="indexterm"/>various deep learning algorithms so far; you may have noticed that they have one parameter in common: the learning rate. The learning rate is defined in the equations to update the model parameters. So, why not think of algorithms to optimize the learning rate? Originally, these equations were described as follows:</p><div class="mediaobject"><img src="graphics/B04779_05_17.jpg" alt="Learning rate optimization"/></div><p>Here:</p><div class="mediaobject"><img src="graphics/B04779_05_18.jpg" alt="Learning rate optimization"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_05_19.jpg" alt="Learning rate optimization"/></span> is the number of steps and <span class="inlinemediaobject"><img src="graphics/B04779_05_20.jpg" alt="Learning rate optimization"/></span> is the learning rate. It is well known that decreasing the value of the learning rate with each iteration lets the model have better precision rates, but we should determine the decline carefully because a sudden drop in the value would collapse the model. The learning rate is one of the model parameters, so why not optimize it? To do so, we need to know what the best rate could be.</p><p>The simplest way of setting the rate is using the momentum, represented as follows:</p><div class="mediaobject"><img src="graphics/B04779_05_21.jpg" alt="Learning rate optimization"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_05_22.jpg" alt="Learning rate optimization"/></span>, called the <a id="id383" class="indexterm"/>
<span class="strong"><strong>momentum coefficient</strong></span>. This hyper parameter is often set to be 0.5 or 0.9 first and then fine-tuned.</p><p>Momentum is actually <a id="id384" class="indexterm"/>a simple but effective way of adjusting the <a id="id385" class="indexterm"/>learning rate but <span class="strong"><strong>ADAGRAD</strong></span>, proposed by Duchi et al. (<a class="ulink" href="http://www.magicbroom.info/Papers/DuchiHaSi10.pdf">http://www.magicbroom.info/Papers/DuchiHaSi10.pdf</a>), is known to be a better way. The equation is described as follows:</p><div class="mediaobject"><img src="graphics/B04779_05_23.jpg" alt="Learning rate optimization"/></div><p>Here:</p><div class="mediaobject"><img src="graphics/B04779_05_24.jpg" alt="Learning rate optimization"/></div><p>Theoretically, this works well, but practically, we often use the following equations to prevent divergence:</p><div class="mediaobject"><img src="graphics/B04779_05_25.jpg" alt="Learning rate optimization"/></div><p>Or we use:</p><div class="mediaobject"><img src="graphics/B04779_05_26.jpg" alt="Learning rate optimization"/></div><p>ADAGRAD is easier to <a id="id386" class="indexterm"/>use than momentum because the value is set automatically and we don't have to set additional hyper parameters.</p><p>ADADELTA, suggested by Zeiler (<a class="ulink" href="http://arxiv.org/pdf/1212.5701.pdf">http://arxiv.org/pdf/1212.5701.pdf</a>), is known to be an even better <a id="id387" class="indexterm"/>optimizer. This is an algorithm-based optimizer and cannot be written in a single equation. Here is a description of ADADELTA:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Initialization:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Initialize accumulation variables:<div class="mediaobject"><img src="graphics/B04779_05_27.jpg" alt="Learning rate optimization"/></div></li></ul></div><p>And:</p><div class="mediaobject"><img src="graphics/B04779_05_28.jpg" alt="Learning rate optimization"/></div></li><li class="listitem" style="list-style-type: disc">
Iteration <span class="inlinemediaobject"><img src="graphics/B04779_05_29.jpg" alt="Learning rate optimization"/></span>:
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Compute:<div class="mediaobject"><img src="graphics/B04779_05_30.jpg" alt="Learning rate optimization"/></div></li><li class="listitem" style="list-style-type: disc">Accumulate gradient:<div class="mediaobject"><img src="graphics/B04779_05_31.jpg" alt="Learning rate optimization"/></div></li><li class="listitem" style="list-style-type: disc">Compute <a id="id388" class="indexterm"/>update:<div class="mediaobject"><img src="graphics/B04779_05_32.jpg" alt="Learning rate optimization"/></div></li><li class="listitem" style="list-style-type: disc">Accumulate updates:<div class="mediaobject"><img src="graphics/B04779_05_33.jpg" alt="Learning rate optimization"/></div></li><li class="listitem" style="list-style-type: disc">Apply update:<div class="mediaobject"><img src="graphics/B04779_05_17.jpg" alt="Learning rate optimization"/></div></li></ul></div></li></ul></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_05_34.jpg" alt="Learning rate optimization"/></span> and <span class="inlinemediaobject"><img src="graphics/B04779_05_35.jpg" alt="Learning rate optimization"/></span> are the <a id="id389" class="indexterm"/>hyper parameters. You may think ADADELTA is rather complicated but you don't need to worry about this complexity when implementing with DL4J.</p><p>There <a id="id390" class="indexterm"/>are still other optimizers supported in DL4J such as <span class="strong"><strong>RMSProp</strong></span>, <span class="strong"><strong>RMSProp</strong></span> + <a id="id391" class="indexterm"/>momentum, and <span class="strong"><strong>Nesterov's Accelerated Gradient </strong></span><a id="id392" class="indexterm"/>
<span class="strong"><strong>Descent</strong></span>. However, we won't dig into them because, practically, momentum, ADAGRAD, and ADADELTA are enough to optimize the <a id="id393" class="indexterm"/>learning rate.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec30"/>Summary</h1></div></div></div><p>In this chapter, you learned how to implement deep learning models with the libraries ND4J and DL4J. Both support GPU computing and both give us the ability to implement them without any difficulties. ND4J is a library for scientific computing and enables vectorization, which makes it easier to implement a calculation among arrays because we don't need to write iterations within them. Since machine learning and deep learning algorithms have many equations with vector calculations, such as inner products and element-wise multiplication, ND4J also helps implement them.</p><p>DL4J is a library for deep learning, and by following some examples with the library, you saw that we can easily build, train, and evaluate various types of deep learning models. Additionally, while building the model, you learned why regularization is necessary to get better results. You also got to know some optimizers of the learning rate: momentum, ADAGRAD, and ADADELTA. All of these can be implemented easily with DL4J.</p><p>You gained knowledge of the core theories and implementations of deep learning algorithms and you now know how to implement them with little difficulty. We can say that we've completed the theoretical part of this book. Therefore, in the next chapter, we'll look at how deep learning algorithms are adapted to practical applications first and then look into other possible fields and ideas to apply the algorithms.</p></div></body></html>