<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deep Neural Networks for Multi-Class Classification</h1>
                </header>
            
            <article>
                
<p>When developing prediction and classification models, depending on the type of response or target variable, we come across two potential type of problems: the target variable is of categorical type (this is a classification type of problem) or the target variable is of a numeric type (this is a regression type of problem). It has been observed that about 70% of the data belongs to <span>problems</span><span> arising from</span> classification categories and the remaining 30% are regression problems (here is the reference: <a href="https://www.topcoder.com/role-of-statistics-in-data-science/">https://www.topcoder.com/role-of-statistics-in-data-science/</a>). <span>In this chapter, we will provide steps for applying deep learning neural networks for classification problems. The steps are illustrated using the fetal cardiotocograms, or CTGs.</span></p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>A brief understanding of the fetal cardiotocogram (or CTG) dataset</li>
<li>Steps for data preparation, including normalization, data partitioning, and one-hot encoding</li>
<li>Creating and fitting a deep neural network model for classification problems</li>
<li>Evaluating classification model performance and making predictions using the model</li>
<li>Fine-tuning the model for performance optimization and best practices</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cardiotocogram dataset</h1>
                </header>
            
            <article>
                
<p>In this section, we will provide information about the data used for developing a multiclass classification model. We will use only one library, which is Keras.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dataset (medical)</h1>
                </header>
            
            <article>
                
<p>The dataset used in this chapter is publicly available at the UCI Machine Learning Repository maintained by the <span>School of Information and Computer Science at the </span><span>University of California. You can access this at </span><a href="https://archive.ics.uci.edu/ml/datasets/cardiotocography">https://archive.ics.uci.edu/ml/datasets/cardiotocography</a><span>.</span></p>
<p>It is to be noted that this URL enables you to download an Excel data file. This file can be easily converted to a <kbd>.csv</kbd> format by saving the file as a <kbd>.csv</kbd> file.</p>
<p>For data we should use the formatting which is used for <kbd>.csv</kbd>, as shown in the following code:</p>
<pre># Read data<br/>library(keras)<br/>data &lt;- read.csv('~/Desktop/data/CTG.csv', header=T)<br/>str(data)<br/><br/>OUTPUT<br/> ## 'data.frame': 2126 obs. of 22 variables:<br/> ## $ LB : int 120 132 133 134 132 134 134 122 122 122 ...<br/> ## $ AC : num 0 0.00638 0.00332 0.00256 0.00651 ...<br/> ## $ FM : num 0 0 0 0 0 0 0 0 0 0 ...<br/> ##  $ UC      : num  0 0.00638 0.00831 0.00768 0.00814 ...<br/> ##  $ DL      : num  0 0.00319 0.00332 0.00256 0 ...<br/> ##  $ DS      : num  0 0 0 0 0 0 0 0 0 0 ...<br/> ##  $ DP      : num  0 0 0 0 0 ...<br/> ##  $ ASTV    : int  73 17 16 16 16 26 29 83 84 86 ...<br/> ##  $ MSTV    : num  0.5 2.1 2.1 2.4 2.4 5.9 6.3 0.5 0.5 0.3 ...<br/> ##  $ ALTV    : int  43 0 0 0 0 0 0 6 5 6 ...<br/> ##  $ MLTV    : num  2.4 10.4 13.4 23 19.9 0 0 15.6 13.6 10.6 ...<br/> ##  $ Width   : int  64 130 130 117 117 150 150 68 68 68 ...<br/> ##  $ Min     : int  62 68 68 53 53 50 50 62 62 62 ...<br/> ##  $ Max     : int  126 198 198 170 170 200 200 130 130 130 ...<br/> ##  $ Nmax    : int  2 6 5 11 9 5 6 0 0 1 ...<br/> ##  $ Nzeros  : int  0 1 1 0 0 3 3 0 0 0 ...<br/> ##  $ Mode    : int  120 141 141 137 137 76 71 122 122 122 ...<br/> ##  $ Mean    : int  137 136 135 134 136 107 107 122 122 122 ...<br/> ##  $ Median  : int  121 140 138 137 138 107 106 123 123 123 ...<br/> ##  $ Variance: int  73 12 13 13 11 170 215 3 3 1 ...<br/> ##  $ Tendency: int  1 0 0 1 1 0 0 1 1 1 ...<br/> ##  $ NSP     : int  2 1 1 1 1 3 3 3 3 3 ...</pre>
<p class="mce-root"/>
<p class="mce-root">This data consists of fetal CTGs, and the target variable classifies a patient into one of three categories: normal, suspect, and pathological. There are 2,126 rows in this dataset. The CTGs are classified by three expert obstetricians, and a consensus classification label is assigned to each of them as normal (N) (represented by 1), suspect (S) (represented by 2), and pathological (P) (represented by 3). There are 21 independent variables, and the main objective is to develop a classification model to correctly classify each patient into one of the three categories represented by N, S, and P.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the data for model building</h1>
                </header>
            
            <article>
                
<p>In this section, we will prepare the data for building the classification model. Data preparation will involve normalizing the data, partitioning the data into training and test data, and carrying out one-hot encoding of the response variable.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Normalizing numeric variables</h1>
                </header>
            
            <article>
                
<p>For developing deep network models, we carry out the normalization of numeric variables to bring them to a common scale. When dealing with several variables, it is likely that different variables have different scales—for example, there could be a variable that shows revenues earned by a company and the values could be in millions of dollars. In another example, there could be a variable that shows the dimension of a product in centimeters. Such extreme differences in scale create difficulties when training a network, and normalization helps to address this issue. For normalization, we will use the following code:</p>
<pre># Normalize data <br/>data &lt;- as.matrix(data)<br/> dimnames(data) &lt;- NULL  <br/> data[,1:21] &lt;- normalize(data[,1:21])<br/> data[,22] &lt;- as.numeric(data[,22]) -1</pre>
<p>As you can see from the preceding code, we first change the data to matrix format, and then we remove the default names by assigning <kbd>NULL</kbd> to the dimension names. In this step, the names of 22 variables will be changed to <kbd>V1</kbd>, <kbd>V2</kbd>, <kbd>V3</kbd>,..., <kbd>V22</kbd>. If you run <kbd>str(data)</kbd> at this stage, you will notice the change in format of the original data. We normalize the 21 independent variables using the <kbd>normalize</kbd> function, which is a part of the Keras package. When you run this line of code, you will notice that it uses TensorFlow as a backend. We also change the target variable, NSP, to numeric from the default integer type. In addition, in the same line of code, we also change values from <kbd>1</kbd>, <kbd>2</kbd>, and <kbd>3</kbd> to <kbd>0</kbd>, <kbd>1</kbd>, and <kbd>2</kbd> respectively.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Partitioning the data</h1>
                </header>
            
            <article>
                
<p>Next, we will partition this data into training and test datasets. To carry out data partitioning, we use the following code:</p>
<pre class="SourceCode"><span class="KeywordTok"># Data partition </span><span class="NormalTok"><br/></span><span class="KeywordTok">set.seed</span><span class="NormalTok">(</span><span class="DecValTok">1234</span><span class="NormalTok">)</span><br/> <span class="NormalTok">ind &lt;-</span><span class="StringTok"> </span><span class="KeywordTok">sample</span><span class="NormalTok">(</span><span class="DecValTok">2</span><span class="NormalTok">, </span><span class="KeywordTok">nrow</span><span class="NormalTok">(data), </span><span class="DataTypeTok">replace =</span><span class="NormalTok"> T, </span><span class="DataTypeTok">prob=</span><span class="KeywordTok">c</span><span class="NormalTok">(.</span><span class="DecValTok">7</span><span class="NormalTok">, .</span><span class="DecValTok">3</span><span class="NormalTok">))</span><br/> <span class="NormalTok">training &lt;-</span><span class="StringTok"> </span><span class="NormalTok">data[ind</span><span class="OperatorTok">==</span><span class="DecValTok">1</span><span class="NormalTok">, </span><span class="DecValTok">1</span><span class="OperatorTok">:</span><span class="DecValTok">21</span><span class="NormalTok">]</span><br/> <span class="NormalTok">test &lt;-</span><span class="StringTok"> </span><span class="NormalTok">data[ind</span><span class="OperatorTok">==</span><span class="DecValTok">2</span><span class="NormalTok">, </span><span class="DecValTok">1</span><span class="OperatorTok">:</span><span class="DecValTok">21</span><span class="NormalTok">]</span><br/> <span class="NormalTok">trainingtarget &lt;-</span><span class="StringTok"> </span><span class="NormalTok">data[ind</span><span class="OperatorTok">==</span><span class="DecValTok">1</span><span class="NormalTok">, </span><span class="DecValTok">22</span><span class="NormalTok">]</span><br/> <span class="NormalTok">testtarget &lt;-</span><span class="StringTok"> </span><span class="NormalTok">data[ind</span><span class="OperatorTok">==</span><span class="DecValTok">2</span><span class="NormalTok">, </span><span class="DecValTok">22</span><span class="NormalTok">]</span></pre>
<p>As you can see from the preceding code, to obtain the same samples in the training and test datasets for repeatability purposes, we use <kbd>set.seed</kbd> with a specific number, which in this case is <kbd>1234</kbd>. This will ensure that the reader can also obtain the same samples in the training and test data. For data partitioning, a 70:30 split is used here, but any other ratio can be used too. In machine learning applications, this is a commonly used step to ensure that the prediction model works well with unseen data that is stored in the form of test data. Training data is used for developing the model and test data is used to assess the performance of the model. Sometimes, a prediction model may perform very well or even perfectly well with training data; however, when it is evaluated with test data that has not been seen by the model, the performance may turn out to be very disappointing. In machine learning, this problem is termed as over fitting the model. Test data helps to assess and ensure that the prediction model can be reliably implemented for making the appropriate decisions.</p>
<p>We use <kbd>training</kbd> and <kbd>test</kbd> names to store independent variables and we use <kbd>trainingtarget</kbd> and <kbd>testtarget</kbd> names to store target variables stored in the 22<sup>nd</sup> column of the dataset. After data partitioning, we will have 1,523 observations in the training data and the remaining 603 observations will be in the test data. Note that although we use a 70:30 split here, the actual ratio after data partitioning may not be exactly 70:30.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">One-hot encoding</h1>
                </header>
            
            <article>
                
<p>After data partitioning, we will carry out a one-hot encoding of the response variable. One-hot encoding helps to represent a categorical variable in zeros and ones. The code and output for one-hot encoding is as follows:</p>
<pre class="SourceCode"><span class="CommentTok"># One-hot encoding<br/> </span><span class="NormalTok">trainLabels &lt;-</span><span class="StringTok"> </span><span class="KeywordTok">to_categorical</span><span class="NormalTok">(trainingtarget)</span><br/> <span class="NormalTok">testLabels &lt;-</span><span class="StringTok"> </span><span class="KeywordTok">to_categorical</span><span class="NormalTok">(testtarget)</span><br/> <span class="KeywordTok">print</span><span class="NormalTok">(testLabels[1:10,])<br/><br/>OUTPUT<br/><strong> ##        [,1] [,2] [,3]</strong><br/><strong> ##   [1,]    1    0    0</strong><br/><strong> ##   [2,]    1    0    0</strong><br/><strong> ##   [3,]    1    0    0</strong><br/><strong> ##   [4,]    0    0    1</strong><br/><strong> ##   [5,]    0    0    1</strong><br/><strong> ##   [6,]    0    1    0</strong><br/><strong> ##   [7,]    1    0    0</strong><br/><strong> ##   [8,]    1    0    0</strong><br/><strong> ##   [9,]    1    0    0</strong><br/><strong> ##  [10,]    1    0    0</strong><br/></span></pre>
<p>As you can see in the preceding code, with the help of the <kbd>to_categorical</kbd> function from the Keras package, we convert the target variable to a binary class matrix, where the presence or absence of a class is simply represented by 1 or 0 respectively. In this example, we have three classes for the target variable, which are converted to three dummy variables. This process is also called <strong>one-hot encoding</strong>. First, 10 rows from <kbd>testLabels</kbd> are printed. The first row indicates the normal category for the patient <span>with (1,0,0)</span><span>, the sixth row indicates the suspect category for the patient </span><span>with (0,1,0)</span><span>, and the fourth row provides an example of the pathologic category of a patient </span><span>with (0,0,1)</span><span>.</span></p>
<p>Once we complete these steps for data preparation, we move to the next step, where we create the classification model to classify a patient as normal, suspect, or pathological.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating and fitting a deep neural network model</h1>
                </header>
            
            <article>
                
<p>In this section, we will develop the model architecture, compile the model, and then fit the model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Developing model architecture</h1>
                </header>
            
            <article>
                
<p>The code used for developing the model is as follows:</p>
<pre># Initializing the model<br/> model &lt;- keras_model_sequential()<br/><br/># Model architecture<br/> model %&gt;% <br/> layer_dense(units = 8, activation = 'relu', input_shape = c(21)) %&gt;% <br/> layer_dense(units = 3, activation = 'softmax')</pre>
<p>As shown in the preceding code, we start by creating a sequential model using the <kbd>keras_model_sequential()</kbd> <span>function, which </span><span>allows a linear stack of layers to be added. Next, we add layers to the model using the pipe operator, <kbd>%&gt;%</kbd>. This pipe operator takes information from the left as output and feeds that information as input to what is on the right. We use a fully connected or densely connected neural network using the </span><kbd>layer_dense</kbd> <span>function and then specify various inputs. In this dataset, we have 21 independent variables, and as such, the </span><kbd>input_shape</kbd> function <span>is specified as 21 neurons or units in the neural network. This layer is also termed as the input layer in the network. The first hidden layer has 8 units and the activation function that we use here is a rectified linear unit, or</span> <kbd>relu</kbd><span>, which is the most popular activation function used in these situations. The first hidden layer is connected to the output layer with 3 units using the pipe operator. We use 3 units since our target variable has 3 classes. The activation function used in the output layer is</span> <kbd>'softmax'</kbd><span>, which helps to keep the range of output values between 0 and 1. Keeping the range of output values between 0 and 1 will help us to interpret results in the form of familiar probability values.</span></p>
<div class="packt_quote packt_tip">For typing the pipe operator, <kbd>%&gt;%</kbd>, in RStudio, you can use the <em>Shift</em><span> </span>+ <em>Command</em> + <em>M</em> shortcut for Mac, and for Windows, <em>Shift</em> + <em>Ctrl</em> + <em>M</em>.</div>
<p>To obtain a summary of the model architecture that we have created, we can run the <kbd>summary</kbd> function, as shown in the following code:</p>
<pre># Model summary<br/> summary(model)<br/> <br/> OUTPUT<br/> ## ___________________________________________________________________________<br/> ## Layer    (type) Output Shape Param #<br/> ## ===========================================================================<br/> ## dense_1 (Dense) (None, 8)      176<br/> ## ___________________________________________________________________________<br/> ## dense_2 (Dense) (None, 3)       27<br/> ## ===========================================================================<br/> ## Total params: 203<br/> ## Trainable params: 203<br/> ## Non-trainable params: 0<br/> ## ___________________________________________________________________________</pre>
<p>Since the input layer has 21 units that are connected to each of the 8 units in the first hidden layer, we end up with 168 <span>weights </span><span>(</span><span>21 x 8</span><span>)</span><span>. We also obtain one bias term for each unit in the hidden layer, with a total of 8 such terms. So, at the first and only hidden layer stage, we have a total of 176 parameters (</span><span>168 + 8)</span><span>. Similarly, 8 units in the hidden layer are connected to 3 units in the output layer, yielding 24 </span><span>weights (</span><span>8 x 3)</span><span>. This way, we have 24 weights and 3 bias terms </span><span>at the output layer</span><span> </span><span>that account for a total of 27 parameters. Finally, the total number of parameters for this neural network architecture will be 203.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compiling the model</h1>
                </header>
            
            <article>
                
<p>To configure the learning process for the neural network, we compile the model by specifying the loss, optimizer, and metrics, as shown in the following code:</p>
<pre># Compile model <br/>model %&gt;% <br/>   compile(loss = 'categorical_crossentropy', <br/>   optimizer = 'adam',<br/>   metrics = 'accuracy')</pre>
<p>We use <kbd>loss</kbd> for specifying the objective function that we want to optimize. As shown in the preceding code, for the loss, we use <kbd>'categorical_crossentropy'</kbd>, since our target variable has three categories. For situations where the target variable has two categories, we use <kbd>binary_crossentropy</kbd>. For the optimizer, we use the <kbd>'adam'</kbd> optimization algorithm, which is a popular algorithm for deep learning. Its popularity is mainly due to the fact that it gives good results faster than other stochastic optimization methods, such as the <strong>adaptive gradient algorithm</strong> (<strong>AdaGrad</strong>) and <strong>root mean square propagation</strong> (<strong>RMSProp</strong>). We specify the metrics for evaluating the model performance during training and testing. For <kbd>metrics</kbd>, we use <kbd>accuracy</kbd> to assess the classification performance of the model.</p>
<p>Now we are ready to fit the model, which we will do in the next section. </p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fitting the model</h1>
                </header>
            
            <article>
                
<p>To fit the model, we make use of the following code:</p>
<pre># Fit model<br/>model_one &lt;- model %&gt;%   <br/> fit(training, <br/>   trainLabels, <br/>   epochs = 200,<br/>   batch_size = 32, <br/>   validation_split = 0.2)<br/><br/>OUTPUT (last 3 epochs)<br/>Epoch 198/200<br/>1218/1218 [==============================] - 0s 43us/step - loss: 0.3662 - acc: 0.8555 - val_loss: 0.5777 - val_acc: 0.8000<br/>Epoch 199/200<br/>1218/1218 [==============================] - 0s 41us/step - loss: 0.3654 - acc: 0.8530 - val_loss: 0.5763 - val_acc: 0.8000<br/>Epoch 200/200<br/>1218/1218 [==============================] - 0s 40us/step - loss: 0.3654 - acc: 0.8571 - val_loss: 0.5744 - val_acc: 0.8000</pre>
<p>As seen from the preceding code, we see the following observations:</p>
<ul>
<li>To fit the model, we provide training data that has data for 21 independent variables and <kbd>trainLabels</kbd>, which contains data for the target variable.</li>
<li>The number of iterations or epochs is specified as 200. An epoch is a single pass of the training data followed by model assessment using validation data.</li>
<li>To avoid overfitting, we specified that the validation split is 0.2, which means that 20% of the training data will be used to assess the model performance as the training proceeds.</li>
<li>Note that this 20% of data is the bottom 20% of the data points in the training data. We stored data on the loss and accuracy values for the training and validation data generated during the training of the model in <kbd>model_one</kbd> for later use.</li>
<li>For <kbd>batch_size</kbd>, we used the default value of 32, which represents the number of samples that will be used per gradient.</li>
<li>As the training of the model proceeds, we get a visual display of plots for loss and accuracy based on training and validation data after each epoch.</li>
</ul>
<ul>
<li>For accuracy, we would like the model to have higher values, as accuracy is a t<kbd>he-higher-the-better</kbd> type of metric, whereas for loss, which is a <kbd>the-lower-the-better</kbd> type of metric, we would like the model to have lower values.</li>
<li>In addition, we also obtained the numeric summary of the loss output based on the last 3 epochs, as shown in the preceding code output. For each epoch, we saw that 1,218 samples out of 1,523 samples of the training data (about 80%) were used for fitting the model. The remaining 20% of the data was used for calculating accuracy and loss values for the validation data.</li>
</ul>
<div class="packt_tip">A word of caution. When using <kbd>validation_split</kbd>, note that the validation data is not selected randomly from the training data—for example, when <kbd>validation_split = 0.2</kbd>, the last 20% of the training data is used for validation and the first 80% is used for training. Therefore, if the values of the target variable are not random, then <kbd>validation_split</kbd> may introduce bias in the classification model.</div>
<p>After the training process completes 200 epochs, we can plot the training progress in terms of loss and accuracy for training and validation data using the <kbd>plot</kbd> function, as shown in the following code:</p>
<pre> plot(model_one)</pre>
<p><span>The following graph provides a plot that has accuracy in the top window and loss in the lower window:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e7d38d49-2ace-4e8d-9365-ba2e65201ed1.png" style="width:32.00em;height:19.67em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Accuracy and loss for training and validation data</div>
<p>From the <span>preceding </span>plot for loss and accuracy, we can make the following observations:</p>
<ul>
<li>From the plot for accuracy in the top graph, you can see that accuracy values increase significantly after about 25 epochs and then continue to increase gradually for the training data.</li>
<li>For validation data, the progress is more uneven, with a drop in accuracy between the 25<sup>th</sup> and 50<sup>th</sup> epochs.</li>
<li>A somewhat similar pattern is observed in the opposite direction for loss values.</li>
<li>Note that if the training data accuracy increases with the number of epochs, but the validation data accuracy decreases, that would suggest an overfitting of the model. We do not see any major pattern suggesting model overfitting from this plot.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model evaluation and predictions</h1>
                </header>
            
            <article>
                
<p>In this section, we will use test data to evaluate the model performance. We can certainly calculate loss and accuracy values using the training data; however, the real test of a classification model is when it is used with unseen data. And since test data is kept separate from the model building process, we can now use it for model evaluation. We will first calculate loss and accuracy values with the test data and then develop a confusion matrix.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loss and accuracy calculation</h1>
                </header>
            
            <article>
                
<p>The code for obtaining loss and accuracy values using the test data along with the output is shown in the following:</p>
<pre># Model evaluation<br/> model %&gt;% <br/> evaluate(test, testLabels) <br/><br/>OUTPUT<br/> ## $loss<br/> ## [1] 0.4439415<br/> ##<br/> ## $acc<br/> ## [1] 0.8424544</pre>
<p>As you can see in the preceding code, using the <kbd>evaluate</kbd> function, we can obtain loss and accuracy values as <kbd>0.4439</kbd> and <kbd>0.8424</kbd> respectively. Using <kbd>colSums(testLabels)</kbd>, we can find that there are 460, 94, and 49 cases of normal, suspect, and pathological patients respectively in the test data. Converting these numbers to percentages using a total of 603 samples in the test data, we obtain 76.3%, 15.6%, and 8.1% respectively. The highest number of samples belongs to the normal category of patients, and we can use 76.3% as a benchmark for the model performance. If we do not use any model and simply classify all cases in the test data as belonging to the normal category of patients, then we will still be correct about 76.3% of the time since we will be right about all normal patients and incorrect about the other two categories.</p>
<p>In other words, the accuracy of our prediction will be as high as 76.3%; therefore, the model that we develop here should perform at least better than this benchmark number. If it functions below this number, then it is <span>not</span><span> </span><span>likely to be </span><span>of</span> <span>much practical use. Since we get an accuracy of 84.2% </span><span>for the test data</span><span>, we are definitely doing better than the benchmark value, but clearly we must also try to improve our model in order to perform even better. To do that, let's dig even deeper and learn about model performance for each category of the response variable with the help of a confusion matrix.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Confusion matrix</h1>
                </header>
            
            <article>
                
<p>To obtain a confusion matrix, let's start by making a prediction for the test data and save it in <kbd>pred</kbd>. We use <kbd>predict_classes</kbd> to make this prediction and then use the <kbd>table</kbd> function to create a summary of predicted versus actual values for the test data to create a confusion matrix, as shown in the following code:</p>
<pre># Prediction and confusion matrix<br/> pred &lt;- model %&gt;%<br/>   predict_classes(test)<br/> table(Predicted=pred, Actual=testtarget)<br/><br/>OUTPUT<br/>          Actual<br/> ## Predicted   0   1   2<br/> ##         0 435  41  11<br/> ##         1  24  51  16<br/> ##         2   1   2  22</pre>
<p>In the preceding confusion matrix, shown as output", values <kbd>0</kbd>, <kbd>1</kbd>, and <kbd>2</kbd> represent normal, suspect, and pathological categories respectively. From the confusion matrix, we can make the following observations:</p>
<ul>
<li>There were <kbd>435</kbd> patients in the test data who were actually normal and the model also predicted them as being normal.</li>
<li>Similarly, there were <kbd>51</kbd> correct predictions for the suspect group and <kbd>22</kbd> correct predictions for the pathological group.</li>
<li>If we add all the numbers on the diagonal of the confusion matrix, which are the correct classifications, we obtain 508 (435 + 51 + 22), or an accuracy level of 84.2% ((508 ÷ 603) x 100).</li>
<li>In the confusion matrix, the off diagonal numbers indicate the number of patients who are misclassified. The highest number of misclassifications is 41, where the patients actually belong to the suspect group but the model incorrectly classified them in the normal category of patients.</li>
<li>The instance of misclassification with the lowest number involved one patient who actually belonged to the normal category, but the model incorrectly classified this patient in the pathological category.</li>
</ul>
<p>Let's also look at the predictions in terms of probabilities instead of only classes, which was the approach that we used previously. To predict probabilities, we can use the <kbd>predict_prob</kbd> function. We can then look at the first seven rows from the test data using the <kbd>cbind</kbd> function for comparison, as shown in the following code:</p>
<pre># Prediction probabilities<br/>prob &lt;- model %&gt;%<br/>    predict_proba(test)<br/>cbind(prob, pred, testtarget)[1:7,]<br/><br/>OUTPUT<br/>                                         pred testtarget<br/>[1,] 0.993281245 0.006415705 0.000302993    0          0<br/>[2,] 0.979825318 0.018759586 0.001415106    0          0<br/>[3,] 0.982333243 0.014519051 0.003147765    0          0<br/>[4,] 0.009040437 0.271216542 0.719743013    2          2<br/>[5,] 0.008850170 0.267527819 0.723622024    2          2<br/>[6,] 0.946622312 0.030137880 0.0232398603   0          1<br/>[7,] 0.986279726 0.012411724 0.0013086179   0          0</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>In the preceding output, we have probability values for three categories based on the model and we also have the predicted category represented by <kbd>pred</kbd> and the actual category represented by <kbd>testtarget</kbd> in the test data. From the preceding output, we can make the following observations:</p>
<ul>
<li>For the first sample, the highest probability of <kbd>0.993</kbd> is for the normal category of patients, and that is the reason the predicted class is identified as <kbd>0</kbd>. Since this prediction matches the actual result in the test data, we treat this as the correct classification.</li>
<li>Similarly, since the fourth sample shows the highest probability of <kbd>0.7197</kbd> for the third category, the predicted class is labeled as <kbd>2</kbd>, which turns out to be a correct prediction.</li>
<li>However, the sixth sample has the highest probability of <kbd>0.9466</kbd> for the first category represented by <kbd>0</kbd>, whereas the actual class is <kbd>1</kbd>. In this case, our model misclassifies the sample.</li>
</ul>
<p>Next, we will explore the option of improving the classification performance of the model to obtain better accuracy. Two key strategies that we can follow are to increase the number of hidden layers for building a deeper neural network and to change the number of units in the hidden layer. We will explore these options in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance optimization tips and best practices</h1>
                </header>
            
            <article>
                
<p>In this section, we fine-tune the previous classification model to explore its functions and see whether its performance can be <span>further </span><span>improved.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Experimenting with an additional hidden layer</h1>
                </header>
            
            <article>
                
<p>In this experiment, we will add an additional hidden layer to the previous model. The code and output of the summary of the model is given as follows:</p>
<pre># Model architecture<br/>model &lt;- keras_model_sequential()<br/>model %&gt;% <br/>   layer_dense(units = 8, activation = 'relu', input_shape = c(21)) %&gt;%   <br/>   layer_dense(units = 5, activation = 'relu') %&gt;% <br/>   layer_dense(units = 3, activation = 'softmax')<br/><br/>summary(model)<br/><br/>OUTPUT<br/>___________________________________________________________________________<br/>Layer (type)                   Output Shape                Param #     <br/>===========================================================================<br/>dense_1 (Dense)                (None, 8)                    176          <br/>___________________________________________________________________________<br/>dense_2 (Dense)                (None, 5)                    45           <br/>___________________________________________________________________________<br/>dense_3 (Dense)                (None, 3)                    18           <br/>===========================================================================<br/>Total params: 239<br/>Trainable params: 239<br/>Non-trainable params: 0<br/>___________________________________________________________________________</pre>
<p>As shown in the preceding code and output, we have added a second hidden layer with 5 units. In this hidden layer too, we use <kbd>relu </kbd>as the activation function. Note that as a result of this change, we have increased the total number of parameters from 203 in the previous model to 239 in this model.</p>
<p>Next, we compile and then fit the model using the following code:</p>
<pre># Compile and fit model<br/>model %&gt;% <br/> compile(loss = 'categorical_crossentropy', <br/> optimizer = 'adam',<br/> metrics = 'accuracy')<br/>model_two &lt;- model %&gt;%   <br/>   fit(training, <br/>       trainLabels, <br/>       epochs = 200,<br/>       batch_size = 32,  <br/>       validation_split = 0.2)<br/> plot(model_two)</pre>
<p><span>As shown in the preceding code, we have compiled the model with same settings that we used earlier. We have also kept the setting for the <kbd>fit</kbd> function the same as earlier. The model-output-related information is stored in <kbd>model_two</kbd>. The following diagram provides the plot of accuracy and loss for <kbd>model_two</kbd>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/81a4aa6a-1a94-4f86-a533-997d3a1f4cb0.png" style="width:50.42em;height:31.08em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Accuracy and loss for training and validation data</div>
<p>From the preceding diagram, we can make the following observations:</p>
<ul>
<li>The accuracy values based on training and validation data remain relatively constant for the first few epochs.</li>
<li>After about 20 epochs, the accuracy for the training data starts to increase and then continues to increase for the remaining epochs. The rate of increase, however, slows down after about 100 epochs.</li>
</ul>
<ul>
<li>On the other hand, the accuracy based on validation data drops for approximately 50 epochs, then starts to increase, and then becomes more or less <span>constant </span>after about 125 epochs.</li>
<li>Similarly, loss values initially drop significantly for training data, but after about 50 epochs, the rate of decrease drops.</li>
<li>The loss values for the validation data drop during the initial few epochs and then increase and stabilize after about 25 epochs. </li>
</ul>
<p>Using class predictions based on the test data, we can also obtain a confusion matrix to assess the performance of this classification model. The following code is used to obtain a confusion matrix:</p>
<pre># Prediction and confusion matrix<br/>pred &lt;- model %&gt;% <br/>    predict_classes(test)<br/>table(Predicted=pred, Actual=testtarget)<br/><br/>OUTPUT<br/>          Actual<br/> ## Predicted   0   1   2<br/> ##         0 429  38   4<br/> ##         1  29  54  33<br/> ##         2   2   2  12</pre>
<p>From the preceding confusion matrix, we can make the following observations:</p>
<ul>
<li>By comparing correct classifications for <kbd>0</kbd>, <kbd>1</kbd>, and <kbd>2</kbd> classes with the previous model, we notice that improvement is only seen for class <kbd>1</kbd>, whereas the correct classifications for classes <kbd>0</kbd> and <kbd>2</kbd> have, in fact, reduced.</li>
<li>The overall accuracy for this model is 82.1%, which is below the accuracy value of 84.2% that we obtained earlier. So, our attempt to make our model slightly deeper did not improve accuracy, in this case.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Experimenting with a higher number of units in the hidden layer</h1>
                </header>
            
            <article>
                
<p>Now, let's fine-tune the first model by changing the number of units in the first and only hidden layer using the following code:</p>
<pre># Model architecture<br/> model &lt;- keras_model_sequential()<br/> model %&gt;% <br/>   layer_dense(units = 30, activation = 'relu', input_shape = c(21)) %&gt;% <br/>   layer_dense(units = 3, activation = 'softmax') <br/><br/>summary(model)<br/>OUTPUT<br/>__________________________________________________________________________<br/>Layer (type)                   Output Shape               Param #      <br/>==========================================================================<br/>dense_1 (Dense)                (None, 30)                  660          <br/>__________________________________________________________________________<br/>dense_2 (Dense)                (None, 3)                   93           <br/>==========================================================================<br/>Total params: 753<br/>Trainable params: 753<br/>Non-trainable params: 0<br/>__________________________________________________________________________<br/><br/># Compile model<br/> model %&gt;% <br/>   compile(loss = 'categorical_crossentropy', <br/>           optimizer = 'adam',<br/>           metrics = 'accuracy')<br/><br/># Fit model<br/>model_three &lt;- model %&gt;%<br/>   fit(training, <br/>       trainLabels, <br/>       epochs = 200,<br/>       batch_size = 32,<br/>       validation_split = 0.2)<br/> plot(model_three )</pre>
<p>As shown in the preceding code and output, we have increased the number of units in the first and only hidden layer from <kbd>8</kbd> to <kbd>30</kbd>. The total number of parameters for this model is <kbd>753</kbd>. We compile and fit the model with the same setting that we used earlier. We store the accuracy and loss values while fitting the model in <kbd>model_three</kbd>.</p>
<p>The following screenshot provides the plot for accuracy and loss for training and validation data based on the new classification model, as shown in the following graph:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/469ca0d2-5a59-477c-b70e-e5e2c7c59d52.png" style="width:43.92em;height:27.08em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Accuracy and loss for training and validation data</div>
<p>We can make the following observations from the preceding plot:</p>
<ul>
<li>There is no evidence of overfitting.</li>
<li>After about 75 epochs, we do not see any major improvement in the model performance.</li>
</ul>
<p>The prediction of classes using the test data and confusion matrix is obtained using the following code:</p>
<pre># Prediction and confusion matrix<br/> pred &lt;- model %&gt;%<br/>    predict_classes(test)<br/> table(Predicted=pred, Actual=testtarget)<br/><br/>OUTPUT<br/>          Actual<br/> ## Predicted   0   1   2<br/> ##         0 424  35   5<br/> ##         1  28  55   5<br/> ##         2   8   4  39</pre>
<p>From the preceding confusion matrix, we can make the following observations:</p>
<ul>
<li>We see improvements in the classification of 1 suspect and 2 pathological categories compared to the first model.</li>
<li>The correct classifications for the <kbd>0</kbd>, <kbd>1</kbd>, and <kbd>2</kbd> categories are <kbd>424</kbd>, <kbd>55</kbd>, and <kbd>39</kbd> respectively.</li>
<li>The overall accuracy using the test data comes to 85.9%, which is better than the first two models.</li>
</ul>
<p>We can also obtain percentages that show how often this model correctly classifies each class by dividing the number of correct classifications in each column by the total of that column. We find that this classification model correctly classifies normal, suspect, and pathological cases with percentages of about 92.2%, 58.5%, and 79.6% respectively. So the model performance is at its highest when correctly classifying normal patients; however, the model accuracy drops to just 58.5% when correctly classifying patients in the suspect category. From the confusion matrix, we can see that the highest number of samples associated with misclassification is 35. Thus, there are 35 patients who actually belong to the suspect category, but the classification model incorrectly puts these patients in the normal category.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Experimenting using a deeper network with more units in the hidden layer</h1>
                </header>
            
            <article>
                
<p>After building three different neural network models with 203, 239, and 753 parameters respectively, we will now build a deeper neural network model containing a larger number of units in the hidden layers. The code used for this experiment is as follows:</p>
<pre># Model architecture<br/>model &lt;- keras_model_sequential()<br/>model %&gt;%<br/>         layer_dense(units = 40, activation = 'relu', input_shape = c(21)) %&gt;%<br/>         layer_dropout(rate = 0.4) %&gt;%<br/>         layer_dense(units = 30, activation = 'relu') %&gt;%<br/>         layer_dropout(rate = 0.3) %&gt;%<br/>         layer_dense(units = 20, activation = 'relu') %&gt;%<br/>         layer_dropout(rate = 0.2) %&gt;%<br/>         layer_dense(units = 3, activation = 'softmax')<br/>summary(model)<br/><br/>OUTPUT<br/>__________________________________________________________________________<br/>Layer (type)                  Output Shape                 Param #     <br/>==========================================================================<br/>dense_1 (Dense)                (None, 40)                   880          <br/>__________________________________________________________________________<br/>dropout_1 (Dropout)            (None, 40)                    0            <br/>__________________________________________________________________________<br/>dense_2 (Dense)                (None, 30)                   1230         <br/>__________________________________________________________________________<br/>dropout_2 (Dropout)            (None, 30)                    0            <br/>__________________________________________________________________________<br/>dense_3 (Dense)                (None, 20)                   620          <br/>__________________________________________________________________________<br/>dropout_3 (Dropout)            (None, 20)                    0            <br/>__________________________________________________________________________<br/>dense_4 (Dense)                (None, 3)                     63           <br/>==========================================================================<br/>Total params: 2,793<br/>Trainable params: 2,793<br/>Non-trainable params: 0<br/>___________________________________________________________________________<br/><br/># Compile model<br/> model %&gt;% <br/>   compile(loss = 'categorical_crossentropy', <br/>           optimizer = 'adam',<br/>           metrics = 'accuracy')<br/><br/># Fit model<br/>model_four &lt;- model %&gt;% <br/> fit(training, <br/> trainLabels, <br/> epochs = 200,<br/> batch_size = 32, <br/> validation_split = 0.2)<br/>plot(model_four)</pre>
<p>You can see from the preceding code and output that to try and improve the classification performance, this model has a total of 2,793 parameters. This model has three hidden layers with 40, 30, and 20 units in the three hidden layers. After each hidden layer, we have also added a dropout layer with <span>dropout rates of </span><span>40%, 30%, and 20%</span><span> to avoid overfitting—for example, with a dropout rate of 0.4 (or 40%) after the first hidden layer, 40% of the units in the first hidden layer are dropped to zero </span><span>at random </span><span>at the time of training. This helps to avoid any overfitting that may occur because of the higher number of units in the hidden layers. We compile the model and then run the model with same settings that we used earlier. We also store the loss and accuracy values after each epoch in</span> <kbd>model_four</kbd><span>.</span></p>
<p>A plot for accuracy and loss values for training and validation data is shown in the following graph:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ef6be428-c579-42c5-8a68-0580a8f217aa.png" style="width:49.33em;height:30.42em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Accuracy and loss for training and validation data</div>
<p>From the preceding plot, we can make the following observations:</p>
<ul>
<li>Training loss and accuracy values stay approximately constant after about 150 epochs.</li>
<li>Accuracy values for validation data are mainly flat after about 75 epochs.</li>
<li>However, for loss, we see some divergence between training and validation data after about 75 epochs, with loss from validation data increasing gradually. This suggests the presence of overfitting after about 75 epochs.</li>
</ul>
<p>Let's now make predictions using test data and review the resulting confusion matrix to assess model performance, as shown in the following code:</p>
<pre># Predictions and confusion matrix<br/>pred &lt;- model %&gt;% <br/>         predict_classes(test)<br/>table(Predicted=pred, Actual=testtarget)<br/><br/>OUTPUT<br/>         Actual<br/>Predicted   0   1   2<br/>        0 431  34   7<br/>        1  20  53   2<br/>        2   9   7  40</pre>
<p>From the preceding confusion matrix, the following observations can be made:</p>
<ul>
<li>The correct classifications for the <kbd>0</kbd>, <kbd>1</kbd>, and <kbd>2</kbd> categories are <kbd>431</kbd>, <kbd>53</kbd>, and <kbd>40</kbd> respectively.</li>
<li>The overall accuracy comes to 86.9%, which is better than the first three models.</li>
<li>We can also find that this classification model correctly classifies normal, suspect, and pathological cases with percentages of about 93.7%, 56.4%, and 81.6% respectively.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Experimenting by addressing the class imbalance problem</h1>
                </header>
            
            <article>
                
<p>In this dataset, the number of patients in the normal, suspect, and pathological categories is not the same. In the original dataset, the number of normal, suspect, and pathological patients are 1,655, 295, and 176, respectively.</p>
<p>We will make use of the following code to develop a bar plot:</p>
<pre># Bar plot<br/>barplot(prop.table(table(data$NSP)),<br/>        col = rainbow(3),<br/>        ylim = c(0, 0.8),<br/>        ylab = 'Proportion',<br/>        xlab = 'NSP',<br/>        cex.names = 1.5)</pre>
<p>After running the preceding code, we obtain the following bar plot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/290b5c4d-c759-4931-bd19-0893b0770a47.png" style="width:35.50em;height:21.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Proportion of samples in each of the three classes</div>
<p>In the preceding bar plot, the percentages of normal, suspect, and pathological patients are approximately 78%, 14%, and 8% respectively. When we compare these classes, we observe that the number of normal patients is about 5.6 times (1,655/295) greater than the number of suspect patients and about 9.4 times greater than the number of pathological patients. The dataset exhibiting a pattern where classes are not balanced but contain significantly different numbers of cases per class is described as having a class imbalance problem. The class that has a significantly higher number of cases may benefit from this at the time of training the model, but at the cost of the other classes.</p>
<p>As a result, a classification model may contain a bias toward the class that has a significantly higher number of cases, and provide results with higher classification accuracy for this class compared to the other classes. When data is influenced by such a class imbalance, it is important to address the issue to avoid bias in the final classification model. In such situations, we can make use of class weights to address the class imbalance issue in a dataset.</p>
<div class="packt_tip">Very often, datasets that are used for developing classification models have an unequal number of samples for each class. Such class imbalance issues can easily be handled using the <kbd>class_weight</kbd> function.</div>
<p>The code that includes <kbd>class_weight</kbd> to incorporate class imbalance information is shown in the following code:</p>
<pre># Fit model<br/>model_five &lt;- model %&gt;% <br/>  fit(training, <br/>      trainLabels,<br/>      epochs = 200,<br/>      batch_size = 32,<br/>      validation_split = 0.2,<br/>      class_weight = list("0"=1,"1"=5.6, "2" = 9.4))<br/>plot(model_five)</pre>
<p>As you can see in the preceding code, we have specified a weight of <kbd>1</kbd> for the <kbd>normal</kbd> class, a weight of <kbd>5.6</kbd> for the suspect class, and a weight of <kbd>9.4</kbd> for the pathological class. Assigning these weights creates a level playing field for all three categories. We have kept all other settings the same as they were in the previous model. After training the network, the loss and accuracy values for each epoch are stored in <kbd>model_five</kbd>.</p>
<p>The loss and accuracy plot for this experiment is shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c9b36df6-2112-4e24-9488-ac26dcab752e.png" style="width:56.00em;height:34.58em;"/></p>
<p>From the accuracy and loss plot based on training and validation data, we do not see any obvious pattern suggesting overfitting. After about 100 epochs, we do not see any major improvement in model performance in terms of loss and accuracy values.</p>
<p>The code for the predictions from the model and the resulting confusion matrix is as follows:</p>
<pre># Prediction and confusion matrix<br/>pred &lt;- model %&gt;% <br/>  predict_classes(test)<br/>table(Predicted=pred, Actual=testtarget)<br/><br/>OUTPUT<br/>         Actual<br/>Predicted   0   1   2<br/>        0 358  12   3<br/>        1  79  74   5<br/>        2  23   8  41</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>From the preceding confusion matrix, we can make the following observations:</p>
<ul>
<li>The correct classifications for the <kbd>0</kbd>, <kbd>1</kbd>, and <kbd>2</kbd> categories are <kbd>358</kbd>, <kbd>74</kbd>, and <kbd>41</kbd> respectively.</li>
<li>The overall accuracy is now reduced to 78.4%, which is mainly due to the drop in accuracy for the normal class, since we increased the weights for the other two classes.</li>
<li>We can also find that this classification model correctly classifies normal, suspect, and pathological cases with percentages of about 77.8%, 78.7%, and 83.7% respectively.</li>
<li>Clearly, the biggest gains are for the suspect class, which is now correctly classified at the rate of 78.7% versus the earlier rate of only 56.4%.</li>
<li>In the pathological class, we do not see any major gain or loss in accuracy value.</li>
<li>These results clearly indicate the influence of using weights to address class imbalance problems, as now the classification performance across the three classes is more consistent.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Saving and reloading a model</h1>
                </header>
            
            <article>
                
<p>We know that each time we run a model in Keras, the model starts with different starting points due to <span class="st">random</span> <span class="st">initial</span> weights<em>.</em> Once we arrive at a model with an acceptable level of performance and would like to reuse the same model in the future, we can save the model using the <kbd>save_model_hdf5</kbd> function. We can then load this same model using the <kbd>load_model_hdf5</kbd> function:</p>
<pre># Save and reload model<br/>save_model_hdf5(model, <br/> filepath, <br/> overwrite = TRUE,<br/> include_optimizer = TRUE)<br/>model_x &lt;- load_model_hdf5(filepath, <br/> custom_objects = NULL, <br/> compile = TRUE)</pre>
<p>The preceding code will allow us to save the model architecture and the model weights, and, if needed, will allow us to resume the training of the model from the previous training session. </p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we saw how to develop a neural network model that helps to solve a classification type of problem. We started with a simple classification model and explored how to change the number of hidden layers and the number of units in the hidden layers. The idea behind exploring and fine-tuning a classification model was to illustrate how to explore and improve the performance of the classification model. We also saw how to dig deeper to understand the performance of a classification model with the help of a confusion matrix. We purposefully looked at a relatively smaller neural network model at the beginning of this chapter and finished with an example of a relatively deeper neural network model. Deeper networks involving several hidden layers can also lead to overfitting problems, where a classification model may have excellent performance with training data but doesn't do well with testing data. To avoid such situations, we can make use of dropout layers after each dense layer, as was illustrated previously. We also illustrated the use of class weights for situations where the class imbalance could cause a classification model to be more biased toward a specific class. Finally, we also saw how we can save the model details for future use when we don't need to rerun the model.</p>
<p>For the models that we used in this chapter, there were certain parameters that we kept constant during the various experiments—for example, when compiling a model, we always used <kbd>adam</kbd> as an optimizer. One of the reasons for the popularity of using <kbd>adam</kbd> is that it doesn't require much tuning, and provides good results in less time; however, the reader is encouraged to try out other optimizers, such as <kbd>adagrad</kbd>, <kbd>adadelta</kbd>, and <kbd>rmsprop</kbd>, and observe the impact on the classification performance of the model. Another setting that we kept constant in this chapter is the batch size of 32 at the time of training the network. The reader is also encouraged to experiment with higher (such as 64) and lower (such as 16) batch sizes and observe what impact this has on the classification performance.</p>
<p>As we go on to future chapters, we will gradually develop more and more complex and deeper neural network models. Having addressed a classification model where the response variables are categorical, in the next chapter, we will go over the steps for developing and improving the prediction model for the regression type of problems, where the target variable is numeric.</p>


            </article>

            
        </section>
    </body></html>