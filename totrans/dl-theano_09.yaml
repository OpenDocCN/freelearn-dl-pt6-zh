- en: Chapter 9. Selecting Relevant Inputs or Memories with the Mechanism of Attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduces a mechanism of attention to neural network performance,
    and enables networks to improve their performance by focusing on relevant parts
    of their inputs or memories.
  prefs: []
  type: TYPE_NORMAL
- en: With such a mechanism, translations, annotations, explanations, and segmentations,
    as seen in previous chapter, enjoy greater accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs of a neural network may also be connected to *reads* and
    *writes* to an external memory. These networks, **memory networks**, are enhanced
    with an external memory and capable of deciding what information, and from where,
    to store or retrieve.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll discuss:'
  prefs: []
  type: TYPE_NORMAL
- en: The mechanism of attention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aligning translations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Focus in images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural Turing Machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic memory networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differentiable mechanism of attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When translating a sentence, describing the content of an image, annotating
    a sentence, or transcribing an audio, it sounds natural to focus on one part at
    a time of the input sentence or image, to get the sense of the block and transform
    it, before moving to the next part, under a certain order for global understanding.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the German language, under certain conditions, verbs come at
    the end of the sentence, so, when translating to English, once the subject has
    been read and translated, a good machine translation neural network could move
    its focus to the end of the sentence to find the verb and translate it into English.
    This process of matching input positions to current output predictions is possible
    through the *mechanism of attention*.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s come back to classification networks that have been designed
    with a softmax layer (see [Chapter 2](part0026_split_000.html#OPEK1-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 2. Classifying Handwritten Digits with a Feedforward Network"), *Classifying
    Handwritten Digits with a Feedforward Network*) that outputs a non-negative weight
    vector ![Differentiable mechanism of attention](img/00130.jpeg) that sums to *1*
    given an input X:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Differentiable mechanism of attention](img/00131.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Differentiable mechanism of attention](img/00132.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The objective of classification is to have ![Differentiable mechanism of attention](img/00133.jpeg)
    as close as possible to *1* for the correct class *k*, and near zero for the other
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'But ![Differentiable mechanism of attention](img/00133.jpeg) is a probability
    distribution, and can also be used as a weight vector to pay attention to some
    values of a memory vector ![Differentiable mechanism of attention](img/00134.jpeg)
    at a position *k*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Differentiable mechanism of attention](img/00135.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: It returns ![Differentiable mechanism of attention](img/00136.jpeg) if the weights
    focus on position *k*. Depending on the sharpness of the weights, the output will
    be more or less blurry.
  prefs: []
  type: TYPE_NORMAL
- en: 'This mechanism of addressing the value of the vector *m* at a particular position
    is an **attention mechanism**: that is, it''s linear, differentiable, and has
    a back-propagation gradient descent for training on specific tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Better translations with attention mechanism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The applications for attention mechanisms are very large. To get a better understanding,
    let us first illustrate it with the example of machine translation. Attention
    mechanism aligns the source sentence and the target sentence (predicted translation),
    and avoids translation degradation for long sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Better translations with attention mechanism](img/00137.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous chapter, we addressed the machine translation with an encoder-decoder
    framework and a fixed-length encoded vector *c* provided by the encoder to the
    decoder. With the attention mechanism, if each step of the encoding recurrent
    network produces a hidden state *h* *i*, the vector provided to the decoder at
    each decoding time step *t* will be variable and given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Better translations with attention mechanism](img/00138.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'With ![Better translations with attention mechanism](img/00139.jpeg) the alignment
    coefficients produced by a softmax function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Better translations with attention mechanism](img/00140.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Depending on the previous hidden state of the decoder ![Better translations
    with attention mechanism](img/00141.jpeg) and the encoding hidden states ![Better
    translations with attention mechanism](img/00142.jpeg), the embedded dot product
    between the previous decoder hidden state and each encoder hidden state produces
    a weight that describes how they should match:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Better translations with attention mechanism](img/00143.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'After a few epochs of training, the model predicts each next word by focusing
    on a part of the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Better translations with attention mechanism](img/00144.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: To learn to align better, it is possible to use the alignment annotations present
    in the dataset, and add a cross entropy loss for the weights produced by the attention
    mechanism, to be used in the first epochs of training.
  prefs: []
  type: TYPE_NORMAL
- en: Better annotate images with attention mechanism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The same mechanism of attention can be applied to the tasks of annotating images
    or transcribing audio.
  prefs: []
  type: TYPE_NORMAL
- en: 'For images, the attention mechanism focuses on the relevant part of the features
    at each predicting time step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Better annotate images with attention mechanism](img/00145.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Show, attend and tell: neural image caption generation with visual attention'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at the point of attention on images for a trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Better annotate images with attention mechanism](img/00146.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '(*Show, Attend and Tell: Neural Image Caption Generation with Visual Attention*,
    by Kelvin Xu et al., 2015)'
  prefs: []
  type: TYPE_NORMAL
- en: Store and retrieve information in Neural Turing Machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Attention mechanism can be used as an access to a part of memory in the memory-augmented
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of memory in Neural Turing Machines has been inspired by both neuroscience
    and computer hardware.
  prefs: []
  type: TYPE_NORMAL
- en: RNN hidden states to store information is not capable of storing sufficiently
    large amounts of data and retrieving it, even when the RNN is augmented with a
    memory cell, such as in the case of LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: To solve this problem, **Neural Turing Machines** (**NTM**) have been first
    designed with an **external memory bank** and read/write heads, whilst retaining
    the magic of being trained via gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reading the memory bank is given by an attention on the variable memory bank
    as the attention on inputs in the previous examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Store and retrieve information in Neural Turing Machines](img/00147.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Which can be illustrated the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Store and retrieve information in Neural Turing Machines](img/00148.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'While writing a value to the memory bank consists of assigning our new value
    to part of the memory, thanks to another attention mechanism:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Store and retrieve information in Neural Turing Machines](img/00149.jpeg)![Store
    and retrieve information in Neural Turing Machines](img/00150.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'describes the information to store, and ![Store and retrieve information in
    Neural Turing Machines](img/00151.jpeg) the information to erase, and are each
    the size of the memory bank:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Store and retrieve information in Neural Turing Machines](img/00152.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The read and write heads are designed as in a hard drive and their mobility
    is imagined by the attention weights ![Store and retrieve information in Neural
    Turing Machines](img/00153.jpeg) and ![Store and retrieve information in Neural
    Turing Machines](img/00154.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: The memory ![Store and retrieve information in Neural Turing Machines](img/00155.jpeg)
    will evolve at every timestep as the cell memory of a LSTM; but, since the memory
    bank is designed to be large, the network tends to store and organize the incoming
    data at every timestep with less interference than for any classical RNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process to work with the memory is naturally been driven with a recurrent
    neural network acting as a **controller** at each time step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Store and retrieve information in Neural Turing Machines](img/00156.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The controller network outputs at each timestep:'
  prefs: []
  type: TYPE_NORMAL
- en: The positioning or attention coefficients for each write/read head
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The value to store or erase for the write heads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The original NTM proposes two approaches to define the *head positioning*,
    also named *addressing*, defined by the weights ![Store and retrieve information
    in Neural Turing Machines](img/00157.jpeg):'
  prefs: []
  type: TYPE_NORMAL
- en: A content-based positioning, to place similar content in the same area of the
    memory, which is useful for retrieval, sorting or counting tasks:![Store and retrieve
    information in Neural Turing Machines](img/00158.jpeg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A location-based positioning, which is based on previous position of the head,
    and can be used in copy tasks. A gate ![Store and retrieve information in Neural
    Turing Machines](img/00159.jpeg) defines the influence of the previous weights
    versus newly generated weights to compute the position of the head. A shift weight
    ![Store and retrieve information in Neural Turing Machines](img/00160.jpeg) defines
    how much to translate from the position with respect to this position.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Last, a sharpening weight ![Store and retrieve information in Neural Turing
    Machines](img/00161.jpeg) reduces the blur on the head position:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Store and retrieve information in Neural Turing Machines](img/00162.jpeg)![Store
    and retrieve information in Neural Turing Machines](img/00163.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: All operations are differentiable.
  prefs: []
  type: TYPE_NORMAL
- en: Many more than two heads are possible, in particular for tasks such as the addition
    of two stored values where a single read head would be limiting.
  prefs: []
  type: TYPE_NORMAL
- en: These NTM have demonstrated better capability than LSTM in tasks such as retrieving
    the next item in an input sequence, repeating the input sequence many times, or
    sampling from distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Memory networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Answering questions or resolving problems given a few facts or a story have
    led to the design of a new type of networks, **memory networks**. In this case,
    the facts or the story are embedded into a memory bank, as if they were inputs.
    To solve tasks that require the facts to be ordered or to create transitions between
    the facts, memory networks use a recurrent reasoning process in multiple steps
    or hops on the memory banks.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the query or question *q* is converted into a constant input embedding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Memory networks](img/00164.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'While, at each step of the reasoning, the facts *X* to answer the question
    are embedded into two memory banks, where the embedding coefficients are a function
    of the timestep:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Memory networks](img/00165.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'To compute attention weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Memory networks](img/00166.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'And:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Memory networks](img/00167.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Selected with the attention:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Memory networks](img/00168.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The output at each reasoning time step is then combined with the identity connection,
    as seen previously to improve the efficiency of the recurrency:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Memory networks](img/00169.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A linear layer and classification softmax layer are added to the last ![Memory
    networks](img/00170.jpeg):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Memory networks](img/00171.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Episodic memory with dynamic memory networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another design has been introduced with dynamic memory networks. First, the
    N facts are concatenated with a separator token and then encoded with a RNN: the
    output of the RNN at each separation ![Episodic memory with dynamic memory networks](img/00172.jpeg)
    is used as input embedding. This way to encode facts is more natural and also
    preserves time dependency. The question is also encoded with an RNN to produce
    a vector *q*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Secondly, the memory bank is replaced with an episodic memory, relying on an
    attention mechanism mixed with an RNN, in order to preserve time dependency between
    the facts as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Episodic memory with dynamic memory networks](img/00173.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The gates ![Episodic memory with dynamic memory networks](img/00174.jpeg) are
    given by a multilayer perceptron depending on the previous state of reasoning
    ![Episodic memory with dynamic memory networks](img/00175.jpeg), the question
    and the input embedding ![Episodic memory with dynamic memory networks](img/00176.jpeg)
    as inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reasoning occurs the same way with a RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Episodic memory with dynamic memory networks](img/00177.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following picture illustrates the interactions between inputs and outputs
    to compute the episodic memories:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Episodic memory with dynamic memory networks](img/00178.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Ask Me Anything: dynamic memory networks for natural language processing'
  prefs: []
  type: TYPE_NORMAL
- en: To benchmark these networks, Facebook research has synthetized the bAbI dataset,
    using NLP tools to create facts, questions, and answers for some random modeled
    stories. The dataset is composed of different tasks to test different reasoning
    skills, such as reasoning on one, two, or three facts, in time, size, or position,
    counting, listing, or understanding relations between arguments, negations, motivations,
    and finding paths.
  prefs: []
  type: TYPE_NORMAL
- en: 'As for guided alignment in machine translation, when the dataset also contains
    the annotations for the facts leading to the answer, it is also possible to use
    supervised training for:'
  prefs: []
  type: TYPE_NORMAL
- en: The attention mechanism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When to stop the reasoning loop, producing a stop token, when the number of
    facts used is sufficient to answer the question
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can refer to these topics for more insights:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Ask Me Anything: Dynamic Memory Networks for Natural Language Processing*,Ankit
    Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani,
    Victor Zhong, Romain Paulus, Richard Socher, 2015'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Attention and Augmented Recurrent Neural Networks*, Chris Olah, Shan Carter,
    Sept 2016 [http://distill.pub/2016/augmented-rnns/](http://distill.pub/2016/augmented-rnns/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Guided Alignment training for Topic Aware Neural Machine Translation*, Wenhu
    Chen, Evgeny Matusov, Shahram Khadivi, Jan-Thorsten Peter, Jul 2016'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Show, Attend and Tell: Neural Image Caption Generation with Visual Attention*,
    Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard
    Zemel, Yoshua Bengio, Fev 2015'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks*,
    Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M. Rush, Bart van Merriënboer,
    Armand Joulin, Tomas Mikolov,2015'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Memory Networks*, Jason Weston, Sumit Chopra, Antoine Bordes,2014'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*End-To-End Memory Networks*, Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,
    Rob Fergus, 2015'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Neural Turing Machines*, Alex Graves, Greg Wayne, Ivo Danihelka, 2014'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Visual-Semantic Alignments for Generating Image Descriptions*, Andrej
    Karpathy, Li Fei-Fei, 2014'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The attention mechanism is a smart option to help neural networks select the
    right information and focus to produce the correct output. It can be placed either
    directly on the inputs or the features (inputs processed by a few layers). Accuracies
    in the cases of translation, image annotation, and speech recognition, are increased,
    in particular when the dimension of the inputs is important.
  prefs: []
  type: TYPE_NORMAL
- en: 'Attention mechanism has led to new types of networks enhanced with external
    memory, working as an input/output, from which to read or to which to write. These
    networks have proved to be very powerful in question-answering challenges, into
    which most tasks in natural language processing can can be cast: tagging, classification,
    sequence-to-sequence, or question answering tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll see more advanced techniques and their application
    to the more general case of recurrent neural networks, to improve accuracy.
  prefs: []
  type: TYPE_NORMAL
