<html><head></head><body>
		<div id="_idContainer029" class="Content">
			<h1 id="_idParaDest-30"><em class="italics"><a id="_idTextAnchor032"/>Chapter 2</em></h1>
		</div>
		<div id="_idContainer030" class="Content">
			<h1 id="_idParaDest-31"><a id="_idTextAnchor033"/>Introduction to Computer Vision</h1>
		</div>
		<div id="_idContainer031" class="Content">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Explain the impact of artificial intelligence and computer vision</li>
				<li class="bullets">Deploy some of the basic computer vision algorithms</li>
				<li class="bullets">Develop some of the basic machine learning algorithms</li>
				<li class="bullets">Construct your first neural network</li>
			</ul>
			<p>This chapter covers an introduction to computer vision followed by a few important basic computer vision and machine learning algorithms.</p>
		</div>
		<div id="_idContainer069" class="Content">
			<h2 id="_idParaDest-32"><a id="_idTextAnchor034"/>Introduction</h2>
			<p><strong class="bold">Artificial Intelligence</strong> (<strong class="bold">AI</strong>) is changing everything. It tries to mimic human intelligence in order to achieve different tasks.</p>
			<p>The section of AI that deals with images is called computer vision. Computer vision is an interdisciplinary scientific field that tries to mimic human eyes. It not only makes sense out of the pixels that are extracted from an image, but also gains a higher level of understanding from that specific image by performing automated tasks and using algorithms.</p>
			<p>Some of these algorithms are better at object recognition, recognizing faces, classifying images, editing images, and even generating images.</p>
			<p>This chapter will begin with an introduction to computer vision, starting with some of the most basic algorithms and an exercise to put them into practice. Later, an introduction to machine learning will be given, starting from the most basic algorithms to neural networks, involving several exercises to strengthen the knowledge acquired.</p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor035"/>Basic Algorithms in Computer Vision</h2>
			<p>In this topic, we will be addressing how images are formed. We will introduce a library that is very useful for performing computer vision tasks and we will learn about the workings of some of these tasks and algorithms and how to code them.</p>
			<h3 id="_idParaDest-34"><a id="_idTextAnchor036"/>Image Terminology</h3>
			<p>To understand computer vision, we first need to know how images work and how a computer interprets them. </p>
			<p>A computer understands an image as a set of numbers grouped together. To be more specific, the image is seen as a two-dimensional array, a matrix that contains values from 0 to 255 (0 being for black and 255 for white in grayscale images) representing the values of the pixels of an image (<strong class="keyword">pixel values</strong>), as shown in the following example:</p>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/C13550_02_01.jpg" alt="Figure 2.1: Image representation without and with pixel values"/>
				</div>
			</div>
			<h6>Figure 2.1: Image representation without and with pixel values</h6>
			<p>In the image on the left-hand side, the number 3 is shown in a low resolution. On the right-hand side, the same image is shown along with the value of every pixel. As this value rises, a brighter color is shown, and if the value decreases, the color gets darker. </p>
			<p>This particular image is in grayscale, which means it is only a two-dimensional array of values from 0 to 255, but what about colored images? Colored images (or red/green/blue (RGB) images) have three layers of two-dimensional arrays stacked together. Every layer represents one color each and putting them all together forms a colored image.</p>
			<p>The preceding image has 14x14 pixels in its matrix. In grayscale, it is represented as 14x14x1, as it only has one matrix, and one channel. For the RGB format, the representation is 14x14x3 as it has 3 channels. From this, all that computers need to understand is that the images come from these pixels.</p>
			<h3 id="_idParaDest-35"><a id="_idTextAnchor037"/>OpenCV</h3>
			<p>OpenCV is an open source computer vision library that has C++, Python, and Java interfaces and supports Windows, Linux, macOS, iOS, and Android.</p>
			<p>For all the algorithms mentioned in this chapter, we will be using OpenCV. OpenCV helps us perform these algorithms using Python. If you want to practice one of these algorithms, we recommend using Google Colab. You will need to install Python 3.5 or above, OpenCV, and NumPy to carry on with this chapter. T<a id="_idTextAnchor038"/><a id="_idTextAnchor039"/>o display them on our screens, we will use Matplotlib. Both of these are great libraries for AI.</p>
			<h3 id="_idParaDest-36"><a id="_idTextAnchor040"/>Basic Image Processing Algorithms </h3>
			<p>In order for a computer to understand an image, the image has to be processed first. There are many algorithms that can be used to process images and the output depends on the task at hand.</p>
			<p>Some of the most basic algorithms are: </p>
			<ul>
				<li>Thresholding </li>
				<li>Morphological transformations</li>
				<li>Blurring</li>
			</ul>
			<h3 id="_idParaDest-37"><a id="_idTextAnchor041"/>Thresholding</h3>
			<p><strong class="bold">Thresholding</strong> is commonly used to simplify how an image is visualized by both the computer and the user in order to make analysis easier. It is based on a value that the user sets and every pixel is converted to white or black depending on whether the value of every pixel is higher or lower than the set value. If the image is in grayscale, the output image will be white and black, but if you choose to keep the RGB format for your image, the threshold will be applied for every channel, which means it will still output a colored image.</p>
			<p>There are different methods for thresholding, and these are some of the most used ones:</p>
			<ol>
				<li><strong class="bold">Simple Thresholding: </strong>If the pixel value is lower than the threshold set by the user, this pixel will be assigned a 0 value (black), or 255 (white). There are also different styles of thresholding within simple thresholding: <p>Threshold binary </p><p>Threshold binary inverted </p><p>Truncate </p><p>Threshold to zero</p><p>Threshold to zero inverted </p><p>The different types of thresholds are shown in figure 2.2</p><div id="_idContainer033" class="IMG---Figure"><img src="image/C13550_02_02.jpg" alt="Figure 2.2: Different types of thresholds"/></div><h6>Figure 2.2: Different types of thresholds</h6><p>Threshold binary inverted works like binary but the pixels that were black are white and vice versa. Global thresholding is another name given to binary thresholding under simple thresholding.</p><p>Truncate shows the exact value of the threshold if the pixel is above the threshold and the pixel value. </p><p>Threshold to zero outputs the pixel value (which is the actual value of the pixel) if the pixel value is above the threshold value, otherwise it will output a black image, whereas threshold to zero inverted does the exact opposite.</p><h4>N<a id="_idTextAnchor042"/>ote</h4><p class="callout">The threshold value can be modified depending on the image or what the user wants to achieve.</p></li>
				<li><strong class="bold">Adaptive Thresholding</strong>: Simple thresholding uses a global value as the threshold. If the image has different lighting conditions in some parts, the algorithm does not perform that well. In such cases, adaptive thresholding automatically guesses different threshold values for different regions within the image, giving us a better overall result with varying lighting conditions. <p>There are two types of adaptive thresholding: </p><p>Adaptive mean thresholding</p><p>Adaptive Gaussian thresholding</p><p>The difference between the adaptive thresholding and simple thresholding is shown in figure 2.3</p><div id="_idContainer034" class="IMG---Figure"><img src="image/C13550_02_03.jpg" alt="Figure 2.3: Difference between adaptive thresholding and simple thresholding"/></div><h6>Figure 2.3: Difference between adaptive thresholding and simple thresholding</h6><p>In adaptive mean thresholding, the threshold value is the mean of the neighborhood area, while in adaptive Gaussian thresholding, the threshold value is the weighted sum of the neighborhood values where weights are a Gaussian window.</p></li>
				<li><strong class="bold">Ots<a id="_idTextAnchor043"/>u's Binarization:</strong> In global thresholding, we used an arbitrary value to assign a threshold value. Consider a bimodal image (an image where the pixels are distributed over two dominant regions). How would you choose the correct value? Otsu's binarization automatically calculates a threshold value from the image histogram for a bimodal image. An <strong class="bold">image histogram</strong> is a type of <a href="https://en.wikipedia.org/wiki/Histogram">histogram</a> that acts as a <a href="https://en.wikipedia.org/wiki/Graphical_representation">graphical representation</a> of the <a href="https://en.wikipedia.org/wiki/Lightness_(color)">tonal</a> distribution in a <a href="https://en.wikipedia.org/wiki/Digital_image">digital image</a>:</li>
			</ol>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/C13550_02_04.jpg" alt="Figure 2.4: Otsu’s thresholding"/>
				</div>
			</div>
			<h6>Figure 2.4: Otsu's thresholding</h6>
			<h3 id="_idParaDest-38">Exe<a id="_idTextAnchor044"/>rcise 4: Applying Various Thresholds to an Image</h3>
			<h4>NOTE</h4>
			<p class="callout">As we are training artificial neural networks on Google Colab, we should use the GPU that Google Colab provides us. In order to do that, we would have to go to <strong class="inline">runtime &gt; Change runtime type &gt; Hardware accelerator: GPU &gt; Save</strong>. </p>
			<p class="callout">All the exercises and activities will be primarily developed in Google Colab. It is recommended to keep a separate folder for different assignments, unless advised not to.</p>
			<p class="callout">The <strong class="inline">Dataset</strong> folder is available on GitHub in the Lesson02 | Activity02 folder.</p>
			<p>In this exercise, we will be loading an image of a subway, to which we will apply thresholding:</p>
			<ol>
				<li value="1">Open up your Google Colab interface.</li>
				<li>Create a folder for the book, download the <strong class="inline">Dataset</strong> folder from GitHub, and upload it in the folder.</li>
				<li>Import the drive and mount it as follows:<p class="snippet">from google.colab import drive</p><p class="snippet">drive.mount('/content/drive')</p><h4>Note</h4><p class="callout">Every time you use a new collaborator, mount the drive to the desired folder.</p><p>Once you have mounted your drive for the first time, you will have to enter the authorization code that you would get by clicking on the URL given by Google and pressing the <strong class="bold">Enter</strong> key on your keyboard:</p><div id="_idContainer036" class="IMG---Figure"><img src="image/C13550_02_05.jpg" alt="Figure 2.5: Image displaying the Google Colab authorization step"/></div><h6>Figure 2.5: Image displaying the Google Colab authorization step</h6></li>
				<li>Now that you have mounted the drive, you need to set the path of the directory:<p class="snippet">cd /content/drive/My Drive/C13550/Lesson02/Exercise04/</p><h4>Note</h4><p class="callout">The path mentioned in step 5 may change as per your folder setup on Google Drive. The path will always begin with <strong class="inline">cd /content/drive/My Drive/</strong>.</p><p class="callout">The <strong class="inline">Dataset</strong> folder must be present in the path you are setting up.</p></li>
				<li>Now you need to import the corresponding dependencies: OpenCV <strong class="inline">cv2</strong> and Matplotlib:<p class="snippet">import cv2</p><p class="snippet">from matplotlib import pyplot as plt</p></li>
				<li>Now type the code to load the <strong class="inline">subway.jpg</strong> image, which we are going to process in grayscale using OpenCV and show using Matplotlib:<h4>Note</h4><p class="callout">The <strong class="inline">subway.jpg</strong> image can be found on GitHub in the Lesson02 | Exercise04 folder.</p><p class="snippet">img = cv2.imread('subway.jpg',0)</p><p class="snippet">plt.imshow(img,cmap='gray')</p><p class="snippet">plt.xticks([]),plt.yticks([])</p><p class="snippet">plt.show()</p><div id="_idContainer037" class="IMG---Figure"><img src="image/C13550_02_06.jpg" alt="Figure 2.6: Result of plotting the loaded subway image"/></div><h6>Figure 2.6: Result of plotting the loaded subway image</h6></li>
				<li>Let's apply simple thresholding by using OpenCV methods. <p>The method for doing so in OpenCV is called <strong class="bold">cv2.threshold</strong> and it takes three parameters: <strong class="bold">image</strong> (grayscale), <strong class="bold">threshold value</strong> (used to classify the pixel values), and <strong class="bold">maxVal</strong>, which represents the value to be given if the pixel value is more than (sometimes less than) the threshold value:</p><p class="snippet">_,thresh1 = cv2.threshold(img,107,255,cv2.THRESH_BINARY)</p><p class="snippet">_,thresh2 = cv2.threshold(img,107,255,cv2.THRESH_BINARY_INV) </p><p class="snippet">_,thresh3 = cv2.threshold(img,107,255,cv2.THRESH_TRUNC) </p><p class="snippet">_,thresh4 = cv2.threshold(img,107,255,cv2.THRESH_TOZERO)</p><p class="snippet">_,thresh5 = cv2.threshold(img,107,255,cv2.THRESH_TOZERO_INV) </p><p class="snippet">titles = ['Original Image','BINARY', 'BINARY_INV', 'TRUNC','TOZERO','TOZERO_INV']</p><p class="snippet">images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]</p><p class="snippet">for i in range(6):</p><p class="snippet">    plt.subplot(2,3,i+1),plt.imshow(images[i],'gray')</p><p class="snippet">    plt.title(titles[i])</p><p class="snippet">    plt.xticks([]),plt.yticks([])</p><p class="snippet">plt.show()</p><div id="_idContainer038" class="IMG---Figure"><img src="image/C13550_02_07.jpg" alt="Figure 2.7: Simple thresholding using OpenCV"/></div><h6>Figure 2.7: Simple thresholding using OpenCV</h6></li>
				<li>We are going to do the same with adaptive thresholding. <p>The method for doing so is <strong class="bold">cv2.adaptiveThreshold</strong> and it has three special input parameters and only one output argument. Adaptive method, block size (the size of the neighborhood area), and C (a constant that is subtracted from the mean or weighted mean calculated) are the inputs, whereas you only obtain the thresholded image as the output. This is unlike global thresholding, where there are two outputs:</p><p class="snippet">th2=cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,71,7)</p><p class="snippet">th3=cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,71,7)</p><p class="snippet">titles = ['Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']</p><p class="snippet">images = [th2, th3]</p><p class="snippet">for i in range(2):</p><p class="snippet">    plt.subplot(1,2,i+1),plt.imshow(images[i],'gray')</p><p class="snippet">    plt.title(titles[i])</p><p class="snippet">    plt.xticks([]),plt.yticks([])</p><p class="snippet">plt.show()</p><div id="_idContainer039" class="IMG---Figure"><img src="image/C13550_02_08.jpg" alt="Figure 2.8: Adaptive thresholding using OpenCV"/></div><h6>Figure 2.8: Adaptive thresholding using OpenCV</h6></li>
				<li>Finally, let's put Otsu's binarization into practice.</li>
				<li>The method is the same as for simple thresholding, <strong class="bold">cv2.threshold</strong>, but with an extra flag, <strong class="bold">cv2.THRESH_OTU</strong>:<p class="snippet">ret2,th=cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)</p><p class="snippet">titles = ['Otsu\'s Thresholding']</p><p class="snippet">images = [th]</p><p class="snippet">for i in range(1):</p><p class="snippet">    plt.subplot(1,1,i+1),plt.imshow(images[i],'gray')</p><p class="snippet">    plt.title(titles[i])</p><p class="snippet">    plt.xticks([]),plt.yticks([])</p><p class="snippet">plt.show()</p></li>
			</ol>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/C13550_02_09.jpg" alt="Figure 2.9: Otsu’s binarization using OpenCV"/>
				</div>
			</div>
			<h6>Figure 2.9: Otsu's binarization using OpenCV</h6>
			<p>Now you are able to apply different thresholding transformations to any image.</p>
			<h3 id="_idParaDest-39"><a id="_idTextAnchor045"/>Morphological Transformations</h3>
			<p>A morphological transformation consists of a set of simple image operations based on an image shape, and they are usually used on binary images. They are commonly used to differentiate text from the background or any other shapes. They need two inputs, one being the original image, and the other is called the <strong class="bold">structuring element</strong> or <strong class="keyword">kernel</strong>, which decides the nature of the operation. The <strong class="keyword">kernel</strong> is usually a matrix that slides through the image, multiplying its values by the values of the pixels of the image. Two basic morphological operators are erosion and dilation. Their variant forms are opening and closing. The one that should be used depends on the task at hand:</p>
			<ul>
				<li><strong class="bold">Erosion</strong>: When given a binary image, it shrinks the thickness by one pixel both on the interior and the exterior of the image, which is represented by white pixels. This method can be applied several times. It can be used for different reasons, depending on what you want to achieve, but normally it is used with dilation (which is explained in figure 2.10) in order to get rid of holes or noise. An example of erosion is shown here with the same digit, 3:</li>
			</ul>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/C13550_02_10.jpg" alt="Figure 2.10: Example of erosion"/>
				</div>
			</div>
			<h6>Figure 2.10: Example of erosion</h6>
			<ul>
				<li><strong class="bold">Dilation</strong>: <a id="_idTextAnchor046"/>This method does the opposite of erosion. It increases the thickness of the object in a binary image by one pixel both on the interior and the exterior. It can also be applied to an image several times. This method can be used for different reasons, depending on what you want to achieve, but normally it is implemented along with erosion in order to get rid of holes in an image or noise. An example of dilation is shown here (we have implemented dilation on the image several times):</li>
			</ul>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/C13550_02_11.jpg" alt="Figure 2.11: Example of dilation"/>
				</div>
			</div>
			<h6>Figure 2.11: Example of dilation</h6>
			<ul>
				<li><strong class="bold">Opening</strong>: T<a id="_idTextAnchor047"/>his method performs erosion first, followed by dilation, and it is usually used for removing noise from an image.</li>
				<li><strong class="bold">Closing</strong>: This algorithm does the opposite of opening, as it performs dilation first before erosion. It is usually used for removing holes within an object:</li>
			</ul>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/C13550_02_12.jpg" alt="Figure 2.12: Examples of opening and closing"/>
				</div>
			</div>
			<h6>Figure 2.12: Examples of opening and closing</h6>
			<p>As you can see, the opening method removes random noise from the image and the closing method works perfectly in fixing the small random holes within the image. In order to get rid of the holes of the output image from the opening method, a closing method could be applied.</p>
			<p>There are mo<a id="_idTextAnchor048"/>re binary operations, but these are the basic ones.</p>
			<h3 id="_idParaDest-40"><a id="_idTextAnchor049"/>Exercise 5: Applying the Various Morphological Transformations to an Image</h3>
			<p>In this exercise, we will be loading an image of a number, on which we will apply the morphological transformations that we have just learned about:</p>
			<ol>
				<li value="1">Open up your Google Colab interface.</li>
				<li>Set the path of the directory:<p class="snippet">cd /content/drive/My Drive/C13550/Lesson02/Exercise05/</p><h4>Note</h4><p class="callout">The path mentioned in step 2 may change, as per your folder setup on Google Drive.</p></li>
				<li>Import the OpenCV, Matplotlib, and NumPy libraries. NumPy here is the fundamental package for scientific computing with Python and will help us create the kernels applied:<p class="snippet">import cv2</p><p class="snippet">import numpy as np</p><p class="snippet">from matplotlib import pyplot as plt</p></li>
				<li>Now type the code to load the <strong class="inline">Dataset/three.png</strong> image, which we are going to process in grayscale using OpenCV and show using Matplotlib:<h4>Note</h4><p class="callout">The <strong class="inline">three.png</strong> image can be found on GitHub in the Lesson02 | Exercise05 folder.</p><p class="snippet">img = cv2.imread('Dataset/three.png',0)</p><p class="snippet">plt.imshow(img,cmap='gray')</p><p class="snippet">plt.xticks([]),plt.yticks([])</p><p class="snippet">plt.savefig('ex2_1.jpg', bbox_inches='tight')</p><p class="snippet">plt.show()</p><div id="_idContainer044" class="IMG---Figure"><img src="image/C13550_02_13.jpg" alt="Figure 2.13: Result of plotting the loaded image"/></div><h6>Figure 2.13: Result of plotting the loaded image</h6></li>
				<li>Let's apply erosion by using OpenCV methods. <p>The method used here is <strong class="bold">cv2.erode</strong>, and it takes three parameters: the image, a kernel that slides through the image, and the number of iterations, which is the number of times that it is executed:</p><p class="snippet">kernel = np.ones((2,2),np.uint8)</p><p class="snippet">erosion = cv2.erode(img,kernel,iterations = 1)</p><p class="snippet">plt.imshow(erosion,cmap='gray')</p><p class="snippet">plt.xticks([]),plt.yticks([])</p><p class="snippet">plt.savefig('ex2_2.jpg', bbox_inches='tight')</p><p class="snippet">plt.show()</p><div id="_idContainer045" class="IMG---Figure"><img src="image/C13550_02_14.jpg" alt="Figure 2.14: Output of the erosion method using OpenCV"/></div><h6>Figure 2.14: Output of the erosion method using OpenCV</h6><p>As we can see, the thickness of the figure has decreased.</p></li>
				<li>We are going to do the same with dilation. <p>The method used here is <strong class="bold">cv2.dilate</strong>, and it takes three parameters: the image, the kernel, and the number of iterations:</p><p class="snippet">kernel = np.ones((2,2),np.uint8)</p><p class="snippet">dilation = cv2.dilate(img,kernel,iterations = 1)</p><p class="snippet">plt.imshow(dilation,cmap='gray')</p><p class="snippet">plt.xticks([]),plt.yticks([])</p><p class="snippet">plt.savefig('ex2_3.jpg', bbox_inches='tight')</p><p class="snippet">plt.show()</p><div id="_idContainer046" class="IMG---Figure"><img src="image/C13550_02_15.jpg" alt="Figure 2.15: Output of the dilation method using OpenCV"/></div><h6>Figure 2.15: Output of the dilation method using OpenCV</h6><p>As we can see, the thickness of the figure has increased.</p></li>
				<li>Finally, let's put opening and closing into practice.<p>The method used here is <strong class="bold">cv2.morphologyEx</strong>, and it takes three parameters: the image, the method applied, and the kernel:</p><p class="snippet">import random</p><p class="snippet">random.seed(42)</p><p class="snippet">def sp_noise(image,prob):</p><p class="snippet">    '''</p><p class="snippet">    Add salt and pepper noise to image</p><p class="snippet">    prob: Probability of the noise</p><p class="snippet">    '''</p><p class="snippet">    output = np.zeros(image.shape,np.uint8)</p><p class="snippet">    thres = 1 - prob </p><p class="snippet">    for i in range(image.shape[0]):</p><p class="snippet">        for j in range(image.shape[1]):</p><p class="snippet">            rdn = random.random()</p><p class="snippet">            if rdn &lt; prob:</p><p class="snippet">                output[i][j] = 0</p><p class="snippet">            elif rdn &gt; thres:</p><p class="snippet">                output[i][j] = 255</p><p class="snippet">            else:</p><p class="snippet">                output[i][j] = image[i][j]</p><p class="snippet">    return output</p><p class="snippet">def sp_noise_on_figure(image,prob):</p><p class="snippet">    '''</p><p class="snippet">    Add salt and pepper noise to image</p><p class="snippet">    prob: Probability of the noise</p><p class="snippet">    '''</p><p class="snippet">    output = np.zeros(image.shape,np.uint8)</p><p class="snippet">    thres = 1 - prob </p><p class="snippet">    for i in range(image.shape[0]):</p><p class="snippet">        for j in range(image.shape[1]):</p><p class="snippet">            rdn = random.random()</p><p class="snippet">            if rdn &lt; prob:</p><p class="snippet">                if image[i][j] &gt; 100:</p><p class="snippet">                    output[i][j] = 0</p><p class="snippet">            else:</p><p class="snippet">                output[i][j] = image[i][j]</p><p class="snippet">    return output</p><p class="snippet">kernel = np.ones((2,2),np.uint8) </p><p class="snippet"># Create thicker figure to work with</p><p class="snippet">dilation = cv2.dilate(img, kernel, iterations = 1)</p><p class="snippet"># Create noisy image</p><p class="snippet">noise_img = sp_noise(dilation,0.05)</p><p class="snippet"># Create image with noise in the figure</p><p class="snippet">noise_img_on_image = sp_noise_on_figure(dilation,0.15)</p><p class="snippet"># Apply Opening to image with normal noise</p><p class="snippet">opening = cv2.morphologyEx(noise_img, cv2.MORPH_OPEN, kernel)</p><p class="snippet"># Apply Closing to image with noise in the figure</p><p class="snippet">closing = cv2.morphologyEx(noise_img_on_image, cv2.MORPH_CLOSE, kernel)</p><p class="snippet">images = [noise_img,opening,noise_img_on_image,closing]</p><p class="snippet">for i in range(4):</p><p class="snippet">    plt.subplot(1,4,i+1),plt.imshow(images[i],'gray')</p><p class="snippet">    plt.xticks([]),plt.yticks([])</p><p class="snippet">plt.savefig('ex2_4.jpg', bbox_inches='tight')</p><p class="snippet">plt.show()</p></li>
			</ol>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/C13550_02_16.jpg" alt="Figure 2.16: Output of the opening method (left) and closing method (right) using OpenCV"/>
				</div>
			</div>
			<h6>Figure 2.16: Output of the opening method (left) and closing method (right) using OpenCV</h6>
			<h4>Note</h4>
			<p class="callout">The entire code file can be found on GitHub in the Lesson02 | Exercise05 folder.</p>
			<h3 id="_idParaDest-41"><a id="_idTextAnchor050"/>Blurring (Smoothing)</h3>
			<p>Image blurring performs convolution over an image with a filter kernel, which in simpler terms is multiplying a matrix of specific values on every part of the image, in order to smooth it. It is useful for removing noise and edges:</p>
			<ul>
				<li><strong class="bold">Averaging</strong>: In this method, we consider a box filter or kernel that takes the average of the pixels within the area of the kernel, replacing the central element by using convolution over the entire image.</li>
				<li><strong class="bold">Gaussian Blurring</strong>: The kernel applied here is Gaussian, instead of the box filter. It is used for removing Gaussian noise in a particular image.</li>
				<li><strong class="bold">Median Blurring</strong>: Similar to averaging, but this one replaces the central element with the median value of the pixels of the kernel. It actually has a very good effect on salt-and-pepper noise (that is, visible black or white spots in an image).</li>
			</ul>
			<p>In Figure 2.17, we have applied the aforementioned methods:</p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/C13550_02_17.jpg" alt="Figure 2.17: Result of comparing different blurring methods"/>
				</div>
			</div>
			<h6>Figure 2.17: Result of comparing different blurring methods</h6>
			<p>There are many mo<a id="_idTextAnchor051"/>re algorithms that could be applied, but these are the most important ones.</p>
			<h3 id="_idParaDest-42"><a id="_idTextAnchor052"/>Exercise 6: Applying the Various Blurring Methods to an Image</h3>
			<p>In this exercise, we will be loading an image of a subway, to which we will apply the blurring method:</p>
			<ol>
				<li value="1">Open up your Google Colab interface.</li>
				<li>Set the path of the directory:<p class="snippet">cd /content/drive/My Drive/C13550/Lesson02/Exercise06/</p><h4>Note</h4><p class="callout">The path mentioned in step 2 may be different according to your folder setup on Google Drive.</p></li>
				<li>Import the OpenCV, Matplotlib, and NumPy libraries:<p class="snippet">import cv2</p><p class="snippet">from matplotlib import pyplot as plt</p><p class="snippet">import numpy as np</p></li>
				<li>Type the code to load the <strong class="inline">Dataset/subway.png</strong> image that we are going to process in grayscale using OpenCV and show it using Matplotlib:<h4>Note</h4><p class="callout">The <strong class="inline">subway.png</strong> image can be found on GitHub in the Lesson02 | Exercise06 folder.</p><p class="snippet">img = cv2.imread('Dataset/subway.jpg')</p><p class="snippet">#Method to convert the image to RGB</p><p class="snippet">img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</p><p class="snippet">plt.imshow(img)</p><p class="snippet">plt.savefig('ex3_1.jpg', bbox_inches='tight')</p><p class="snippet">plt.xticks([]),plt.yticks([])</p><p class="snippet">plt.show()</p><div id="_idContainer049" class="IMG---Figure"><img src="image/C13550_02_18.jpg" alt="Figure 2.18: Result of plotting the loaded subway image in RGB"/></div><h6>Figure 2.18: Result of plotting the loaded subway image in RGB</h6></li>
				<li>Let's apply all the blurring methods:<p>The methods applied are <strong class="bold">cv2.blur</strong>, <strong class="bold">cv2.GaussianBlur</strong>, and <strong class="bold">cv2.medianBlur</strong>. All of them take an image as the first parameter. The first method takes only one argument, that is, the kernel. The second method takes the kernel and the standard deviation (sigmaX and sigmaY), and if both are given as zeros, they are calculated from the kernel size. The method mentioned last only takes one more argument, which is the kernel size:</p><p class="snippet">blur = cv2.blur(img,(51,51)) # Apply normal Blurring</p><p class="snippet">blurG = cv2.GaussianBlur(img,(51,51),0) # Gaussian Blurring</p><p class="snippet">median = cv2.medianBlur(img,51) # Median Blurring</p><p class="snippet">titles = ['Original Image','Averaging', 'Gaussian Blurring', 'Median Blurring']</p><p class="snippet">images = [img, blur, blurG, median]</p><p class="snippet">for i in range(4):</p><p class="snippet">    plt.subplot(2,2,i+1),plt.imshow(images[i])</p><p class="snippet">    plt.title(titles[i])</p><p class="snippet">    plt.xticks([]),plt.yticks([])</p><p class="snippet">plt.savefig('ex3_2.jpg', bbox_inches='tight')</p><p class="snippet">plt.show()</p></li>
			</ol>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/C13550_02_19.jpg" alt="Figure 2.19: Blurring methods with OpenCV"/>
				</div>
			</div>
			<h6>Figure 2.19: Blurring methods with OpenCV</h6>
			<p>Now you know how to apply several blurring techniques to any image.</p>
			<h3 id="_idParaDest-43"><a id="_idTextAnchor053"/>Exercise 7: Loading an Image and Applying the Learned Methods</h3>
			<p>In this exercise, we will be loading an image of a number and we will apply the methods that we have learned so far. </p>
			<h4>Note</h4>
			<p class="callout">The entire code is available on GitHub in the Lesson02 | Exercise07-09 folder.</p>
			<ol>
				<li value="1">Open up a new Google Colab interface, and mount your drive as mentioned in <em class="italics">Exercise 4</em>, <em class="italics">Applying the Various Thresholds to an Image</em>, of this chapter.</li>
				<li>Set the path of the directory:<p class="snippet">cd /content/drive/My Drive/C13550/Lesson02/Exercise07/</p><h4>Note</h4><p class="callout">The path mentioned in step 2 may be different according to your folder setup on Google Drive.</p></li>
				<li>Import the correspo<a id="_idTextAnchor054"/>nding dependencies: NumPy, OpenCV, and Matplotlib:<p class="snippet">import numpy as np  #Numpy</p><p class="snippet">import cv2          #OpenCV</p><p class="snippet">from matplotlib import pyplot as plt #Matplotlib</p><p class="snippet">count = 0</p></li>
				<li>Type the code to load the <strong class="inline">Dataset/number.jpg</strong> image, which we are going to process in grayscale using OpenCV and show using Matplotlib:<h4>Note</h4><p class="callout">The <strong class="inline">number.jpg</strong> image can be found on GitHub in the Lesson02 | Exercise07-09 | Dataset folder.</p><p class="snippet">img = cv2.imread('D<a id="_idTextAnchor055"/>ataset/number.jpg',0)</p><p class="snippet">plt.imshow(img,cmap='gray')</p><p class="snippet">plt.xticks([]),plt.yticks([])</p><p class="snippet">plt.show()</p><div id="_idContainer051" class="IMG---Figure"><img src="image/C13550_02_20.jpg" alt="Figure 2.20: Result of loading the image with the number"/></div><h6>Figure 2.20: Result of loading the image with the number</h6></li>
				<li>If you want to recognize those digits using machine learning or any other algorithm, you need to simplify the visualization of them. Using thresholding seems to be the first logical step to proceed with this exercise. We have learned some thresholding methods, but the most commonly used one is Otsu's binarization, as it automatically calculates the threshold value without the user providing the details manually.<p>Apply Otsu's binarization to the grayscale image and show it using Matplotlib: </p><p class="snippet">_,th1=cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU</p><p class="snippet">th1 = (255-th1) </p><p class="snippet"># This step changes the black with white and vice versa in order to have white figures</p><p class="snippet">plt.imshow(th1,cmap='gray')</p><p class="snippet">plt.xticks([]),plt.yticks([])</p><p class="snippet">plt.show()</p><div id="_idContainer052" class="IMG---Figure"><img src="image/C13550_02_21.jpg" alt="Figure 2.21: Using Otsu’s binarization thresholding on the image"/></div><h6>Figure 2.21: Using Otsu's binarization thresholding on the image</h6></li>
				<li>In order to get rid of the lines in the background, we need to do some morphological transformations. First, start by applying the closing method:<p class="snippet">open1 = cv2.morpholo<a id="_idTextAnchor056"/>gyEx(th1, cv2.MORPH_OPEN, np.ones((4, 4),np.uint8))</p><p class="snippet">plt.imshow(open1,cmap='gray')</p><p class="snippet">plt.xticks([]),plt.yticks([])</p><p class="snippet">plt.show()</p><div id="_idContainer053" class="IMG---Figure"><img src="image/C13550_02_22.jpg" alt="Figure 2.22: Applying the closing method"/></div><h6>Figure 2.22: Applying the closing method</h6><h4>Note </h4><p class="callout">The lines in the background have been removed completely. Now a number prediction will be much easier.</p></li>
				<li>In order to fill the holes that are visible in these digits, we need to apply the opening method. Apply the opening method to the preceding image:<p class="snippet">close1 = cv2.morpholog<a id="_idTextAnchor057"/>yEx(open1, cv2.MORPH_CLOSE, np.ones((8, 8), np.uint8))</p><p class="snippet">plt.imshow(close1,cmap='gray')</p><p class="snippet">plt.xticks([]),plt.yticks([])</p><p class="snippet">plt.show()</p><div id="_idContainer054" class="IMG---Figure"><img src="image/C13550_02_23.jpg" alt="Figure 2.23: Applying the opening method"/></div><h6>Figure 2.23: Applying the opening method</h6></li>
				<li>There are still leftovers and imperfections around the digits. In order to remove these, a closing method with a bigger kernel would be the best choice. Now apply the corresponding method:<p class="snippet">open2 = cv2.morphologyE<a id="_idTextAnchor058"/>x(close1, cv2.MORPH_OPEN,np.ones((7,12),np.uint8))</p><p class="snippet">plt.imshow(open2,cmap='gray')</p><p class="snippet">plt.xticks([]),plt.yticks([])</p><p class="snippet">plt.show()</p><div id="_idContainer055" class="IMG---Figure"><img src="image/C13550_02_24.jpg" alt="Figure 2.24: Applying the closing method with a kernel of a bigger size"/></div><h6>Figure 2.24: Applying the closing method with a kernel of a bigger size</h6><p>Depending on the classifier that you use to predict the digits or the conditions of the given image, some other algorithms would be applied. </p></li>
				<li>If you want to predict the numbers, you will need to predict them one by one. Thus, you should divide the numbers into smaller numbers.<p>Thankfully, OpenCV has a method to do this, and it's called <strong class="bold">cv2.findContours</strong>. In order to find contours, we need to invert blacks into whites. This piece of code is larger, but it is only required if you want to predict character by character:</p><p class="snippet">_, contours, _ = cv2.fin<a id="_idTextAnchor059"/>dContours(open2, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) #Find contours</p><p class="snippet">cntsSorted = sorted(contours, key=lambda x: cv2.contourArea(x), reverse=True) #Sort the contours</p><p class="snippet">cntsLength = len(cntsSorted)</p><p class="snippet">images = []</p><p class="snippet">for idx in range(cntsLength): #Iterate over the contours</p><p class="snippet">	x, y, w, h = cv2.boundingRect(contour_no) #Get its position and size</p><p class="snippet">	... # Rest of the code in Github</p><p class="snippet">	images.append([x,sample_no]) #Add the image to the list of images and the X position</p><p class="snippet">images = sorted(images, key=lambda x: x[0]) #Sort the list of images using the X position</p><p class="snippet">{…}</p><h4>Note</h4><p class="callout">The entire code with added comments is available on GitHub in the Lesson02 | Exercise07-09 folder.</p></li>
			</ol>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/C13550_02_25.jpg" alt="Figure 2.25: Extracted digits as the output"/>
				</div>
			</div>
			<h6>Figure 2.25: Extracted digits as the output</h6>
			<p>In the first part of the <a id="_idTextAnchor060"/>code, we are finding the <strong class="keyword">contours</strong> of the image (the curve joining all the continuous points along the boundary and of the same color or intensity) to find every digit, which we then sort depending on the area of each contour (each digit). </p>
			<p>After this, we loop over the contours, cropping the original image with the given contours, ending up with every number in a different image. </p>
			<p>After this, we need to have all the images with the same shape, so we adapt the image to a given shape using NumPy and append the image to a list of images along with the X position.</p>
			<p>Finally, we sort the list of images using the X position (from left to right, so they remain in order) and plot the results. We also save every single digit as an image so that we can use every digit separately afterward for any task we want.</p>
			<p>Congratulations! You have successfully processed an image with text in it, obtained the text, and extracted every single character, and now the magic of machine learning can begin.</p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor061"/>Introduction to Machine Learning</h2>
			<p><strong class="bold">Machine learning</strong> (<strong class="bold">ML</strong>) is the science of making computers learn from data without stating any rules. ML is mostly based on models that are trained with a lot of data, such as images of digits or features of different objects, with their corresponding labels, such as the number of those digits or the type of the object. This is called <strong class="bold">supervised learning</strong>. There are other types of learning, such as <strong class="bold">unsupervised learning</strong> and <strong class="bold">reinforcement learning</strong>, but we will be focusing on supervised learning. The main difference between supervised learning and unsupervised learning is that the model learns clusters from the data (depending on how many clusters you specify), which are translated into classes. Reinforcement learning, on the other hand, is concerned with how software agents should take action in an environment in order to increase a reward that is given to the agent, which will be positive if the agent is performing the right actions and negative otherwise.</p>
			<p>In this part of the chapter, we will gain an understanding of machine learning and check a variety of models and algorithms, going from the most basic models to explaining artificial neural networks. </p>
			<h3 id="_idParaDest-45"><a id="_idTextAnchor062"/>Decision Trees and Boosting Algorithms </h3>
			<p>In this section, we will be explaining decision trees and boosting algorithms as some of the most basic machine learning algorithms.</p>
			<p><strong class="keyword">Bagging</strong> (decision trees and random forests) and <strong class="keyword">boosting</strong> (AdaBoost) will be explained in this topic.</p>
			<h3 id="_idParaDest-46"><a id="_idTextAnchor063"/>Bagging:</h3>
			<p><strong class="keyword">Decision trees</strong> are perhaps the most basic machine learning algorithms, and are used for classification and regression, but on a basic level, they are used for teaching and performing tests.</p>
			<p>In a decision tree, every node represents an attribute of the data that is being trained on (whether something is true or false), where every branch (line between nodes) represents a decision (if something is true, go this way; otherwise, the other way) and every leaf represents a final outcome (if all conditions are fulfilled, it's a sunflower or a daisy).</p>
			<p>We are now going to use the Iris dataset. This dataset considers sepal width and length, along with petal width and length, in order to classify Iris flowers as setosa, versicolour, or virginica.</p>
			<h4>Note</h4>
			<p class="callout">The Iris dataset can be downloaded from scikit-learn using Python: </p>
			<p class="callout"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html">https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html</a></p>
			<p class="callout">Scikit-learn is a library that provides useful tools for data mining and data analysis.</p>
			<p>The following flowchart shows the learning representation of a decision tree trained on this dataset. X represents features from the dataset, X0 being sepal length, X1 being sepal width, X2 being petal length, and X3 petal width. The 'value' tag is how many samples of each category fall into each node. We can see that, in the first step, the decision tree already distinguishes setosa from the other two by only considering the X2 feature, petal length:</p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/C13550_02_26.jpg" alt="Figure 2.26: Graph of a decision tree for the Iris dataset"/>
				</div>
			</div>
			<h6>Figure 2.26: Graph of a decision tree for the Iris dataset</h6>
			<p>Decision trees can be implemented in Python using only a couple of lines thanks to scikit-learn:</p>
			<p class="snippet">from sklearn.tree import DecisionTreeClassifier</p>
			<p class="snippet">dtree=DecisionTreeClassifier()</p>
			<p class="snippet">dtree.fit(x,y)</p>
			<p><strong class="inline">x</strong> and <strong class="inline">y</strong> are the features and the labels of the training set, respectively.</p>
			<p><strong class="inline">x</strong>, apart from being only columns of data representing those lengths and widths, could also be every pixel of the image. In machine learning, when the input data is images, every pixel is treated as a feature.</p>
			<p>Decision trees are trained for one specific task or dataset and cannot be transferred to another similar problem. Nevertheless, several decision trees can be combined in order to create bigger models and learn how to generalize. These are called <strong class="keyword">random forests</strong>.</p>
			<p>The name forest refers to an ensemble of many decision tree algorithms, following the <strong class="keyword">bagging </strong>method, which states that the combination of several algorithms achieves the best result overall. The appearance of the word "random" refers to the randomness of the algorithm when selecting the features to take into account to split a node.</p>
			<p>Thanks again to scikit-learn, we can implement the random forest algorithm with only a couple of lines, fairly similar to the previous lines:</p>
			<p class="snippet">from sklearn.ensemble import RandomForestClassifier</p>
			<p class="snippet">rndForest=RandomForestClassifier(n_estimators=10)</p>
			<p class="snippet">rndForest.fit(x,y)</p>
			<p><strong class="inline">n_estimators</strong> stands for the number of underlying decision trees. If you test the results with this method, the results will improve for sure.</p>
			<p>There are other methods that follow the <strong class="bold">boosting</strong> methodology as well. Boosting consists of algorithms called <strong class="bold">weak learners</strong> that are put together into a weighted sum and generate a strong learner, which gives an output. These weak learners are trained sequentially, meaning each one of them tries to solve the mistakes made by its predecessor.</p>
			<p>There are many algorithms that use this approach. The most famous ones are AdaBoost, gradient boosting, and XGBoost. We are only going to look at AdaBoost as it is the most well known and easy to understand.</p>
			<h3 id="_idParaDest-47"><a id="_idTextAnchor064"/>Boosting</h3>
			<p><strong class="keyword">AdaBoost </strong>puts together weak learners in order to form a strong learner. The name AdaBoost stands for adaptive boosting, which means that this strategy would weigh differently at each point in time. Those examples that are incorrectly classified in a single iteration, get a higher weight than the next iteration, and vice versa.</p>
			<p>The code for this method is as follows:</p>
			<p class="snippet">from sklearn.ensemble import AdaBoostClassifier</p>
			<p class="snippet">adaboost=AdaBoostClassifier(n_estimators=100)</p>
			<p class="snippet">adaboost.fit(x_train, y_train)</p>
			<p><strong class="inline">n_estimators</strong> is the maximum number of estimators once boosting is completed.</p>
			<p>This method is initialized with a decision tree underneath; thus, the performance might not be as good as the random forest. But in order to make a better classifier, the random forest algorithm should be used instead:</p>
			<p class="snippet">AdaBoostClassifier(RandomForestClassifier(n_jobs=-1,n_estimators=500,max_features='auto'),n_estimators=100)</p>
			<h3 id="_idParaDest-48"><a id="_idTextAnchor065"/>Exercise 8: Predicting Numbers Using the Decision Tree, Random Forest, and AdaBoost Algorithms</h3>
			<p>In this exercise, we are going to use the digits obtained from the last exercise and the models that we have learned in this topic to correctly predict every number. To do that, we are going to extract several digits from some samples inside the <strong class="inline">Dataset/numbers</strong> folder, along with the MNIST dataset to have enough data, so the models learn properly. The MNIST dataset is a compound of handwritten digits, which go from 0 to 9 with a shape of 28 x 28 x 3, and it is mostly used for researchers to test their methods or to play around with. Nevertheless, it can help to predict some numbers even though they are not of the same kind. You can check out this dataset at <a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>.</p>
			<p>As the installation of Keras requires TensorFlow, we propose to use Google Colab, which is just like a Jupyter notebook but with the difference that your system is not being used. Instead, a remote virtual machine is used and everything for machine learning and Python is already installed. </p>
			<p>Let's begin the exercise:</p>
			<h4>Note</h4>
			<p class="callout">We will be continuing the code from Exercise 7, here in the same notebook.</p>
			<ol>
				<li value="1">Head to the interface on Google Colab, where you executed the code for <em class="italics">Exercise 7</em>, <em class="italics">Loading an Image and Applying the Learned Methods.</em></li>
				<li>Import the libraries:<p class="snippet">import numpy as np</p><p class="snippet">import random</p><p class="snippet">from sklearn import metrics</p><p class="snippet">from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier</p><p class="snippet">from sklearn.tree import DecisionTreeClassifier</p><p class="snippet">from sklearn.utils import shuffle</p><p class="snippet">from matplotlib import pyplot as plt</p><p class="snippet">import cv2</p><p class="snippet">import os</p><p class="snippet">import re</p><p class="snippet">random.seed(42)</p><h4>Note </h4><p class="callout">We are setting the seed of the random method to 42, which is for reproducibility: all random steps have the same randomness and always give the same output. It could be set to any number that does not vary.</p></li>
				<li>Now we are going to import the MNIST dataset:<p class="snippet">from keras.datasets import mnist</p><p class="snippet">(x_train, y_train), (x_test, y_test) = mnist.load_data()</p><p>In the last line of the code, we are loading the data in <strong class="inline">x_train</strong>, which is the training set (60,000 examples of digits), <strong class="inline">y_train</strong>, which are the labels of those digits, <strong class="inline">x_test</strong>, which is the testing set, and <strong class="inline">y_test</strong>, which are the corresponding labels. These are in NumPy format.</p></li>
				<li>Let's show some of those digits using Matplotlib:<p class="snippet">for idx in range(5):</p><p class="snippet">    rnd_index = random.randint(0, 59999)</p><p class="snippet">    plt.subplot(1,5,idx+1),plt.imshow(x_train[idx],'gray')</p><p class="snippet">    plt.xticks([]),plt.yticks([])</p><p class="snippet">plt.show()</p><div id="_idContainer058" class="IMG---Figure"><img src="image/C13550_02_27.jpg" alt="Figure 2.27: MNIST dataset"/></div><h6>Figure 2.27: MNIST dataset</h6><h4>Note </h4><p class="callout">These digits do not look like the ones that we extracted in the previous exercise. In order to make the models properly predict the digits from the image processed in the first exercise, we will need to add some of those digits to this dataset.</p><p>Here's the process for adding new digits that look like the ones we want to predict:</p><p>Add a Dataset folder with subfolders numbered from 0 to 9 (already done).</p><p>Get the code from the previous exercise.</p><p>Use the code to extract all the digits from the images that are stored in '<strong class="inline">Dataset/numbers/</strong>' (already done).</p><p>Paste the generated digits to the corresponding folders with the name that corresponds to the digit generated (already done).</p><p>Add those images to the original dataset (step 5 in this exercise).</p></li>
				<li>To add those images to your training set, these two methods should be declared:<p class="snippet"># ---------------------------------------------------------</p><p class="snippet">def list_files(directory, ext=None):</p><p class="snippet">    return [os.path.join(directory, f) for f in os.listdir(directory)</p><p class="snippet">            if os.path.isfile(os.path.join(directory, f)) and ( ext==None or re.match('([\w_-]+\.(?:' + ext + '))', f) )]</p><p class="snippet">   # -------------------------------------------------------</p><p class="snippet">def load_images(path,label):</p><p class="snippet">    X = []</p><p class="snippet">    Y = []</p><p class="snippet">    label = str(label)</p><p class="snippet">    for fname in list_files( path, ext='jpg' ): </p><p class="snippet">        img = cv2.imread(fname,0)</p><p class="snippet">        img = cv2.resize(img, (28, 28))</p><p class="snippet">        X.append(img)</p><p class="snippet">        Y.append(label)</p><p class="snippet">    if maximum != -1 :</p><p class="snippet">        X = X[:maximum]</p><p class="snippet">        Y = Y[:maximum]</p><p class="snippet">    X = np.asarray(X)</p><p class="snippet">    Y = np.asarray(Y)</p><p class="snippet">    return X, Y</p><p>The first method, <strong class="inline">list_files()</strong>, lists all the files within a folder with the specified extension, which in this case is <strong class="inline">jpg</strong>.</p><p>In the main method, <strong class="inline">load_images()</strong>, we are loading the images from those folders, which are from the digit folder, with its corresponding label. If the maximum is different to –1, we establish a limit to the quantity that is loaded for every digit. We do this because there should be similar samples for every digit. Finally, we convert the lists to NumPy arrays.</p></li>
				<li>Now we need to add these arrays to the training set so that our models can learn how to recognize the extracted digits:<p class="snippet">print(x_train.shape)</p><p class="snippet">print(x_test.shape)</p><p class="snippet">X, Y = load_images('Dataset/%d'%(0),0,9)</p><p class="snippet">for digit in range(1,10):</p><p class="snippet">  X_aux, Y_aux = load_images('Dataset/%d'%(digit),digit,9)</p><p class="snippet">  print(X_aux.shape)</p><p class="snippet">  X = np.concatenate((X, X_aux), axis=0)</p><p class="snippet">  Y = np.concatenate((Y, Y_aux), axis=0)</p><p>After adding those digits using the method declared in the preceding code, we concatenate those arrays to the sets created before the for loop mentioned:</p><p class="snippet">from sklearn.model_selection import train_test_split</p><p class="snippet">x_tr, x_te, y_tr, y_te = train_test_split(X, Y, test_size=0.2)</p><p>After this, the <strong class="inline">train_test_split</strong> method from <strong class="inline">sklearn</strong> is used in order to separate those digits – 20% for testing and the rest for training:</p><p class="snippet">x_train = np.concatenate((x_train, x_tr), axis=0)</p><p class="snippet">y_train = np.concatenate((y_train, y_tr), axis=0)</p><p class="snippet">x_test = np.concatenate((x_test, x_te), axis=0)</p><p class="snippet">y_test = np.concatenate((y_test, y_te), axis=0)</p><p class="snippet">print(x_train.shape)</p><p class="snippet">print(x_test.shape)</p><p>Once done, we concatenate those to the original training and testing sets. We have printed the shape of x_train and x_test before and after so those extra 60 digits can be seen. It goes from shape (60,000, 28, and 28) and (10,000, 28, and 28) to shape (60,072, 28, and 28) and (10,018, 28, and 28).</p></li>
				<li>For the models imported from sklearn that we are going to use in this exercise, we need to format the arrays to the shape (n samples and array), and now we have (n samples, array_height, and array_width): <p class="snippet">x_train = x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2])</p><p class="snippet">x_test = x_test.reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2])</p><p class="snippet">print(x_train.shape)</p><p class="snippet">print(x_test.shape)</p><p>We multiply the height and the width of the array in order to get the total length of the array, but only in one dimension: (28*28) = (784).</p></li>
				<li>Now we are ready to feed the data into the models. We will start training a decision tree:<p class="snippet">print ("Applying Decision Tree...")</p><p class="snippet">dtc = DecisionTreeClassifier()</p><p class="snippet">dtc.fit(x_train, y_train)</p><p>In order to see how well this model performs, metric accuracy is used. This represents the number of samples from <strong class="inline">x_test</strong> that have been predicted, which we have already imported from the <strong class="inline">metrics</strong> module and from sklearn. Now we will be using <strong class="inline">accuracy_score()</strong> from that module to calculate the accuracy of the model. We need to predict the results from <strong class="inline">x_test</strong> using the <strong class="inline">predict()</strong> function from the model and see whether the output matches the <strong class="inline">y_test</strong> labels:</p><p class="snippet">y_pred = dtc.predict(x_test)</p><p class="snippet">accuracy = metrics.accuracy_score(y_test, y_pred)</p><p class="snippet">print(accuracy*100)</p><p>After that, the accuracy is calculated and printed. The resulting accuracy percentage is <strong class="bold">87.92%</strong>, which is not a bad result for a decision tree. It can be improved though.</p></li>
				<li>Let's try the random forest algorithm:<p class="snippet">print ("Applying RandomForest...")</p><p class="snippet">rfc = RandomForestClassifier(n_estimators=100)</p><p class="snippet">rfc.fit(x_train, y_train)</p><p>Following the same methodology to calculate the accuracy, the accuracy obtained is <strong class="bold">94.75%</strong>, which is way better and could be classified as a good model.</p></li>
				<li>Now, we will try AdaBoost initialized with random forest:<p class="snippet">print ("Applying Adaboost...")</p><p class="snippet">adaboost = AdaBoostClassifier(rfc,n_estimators=10)</p><p class="snippet">adaboost.fit(x_train, y_train)</p><p>The accuracy obtained using AdaBoost is <strong class="bold">95.67%</strong>. This algorithm takes much more time than the previous ones but gets better results.</p></li>
				<li>We are now going to apply random forest to the digits that were obtained in the last exercise. We apply this algorithm because it takes much less time than AdaBoost and gives better results. Before checking the following code, you need to run the code from the exercise one for the image stored in the <strong class="inline">Dataset/number.jpg</strong> folder, which is the one used in the first exercise, and for the other two images that are extracted for testing in the <strong class="inline">Dataset/testing/</strong> folder. Once you have done that, you should have five images of digits in your directory for every image, ready to be loaded. Here's the code: <p class="snippet">for number in range(5):</p><p class="snippet">    imgLoaded = cv2.imread('number%d.jpg'%(number),0)</p><p class="snippet">    img = cv2.resize(imgLoaded, (28, 28))</p><p class="snippet">    img = img.flatten()</p><p class="snippet">    img = img.reshape(1,-1)</p><p class="snippet">    plt.subplot(1,5,number+1),</p><p class="snippet">    plt.imshow(imgLoaded,'gray')</p><p class="snippet">    plt.title(rfc.predict(img)[0])</p><p class="snippet">    plt.xticks([]),plt.yticks([])</p><p class="snippet">plt.show()</p></li>
			</ol>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/C13550_02_28.jpg" alt="Figure 2.28: Random forest prediction for the digits 1, 6, 2, 1, and 6"/>
				</div>
			</div>
			<h6>Figure 2.28: Random forest prediction for the digits 1, 6, 2, 1, and 6</h6>
			<p>Here, we are applying the <strong class="inline">predict()</strong> function of the random forest model, passing every image to it. Random forest seems to perform pretty well, as it has predicted all of the numbers correctly. Let's try another number that has not been used (there is a folder with some images for testing inside the <strong class="inline">Dataset</strong> folder):</p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/C13550_02_29.jpg" alt="Figure 2.29: Random forest prediction for the digits 1, 5, 8, 3, and 4"/>
				</div>
			</div>
			<h6>Figure 2.29: Random forest prediction for the digits 1, 5, 8, 3, and 4</h6>
			<p>It is still performing well with the rest of the digits. Let's try another number:</p>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/C13550_02_30.jpg" alt="Figure 2.30: Random forest prediction for the digits 1, 9, 4, 7, and 9"/>
				</div>
			</div>
			<h6>Figure 2.30: Random forest prediction for the digits 1, 9, 4, 7, and 9</h6>
			<p>With the number 7, it seems to be having problems. It is probably because we have not introduced enough samples, and due to the simplicity of the model.</p>
			<h4>Note</h4>
			<p class="callout">The entire code for this exercise is available on GitHub in the Lesson02 | Exercise07-09 folder.</p>
			<p>Now, in the next topic, we are going to explore the world of artificial neural networks, which are far more capable of achieving these tasks.</p>
			<h3 id="_idParaDest-49"><a id="_idTextAnchor066"/>Artificial Neural Networks (ANNs)</h3>
			<p><strong class="keyword">Artificial neural networks</strong> <strong class="keyword">(ANNs)</strong> are information processing systems that are modeled on and inspired by the human brain, which they try to mimic by learning how to recognize patterns in data. They accomplish tasks by having a well structured architecture. This architecture is composed of several small processing units called neurons, which are interconnected in order to solve major problems.</p>
			<p>ANNs learn by having enough examples in the dataset that they are processing, and enough examples means thousands of examples, or even millions. The amount of data here can be a disadvantage, since if you do not have this data, you will have to create it yourself, and that means that you will probably need a lot of money to gather sufficient data.</p>
			<p>Another disadvantage of these algorithms is that they need to be trained on specific hardware and software. They are well trained on high-performance GPUs, which are expensive. You can still do certain things using a GPU that does not cost that much, but the data will take much longer to be trained. You also need to have specific software, such as <strong class="bold">TensorFlow</strong>, <strong class="bold">Keras</strong>, <strong class="bold">PyTorch</strong>, or <strong class="bold">Fast.AI</strong>. For this book, we will be using TensorFlow and Keras, which runs on top of TensorFlow.</p>
			<p>These algorithms work by taking all of the data as input, in which the first layer of neurons acts as the input. After that, every entry is passed to the next layer of neurons, where these are multiplied by some value and processed by an activation function, which makes "decisions" and passes those values to the next layer. The layers in the middle of the network are called hidden layers. This process keeps going until the last layer, where the output is given. When introducing the MNIST images as input to the neural network, the end of the network should have 10 neurons, each neuron representing each digit, and if the neural network guesses that an image is a specific digit, then the corresponding neuron will be activated. The ANN checks whether it has succeeded for the decision, and if not, it performs a correction process called <strong class="keyword">backpropagation</strong>, where every pass of the network is checked and corrected, adjusting the weights of the neurons. In Figure 2.31, backpropagation is shown:</p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/C13550_02_31.jpg" alt="Figure 2.31: Backpropagation process"/>
				</div>
			</div>
			<h6>Figure 2.31: Backpropagation process</h6>
			<p>Here is a graphical representation of an ANN:</p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/C13550_02_32.jpg" alt="Figure 2.32: ANN architecture"/>
				</div>
			</div>
			<h6>Figure 2.32: ANN architecture</h6>
			<p>In the preceding diagram, we can see the neurons, which is where all the processing occurs, and the connections between them, which are the weights of the network.</p>
			<p>We are going to gain an understanding of how to create one of these neural networks, but first, we need to take a look at the data that we have.</p>
			<p>In the previous exercise, we had the shapes (60,072 and 784) and (10,018 and 784) as integer types, and 0 to 255 as pixel values, for training and testing, respectively. ANNs perform better and faster with <strong class="keyword">normalized data</strong>, but what is that?</p>
			<p>Having normalized data means converting that 0-255 range of values to a range of 0-1. The values must be adapted to fit between 0 and 1, which means they will be float numbers, because there is no other way to fit a higher range of numbers into a shorter range So, first we need to convert the data to a float and then normalize it. Here's the code for doing so:</p>
			<p class="snippet">x_train = (x_train.astype(np.float32))/255.0 #Converts to float and then normalize</p>
			<p class="snippet">x_test = (x_test.astype(np.float32))/255.0 #Same for the test set</p>
			<p class="snippet">x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)</p>
			<p class="snippet">x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)</p>
			<p>For the labels, we also need to change the format to one-hot encoding.</p>
			<p>In order to do that, we need to use a function from Keras, from its <strong class="inline">utils</strong> package (the name has changed to <strong class="inline">np_utils</strong>), called <strong class="inline">to_categorical()</strong>, which transforms the number of the digit of every label to <strong class="keyword">one-hot encoding</strong>. Here's the code:</p>
			<p class="snippet">y_train = np_utils.to_categorical(y_train, 10)</p>
			<p class="snippet">y_test = np_utils.to_categorical(y_test, 10)</p>
			<p>If we print the first label of <strong class="inline">y_train</strong>, 5, and then we print the first value of <strong class="inline">y_train</strong> after the conversion, it will output [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]. This format puts a 1 in the sixth place of an array of 10 positions (because there are 10 numbers) for the number 5 (in the sixth place because the first one is for the 0, and not for the 1). Now we are ready to go ahead with the architecture of the neural network.</p>
			<p>For a basic neural network, dense layers (or <strong class="keyword">fully connected layers</strong>) are employed. These neural networks are also called <strong class="keyword">fully connected neural networks</strong>. These contain a series of neurons that represent the neurons of the human brain. They need an activation function to be specified. An activation function is a function that takes the input and calculates a weighted sum of it, adding a bias and deciding whether it should be activated or not (outputs 1 and 0, respectively).</p>
			<p>The two most used activation functions are sigmoid and ReLU, but ReLU has demonstrated better performance overall. They are represented on the following chart: </p>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/C13550_02_33.jpg" alt="Figure 2.33: The sigmoid and ReLU functions"/>
				</div>
			</div>
			<h6>Figure 2.33: The sigmoid and ReLU functions</h6>
			<p>The sigmoid and ReLU functions calculate the weighted sum and add the bias. They then output a value depending on the value of that calculation. The sigmoid function will give different values depending on the value of the calculation, from 0 to 1. But ReLU will give 0 for negative values or return the value of the calculation for positive values. </p>
			<p>Toward the end of a neural network, normally the <strong class="keyword">softmax</strong> activation function takes place, which will output a non-probabilistic number for every class, which is higher for the class that has the highest chance of corresponding to the input image. There are other activation functions, but this one is the best for the output of a network for multi-classification problems.</p>
			<p>In <strong class="bold">Keras</strong>, a neural network could be coded as follows:</p>
			<p class="snippet">model = Sequential()</p>
			<p class="snippet">model.add(Dense(16, input_shape=input_shape))</p>
			<p class="snippet">model.add(Activation('relu'))</p>
			<p class="snippet">model.add(Dense(8))</p>
			<p class="snippet">model.add(Activation('relu'))</p>
			<p class="snippet">model.add(Flatten())</p>
			<p class="snippet">model.add(Dense(10, activation="softmax"))</p>
			<p>The model is created as <strong class="inline">Sequential()</strong> as the layers are created sequentially. First, we add a dense layer with 16 neurons and the shape of the input is passed so that the neural network knows the shape of the input. After which, the <strong class="inline">ReLU</strong> activation function is applied. We use this function because it generally gives good results. We stack another layer with eight neurons and the same activation function.</p>
			<p>At the end, we use the <strong class="inline">Flatten</strong> function to convert the array to one dimension and then the last dense layer is stacked, where the number of classes should represent the number of neurons (in this case, there would be 10 classes for the MNIST dataset). The softmax function is applied in order to get the results as a one-hot encoder, as we have mentioned before.</p>
			<p>Now we have to compile the model. In order to do that, we use the <strong class="inline">compile</strong> method as follows:</p>
			<p class="snippet">model.compile(loss='categorical_crossentropy', optimizer=Adadelta(), metrics=['accuracy'])</p>
			<p>We pass the loss function, which is used to calculate the error for the backpropagation process. For this problem, we will be using categorial cross-entropy as the loss function, as this is a categorical problem. The optimizer used is <strong class="bold">Adadelta</strong>, which performs very well in most situations. We establish accuracy as the main metric to be considered in the model.</p>
			<p>We are going to use what is called a callback in Keras. These are called in every epoch during training. We will be using the <strong class="inline">Checkpoint</strong> function in order to save our model with the best validation result on every epoch:</p>
			<p class="snippet">ckpt = ModelCheckpoint('model.h5', save_best_only=True,monitor='val_loss', mode='min', save_weights_only=False)</p>
			<p>The function to train this model is called <strong class="inline">fit()</strong> and is implemented as follows:</p>
			<p class="snippet">model.fit(x_train, y_train, batch_size=64, epochs=10, verbose=1, validation_data=(x_test, y_test),callbacks=[ckpt])</p>
			<p>We pass the training set with its labels, and we establish a batch size of 64 (these are the images that are passed on every step of every epoch), out of which we choose to have 10 training epochs (on every epoch the data is processed). The validation set is also passed in order to see how the model performs on unseen data, and at the end, we set the callback that we created before.</p>
			<p>All these parameters have to be adjusted according to the problem that we are facing. In order to put all of this into practice, we are going to perform an exercise – the same exercise that we did with decision trees, but with neural networks.</p>
			<h3 id="_idParaDest-50"><a id="_idTextAnchor067"/>Exercise 9: Building Your First Neural Network</h3>
			<h4>Note</h4>
			<p class="callout">We will be continuing the code from Exercise 8 here.</p>
			<p class="callout">The entire code for this exercise can be found on GitHub in the Lesson02 | Exercise07-09 folder.</p>
			<ol>
				<li value="1">Head to the interface on Google Colab where you executed the code for <em class="italics">Exercise 8</em>, <em class="italics">Predicting Numbers Using the Decision Tree, Random Forest, and AdaBoost Algorithms</em>.</li>
				<li>Now import the packages from the Keras library:<p class="snippet">from keras.callbacks import ModelCheckpoint</p><p class="snippet">from keras.layers import Dense, Flatten, Activation, BatchNormalization, Dropout</p><p class="snippet">from keras.models import Sequential</p><p class="snippet">from keras.optimizers import Adadelta</p><p class="snippet">from keras import utils as np_utils</p></li>
				<li>We normalize the data as we explained in this part of the chapter. We also declare the <strong class="inline">input_shape</strong> instance that will be passed to the neural network, and we print it:<p class="snippet">x_train = (x_train.astype(np.float32))/255.0</p><p class="snippet">x_test = (x_test.astype(np.float32))/255.0</p><p class="snippet">x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)</p><p class="snippet">x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)</p><p class="snippet">y_train = np_utils.to_categorical(y_train, 10)</p><p class="snippet">y_test = np_utils.to_categorical(y_test, 10)</p><p class="snippet">input_shape = x_train.shape[1:]</p><p class="snippet">print(input_shape)</p><p class="snippet">print(x_train.shape)</p><p>The output is as follows:</p><div id="_idContainer065" class="IMG---Figure"><img src="image/C13550_02_34.jpg" alt="Figure 2.34: Data output when passed for normalization using neural networks"/></div><h6>Figure 2.34: Data output when passed for normalization using neural networks</h6></li>
				<li>Now we are going to declare the model. The model that we built before was never going to perform well enough on this problem, so we have created a deeper model with more neurons and with a couple of new methods:<p class="snippet">def DenseNN(input_shape):</p><p class="snippet">    model = Sequential()</p><p class="snippet">    model.add(Dense(512, input_shape=input_shape))</p><p class="snippet">    model.add(Activation('relu'))</p><p class="snippet">    model.add(BatchNormalization())</p><p class="snippet">    model.add(Dropout(0.2))</p><p class="snippet">    model.add(Dense(512))</p><p class="snippet">    model.add(Activation('relu'))</p><p class="snippet">    model.add(BatchNormalization())</p><p class="snippet">    model.add(Dropout(0.2))</p><p class="snippet">    model.add(Dense(256))</p><p class="snippet">    model.add(Activation('relu'))</p><p class="snippet">    model.add(BatchNormalization())</p><p class="snippet">    model.add(Dropout(0.2))</p><p class="snippet">    model.add(Flatten())</p><p class="snippet">    model.add(Dense(256))</p><p class="snippet">    model.add(Activation('relu'))</p><p class="snippet">    model.add(BatchNormalization())</p><p class="snippet">    model.add(Dropout(0.2))</p><p class="snippet">    model.add(Dense(10, activation="softmax"))</p><p>We have added a <strong class="inline">BatchNormalization()</strong> method, which helps the network converge faster and may give better results overall. </p><p>We have also added the <strong class="inline">Dropout()</strong> method, which helps the network to avoid <strong class="keyword">overfitting </strong>(the accuracy of the training set is much higher than the accuracy of the validation set). It does that by disconnecting some neurons during training (0.2 -&gt; 20% of neurons), which allows better generalization of the problem (better classification of unseen data).</p><p>Furthermore, the number of neurons has increased drastically. Also, the number of layers has increased. The more layers and neurons are added, the deeper the understanding is and more complex features are learned.</p></li>
				<li>Now we compile the model using categorical cross-entropy, as there are several classes, and we use Adadelta, which is great overall for these kinds of tasks. Also, we use accuracy as the main metric:<p class="snippet">model.compile(loss='categorical_crossentropy', optimizer=Adadelta(), metrics=['accuracy'])</p></li>
				<li>Let's create the <strong class="inline">Checkpoint</strong> callback, where the model will be stored in the <strong class="inline">Models</strong> folder with the name <strong class="inline">model.h5</strong>. We will be using validation loss as the main method to be tracked and the model will be saved in its entirety:<p class="snippet">ckpt = ModelCheckpoint('Models/model.h5', save_best_only=True,monitor='val_loss', mode='min', save_weights_only=False)</p></li>
				<li>Start to train the network with the <strong class="inline">fit()</strong> function, just like we explained before. We use 64 as the batch size, 10 epochs (which is enough as every epoch is going to last a very long time and between epochs it will not improve that much), and we will introduce the Checkpoint callback:<p class="snippet">model.fit(x_train, y_train, </p><p class="snippet">          batch_size=64,</p><p class="snippet">          epochs=10,</p><p class="snippet">          verbose=1,</p><p class="snippet">          validation_data=(x_test, y_test),</p><p class="snippet">          callbacks=[ckpt])</p><p>This is going to take a while.</p><p>The output should look like this:</p><div id="_idContainer066" class="IMG---Figure"><img src="image/C13550_02_35.jpg" alt="Figure 2.35: Neural network output"/></div><h6>Figure 2.35: Neural network output</h6><p>The final accuracy of the model corresponds to the last <strong class="inline">val_acc</strong>, which is <strong class="bold">97.83%.</strong> This is a better result than we got using AdaBoost or random forest.</p></li>
				<li>Now let's make some predictions:<p class="snippet">for number in range(5):</p><p class="snippet">    imgLoaded = cv2.imread('number%d.jpg'%(number),0)</p><p class="snippet">    img = cv2.resize(imgLoaded, (28, 28))</p><p class="snippet">    img = (img.astype(np.float32))/255.0</p><p class="snippet">    img = img.reshape(1, 28, 28, 1)</p><p class="snippet">    plt.subplot(1,5,number+1),plt.imshow(imgLoaded,'gray')</p><p class="snippet">    plt.title(np.argmax(model.predict(img)[0]))</p><p class="snippet">    plt.xticks([]),plt.yticks([])</p><p class="snippet">plt.show()</p><p>The code looks similar to the code used in the last exercise but has some minor differences. One is that, as we changed the input format, we have to change the format of the input image too (float and normalize). The other is that the prediction is in one-hot encoding, so we use the <strong class="inline">argmax()</strong> NumPy function in order to get the position of the maximum value of the one-hot output vector, which would be the predicted digit.</p><p>Let's see the output of the last number that we tried using random forest:</p></li>
			</ol>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/C13550_02_36.jpg" alt="Figure 2.36: Prediction of numbers using neural networks"/>
				</div>
			</div>
			<h6>Figure 2.36: Prediction of numbers using neural networks</h6>
			<p>The output has been successful – even the 7 that the random forest model struggled with.</p>
			<h4>Note</h4>
			<p class="callout">The entire code can be found on GitHub in the Lesson02 | Exercise07-09 folder.</p>
			<p>If you try the other numbers, it will classify them all very well – it has learned how to.</p>
			<p>Congratulations! You have built your first neural network and you have applied it to a real-world problem! Now you are ready to go through the activity for this chapter.</p>
			<h3 id="_idParaDest-51"><a id="_idTextAnchor068"/>Activity 2: Classify 10 Types of Clothes from the Fashion-MNIST Database</h3>
			<p>Now you are going to face a similar problem to the previous one but with types of clothes. This database is very similar to the original MNIST. It has 60,000 images – 28x28 in grayscale – for training and 10,000 for testing. You will have to follow the steps mentioned in the first exercise as this activity is not focused on the real world. You will have to put into practice the abilities learned in the last exercise by building a neural network on your own. For this, you will have to open a Google Colab notebook. The following steps will guide you in the right direction:</p>
			<ol>
				<li value="1">Load the dataset from Keras:<p class="snippet">from keras.datasets import fashion_mnist</p><p class="snippet">(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()</p><h4>Note</h4><p class="callout">The data is preprocessed like MNIST, so the next steps should be similar to <em class="italics">Exercise 5</em>, <em class="italics">Applying the Various Morphological Transformations to an Image</em>.</p></li>
				<li>Import <strong class="inline">random</strong> and set the seed to 42. Import <strong class="inline">matplotlib</strong> and plot five random samples of the dataset, just as we did in the last exercise.</li>
				<li>Now normalize the data and reshape it to fit properly into the neural network and convert the labels to one-hot encoder.</li>
				<li>Start to build the architecture of the neural network by using dense layers. You have to build it inside a method that will return the model.<h4>Note</h4><p class="callout">We recommend starting off by building a very small, easy architecture and improving it by testing it with the given dataset.</p></li>
				<li>Compile the model with the appropriate parameters and start training the neural network.</li>
				<li>Once trained, we should make some predictions in order to test the model. We have uploaded some images into the same <strong class="inline">testing</strong> folder inside the <strong class="inline">Dataset</strong> folder of the last exercise. Make predictions using those images, just as we did in the last exercise.<h4>Note</h4><p class="callout">You have to consider that the images that were fed into the neural network had a black background and the clothes were white, so you should make corresponding adjustments to make the image look like those. If needed, you should invert white as black and vice versa. NumPy has a method that does that: <strong class="inline">image = np.invert(image)</strong>.</p></li>
				<li>Check the results:</li>
			</ol>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/C13550_02_37.jpg" alt="Figure 2.37: The output of the prediction is the index of the position in this list"/>
				</div>
			</div>
			<h6>Figure 2.37: The output of the prediction is the<a id="_idTextAnchor069"/> index of the position in this list</h6>
			<h4>Note</h4>
			<p class="callout">The solution for this activity is available on page 302.</p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor070"/>Summary</h2>
			<p>Computer vision is a big field within AI. By understanding this field, you can achieve results such as extracting information from an image or generating images that look just like they do in real life, for example. This chapter has covered image preprocessing for feature extraction using the OpenCV library, which allows easy training and prediction for machine learning models. Some basic machine learning models have also been covered, such as decision trees and boosting algorithms. These served as an introduction to machine learning and were mostly used to play around. Finally, neural networks were introduced and coded using Keras and TensorFlow as a backend. Normalization was explained and put into practice, along with dense layers, though convolutional layers are known to work better with images than dense layers do, and they will be explained later in the book. </p>
			<p>Concepts for avoiding overfitting were also covered, and toward the end, we used the model to make predictions and put it into practice using real-world images.</p>
			<p>In the next chapter, the fundamentals of <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) will be introduced, along with the most widely used techniques for extracting information from a corpus in order to create basic models for language prediction.</p>
		</div>
	</body></html>