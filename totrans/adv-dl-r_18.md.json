["```py\n# Fitting model and TensorBoard\nsetwd(\"~/Desktop/\")\nmodel_one <- model %>% fit(training, \n                         trainLabels, \n                         epochs = 200,  \n                         batch_size = 32,\n                         validation_split = 0.2,\n                         callbacks = callback_tensorboard('ctg/one'))\ntensorboard('ctg/one')\n```", "```py\n# TensorBoard visualization for multiple models\ntensorboard(c('ctg/one', 'ctg/two', 'ctg/three', 'ctg/four'))\n```", "```py\n# LIME package\nlibrary(lime)\n\n# Using LIME with keras\nmodel_type.keras.engine.sequential.Sequential <- \nfunction(x, ...) {\"classification\"}\npredict_model.keras.engine.sequential.Sequential <- \n  function(x,newdata,type, ...) {p <- predict_proba(object=x, x=as.matrix(newdata))\n         data.frame(p)}\n\n# Create explainer using lime\nexplainer <- lime(x = data.frame(training), \n             model = model, \n             bin_continuous = FALSE)\n\n# Create explanation\nexplanation <- explain(data.frame(test)[1:5,],  \n                  explainer    = explainer, \n                  n_labels     = 1,  \n                  n_features   = 4,  \n                  kernel_width = 0.5)\ntesttarget[1:5]\n[1] 0 0 0 2 2\n```", "```py\nlibrary(tfruns)\ntraining_run(\"mlp_ctg.R\")\n```", "```py\n# Training network for classification with CTG data (chapter-2)\nmodel <- keras_model_sequential()\nmodel %>% \n  layer_dense(units = 25, activation = 'relu', input_shape = c(21)) %>%\n  layer_dense(units = 3, activation = 'softmax') \nmodel %>% compile(loss = 'categorical_crossentropy', \n                  optimizer = 'adam',\n                  metrics = 'accuracy')\nhistory <- model %>% fit(training, \n                         trainLabels, \n                         epochs = 50,  \n                         batch_size = 32,\n                         validation_split = 0.2)\nplot(history)\n```", "```py\n# Training network with callback\nmodel <- keras_model_sequential()\nmodel %>% \n  layer_dense(units = 25, activation = 'relu', input_shape = c(21)) %>%\n  layer_dense(units = 3, activation = 'softmax') \nmodel %>% compile(loss = 'categorical_crossentropy', \n                  optimizer = 'adam',\n                  metrics = 'accuracy')\nhistory <- model %>% fit(training, \n                         trainLabels, \n                         epochs = 50,  \n                         batch_size = 32,\n                         validation_split = 0.2,\n                         callbacks = callback_early_stopping(monitor = \"val_loss\", \n                                                   patience = 10))\nplot(history)\n```"]