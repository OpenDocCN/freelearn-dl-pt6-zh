<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Getting Started with Deep Learning</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will explain some basic concepts of <strong>M<span>achine Learning</span></strong> (<strong>ML</strong>) and <strong><span>D</span><span>eep Learning (DL)</span></strong> that will be used in all subsequent chapters. We will start with a brief introduction to ML. Then we will move on to DL, which is one of the emerging branches of <span>ML</span>.</p>
<p>We will briefly discuss some of the most well-known and widely used neural network architectures. Next, we will look at various features of deep learning frameworks and libraries. Then we will see how to prepare a programming environment, before moving on to coding with some open source, deep learning libraries such as <strong>DeepLearning4J (DL4J)</strong>. </p>
<p>Then we will solve a very famous <span>ML</span> problem: the Titanic survival prediction. For this, we will use an Apache Spark-based <strong>Multilayer Perceptron</strong> (<strong>MLP</strong>) classifier to solve this problem. Finally, we'll see some frequently asked questions that will help us generalize our basic understanding of DL. Briefly, the following topics will be covered:</p>
<ul>
<li>A soft introduction to <span>ML</span></li>
<li>Artificial Neural Networks (ANNs)</li>
<li>Deep neural network architectures</li>
<li>Deep learning frameworks</li>
<li>Deep learning from disastersâ€”Titanic survival prediction using MLP</li>
<li>Frequently asked questions (FAQ)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A soft introduction to ML</h1>
                </header>
            
            <article>
                
<p><span>ML</span> approaches are based on a set of statistical and mathematical algorithms in order to carry out tasks such as classification, regression analysis, concept learning, predictive modeling, clustering, and mining of useful patterns. Thus, with the use of ML, we aim at improving the learning experience such that it becomes automatic. Consequently, we may not need complete human interactions, or at least we can reduce the level of such interactions as much as possible.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working principles of ML algorithms</h1>
                </header>
            
            <article>
                
<p>We now refer to a famous definition of <span>ML</span> by Tom M. Mitchell (<em>Machine Learning, Tom Mitchell, McGraw Hill</em>), where he explained what learning really means from a computer science perspective:</p>
<div class="packt_quote">"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."</div>
<p>Based on this definition, we can conclude that a computer program or machine can do the following:</p>
<ul>
<li>Learn from data and histories</li>
<li>Improve with experience</li>
<li>Iteratively enhance a model that can be used to predict outcomes of questions</li>
</ul>
<p>Since they are at the core of predictive analytics, almost every <span>ML</span> algorithm we use can be treated as an optimization problem. This is about finding parameters that minimize an objective function, for example, a weighted sum of two terms like a cost function and regularization. Typically, an objective function has two components:</p>
<ul>
<li>A regularizer, which controls the complexity of the model</li>
<li>The loss, which measures the error of the model on the training data.</li>
</ul>
<p>On the other hand, the regularization parameter defines the trade-off between minimizing the training error and the model's complexity in an effort to avoid overfitting problems. Now, if both of these components are convex, then their sum is also convex; it is non-convex otherwise. More elaborately, when using an <span>ML</span> algorithm, the goal is to obtain the best hyperparameters of a function that return the minimum error when making predictions. Therefore, using a convex optimization technique, we can minimize the function until it converges towards the minimum error.</p>
<p>Given that a problem is convex, it is usually easier to analyze the asymptotic behavior of the algorithm, which shows how fast it converges as the model observes more and more training data. The challenge of ML is to allow training a model so that it can recognize complex patterns and make decisions not only in an automated way but also as intelligently as possible. The entire learning process requires input datasets that can be split (or are already provided) into three types, outlined as follows:</p>
<ul>
<li><strong>A training set</strong> is the knowledge base coming from historical or live data used to fit the parameters of the ML algorithm. During the training phase, the ML model utilizes the training set to find optimal weights of the network and reach the objective function by minimizing the training error. Here, the <strong>back-prop rule</strong> (or another more advanced optimizer with a proper updater; we'll see this later on) is used to train the model, but all the hyperparameters are need to be set before the learning process starts<strong>.<br/></strong></li>
<li><strong>A validation set</strong> is a set of examples used to tune the parameters of an ML model. It ensures that the model is trained well and generalizes towards avoiding overfitting. Some ML practitioners refer to it as a <strong>development set</strong> or <strong>dev set</strong> as well.</li>
<li><strong>A test set</strong> is used for evaluating the performance of the trained model on unseen data. This step is also referred to as <strong>model inferencing</strong>. After assessing the final model on the test set (that is, when we're fully satisfied with the model's performance), we do not have to tune the model any further but the trained model can be deployed in a production-ready environment.</li>
</ul>
<p>A common practice is splitting the input data (after necessary pre-processing and feature engineering) into 60% for training, 10% for validation, and 20% for testing, but it really depends on use cases. Also, sometimes we need to perform up-sampling or down-sampling on the data based on the availability and quality of the datasets. </p>
<p>Moreover, the learning theory uses mathematical tools that derive from probability theory and information theory. Three learning paradigms will be briefly discussed:</p>
<ul>
<li>Supervised learning</li>
<li>Unsupervised learning</li>
<li>Reinforcement learning</li>
</ul>
<p>The following diagram summarizes the three types of learning, along with the problems they address:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-105 image-border" src="assets/417d3a88-3435-4383-a62a-f387d8a98dac.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Types of learning and related problems</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Supervised learning</h1>
                </header>
            
            <article>
                
<p><strong>Supervised learning</strong> is the simplest and most well-known automatic learning task. It is based on a number of pre-defined examples, in which the category to which each of the inputs should belong is already known. <em>Figure 2</em> shows a typical workflow of supervised learning.</p>
<p>An actor (for example, an <span>ML </span>practitioner, data scientist, data engineer, ML engineer, and so on) performs <strong>Extraction Transformation Load</strong> (<strong><span>ETL</span></strong>) and the necessary feature engineering (including feature extraction, selection, and so on) to get the appropriate data having features and labels. Then he does the following:</p>
<ol>
<li>Splits the data into training, development, and test sets</li>
<li>Uses the training set to train an <span>ML</span> model</li>
</ol>
<ol start="3">
<li>The validation set is used to validate the training against the overfitting problem and regularization</li>
<li>He then evaluates the model's performance on the test set (that is unseen data)</li>
<li>If the performance is not satisfactory, he can perform additional tuning to get the best model based on hyperparameter optimization</li>
<li>Finally, he deploys the best model in a production-ready environment</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-106 image-border" src="assets/784a5592-403f-409c-8cbf-5cb2eba4deab.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Supervised learning in action</div>
<p class="mce-root CDPAlignLeft CDPAlign">In the overall life cycle, there might be many actors involved (for example, a data engineer, data scientist, or <span>ML</span> engineer) to perform each step independently or collaboratively.</p>
<p>The supervised learning context includes <strong>classification</strong> and <strong>regression</strong> tasks; classification is used to predict which class a data point is part of (<strong>discrete value</strong>), while regression is used to predict <strong>continuous values</strong>. In other words, a classification task is used to predict the label of the class attribute, while a regression task is used to make a numeric prediction of the class attribute.</p>
<p>In the context of supervised learning, <strong>unbalanced data</strong> refers to classification problems where we have unequal instances for different classes. For example, if we have a classification task for only two classes, <strong>balanced data</strong> would mean 50% pre-classified examples for each of the classes.</p>
<p>If the input dataset is a little unbalanced (for example, 60% data points for one class and 40% for the other class), the learning process will require for the input dataset to be split randomly into three sets, with 50% for the training set, 20% for the validation set, and the remaining 30% for the testing set.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unsupervised learning</h1>
                </header>
            
            <article>
                
<p>In <strong>unsupervised learning</strong>, an input set is supplied to the system during the training phase. In contrast with supervised learning, the input objects are not labeled with their class. For classification, we assumed that we are given a training dataset of correctly labeled data. Unfortunately, we do not always have that advantage when we collect data in the real world.</p>
<p>For example, let's say you have a large collection of totally legal, not pirated, MP3 files in a crowded and massive folder on your hard drive. In such a case, how could we possibly group songs together if we do not have direct access to their metadata? One possible approach could be to mix various <span>ML </span>techniques, but clustering is often the best solution.</p>
<p>Now, what if you can build a clustering predictive model that helps automatically group together similar songs and organize them into your favorite categories, such as <em>country</em>, <em>rap</em>, <em>rock</em>, and so on? In short, unsupervised learning algorithms are commonly used in clustering problems. The following diagram gives us an idea of a clustering technique applied to solve this kind of problem:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-107 image-border" src="assets/9e2bb782-2a63-4dfd-a242-dc8b18df8dc7.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Clustering techniques â€“ an example of unsupervised learning</span></div>
<p>Although the data points are not labeled, we can <span>still </span>do the necessary feature engineering and grouping of a set of objects in such a way that objects in the same group (called a <strong>cluster</strong>) are brought together. This is not easy for a human. Rather, a standard approach is to define a similarity measure between two objects and then look for any cluster of objects that are more similar to each other than they are to the objects in the other clusters. Once we've done the clustering of the data points (that is, MP3 files) and the validation is completed, we know the pattern of the data (that is, what type of MP3 files fall in which group).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reinforcement learning</h1>
                </header>
            
            <article>
                
<p><strong>Reinforcement learning</strong> is an artificial intelligence approach that focuses on the learning of the system through its interactions with the environment. In reinforcement learning, the system's parameters are adapted based on the feedback obtained from the environment, which in turn provides feedback on the decisions made by the system. The following diagram shows a person making decisions in order to arrive at their destination.</p>
<p>Let's take an example of the route you take from home to work. In this case, you take the same route to work every day. However, out of the blue, one day you get curious and decide to try a different route with a view to finding the shortest path. <span>This dilemma of trying out new routes or sticking to the best-known route is an example of </span><strong>exploration versus exploitation</strong>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-108 image-border" src="assets/ed24ebcf-4492-4302-9226-33c35d8f9972.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>An agent always tries to reach the destination</span></div>
<p>We can take a look at one more example in terms of a system modeling a chess player. In order to improve its performance, the system utilizes the result of its previous moves; such a system is said to be a system learning with reinforcement.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Putting ML tasks altogether</h1>
                </header>
            
            <article>
                
<p>We have seen the basic working principles of ML algorithms. Then we have seen what <span>the basic ML tasks </span>are and how they formulate domain-specific problems. Now let's take a look at how can we summarize ML tasks and some applications in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/899ceaf3-c710-4675-ae99-33c76cd6ac2f.png" style=""/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span>ML tasks and some use cases from different application domains<br/></span></div>
<p>However, the preceding figure lists only a few use cases and applications using different ML tasks. In practice, ML is used in numerous use cases and applications. We will try to cover a few of those throughout this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Delving into deep learning</h1>
                </header>
            
            <article>
                
<p>Simple <span>ML</span> methods that were used in normal-size data analysis are not effective anymore and should be substituted by more robust <span>ML</span> methods. Although classical ML techniques allow researchers to identify groups or clusters of related variables, the accuracy and effectiveness of these methods diminish with large and high-dimensional datasets.</p>
<p>Here comes deep learning, which is one of the most important developments in artificial intelligence in the last few years. Deep learning is a branch of <span>ML</span> based on a set of algorithms that attempt to model high-level abstractions in data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How did DL take ML into next level?</h1>
                </header>
            
            <article>
                
<p>In short, deep learning algorithms are mostly a set of ANNs that can make better representations of large-scale datasets, in order to build models that learn these representations very extensively. Nowadays it's not limited to ANNs, but there have been really many theoretical advances and software and hardware improvements that were necessary for us to get to this day. In this regard, Ian Goodfellow et al. (Deep Learning, MIT Press, 2016) defined deep learning as follows:</p>
<div class="mce-root packt_quote"><span>"Deep learning is a particular kind of machine learning that achieves great power and flexibility by learning to represent the world as a nested hierarchy of concepts, with each concept defined in relation to simpler concepts, and more abstract representations computed in terms of less abstract ones."</span></div>
<p>Let's take an example; suppose we want to develop a predictive analytics model, such as an animal recognizer, where our system has to resolve two problems:</p>
<ul>
<li>To classify whether an image represents a cat or a dog</li>
<li>To cluster images of dogs and cats.</li>
</ul>
<p>If we solve the first problem using a typical <span>ML</span> method, we must define the facial features (ears, eyes, whiskers, and so on) and write a method to identify which features (typically nonlinear) are more important when classifying a particular animal.</p>
<p>However, at the same time, we cannot address the second problem because classical ML algorithms for clustering images (such as <strong>k-means</strong>) cannot handle nonlinear features. Deep learning algorithms will take these two problems one step further and the most important features will be extracted automatically after determining which features are the most important for classification or clustering.</p>
<p>In contrast, when using a classical <span>ML</span> algorithm, we would have to provide the features <span>manually</span>. In summary, the deep learning workflow would be as follows:</p>
<ul>
<li>A deep learning algorithm would first identify the edges that are most relevant when clustering cats or dogs. It would then try to find various combinations of shapes and edges hierarchically. This step is called ETL.</li>
<li>After several iterations, hierarchical identification of complex concepts and features is carried out. Then, based on the identified features, the DL algorithm automatically decides which of these features are most significant (statistically) to classify the animal. This step is feature extraction.</li>
<li>Finally, it takes out the label column and performs unsupervised training using <strong>AutoEncoders</strong> (<strong>AEs</strong>) to extract the latent features to be redistributed to k-means for clustering.</li>
<li>Then the clustering assignment hardening loss (CAH loss) and reconstruction loss are jointly optimized towards optimal clustering assignment. Deep Embedding Clustering (see more at <a href="https://arxiv.org/pdf/1511.06335.pdf">https://arxiv.org/pdf/1511.06335.pdf</a>) is an example of such an approach. We will discuss deep learning-based clustering approaches in <a href="b458baf5-6590-4e69-84d4-8c02a898dbca.xhtml" target="_blank">Chapter 11</a>, <em>Discussion, Current Trends, and Outlook</em>. </li>
</ul>
<p>Up to this point, we have seen that deep learning systems are able to recognize what an image represents. A computer does not see an image as we see it because it only knows the position of each pixel and its color. Using deep learning techniques, the image is divided into various layers of analysis.</p>
<p>At a lower level, the software analyzes, for example, a grid of a few pixels with the task of detecting a type of color or various nuances. If it finds something, it informs the next level, which at this point checks whether <span>or not </span>that given color belongs to a larger form, such as a line. The process continues to the upper levels until you understand what is shown in the image. The following diagram shows what we have discussed in the case of an image classification system: </p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/9ee85d48-6b19-4c74-bce0-196adfd06480.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">A deep learning system at work on a dog versus cat classification problem</div>
<p>More precisely, the preceding image classifier can be built layer by layer, as follows:</p>
<ul>
<li><strong>Layer 1</strong>: The algorithm starts identifying the dark and light pixels from the raw images</li>
<li><strong>Layer 2</strong>: The algorithm then identifies edges and shapes</li>
<li><strong>Layer 3</strong>: It then learns more complex shapes and objects</li>
<li><strong>Layer 4</strong>: The algorithm then learns which objects define a human face</li>
</ul>
<p>Although this is a very simple classifier, software capable of doing these types of things is now widespread and is found in systems for recognizing faces, or in those for searching by an image on Google, for example. These pieces of software are based on deep learning algorithms.</p>
<p>On the contrary, by using a linear <span>ML</span> algorithm, we cannot build such applications since these algorithms are incapable of handling nonlinear image features. Also, using ML approaches, we typically handle a few hyperparameters only. However, when neural networks are brought to the party, things become too complex. In each layer, there are millions or even billions of hyperparameters to tune, so much that the cost function becomes non-convex.</p>
<p>Another reason is that activation functions used in hidden layers are nonlinear, so the cost is non-convex. We will discuss this phenomenon in more detail in later chapters but let's take a quick look at ANNs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Artificial Neural Networks</h1>
                </header>
            
            <article>
                
<p>ANNs work on the concept of deep learning. They represent the human nervous system in how the nervous system consists of a number of neurons that communicate with each other using axons. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Biological neurons</h1>
                </header>
            
            <article>
                
<p>The working principles of ANNs are inspired by how a human brain works, depicted in <em>Figure 7</em>. <span class="st">The receptors receive the stimuli either internally or from the external world; then they pass the information into the biological <em>neurons</em> for further processing</span>. There are a number of dendrites, in addition to another long extension called the <strong>axon</strong>.</p>
<p>Towards its extremity, there are minuscule structures called <strong>synaptic terminals,</strong> used to connect one neuron to the dendrites of other neurons. Biological neurons receive short electrical impulses called <strong>signals</strong> from other neurons, and in response, they trigger their own signals:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/3974bdf0-c8d9-4568-a829-fec048f9598c.png" style=""/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref"><span>Working principle of biological neurons</span></div>
<p>We can thus summarize that the neuron comprises a cell body (also known as the soma), one or more <strong>dendrites</strong> for receiving signals from other neurons, and an <strong>axon</strong> for carrying out the signals generated by the neurons.</p>
<p>A neuron is in an active state when it is sending signals to other neurons. However, when it is receiving signals from other neurons, it is in an inactive state. In an idle state, a neuron accumulates all the signals received before reaching a certain activation threshold. This whole thing motivated researchers to introduce an ANN.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A brief history of ANNs</h1>
                </header>
            
            <article>
                
<p>Inspired by the working principles of biological neurons, Warren McCulloch and Walter Pitts proposed the first artificial neuron model in 1943 in terms of a computational model of nervous activity. This simple model of a biological neuron, also known as an <strong>artificial neuron (AN),</strong> has one or more binary (on/off) inputs and one output only.</p>
<p>An AN simply activates its output when more than a certain number of its inputs are active. For example, here we see a few ANNs that perform various logical operations. In this example, we assume that a neuron is activated only when at least two of its inputs are active:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-775 image-border" src="assets/6e2b1c75-5e3e-4236-8a64-38389a64f4cf.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">ANNs performing simple logical computations</div>
<p>The example sounds too trivial, but even with such a simplified model, it is possible to build a network of ANs. Nevertheless, these networks can be combined to compute complex logical expressions too. This simplified model inspired John von Neumann, Marvin Minsky, Frank Rosenblatt, and many others to come up with another model called a <strong>perceptron</strong> back in 1957.</p>
<p>The perceptron is one of the simplest ANN architectures we've seen in the last 60 years. It is based on a slightly different AN called a <strong>Linear Threshold Unit</strong> (<strong>LTU</strong>). The only difference is that the inputs and outputs are now numbers instead of binary on/off values. Each input connection is associated with a weight. The LTU computes a weighted sum of its inputs, then applies a step function (which resembles the action of an activation function) to that sum, and outputs the result:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-774 image-border" src="assets/75a216da-3354-4b5f-bd0a-023f44330222.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">The left-side figure represents an LTU and the right-side figure shows a perceptron</div>
<p>One of the downsides of a perceptron is that its decision boundary is linear. Therefore, they are incapable of learning complex patterns. They are also incapable of solving some simple problems like <strong>Exclusive OR</strong> (<strong>XOR</strong>). However, later on, the limitations of perceptrons were somewhat eliminated by stacking multiple perceptrons, called MLP.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How does an ANN learn?</h1>
                </header>
            
            <article>
                
<p>Based on the concept of biological neurons, the term and the idea of ANs arose. Similarly to biological neurons, the artificial neuron consists of the following:</p>
<ul>
<li>One or more incoming connections that aggregate signals from neurons</li>
<li>One or more output connections for carrying the signal to the other neurons</li>
<li>An <strong>activation function</strong>, which determines the numerical value of the output signal</li>
</ul>
<p>The learning process of a neural network is configured as an <em>iterative process</em> of <em>optimization</em> of the <em>weights</em> (see more in the next section). The weights are updated in each epoch. Once the training starts, the aim is to generate predictions by minimizing the loss function. The performance of the network is then evaluated on the test set.</p>
<p>Now we know the simple concept of an artificial neuron. However, generating only some artificial signals is not enough to learn a complex task. Albeit, a commonly used supervised learning algorithm is the backpropagation algorithm, which is very commonly used to train a complex ANN.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ANNs and the backpropagation algorithm</h1>
                </header>
            
            <article>
                
<p>The backpropagation algorithm aims to minimize the error between the current and the desired output. Since the network is feedforward, the activation flow always proceeds forward from the input units to the output units.</p>
<p>The gradient of the cost function is backpropagated and the network weights get updated; the overall method can be applied to any number of hidden layers recursively. In such a method, the incorporation between two phases is important. In short, the basic steps of the training procedure are as follows:</p>
<ol>
<li>Initialize the network with some random (or more advanced XAVIER) weights</li>
<li>For all training cases, follow the steps of forward and backward passes as outlined next</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Forward and backward passes</h1>
                </header>
            
            <article>
                
<p>In the forward pass, a number of operations are performed to obtain some predictions or scores. In such an operation, a graph is created, connecting all dependent operations in a top-to-bottom fashion. Then the network's error is computed, which is the difference between the predicted output and the actual output.</p>
<p>On the other hand, the backward pass is involved mainly with mathematical operations, such as creating derivatives for all differential operations (that is auto-differentiation methods), top to bottom (for example, measuring the loss function to update the network weights), for all the operations in the graph, and then using them in chain rule.</p>
<p>In this pass, for all layers starting with the output layer back to the input layer, it shows the network layer's output with the correct input (error function). Then it adapts the weights in the current layer to minimize the error function. This is backpropagation's optimization step. By the way, there are two types of auto-differentiation methods:</p>
<ol>
<li><strong>Reverse mode</strong>: Derivation of a single output with respect to all inputs</li>
<li><strong>Forward mode</strong>: Derivation of all outputs with respect to one input</li>
</ol>
<p>The backpropagation algorithm processes the information in such a way that the network decreases the global error during the learning iterations; however, this does not guarantee that the global minimum is reached. The presence of hidden units and the nonlinearity of the output function mean that the behavior of the error is very complex and has many local minimas.</p>
<p>This backpropagation step is typically performed thousands or millions of times, using many training batches, until the model parameters converge to values that minimize the cost function. The training process ends when the error on the validation set begins to increase, because this could mark the beginning of a phase overfitting.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Weights and biases</h1>
                </header>
            
            <article>
                
<p>Besides the state of a neuron, synaptic weight is considered, which influences the connection within the network. Each weight has a numerical value indicated by <em>W<sub>ij</sub></em>, which is the synaptic weight connecting neuron <em>i</em> to neuron <em>j</em>.</p>
<div class="packt_tip"><strong>Synaptic weight</strong>: This concept evolved from biology and refers to the strength or amplitude of a connection between two nodes, corresponding in biology to the amount of influence the firing of one neuron has on another.</div>
<p>For each neuron (also known as, unit) <em>i</em>, an input vector can be defined by <em>x<sub>i</sub></em>= (<em>x<sub>1</sub></em>, <em>x<sub>2</sub></em>,...<em>x<sub>n</sub></em>) and a weight vector can be defined by <em>w<sub>i</sub></em>= (<em>w<sub>i1</sub></em>, <em>w<sub>i2</sub></em>,...<em>w<sub>in</sub></em>). Now, depending on the position of a neuron, the weights and the output function determine the behavior of an individual neuron. Then during forward propagation, each unit in the hidden layer gets the following signal:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/e468c385-a1c8-4d70-a6c5-6ecf83d1cdb4.png" style="width:17.67em;height:2.92em;"/></div>
<p class="mce-root">Nevertheless, among the weights, there is also a special type of weight called <em>bias</em> unit <em>b.</em> Technically, bias units <span class="inline_editor_value"><span class="ui_qtext_rendered_qtext">aren't connected to any previous layer, so they don't have true activity. But still, the</span></span> bias <em>b</em> value allows the neural network to shift the activation function to the left or right. Now, taking the bias unit into consideration, the modified network output can be formulated as follows:<strong> </strong></p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/ebd48a2a-9c42-4031-8d49-48b9a900f105.png" style="width:21.00em;height:3.08em;"/></div>
<p>The preceding equation signifies that each hidden unit gets the sum of inputs multiplied by the corresponding weightâ€”summing junction. Then the resultant in the summing junction is passed through the activation function, which squashes the output as depicted in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-773 image-border" src="assets/64811b83-4195-4c85-97db-d7a5c3cda41d.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Artificial neuron model</span></div>
<p>Now, a tricky question: how do we initialize the weights? Well, if we initialize all weights to the same value (for example, 0 or 1), each hidden neuron will get exactly the same signal. Let's try to break it down:</p>
<ul>
<li>If all weights are initialized to 1, then each unit gets a signal equal to the sum of the inputs</li>
<li>If all weights are 0, which is even worse, every neuron in a hidden layer will get zero signal</li>
</ul>
<p>For network weight initialization, Xavier initialization is nowadays used widely. It is similar to random initialization but often turns out to work much better since it can automatically determine the scale of initialization based on the number of input and output neurons.</p>
<div class="packt_infobox">Interested readers should refer to this publication for detailed info: Xavier Glorot and Yoshua Bengio, <em>Understanding the difficulty of training deep feedforward neural networks</em>: proceedings of the 13<sup>th</sup> international conference on <strong>Artificial Intelligence and Statistics</strong> (<strong>AISTATS</strong>) 2010, Chia Laguna Resort, Sardinia, Italy; Volume 9 of JMLR: W&amp;CP.</div>
<p>You may be wondering whether you can get rid of random initialization while training a regular DNN (for example, MLP or DBN). Well, recently, some researchers have been talking about random orthogonal matrix initializations that perform better than just any random initialization for training DNNs.</p>
<p>When it comes to initializing the biases, we can initialize them to be zero. But setting the biases to a small constant value such as 0.01 for all biases ensures that all <span><strong>Rectified Linear Unit</strong> (</span><strong>ReLU</strong>) units can propagate some gradient. However, it neither performs well nor shows consistent improvement. Therefore, sticking with zero is recommended.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Weight optimization</h1>
                </header>
            
            <article>
                
<p>Before the training starts, the network parameters are set randomly. Then to optimize the network weights, an iterative algorithm called <strong>Gradient Descent</strong> (<strong>GD</strong>) is used. Using GD optimization, our network computes the cost gradient based on the training set. Then, through an iterative process, the gradient <em><span class="emphasis">G</span></em> of the error function <em>E</em> is computed<em>.<br/></em></p>
<p>In following graph, gradient <strong>G</strong> of error function <strong><em><span class="emphasis">E</span></em></strong> provides the direction in which the error function with current values has the steeper slope. Since the ultimate target is to reduce the network error, GD makes small steps in the opposite direction <em>-</em><strong>G</strong>. This iterative process is executed a number of times, so the error <em>E</em> would move down towards the global minima<em>.</em> This way, the ultimate target is to reach a point where <strong>G = 0</strong><em>,</em> where no further optimization is possible:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-772 image-border" src="assets/bd401445-a7a4-4b48-87da-94f13c8d52d1.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Searching for the minimum for the error function E; we move in the direction in which the gradient G of E is minimal</span></div>
<p>The downside is that it takes too long to converge, which makes it impossible to meet the demand of handling large-scale training data. Therefore, a faster GD called <strong>Stochastic Gradient Descent</strong> (<strong>SDG</strong>) is proposed, which is also a widely used optimizer in DNN training. In SGD, we use only one training sample per iteration from the training set to update the network parameters.</p>
<div class="packt_infobox">I'm not saying SGD is the only available optimization algorithm, but there are so many advanced optimizers available nowadays, for example, Adam, RMSProp, ADAGrad, Momentum, and so on. More or less, most of them are either direct or indirect optimized versions of SGD.</div>
<p>By the way, the term <strong>stochastic</strong> comes from the fact that the gradient based on a single training sample per iteration is a stochastic approximation of the true cost gradient.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Activation functions</h1>
                </header>
            
            <article>
                
<p>To allow a neural network to learn complex decision boundaries, we apply a non-linear activation function to some of its layers. Commonly used functions include Tanh, ReLU, softmax, and variants of these. More technically, each neuron receives as input signal the weighted sum of the synaptic weights and the activation values of the neurons connected. One of the most widely used functions for this purpose is the so-called <strong>sigmoid function</strong>. It is a special case of the logistic function, which is defined by the following formula:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/60003fbe-ed08-424d-9478-582a826551bd.png" style="width:10.00em;height:3.17em;"/></div>
<p>The domain of this function includes all real numbers, and the co-domain is (<em>0, 1</em>). This means that any value obtained as an output from a neuron (as per the calculation of its activation state), will always be between zero and one. The sigmoid function, as represented in the following diagram, provides an interpretation of the saturation rate of a neuron, from not being active (<em>= 0</em>) to complete saturation, which occurs at a predetermined maximum value (<em>= 1</em>).</p>
<p>On the other hand, a hyperbolic tangent, or <strong>tanh</strong>, is another form of the activation function. Tanh squashes a real-valued number to the range <em>[-1, 1]</em>. In particular, mathematically, tanh activation function can be expressed as follows:</p>
<div class="CDPAlignCenter CDPAlign"><span class="mn"><img class="fm-editor-equation" src="assets/e72b887a-0d4b-4045-a62c-2db8cd4f3c4e.png" style="width:11.08em;height:1.42em;"/></span></div>
<p>The preceding equation can be represented in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6a367bab-2c52-4abd-ad34-337accfb4b1a.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Sigmoid versus tanh activation function</div>
<p>In general, in the last level of an <strong>feedforward neural network</strong> (<strong>FFNN</strong>), the softmax function is applied as the decision boundary. This is a common case, especially when solving a classification problem. <span>In probability theory, the output of the softmax function is squashed as the probability distribution over</span> <em>K</em> <span>different possible outcomes.</span> Nevertheless, the softmax function is used in various multiclass classification methods, such that the network's output is distributed across classes (that is, probability distribution over the classes) having a dynamic range between <em>-1</em> and <em>1</em> or <em>0</em> and <em>1</em>.</p>
<div class="packt_tip">For a regression problem, we do not need to use any activation function since the network generates continuous valuesâ€”probabilities. However, I've seen people using the IDENTITY activation function for regression problems nowadays. We'll see this in later chapters.</div>
<p>To conclude, choosing proper activation functions and network weights initialization are two problems that make a network perform at its best and help to obtain good training. We'll discuss more in upcoming chapters; we will see where to use which activation function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Neural network architectures</h1>
                </header>
            
            <article>
                
<p>There are various types of architectures in neural networks. We can categorize DL architectures into four groups: <strong>Deep Neural Networks</strong> (<strong>DNNs</strong>), <strong>Convolutional Neural Networks</strong> (<strong>CNNs</strong>), <strong>Recurrent Neural Networks</strong> (<strong>RNNs</strong>), and <strong>Emergent Architectures</strong> (<strong>EAs</strong>).</p>
<p>Nowadays, based on these architectures, researchers come up with so many variants of these for domain-specific use cases and research problems. <span class="Heading2Char">The following sections of this chapter will give a brief introduction to these architectures. More detailed analysis, with examples of applications, will be the subject of later chapters of this book.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep neural networks</h1>
                </header>
            
            <article>
                
<p>DNNs are neural networks having complex and deeper architecture with a large number of neurons in each layer, and there are many connections. The computation in each layer transforms the representations in the subsequent layers into slightly more abstract representations. However, we will use the term DNN to refer specifically to the MLP, the <strong>Stacked Auto-Encoder</strong> (<strong>SAE</strong>), and <strong>Deep Belief Networks</strong> (<strong>DBNs</strong>).</p>
<p>SAEs and DBNs use AEs and <strong>Restricted Boltzmann Machines</strong> (<strong>RBMs</strong>) as building blocks of the architectures. The main difference between these and MLPs is that training is executed in two phases: unsupervised pre-training and supervised fine-tuning.</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-771 image-border" src="assets/a0a9b905-3a15-43a7-8160-3a0c2a2f3d2a.png" style=""/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">SAE and DBN using AE and RBM respectively</div>
<p>In unsupervised pre-training, shown in the preceding diagram, the layers are stacked sequentially and trained in a layer-wise manner, like an AE or RBM using unlabeled data. Afterwards, in supervised fine-tuning, an output classifier layer is stacked and the complete neural network is optimized by retraining with labeled data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multilayer Perceptron</h1>
                </header>
            
            <article>
                
<p>As discussed earlier, a single perceptron is even incapable of approximating an XOR function. To overcome this limitation, multiple perceptrons are stacked together as MLPs, where layers are connected as a directed graph. This way, the signal propagates one way, from input layer to hidden layers to output layer, as shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/25203e1f-35e2-409d-abb0-1059fbc94385.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">An MLP architecture having an input layer, two hidden layers, and an output layer</div>
<p>Fundamentally, an MLP is one the most simple FFNNs having at least three layers: an input layer, a hidden layer, and an output layer. An MLP was first trained with a backpropogation algorithm in the 1980s.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep belief networks</h1>
                </header>
            
            <article>
                
<p>To overcome the overfitting problem in MLPs, the DBN was proposed by Hinton et al. It uses a greedy, layer-by-layer, pre-training algorithm to initialize the network weights through probabilistic generative models.</p>
<p>DBNs are composed of a visible layer and multiple layersâ€”<strong>hidden units</strong>. The top two layers have undirected, symmetric connections in between and form an associative memory, whereas lower layers receive top-down, directed connections from the preceding layer. The building blocks of a DBN are RBMs, as you can see in the following figure, where several RBMs are <em>stacked</em> one after another to form DBNs:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/67d0cd1e-9840-481f-b2a0-749cd099c0bf.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">A DBN configured for semi-supervised learning</div>
<p>A single RBM consists of two layers. The first layer is composed of visible neurons, and the second layer consists of hidden neurons. <em>Figure 16</em> shows the structure of a simple RBM, where the neurons are arranged according to a symmetrical bipartite graph:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/2c7b7adc-9112-4c9e-872e-20a69f6cde71.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">RBM architecture</div>
<p>In DBNs, an RBM is trained first with input data, called unsupervised pre-training, and the hidden layer represents the features learned using a greedy learning approach called supervised fine-tuning. Despite numerous successes, DBNs are being replaced  by <span>AEs</span>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Autoencoders</h1>
                </header>
            
            <article>
                
<p>An AE is a network with three or more layers, where the input layer and the output <span>layer </span>have the same number of neurons, and those intermediate (hidden layers) have a lower number of neurons. The network is trained to reproduce in the output, for each piece of input data, the same pattern of activity as in the input.</p>
<p>Useful applications of AEs are data denoising and dimensionality reduction for data visualization. The following diagram shows how an AE typically works. It reconstructs the received input through two phases: an encoding phase, which corresponds to a dimensional reduction for the original input, and a decoding phase, which is capable of reconstructing the original input from the encoded (compressed) representation:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-770 image-border" src="assets/281f46f7-6d2f-4ad6-9fa3-399aef7aa193.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Encoding and decoding phases of an AE</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional neural networks</h1>
                </header>
            
            <article>
                
<p>CNNs have achieved much and wide adoption in computer vision (for example, image recognition). In CNN networks, the connection scheme that defines the convolutional layer (conv) is significantly different compared to an MLP or DBN.</p>
<p>Importantly, a DNN has no prior knowledge of how the pixels are organized; it does not know that nearby pixels are close. A CNN's architecture embeds this prior knowledge. Lower layers typically identify features in small areas of the image, while higher layers combine lower-level features into larger features. This works well with most natural images, giving CNNs a decisive head start over DNNs:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2227d18b-1d82-4a75-9678-694029831dd2.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>A regular DNN versus a CNN</span></div>
<p>Take a close look at the preceding diagram; on the left is a regular three-layer neural network, and on the right, a CNN arranges its neurons in three dimensions (width, height, and depth). In a CNN architecture, a few convolutional layers are connected in a cascade style, where each layer is followed by a ReLU<strong> </strong>layer, then a pooling layer, then a few more convolutional layers (+ReLU), then another pooling layer, and so on.</p>
<p>The output from each conv layer is a set of objects called feature maps that are generated by a single kernel filter. Then the feature maps can be used to define a new input to the next layer. Each neuron in a CNN network produces an output followed by an activation threshold, which is proportional to the input and not bound. This type of layer is called a convolutional layer. The following diagram is a schematic of the architecture of a CNN used for facial recognition:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/209878a0-02ba-4912-80d8-d8662a4083b4.png" style=""/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">A schematic architecture of a CNN used for facial recognition</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recurrent neural networks </h1>
                </header>
            
            <article>
                
<p>A <strong>recurrent neural network</strong> <strong>(RNN)</strong> is a class of <strong>artificial neural network</strong> (<strong>ANN</strong>) where connections between units form a directed cycle. RNN architecture was originally conceived by Hochreiter and Schmidhuber in 1997. RNN architectures have standard MLPs plus added loops (as shown in the following diagram), so they can exploit the powerful nonlinear mapping capabilities of the MLP; and they have some form of memory:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-769 image-border" src="assets/78c14ba6-ccda-4db1-92b6-cee4f27af9ab.png" style=""/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span>RNN architecture</span></div>
<p>The preceding image shows a a very basic RNN having an input layer, 2 recurrent layers and an output layer. However, this basic RNN suffers from gradient vanishing and exploding problem and cannot model the long-term depedencies. Therefore, more advanced architectures are designed to utilize sequential information of input data with cyclic connections among building blocks such as perceptrons. These architectures include <strong>Long-Short-Term Memory</strong> (<strong>LSTM</strong>), <strong>Gated Recurrent Units</strong> (<strong>GRUs</strong>), <strong>Bidirectional-LSTM</strong> and other variants.</p>
<p>Consequently, LSTM and GR can overcome the drawbacks of regular RNNs: gradient vanishing/exploding problem and the long-short term dependency. We will look at these architectures in chapter 2.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Emergent architectures</h1>
                </header>
            
            <article>
                
<p>Many other emergent DL architectures have been suggested, such as <strong>Deep SpatioTemporal Neural Networks</strong> (<strong>DST-NNs</strong>), <strong>Multi-Dimensional Recurrent Neural Networks</strong> (<strong>MD-RNNs</strong>), and <strong>Convolutional AutoEncoders</strong> (<strong>CAEs</strong>).</p>
<p>Nevertheless, there are a few more emerging networks, such as <strong>CapsNets</strong> (which is an improved version of a CNN, designed to remove the drawbacks of regular CNNs), RNN for image recognition, and <strong>Generative Adversarial Networks</strong> (<strong>GANs</strong>) for simple image generation. Apart from these, factorization machines for personalization and deep reinforcement learning are also being used widely.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Residual neural networks</h1>
                </header>
            
            <article>
                
<p class="mce-root">Since there are sometimes millions of billions of hyperparameters and other practical aspects, it's really difficult to train deeper neural networks. To overcome this limitation, Kaiming He et al. (see <a href="https://arxiv.org/abs/1512.03385v1" target="_blank">https://arxiv.org/abs/1512.03385v1</a>) proposed a residual learning framework to ease the training of networks that are substantially deeper than those used previously.</p>
<p class="mce-root">They also explicitly reformulated the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. This way, these residual networks are easier to optimize and can gain accuracy from considerably increased depth.</p>
<p class="mce-root">The downside is that building a network by simply stacking residual blocks inevitably limits its optimization ability. To overcome this limitation, Ke Zhang et al. also proposed using a Multilevel Residual Network (<a href="https://arxiv.org/abs/1608.02908">https://arxiv.org/abs/1608.02908</a>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generative adversarial networks</h1>
                </header>
            
            <article>
                
<p>GANs are deep neural net architectures that consist of two networks pitted against each other (hence the name "adversarial"). Ian Goodfellow et al. introduced GANs in a paper (see more at <a href="https://arxiv.org/abs/1406.2661v1">https://arxiv.org/abs/1406.2661v1</a>). In GANs, the two main components are the <strong>generator</strong> <strong>and discriminator</strong>.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2cf8b4f1-7163-4af1-aa4b-6066329d554a.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Working principle of Generative Adversarial Networks (GANs)</div>
<p>The Generator will try to generate data samples out of a specific probability distribution, which is very similar to the actual object. The discriminator will judge whether its input is coming from the original training set or from the generator part.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Capsule networks</h1>
                </header>
            
            <article>
                
<p>CNNs perform well at classifying images. However, if the images have rotation, tilt, or any other different orientation, then CNNs show relatively very poor performance. Even the pooling operation in CNNs cannot much help against the positional invariance.</p>
<p>This issue in CNNs has led us to the recent advancement of CapsNet through the paper titled <em>Dynamic Routing Between Capsules</em><em> </em>(see more at <a href="https://arxiv.org/abs/1710.09829">https://arxiv.org/abs/1710.09829</a>) by Geoffrey Hinton et al.</p>
<p>Unlike a regular DNN, where we keep on adding layers, in CapsNets, the idea is to add more layers inside a single layer. This way, a CapsNet is a nested set of neural layers. We'll discuss more in <a href="b458baf5-6590-4e69-84d4-8c02a898dbca.xhtml">Chapter 11</a>, <em>Discussion, Current Trends, and Outlook</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DL frameworks and cloud platforms</h1>
                </header>
            
            <article>
                
<p>In this section, we'll present some of the most popular deep learning frameworks. Then we will discuss some cloud based platforms where you can deploy/run your DL applications. In short, almost all of the libraries provide the possibility of using a graphics processor to speed up the learning process, are released under an open license, and are the result of university research groups.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep learning frameworks</h1>
                </header>
            
            <article>
                
<p><strong>TensorFlow</strong> is mathematical software, and an open source software library for machine intelligence. The Google Brain team developed it in 2011 and open-sourced it in 2015. The main features offered by the latest release of TensorFlow (v1.8 during the writing of this book) are faster computing, flexibility, portability, easy debugging, a unified API, transparent use of GPU computing, easy use, and extensibility. Once you have constructed your neural network model, after the necessary feature engineering, you can simply perform the training interactively using plotting or TensorBoard.</p>
<p><strong>Keras</strong> is a deep learning library that sits atop TensorFlow and Theano, providing an intuitive API inspired by Torch. It is perhaps the best Python API in existence. DeepLearning4J relies on Keras as its Python API and imports models from Keras and through Keras from Theano and TensorFlow.</p>
<p><strong>Theano</strong> is also a deep learning framework written in Python. It allows using GPU, which is 24x faster than a single CPU. Defining, optimizing, and evaluating complex mathematical expressions is very straightforward in Theano.</p>
<p><strong>Neon</strong> is a Python-based deep learning framework developed by Nirvana. Neon has a syntax similar to Theano's high-level framework (for example, Keras). Currently, Neon is considered the fastest tool for GPU-based implementation, especially for CNNs. But its CPU-based implementation is relatively worse than most other libraries.</p>
<p><strong>PyTorch</strong> is a vast ecosystem for ML that offers a large number of algorithms and functions, including for DL and for processing various types of multimedia data, with a particular focus on parallel computing. Torch is a highly portable framework supported on various platforms, including Windows, macOS, Linux, and Android.</p>
<p><strong>Caffe</strong>, developed primarily by <strong>Berkeley Vision and Learning Center</strong> (<strong>BVLC</strong>), is a framework designed to stand out because of its expression, speed, and modularity.</p>
<p><strong>MXNet </strong><em>(</em><a href="http://mxnet.io/" target="_blank">http://mxnet.io/</a>) is a deep learning framework that supports many languages, such as R, Python, C++, and Julia. This is helpful because if you know any of these languages, you will not need to step out of your comfort zone at all to train your deep learning models. Its backend is written in C++ and CUDA and it is able to manage its own memory in a <span>way </span>similar to Theano.</p>
<p>The <strong>Microsoft Cognitive Toolkit</strong> (<strong>CNTK</strong>) is a unified deep learning toolkit from Microsoft Research that makes it easy to train and combine popular model types across multiple GPUs and servers. CNTK implements highly efficient CNN and RNN training for speech, image, and text data. It supports cuDNN v5.1 for GPU acceleration.</p>
<p>DeepLearning4J is one of the first commercial-grade, open source, distributed deep learning libraries written for Java and Scala. This also provides integrated support for Hadoop and Spark. DeepLearning4 is designed to be used in business environments on distributed GPUs and CPUs.</p>
<p>DeepLearning4J aims to be cutting-edge and plug-and-play, with more convention than configuration, which allows for fast prototyping for non-researchers. The following libraries can be integrated with DeepLearning4 and will make your JVM experience easier whether you are developing your ML application in Java or Scala.</p>
<p>ND4J is just like NumPy for JVM. It comes with some basic operations of linear algebra such as matrix creation, addition, and multiplication. ND4S, on the other hand, is a scientific computing library for linear algebra and matrix manipulation. It supports n-dimensional arrays for JVM-based languages.</p>
<p>To conclude, the following figure shows the last 1 year's Google trends concerning the popularity of different DL frameworks:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-768 image-border" src="assets/ea45c7d9-c0ff-44c7-8887-fec8cbdfaa15.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">The trends of different DL frameworks. TensorFlow and Keras are most dominating. Theano is losing its popularity. On the other hand, DeepLearning4J is emerging for JVM.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cloud-based platforms for DL</h1>
                </header>
            
            <article>
                
<p>Apart from the preceding libraries, there have been some recent initiatives for deep learning on the cloud. The idea is to bring deep learning capabilities to big data with millions of billions of data points and high-dimensional data. For example, <strong>Amazon Web Services</strong> (<strong>AWS</strong>), Microsoft Azure, Google Cloud Platform, and <strong>NVIDIA GPU Cloud</strong> (<strong>NGC</strong>) all offer machine and deep learning services that are native to their public clouds.</p>
<p>In October 2017, AWS released deep learning <strong>Amazon Machine Images</strong> (<strong><span>AMIs</span></strong>) for Amazon <strong>Elastic Compute Cloud</strong> (<strong>EC2</strong>) P3 instances. These AMIs come pre-installed with deep learning frameworks, such as TensorFlow, Gluon, and Apache MXNet, that are optimized for the NVIDIA Volta V100 GPUs within Amazon EC2 P3 instances.</p>
<p>The Microsoft Cognitive Toolkit is Azure's open source, deep learning service. Similar to AWS's offering, it focuses on tools that can help developers build and deploy deep learning applications.</p>
<p>On the other hand, NGC empowers AI scientists and researchers with GPU-accelerated containers (see <a href="https://www.nvidia.com/en-us/data-center/gpu-cloud-computing/" target="_blank">https://www.nvidia.com/en-us/data-center/gpu-cloud-computing/</a>). NGC features containerized deep learning frameworks such as TensorFlow, PyTorch, MXNet, and more that are tuned, tested, and certified by NVIDIA to run on the latest NVIDIA GPUs.</p>
<p>Now that we have a minimum of knowledge about available DL libraries, frameworks, and cloud-based platforms for running and deploying our DL applications, we can dive into coding. First, we will start by solving the famous Titanic survival prediction problem. However, we won't use the previously listed frameworks; we will be using the Apache Spark ML library. Since we will be using Spark along with other DL libraries, knowing a little bit of Spark would help us grasp things in the upcoming chapters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep learning from a disaster â€“ Titanic survival prediction</h1>
                </header>
            
            <article>
                
<p>In this section, we are going to solve the famous Titanic survival prediction problem available on Kaggle (see <a href="https://www.kaggle.com/c/titanic/data" target="_blank">https://www.kaggle.com/c/titanic/data</a>). The task is to complete the analysis of what sorts of people are likely to survive using an ML algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem description</h1>
                </header>
            
            <article>
                
<p>Before diving into the coding, let's see a short description of the problem. This paragraph is directly quoted from the Kaggle Titanic survival prediction page:</p>
<div class="packt_quote">"The sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships. One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper class. In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy."</div>
<p>Now, before going even deeper, we need to know about the data of the passengers traveling on the Titanic during the disaster so that we can develop a predictive model that can be used for survival analysis. The dataset can be downloaded from <a href="https://github.com/rezacsedu/TitanicSurvivalPredictionDataset">https://github.com/rezacsedu/TitanicSurvivalPredictionDataset</a>. There are two <kbd>.csv</kbd> files:</p>
<ul>
<li><strong>The training set</strong> (<kbd>train.csv</kbd>): Can be used to build your ML models. This file also includes labels as the <em>ground truth</em> for each passenger for the training set.</li>
<li><strong>The test set</strong> (<kbd>test.csv</kbd>): Can be used to see how well your model performs on unseen data. However, for the test set, we do not provide the ground truth for each passenger.</li>
</ul>
<p>In short, for each passenger in the test set, we have to use the trained model to predict whether they'll survive the sinking of the Titanic. <em>Table 1</em> shows the metadata of the training set:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p><strong>Variable</strong></p>
</td>
<td>
<p><strong>Definition</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>survival</kbd></p>
</td>
<td>
<p>Two labels:</p>
<ul>
<li><em>0 = No</em></li>
<li><em>1 = Yes</em></li>
</ul>
</td>
</tr>
<tr>
<td>
<p><kbd>pclass</kbd></p>
</td>
<td>
<p>This is a proxy for the <strong>Socioeconomic Status</strong> (<strong>SES</strong>) of a passenger and is categorized as upper, middle, and lower. In particular, <em>1 = 1<sup>st</sup></em>, <em>2 = 2<sup>nd</sup></em>, <em>3 = 3<sup>rd</sup>.</em></p>
</td>
</tr>
<tr>
<td>
<p><kbd>sex</kbd></p>
</td>
<td>
<p>Male or female.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>Age</kbd></p>
</td>
<td>
<p>Age in years.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>sibsp</kbd></p>
</td>
<td>
<p>This signifies family relations as follows:</p>
<ul>
<li><em>Sibling = brother, sister, stepbrother, stepsister</em></li>
<li><em>Spouse = husband, wife (mistresses and fiancÃ©s were ignored)</em></li>
</ul>
</td>
</tr>
<tr>
<td>
<p><kbd>parch</kbd></p>
</td>
<td>
<p>In the dataset, family relations are defined as follows:</p>
<ul>
<li><em>Parent = mother, father</em></li>
<li><em>Child = daughter, son, stepdaughter, stepson</em></li>
</ul>
<p>Some children traveled only with a nanny, therefore <em>parch=0</em> for them.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>ticket</kbd></p>
</td>
<td>
<p>Ticket number.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>fare</kbd></p>
</td>
<td>
<p>Passenger ticket fare.</p>
</td>
</tr>
<tr>
<td>
<p>cabin</p>
</td>
<td>
<p>Cabin number.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>embarked</kbd></p>
</td>
<td>
<p>Three ports:</p>
<ul>
<li><em>C = Cherbourg</em></li>
<li><em>Q = Queenstown</em></li>
<li><em>S = Southampton</em></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Now the question would be: using this labeled data, can we draw some straightforward conclusions? Say that being a woman, being in first class, and being a child were all factors that could boost a passenger's chances of survival during this disaster.</p>
<p>To solve this problem, we can start from the basic MLP, which is one of the oldest deep learning algorithms. For this, we use the Spark-based <kbd>MultilayerPerceptronClassifier</kbd>. At this point, you might be wondering why I am talking about Spark since it is not a DL library. However, Spark has an MLP implementation, which would be enough to serve our objective.</p>
<p>Then from the next chapter, we'll gradually start using more robust DNN by using DeepLearning4J, a JVM-based framework for developing deep learning applications. So let's see how to configure our Spark environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring the programming environment</h1>
                </header>
            
            <article>
                
<p>I am assuming that Java is already installed on your machine and the <kbd>JAVA_HOME</kbd> is set too. Also, I'm assuming that your IDE has the Maven plugin installed. If so, then just create a Maven project and add the project properties as follows:</p>
<pre><strong>&lt;properties&gt;</strong><br/>        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;<br/>        &lt;java.version&gt;1.8&lt;/java.version&gt;<br/>        &lt;jdk.version&gt;1.8&lt;/jdk.version&gt;<br/>        &lt;spark.version&gt;2.3.0&lt;/spark.version&gt;<br/><strong>&lt;/properties&gt;</strong></pre>
<p>In the preceding tag, I specified Spark (that is, 2.3.0), but you can adjust it. Then add the following dependencies in the <kbd>pom.xml</kbd> file:</p>
<pre><strong>&lt;dependencies&gt;</strong><br/>        <strong>&lt;dependency&gt;</strong><br/>            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br/>            &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;<br/>            &lt;version&gt;${spark.version}&lt;/version&gt;<br/>        <strong>&lt;/dependency&gt;</strong><br/>        <strong>&lt;dependency&gt;</strong><br/>            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br/>            &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;<br/>            &lt;version&gt;${spark.version}&lt;/version&gt;<br/>        <strong>&lt;/dependency&gt;</strong><br/>        <strong>&lt;dependency&gt;</strong><br/>            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br/>            &lt;artifactId&gt;spark-mllib_2.11&lt;/artifactId&gt;<br/>            &lt;version&gt;${spark.version}&lt;/version&gt;<br/>        <strong>&lt;/dependency&gt;</strong><br/>        <strong>&lt;dependency&gt;</strong><br/>            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br/>            &lt;artifactId&gt;spark-graphx_2.11&lt;/artifactId&gt;<br/>            &lt;version&gt;${spark.version}&lt;/version&gt;<br/>        <strong>&lt;/dependency&gt;</strong><br/>        <strong>&lt;dependency&gt;</strong><br/>            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br/>            &lt;artifactId&gt;spark-yarn_2.11&lt;/artifactId&gt;<br/>            &lt;version&gt;${spark.version}&lt;/version&gt;<br/>        <strong>&lt;/dependency&gt;</strong><br/>        <strong>&lt;dependency&gt;</strong><br/>            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br/>            &lt;artifactId&gt;spark-network-shuffle_2.11&lt;/artifactId&gt;<br/>            &lt;version&gt;${spark.version}&lt;/version&gt;<br/>        <strong>&lt;/dependency&gt;</strong><br/>        <strong>&lt;dependency&gt;</strong><br/>            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;<br/>            &lt;artifactId&gt;spark-streaming-flume_2.11&lt;/artifactId&gt;<br/>            &lt;version&gt;${spark.version}&lt;/version&gt;<br/>        <strong>&lt;/dependency&gt;</strong><br/>        <strong>&lt;dependency&gt;</strong><br/>            &lt;groupId&gt;com.databricks&lt;/groupId&gt;<br/>            &lt;artifactId&gt;spark-csv_2.11&lt;/artifactId&gt;<br/>            &lt;version&gt;1.3.0&lt;/version&gt;<br/>        <strong>&lt;/dependency&gt;</strong><br/><strong>&lt;/dependencies&gt;</strong></pre>
<p>Then if everything goes smoothly, all the JAR files will be downloaded in the project home as Maven dependencies. Alright! Then we can start writing the code.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature engineering and input dataset preparation</h1>
                </header>
            
            <article>
                
<p>In this sub-section, we will see some basic feature engineering and dataset preparation that can be fed into the MLP classifier. So let's start by creating <kbd>SparkSession</kbd>, which is the gateway to access Spark:</p>
<pre><strong>SparkSession</strong> spark = SparkSession<br/>                     .<em>builder</em>()<br/>                     .master("local[*]")<br/>                     .config("spark.sql.warehouse.dir", "/tmp/spark")<br/>                     .appName("SurvivalPredictionMLP")<br/>                     .getOrCreate();</pre>
<p>Then let's read the training set and see a glimpse of it:</p>
<pre><strong>Dataset&lt;Row&gt;</strong> df = spark.sqlContext()<br/>                .read()<br/>                .format("com.databricks.spark.csv")<br/>                .option("header", "true")<br/>                .option("inferSchema", "true")<br/>                .load("data/train.csv");<br/>df.show();</pre>
<p>A snapshot of the dataset can be seen as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/1ac60d70-682f-4073-8ad9-5519b7be3bd8.png" style=""/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">A snapshot of the Titanic survival dataset</div>
<p>Now we can see that the training set has both categorical as well as numerical features. In addition, some features are not important, such as <kbd>PassengerID</kbd>, <kbd>Ticket</kbd>, and so on. The same also applies to the <kbd>Name</kbd> feature unless we manually create some features based on the title. However, let's keep it simple. Nevertheless, some columns contain null values. Therefore, lots of consideration and cleaning are required.</p>
<p>I ignore the <kbd>PassengerId</kbd>, <kbd>Name</kbd>, and <kbd>Ticket</kbd> columns. Apart from these, the <kbd>Sex</kbd> column is categorical, so I've encoded the passengers based on <kbd>male</kbd> and <kbd>female</kbd>. Then the <kbd>Embarked</kbd> column is encoded too. We can encode <kbd>S</kbd> as <kbd>0</kbd>, <kbd>C</kbd> as <kbd>1</kbd>, and <kbd>Q</kbd> as <kbd>2</kbd>.</p>
<p>For this also, we can write user-defined-functions (also known as UDFs) called <kbd>normSex</kbd> and <kbd>normEmbarked</kbd> for <kbd>Sex</kbd> and <kbd>Embarked</kbd>, respectively. Let's see their signatures:</p>
<pre><strong>private</strong> <strong>static</strong> UDF1&lt;String,Option&lt;Integer&gt;&gt; <em>normEmbarked</em>=(String d) -&gt; {<br/>        <strong>if</strong> (<strong>null</strong> == d)<br/>            <strong>return</strong> Option.<em>apply</em>(<strong>null</strong>);<br/>        <strong>else</strong> {<br/>            <strong>if</strong> (d.equals("S"))<br/>                <strong>return</strong> Some.apply(0);<br/>            <strong>else</strong> <strong>if</strong> (d.equals("C"))<br/>                <strong>return</strong> Some.apply(1);<br/>            <strong>else<br/></strong>                <strong>return</strong> Some.apply(2);<br/>        }<br/>    };</pre>
<p>Therefore, this UDF takes a <kbd>String</kbd> type and encodes as an integer. Now the <kbd>normSex</kbd> UDF also works similarly:</p>
<pre><strong>private</strong> <strong>static</strong> UDF1&lt;String, Option&lt;Integer&gt;&gt; normSex = (String d) -&gt; {<br/>      <strong>if</strong> (<strong>null</strong> == d)<br/>          <strong>return</strong> Option.apply(<strong>null</strong>);<br/>      <strong>else</strong> {<br/>        <strong>if</strong> (d.equals("male"))<br/>            <strong>return</strong> Some.apply(0);<br/>        <strong>else<br/></strong>            <strong>return</strong> Some.apply(1);<br/>      }<br/>    };</pre>
<p>So we can now select only useful columns but for the <kbd>Sex</kbd> and <kbd>Embarked</kbd> columns with the aforementioned UDFs:</p>
<pre><strong>Dataset&lt;Row&gt;</strong> projection = df.select(<br/>                col("Survived"),<br/>                col("Fare"),<br/>                callUDF("normSex", col("Sex")).alias("Sex"),<br/>                col("Age"),<br/>                col("Pclass"),<br/>                col("Parch"),<br/>                col("SibSp"),<br/>                 callUDF("normEmbarked",<br/>                col("Embarked")).alias("Embarked"));<br/>projectin.show();</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/100ec195-8342-422b-9db2-22e4838df190.png" style=""/></div>
<p>Now we have been able to convert a categorical column into a numeric; however, as we can see, there are still null values. Therefore, what can we do? We can either drop the <kbd>null</kbd> values altogether or apply some <kbd>null</kbd> imputing techniques with the mean value of those particular columns. I believe the second approach is better.</p>
<p>Now, again for this null imputation, we can write UDFs too. However, for that we need to know some statistics about those numerical columns. Unfortunately, we cannot perform the summary statistics on DataFrame. Therefore, we have to convert the DataFrame into <kbd>JavaRDD&lt;Vector&gt;</kbd>. Well, we also ignore the <kbd>null</kbd> entries for calculating this:</p>
<pre><strong>JavaRDD&lt;Vector&gt;</strong> statsDf =projection.rdd().toJavaRDD().map(row -&gt; Vectors.<em>dense</em>( row.&lt;Double&gt;getAs("Fare"),<br/>               row.isNullAt(3) ? 0d : row.Double&gt;getAs("Age")<br/>                  ));</pre>
<p>Now let's compute the multivariate statistical <kbd>summary</kbd>. The <kbd>summary</kbd> statistical will be further used to calculate the <kbd>meanAge</kbd> and <kbd>meanFare</kbd> for the corresponding missing entries for these two features:</p>
<pre><strong>MultivariateStatisticalSummary</strong> summary = Statistics.<em>colStats</em>(statsRDD.rdd());<br/><strong>double</strong> meanFare = summary.mean().apply(0);<br/><strong>double</strong> meanAge = summary.mean().apply(1); </pre>
<p>Now let's create two more UDFs for the null imputation on the <kbd>Age</kbd> and <kbd>Fare</kbd> columns:</p>
<pre><strong>UDF1</strong>&lt;String, Option&lt;Double&gt;&gt; normFare = (String d) -&gt; {<br/>            <strong>if</strong> (<strong>null</strong> == d) {<br/>                <strong>return</strong> Some.apply(meanFare);<br/>            }<br/>            <strong>else<br/></strong>                <strong>return</strong> Some.apply(Double.parseDouble(d));<br/>        };</pre>
<p>Therefore, we have defined a UDF, which fills in the <kbd>meanFare</kbd> values if the data has no entry. Now let's create another UDF for the <kbd>Age</kbd> column:</p>
<pre><strong>UDF1</strong>&lt;String, Option&lt;Double&gt;&gt; normAge = (String d) -&gt; {<br/>          <strong>if</strong> (<strong>null</strong> == d)<br/>              <strong>return</strong> Some.apply(meanAge);<br/>          <strong>else<br/></strong>              <strong>return</strong> Some.apply(Double.parseDouble(d));<br/>        };</pre>
<p>Now we need to register the UDFs as follows:</p>
<pre>spark.sqlContext().udf().register("normFare", normFare, DataTypes.<strong>DoubleType</strong>);<br/>spark.sqlContext().udf().register("normAge", normAge, DataTypes.<strong>DoubleType</strong>);</pre>
<p>Therefore, let's apply the preceding UDFs for <kbd>null</kbd> imputation:</p>
<pre><strong>Dataset&lt;Row&gt;</strong> finalDF = projection.select(<br/>                <em>col</em>("Survived"),<br/>                <em>callUDF</em>("normFare",<br/>                <em>col</em>("Fare").cast("string")).alias("Fare"),<br/>                <em>col</em>("Sex"),<br/>                <em>callUDF</em>("normAge",<br/>                <em>col</em>("Age").cast("string")).alias("Age"),<br/>                <em>col</em>("Pclass"),<br/>                <em>col</em>("Parch"),<br/>                <em>col</em>("SibSp"),<br/>                <em>col</em>("Embarked"));<br/>finalDF.show();</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e0d3af75-42f6-4d1b-9360-25daec84fbf3.png" style=""/></div>
<p>Great! We now can see that the <kbd>null</kbd> values are replaced with the mean value for the <kbd>Age</kbd> and <kbd>Fare</kbd> columns. However, still the numeric values are not scaled. Therefore, it would be a better idea to scale them. However, for that, we need to compute the mean and variance and then store them as a model to be used for later scaling:</p>
<pre><strong>Vector</strong> stddev = Vectors.dense(Math.sqrt(summary.variance().apply(0)), Math.sqrt(summary.variance().apply(1)));<br/><br/><strong>Vector</strong> mean = Vectors.dense(summary.mean().apply(0), summary.mean().apply(1));<br/><strong>StandardScalerModel</strong> scaler = <strong>new</strong> StandardScalerModel(stddev, mean);</pre>
<p>Then we need an encoder for the numeric values (that is, <kbd>Integer</kbd>; either <kbd>BINARY</kbd> or <kbd>Double</kbd>):</p>
<pre><strong>Encoder&lt;Integer&gt;</strong> integerEncoder = Encoders.INT();<br/><strong>Encoder&lt;Double&gt;</strong> doubleEncoder = Encoders.DOUBLE();<br/>Encoders.BINARY();<br/><br/><strong>Encoder&lt;Vector&gt;</strong> vectorEncoder = Encoders.kryo(Vector.<strong>class</strong>);<br/>Encoders.tuple(integerEncoder, vectorEncoder);<br/>Encoders.tuple(doubleEncoder, vectorEncoder);</pre>
<p>Then we can create a <kbd>VectorPair</kbd> consisting of the label (that is, <kbd>Survived</kbd>) and the features. Here the encoding is, basically, creating a scaled feature vector:</p>
<pre><strong>JavaRDD&lt;VectorPair&gt;</strong> scaledRDD = trainingDF.toJavaRDD().map(row -&gt; {<br/>                <strong>VectorPair</strong> vectorPair = <strong>new</strong> VectorPair();<br/>                vectorPair.setLable(<strong>new<br/></strong>                <strong>Double(row.&lt;Integer&gt;</strong> getAs("Survived")));<br/><br/>                vectorPair.setFeatures(Util.<em>getScaledVector</em>(<br/>                                row.&lt;Double&gt;getAs("Fare"),<br/>                                row.&lt;Double&gt;getAs("Age"),<br/>                                row.&lt;Integer&gt;getAs("Pclass"),<br/>                                row.&lt;Integer&gt;getAs("Sex"),<br/>                                row.isNullAt(7) ? 0d :<br/>                                row.&lt;Integer&gt;getAs("Embarked"),<br/>                                scaler));<br/>                <strong>return</strong> vectorPair;<br/>        });</pre>
<p>In the preceding code block, the <kbd>getScaledVector()</kbd> method does perform the scaling operation. The signature of this method can be seen as follows:</p>
<pre><strong>public</strong> <strong>static</strong> org.apache.spark.mllib.linalg.Vector getScaledVector(<strong>double</strong> fare, <br/><strong>        double</strong> age, <strong>double</strong> pclass,  <strong>double</strong> sex, <strong>double</strong> embarked, StandardScalerModel scaler) {<br/>        org.apache.spark.mllib.linalg.<strong>Vector</strong> scaledContinous = scaler.transform(Vectors.dense(fare, age));<br/>        <strong>Tuple3&lt;Double, Double, Double&gt;</strong> pclassFlat = flattenPclass(pclass);<br/>        <strong>Tuple3&lt;Double, Double, Double&gt;</strong> embarkedFlat = flattenEmbarked(embarked);<br/>        <strong>Tuple2&lt;Double, Double&gt;</strong> sexFlat = flattenSex(sex);<br/><br/>        <strong>return</strong> Vectors.dense(<br/>                scaledContinous.apply(0),<br/>                scaledContinous.apply(1),<br/>                sexFlat._1(),<br/>                sexFlat._2(),<br/>                pclassFlat._1(),<br/>                pclassFlat._2(),<br/>                pclassFlat._3(),<br/>                embarkedFlat._1(),<br/>                embarkedFlat._2(),<br/>                embarkedFlat._3());<br/>    }</pre>
<p>Since we planned to use a Spark ML-based classifier (that is, an MLP implementation), we need to convert this RDD of the vector to an ML vector:</p>
<pre><strong>Dataset&lt;Row&gt;</strong> scaledDF = spark.createDataFrame(scaledRDD, VectorPair.<strong>class</strong>);</pre>
<p>Finally, let's see how the resulting DataFrame looks:</p>
<pre>scaledDF.show();</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0fe08cf5-78c7-4260-bb72-51b3ac6fba75.png" style=""/></div>
<p>Up to this point, we have been able to prepare our features. Still, this is an MLlib-based vector, so we need to further convert this into an ML vector:</p>
<pre><strong>Dataset&lt;Row&gt;</strong> scaledData2 = MLUtils.convertVectorColumnsToML(scaledDF);</pre>
<p>Fantastic! Now were' almost done preparing a training set that can be consumed by the MLP classifier. Since we also need to evaluate the model's performance, we can randomly split the training data for the training and test sets. Let's allocate 80% for training and 20% for testing. These will be used to train the model and evaluate the model, respectively:</p>
<pre><strong>Dataset&lt;Row&gt;</strong> data = scaledData2.toDF("features", "label");<br/><strong>Dataset&lt;Row&gt;</strong>[] datasets = data.randomSplit(<strong>new</strong> <strong>double</strong>[]{0.80, 0.20}, 12345L);<br/><br/><strong>Dataset&lt;Row&gt;</strong> trainingData = datasets[0];<br/><strong>Dataset&lt;Row&gt;</strong> validationData = datasets[1];</pre>
<p>Alright. Now that we have the training set, we can perform training on an MLP model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training MLP classifier </h1>
                </header>
            
            <article>
                
<p>In Spark, an MLP is a classifier that consists of multiple layers. Each layer is fully connected to the next layer in the network. Nodes in the input layer represent the input data, whereas other nodes map inputs to outputs by a linear combination of the inputs with the nodeâ€™s weights and biases and by applying an activation function.</p>
<div class="packt_tip">Interested readers can take a look at <a href="https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier">https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier</a>.</div>
<p>So let's create the layers for the MLP classifier. For this example, let's make a shallow network considering the fact that our dataset is not that highly dimensional.</p>
<p>Let's assume that only 18 neurons in the first hidden layer and <kbd>8</kbd> neurons in the second hidden layer would be sufficient. Note that the input layer has <kbd>10</kbd> inputs, so we set <kbd>10</kbd> neurons and <kbd>2</kbd> neurons in the output layers since our MLP will predict only <kbd>2</kbd> classes. One thing is very importantâ€”the number of inputs has to be equal to the size of the feature vectors and the number of outputs has to be equal to the total number of labels:</p>
<pre><strong>int</strong>[] layers = <strong>new</strong> <strong>int</strong>[] {10, 8, 16, 2};</pre>
<p>Then we instantiate the model with the trainer and set its parameters:</p>
<pre><strong>MultilayerPerceptronClassifier</strong> mlp = <strong>new</strong> MultilayerPerceptronClassifier()<br/>                                          .setLayers(layers)<br/>                                          .setBlockSize(128)<br/>                                          .setSeed(1234L)<br/>                                          .setTol(1E-8)<br/>                                          .setMaxIter(1000);</pre>
<p>So, as you can understand, the preceding <kbd>MultilayerPerceptronClassifier()</kbd> is the classifier trainer based on the MLP. Each layer has a sigmoid activation function except the output layer, which has the softmax activation. Note that Spark-based MLP implementation supports only minibatch GD and LBFGS optimizers.</p>
<p>In short, we cannot use other activation functions such as ReLU or tanh in the hidden layers. Apart from this, other advanced optimizers are also not supported, nor are batch normalization and so on. This is a serious constraint of this implementation. In the next chapter, we will try to overcome this with DL4J.<br/></p>
<p>We have also set the convergence tolerance of iterations as a very small value so that it will lead to higher accuracy with the cost of more iterations. We set the block size for stacking input data in matrices to speed up the computation.</p>
<div class="packt_infobox"><span>If the size of the training set is large, then the data is stacked within partitions. If the block size is more than the remaining data in a partition, then it is adjusted to the size of this data. The recommended size is between 10 and 1,000, but the default block size is 128.</span></div>
<p>Finally, we plan to iterate the training 1,000 times. So let's start training the model using the training set:</p>
<pre><strong>MultilayerPerceptronClassificationModel</strong> model = mlp.fit(trainingData);</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating the MLP classifier</h1>
                </header>
            
            <article>
                
<p>When the training is completed, we compute the prediction on the test set to evaluate the robustness of the model:</p>
<pre><strong>Dataset&lt;Row&gt;</strong> predictions = model.transform(validationData);</pre>
<p>Now, how about seeing some sample predictions? Let's observe both the true labels and the predicted labels:</p>
<pre>predictions.show();</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e47955e9-f0ad-4335-a487-19aaa4c43a47.png" style=""/></div>
<p>We can see that some predictions are correct but some of them are wrong too. Nevertheless, in this way, it is difficult to guess the performance. Therefore, we can compute performance metrics such as precision, recall, and f1 measure:</p>
<pre><strong>MulticlassClassificationEvaluator</strong> evaluator = <strong>new</strong> MulticlassClassificationEvaluator()<br/>                                              .setLabelCol("label")<br/>                                              .setPredictionCol("prediction");<br/><br/><strong>MulticlassClassificationEvaluator</strong> evaluator1 = evaluator.setMetricName("accuracy");<br/><strong>MulticlassClassificationEvaluator</strong> evaluator2 = evaluator.setMetricName("weightedPrecision");<br/><strong>MulticlassClassificationEvaluator</strong> evaluator3 = evaluator.setMetricName("weightedRecall");<br/><strong>MulticlassClassificationEvaluator</strong> evaluator4 = evaluator.setMetricName("f1");</pre>
<p>Now let's compute the classification's <kbd>accuracy</kbd>, <kbd>precision</kbd>, <kbd>recall</kbd>, <kbd>f1</kbd> measure, and error on test data:</p>
<pre><strong>double</strong> accuracy = evaluator1.evaluate(predictions);<br/><strong>double</strong> precision = evaluator2.evaluate(predictions);<br/><strong>double</strong> recall = evaluator3.evaluate(predictions);<br/><strong>double</strong> f1 = evaluator4.evaluate(predictions);<br/><br/>// Print the performance metrics<br/>System.<strong><em>out</em></strong>.println("Accuracy = " + accuracy);<br/>System.<strong><em>out</em></strong>.println("Precision = " + precision);<br/>System.<strong><em>out</em></strong>.println("Recall = " + recall);<br/>System.<strong><em>out</em></strong>.println("F1 = " + f1);<br/><br/>System.<strong><em>out</em></strong>.println("Test Error = " + (1 - accuracy));</pre>
<pre class="mce-root"><span class="packt_screen"><strong><q>&gt;&gt;&gt;</q><br/></strong></span><span class="packt_screen"><strong>Accuracy</strong> = 0.7796476846282568<br/> <strong>Precision</strong> = 0.7796476846282568<br/> <strong>Recall</strong> = 0.7796476846282568<br/> <strong>F1</strong> = 0.7796476846282568<br/> <strong>Test Error</strong> = 0.22035231537174316</span></pre>
<p>Well done! We have been able to achieve a fair accuracy rate, that is, 78%. Still we can improve the with additional feature engineering. More tips will be given in the next section! Now, before concluding this chapter, let's try to utilize the trained model to get the prediction on the test set. First, we read the test set and create the DataFrame:</p>
<pre><strong>Dataset&lt;Row&gt;</strong> testDF = Util.getTestDF();</pre>
<p>Nevertheless, even if you see the test set, it has some null values. So let's do null imputation on the <kbd>Age</kbd> and <kbd>Fare</kbd> columns. If you don't prefer using UDF, you can create a MAP where you include your imputing plan:</p>
<pre><strong>Map&lt;String, Object&gt;</strong> m = <strong>new</strong> HashMap&lt;String, Object&gt;();<br/>m.put("Age", meanAge);<br/>m.put("Fare", meanFare);<br/>       <br/><strong>Dataset&lt;Row&gt;</strong> testDF2 = testDF.na().fill(m);</pre>
<p>Then again, we create an RDD of <kbd>vectorPair</kbd> consisting of features and labels (target column):</p>
<pre><strong>JavaRDD&lt;VectorPair&gt;</strong> testRDD = testDF2.javaRDD().map(row -&gt; {<br/>            VectorPair vectorPair = <strong>new</strong> VectorPair();<br/>            vectorPair.setLable(row.&lt;Integer&gt;getAs("PassengerId"));<br/>            vectorPair.setFeatures(Util.<em>getScaledVector</em>(<br/>                    row.&lt;Double&gt;getAs("Fare"),<br/>                    row.&lt;Double&gt;getAs("Age"),<br/>                    row.&lt;Integer&gt;getAs("Pclass"),<br/>                    row.&lt;Integer&gt;getAs("Sex"),<br/>                    row.&lt;Integer&gt;getAs("Embarked"),<br/>                    scaler));<br/>            <strong>return</strong> vectorPair;<br/>        });</pre>
<p>Then we create a Spark DataFrame:</p>
<pre><strong>Dataset&lt;Row&gt;</strong> scaledTestDF = spark.createDataFrame(testRDD, VectorPair.<strong>class</strong>);</pre>
<p>Finally, let's convert the MLib vectors to ML based vectors:</p>
<pre><strong>Dataset&lt;Row&gt;</strong> finalTestDF = MLUtils.convertVectorColumnsToML(scaledTestDF).toDF("features", "PassengerId");</pre>
<p>Now, let's perform the model inferencing, that is, create a prediction for the <kbd>PassengerId</kbd> column and show the sample <kbd>prediction</kbd>:</p>
<pre><strong>Dataset&lt;Row&gt;</strong> resultDF = model.transform(finalTestDF).select("PassengerId", "prediction"); <br/>resultDF.show();</pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/dadcc05d-8734-4479-85d1-7adeca1c1ac9.png" style=""/></div>
<p>Finally, let's write the result in a CSV file:</p>
<pre>resultDF.write().format("com.databricks.spark.csv").option("header", <strong>true</strong>).save("result/result.csv");</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Frequently asked questions (FAQs)</h1>
                </header>
            
            <article>
                
<p>Now that we have solved the Titanic survival prediction problem with an acceptable level of accuracy, there are other practical aspects of this problem and of overall deep learning phenomena that need to be considered too. In this section, we will see some frequently asked questions that might be already in your mind. Answers to these questions can be found in <em>Appendix A</em>.</p>
<ol>
<li>Draw an ANN using the original artificial neurons that compute the XOR operation: <em>A</em>âŠ• <em>B</em>. Describe this problem formally as a classification problem. Why <span>can't </span>simple neurons solve this problem? How does an MLP solve this problem by stacking multiple perceptrons?</li>
<li>We have briefly seen the history of ANNs. What are the most significant milestones in the era of deep learning? Can we explain the timeline in a single figure?</li>
<li>Can I use another deep learning framework for solving this Titanic survival prediction problem more flexibly?</li>
<li>Can I use <kbd>Name</kbd> as a feature to be used in the MLP in the code?</li>
<li>I understand the number of neurons in the input and output layers. But how many neurons should I set for the hidden layers?</li>
<li>Can't we improve the predictive accuracy by the cross-validation and grid search technique?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we introduced some fundamental themes of DL. We started our journey with a basic but comprehensive introduction to ML. Then we gradually moved on to DL and different neural architectures. Then we got a brief overview of the most important DL frameworks. Finally, we saw some frequently asked questions related to deep <span><span>learning</span></span> and the Titanic survival prediction problem.</p>
<p>In the next chapter, we'll begin our journey into DL by solving the Titanic survival prediction problem using MLP. Then'll we start developing an end-to-end project for cancer type classification using a recurrent LSTM network. A very-high-dimensional gene expression dataset will be used for training and evaluating the model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Answers to FAQs</h1>
                </header>
            
            <article>
                
<p>Answer to question 1: There are many ways to solve this problem:</p>
<ol>
<li><em>A</em> âŠ• <em>B= (A âˆ¨ Â¬ B)âˆ¨ (Â¬ A âˆ§ B)</em></li>
<li><em>A </em>âŠ• <em>B = (A âˆ¨ B) âˆ§ Â¬(A âˆ¨ B)</em></li>
<li><em>A </em>âŠ• <em>B = (A âˆ¨ B) âˆ§ (Â¬ A âˆ¨ âˆ§ B)</em>, and so on</li>
</ol>
<p>If we go with the first approach, the resulting ANNs would look like this:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-767 image-border" src="assets/499ef0aa-0177-40a6-8bf8-8aa808cfef1a.png" style=""/></div>
<p>Now from computer science literature, we know that only two input combinations and one output are associated with the XOR operation. With inputs (0, 0) or (1, 1) the network outputs 0; and with inputs (0, 1) or (1, 0), it outputs 1. So we can formally represent the preceding truth table as follows:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p><strong>X0</strong></p>
</td>
<td>
<p><strong>X1</strong></p>
</td>
<td>
<p><strong>Y</strong></p>
</td>
</tr>
<tr>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Here, each pattern is classified into one of two classes that can be separated by a single line <em>L</em>. They are known as linearly separable patterns, as represented here:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-719 image-border" src="assets/19d184b5-45b8-431d-baa0-a389b9f93625.png" style=""/></div>
<p>Answer to question 2<span>:</span> The most significant progress in ANN and DL can be described in the following timeline. We have already seen how artificial neurons and perceptrons provided the base in 1943s and 1958s respectively. Then, XOR was formulated as a linearly non-separable problem in 1969 by Minsky et al. But later in 1974, Werbos et al. demonstrated the backpropagation algorithm for training the perceptron in 1974.</p>
<p class="mce-root">However, the most significant advancement happened in the 1980s, when John Hopfield et al. proposed the Hopfield Network in 1982. Then, <span>Hinton, </span>one of the godfathers of neural networks and deep learning, and his team proposed the Boltzmann machine in 1985. However, probably one of the most significant advances happened in 1986, when Hinton et al. successfully trained the MLP, and Jordan et. al. proposed RNNs. In the same year, Smolensky et al. also proposed an improved version of the RBM.</p>
<p>In the 1990s, the most significant year was 1997. Lecun et al. proposed LeNet in 1990, and Jordan et al. <span>proposed </span>RNN in 1997. In the same year, Schuster et al. proposed an improved version of LSTM and an improved version of the original RNN, called <strong>bidirectional RNN</strong>.</p>
<p>Despite significant advances in computing, from 1997 to 2005, we hadn't experienced much advancement, until Hinton struck again in 2006. He and his team proposed a DBN by stacking multiple RBMs. Then in 2012, again Hinton invented dropout, which significantly improved regularization and overfitting in a DNN.</p>
<p>After that, Ian Goodfellow et al. introduced GANs, a significant milestone in image recognition. In 2017, Hinton proposed CapsNets to overcome the limitations of regular CNNsâ€”so far one of the most significant milestones.</p>
<p><strong>Answer to question 3</strong><span>:</span> Yes, you can use other deep learning frameworks described in the <em>Deep learning frameworks</em> section. However, since this book is about using Java for deep learning, I would suggest going for DeepLearning4J. We will see how flexibly we can create networks by stacking input, hidden, and output layers using DeepLearning4J in the next chapter.</p>
<p><strong>Answer to question 4</strong><span>: </span>Yes, you can, since the passenger's name containing a different title (for example, Mr., Mrs., Miss, Master, and so on) could be significant too. For example, we can imagine that being a woman (that is, Mrs.) and being a junior (for example, Master.) could give a higher chance of survival.</p>
<p>Even, after watching the famous movie Titanic (1997), we can imagine that being in a relationship, a girl might have a good chance of survival since his boyfriend would try to save her! Anyway, this is just for imagination, so do not take it seriously. Now, we can write a user-defined function to encode this using Apache Spark. Let's take a look at the following UDF in Java:</p>
<pre><strong>private static</strong> <strong>final UDF1&lt;String, Option&lt;String&gt;&gt;</strong> getTitle = (<strong>String</strong> name) -&gt;      {<br/>    <strong>if</strong>(name.contains("Mr.")) { // If it has Mr.<br/>        <strong>return</strong> Some.apply("Mr.");<br/>    } <strong>else if</strong>(name.contains("Mrs.")) { // Or if has Mrs.<br/>        <strong>return</strong> Some.apply("Mrs.");<br/>    } <strong>else if</strong>(name.contains("Miss.")) { // Or if has Miss.<br/>        <strong>return</strong> Some.apply("Miss.");<br/>    } <strong>else if</strong>(name.contains("Master.")) { // Or if has Master.<br/>        <strong>return</strong> Some.apply("Master.");<br/>    } <strong>else</strong>{ // Not any.<br/>        <strong>return</strong> Some.apply("Untitled");<br/>    }<br/>};</pre>
<p>Next, we can register the UDF. Then I had to register the preceding UDF as follows:</p>
<pre>spark.sqlContext().<strong>udf()</strong>.register("getTitle", getTitle, DataTypes.StringType);<br/><br/><strong>Dataset&lt;Row&gt;</strong> categoricalDF = df.select(callUDF("getTitle", col("Name")).alias("Name"), col("Sex"), <br/>                                       col("Ticket"), col("Cabin"), col("Embarked"));<br/>categoricalDF.show();</pre>
<p>The resulting column would look like this:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f2b16249-8d6f-45ec-9709-5f2b1f17060c.png" style=""/></div>
<p><strong>Answer to question 5<span>:</span></strong> For many problems, you can start with just one or two hidden layers. This setting will work just fine using two hidden layers with the same total number of neurons (continue reading to get an idea about a number of neurons) in roughly the same amount of training time. Now let's see some naÃ¯ve estimation about setting the number of hidden layers:</p>
<ul>
<li><strong>0</strong>: Only capable of representing linear separable functions</li>
<li><strong>1</strong>: Can approximate any function that contains a continuous mapping from one finite space to another</li>
<li><strong>2</strong>: Can represent an arbitrary decision boundary to arbitrary accuracy</li>
</ul>
<p>However, for a more complex problem, you can gradually ramp up the number of hidden layers, until you start overfitting the training set. Nevertheless, you can try increasing the number of neurons gradually until the network starts overfitting. This means the upper bound on the number of hidden neurons that will not result in overfitting is:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/9b39b8c6-77fd-459a-83c9-27b442708c37.png" style="width:13.50em;height:3.42em;"/></div>
<p><span class="mi">In the preceding equation:</span></p>
<ul>
<li><span class="mi"><em>N<sub>i</sub></em></span> = number of input neurons</li>
<li><span class="mi"><em>N<sub>o</sub></em></span> = number of output neurons</li>
<li><span class="mi"><em>N<sub>s</sub></em></span> = number of samples in training dataset</li>
<li><span class="mi"><em>Î±</em></span> = an arbitrary scaling factor, usually <em>2-10</em></li>
</ul>
<p>Note that the preceding equation does not come from any research but from my personal working experience.</p>
<p><strong>Answer to question 6<span>:</span></strong> Of course, we can. We can cross-validate the training and create a grid search technique for finding the best hyperparameters. Let's give it a try.</p>
<p>First, we have the layers defined. Unfortunately, we cannot cross-validate layers. Probably, it's either a bug or made intentionally by the Spark guys. So we stick to a single layering:</p>
<pre><strong>int</strong>[] layers = <strong>new </strong><strong>int</strong>[] {10, 16, 16, 2};</pre>
<p>Then we create the trainer and set only the layer and seed parameters:</p>
<pre><strong>MultilayerPerceptronClassifier</strong> mlp = <strong>new</strong> MultilayerPerceptronClassifier()<br/>                     .setLayers(layers)<br/>                     .setSeed(1234L);</pre>
<p>We search through the MLP's different hyperparameters for the best model:</p>
<pre><strong>ParamMap[]</strong> paramGrid = <strong>new</strong> ParamGridBuilder() <br/>                    .addGrid(mlp.blockSize(), <strong>new</strong> <strong>int</strong>[] {32, 64, 128})<br/>                    .addGrid(mlp.maxIter(), <strong>new</strong> <strong>int</strong>[] {10, 50})<br/>                    .addGrid(mlp.tol(), <strong>new</strong> <strong>double</strong>[] {1E-2, 1E-4, 1E-6})<br/>                    .build();<br/><strong>MulticlassClassificationEvaluator</strong> evaluator = <strong>new</strong> MulticlassClassificationEvaluator()<br/>          .setLabelCol("label")<br/>          .setPredictionCol("prediction");</pre>
<p>We then set up the cross-validator and perform 10-fold cross-validation:</p>
<pre><strong>int</strong> numFolds = 10;<br/><strong>CrossValidator</strong> crossval = <strong>new</strong> CrossValidator()<br/>          .setEstimator(mlp)<br/>          .setEvaluator(evaluator)<br/>          .setEstimatorParamMaps(paramGrid)<br/>          .setNumFolds(numFolds);</pre>
<p>Then we perform training using the cross-validated model:</p>
<pre><strong>CrossValidatorModel</strong> cvModel = crossval.fit(trainingData);</pre>
<p>Finally, we evaluate the cross-validated model on the test set, as follows:</p>
<pre><strong>Dataset&lt;Row&gt;</strong> predictions = cvModel.transform(validationData);</pre>
<p>Now we can compute and show the performance metrics, similar to our previous example:</p>
<pre><strong>double</strong> accuracy = evaluator1.evaluate(predictions);<br/><strong>double</strong> precision = evaluator2.evaluate(predictions);<br/><strong>double</strong> recall = evaluator3.evaluate(predictions);<br/><strong>double</strong> f1 = evaluator4.evaluate(predictions);<br/><br/>// Print the performance metrics<br/>System.<strong>out</strong>.println("Accuracy = " + accuracy);<br/>System.<strong>out</strong>.println("Precision = " + precision);<br/>System.<strong>out</strong>.println("Recall = " + recall);<br/>System.<strong>out</strong>.println("F1 = " + f1);<br/>System.<strong>out</strong>.println("Test Error = " + (1 - accuracy));<strong> <br/></strong></pre>
<pre><strong>&gt;&gt;&gt;</strong><q><strong><br/></strong></q><strong>A<span class="packt_screen">ccuracy = 0.7810132575757576<br/> Precision = 0.7810132575757576<br/> Recall = 0.7810132575757576<br/> F1 = 0.7810132575757576<br/> Test Error = 0.21898674242424243</span></strong></pre>


            </article>

            
        </section>
    </body></html>