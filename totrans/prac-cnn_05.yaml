- en: Transfer Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned that a CNN consists of several layers. We
    also studied different CNN architectures, tuned different hyperparameters, and
    identified values for stride, window size, and padding. Then we chose a correct
    loss function and optimized it. We trained this architecture with a large volume
    of images. So, the question here is, how do we make use of this knowledge with
    a different dataset? Instead of building a CNN architecture and training it from
    scratch, it is possible to take an existing pre-trained network and adapt it to
    a new and different dataset through a technique called **transfer learning**. We
    can do so through feature extraction and fine tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning is the process of copying knowledge from an already trained
    network to a new network to solve similar problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer learning example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-task learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature extraction approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a feature extraction approach, we train only the top level of the network;
    the rest of the network remains fixed. Consider a feature extraction approach
    when the new dataset is relatively small and similar to the original dataset.
    In such cases, the higher-level features learned from the original dataset should
    transfer well to the new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a fine-tuning approach when the new dataset is large and similar to
    the original dataset. Altering the original weights should be safe because the
    network is unlikely to overfit the new, large dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us consider a pre-trained convolutional neural network, as shown in the
    following diagram. Using this we can study how the transfer of knowledge can be
    used in different situations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/688a6ad3-849b-42f7-b371-4523211bfcbc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When should we use transfer learning? Transfer learning can be applied in the
    following situations, depending on:'
  prefs: []
  type: TYPE_NORMAL
- en: The size of the new (target) dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarity between the original and target datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are four main use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 1**: New (target) dataset is small and is similar to the original training
    dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Case 2**: New (target) dataset is small but is different from the original
    training dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Case 3**: New (target) dataset is large and is similar to the original training
    dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Case 4**:New (target) dataset is large and is different from the original
    training dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let us now walk through each case in detail in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Target dataset is small and is similar to the original training dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If the target dataset is small and similar to the original dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, replace the last fully connected layer with a new fully connected
    layer that matches with the number of classes of the target dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initialize old weights with randomized weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Train the network to update the weights of the new, fully connected layer:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/daa1d1de-8117-4897-9afd-94588783b1d9.png)'
  prefs: []
  type: TYPE_IMG
- en: Transfer learning can be used as a strategy to avoid overfitting, especially
    when there is a small dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Target dataset is small but different from the original training dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If the target dataset is small but of a different type to the original – for
    example, the original dataset is dog images and the new (target) dataset is flower
    images – then do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Slice most of the initial layers of the network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add to the remaining pre-trained layers a new fully connected layer that matches
    the number of classes of the target dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomize the weights of the new fully connected layer and freeze all the weights
    from the pre-trained network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train the network to update the weights of the new fully connected layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since the dataset is small, overfitting is still a concern here as well. To
    overcome this, we will keep the weights of the original pre-trained network the
    same and update only the weights of the new fully connected layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f7a1081a-47f5-4e46-937c-c84bf035dc03.png)'
  prefs: []
  type: TYPE_IMG
- en: Only fine tune the higher level portion of the network. This is because the
    beginning layers are designed to extract more generic features. In general, the
    first layer of a convolutional neural network is not specific to a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Target dataset is large and similar to the original training dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here we do not have an overfitting concern, as the dataset is large. So, in
    this case, we can retrain the entire network:'
  prefs: []
  type: TYPE_NORMAL
- en: Remove the last fully connected layer and replace it with a fully connected
    layer that matches the number of classes in the target dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Randomly initialize the weights of this newly added, fully connected layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initialize the rest of the weights with pre-trained weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Train the entire network:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/2df8e25a-0f59-43c8-85f6-4a9486cf6aa7.png)'
  prefs: []
  type: TYPE_IMG
- en: Target dataset is large and different from the original training dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If the target dataset is large and different from the original:'
  prefs: []
  type: TYPE_NORMAL
- en: Remove the last fully connected layer and replace it with a fully connected
    layer that matches the number of classes in the target dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Train the entire network from scratch with randomly initialized weights:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/76604751-43f7-4ae8-875d-66e44fa19053.png)'
  prefs: []
  type: TYPE_IMG
- en: The `Caffe` library has ModelZoo, where one can share network weights.
  prefs: []
  type: TYPE_NORMAL
- en: Consider training from scratch when the dataset is large and completely different
    from the original dataset. In this case, we have enough data to train from scratch
    without the fear of overfitting. However, even in this case, it might be beneficial
    to initialize the entire network with pre-trained weights and fine tune it on
    the new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we will take a pre-trained VGGNet and use transfer learning
    to train a CNN classifier that predicts dog breeds, given a dog image. Keras contains
    many pre-trained models, along with the code that loads and visualizes them. Another
    is a flower dataset that can be downloaded here. The Dog breed dataset has 133
    dog breed categories and 8,351 dog images. Download the Dog breed dataset here
    and copy it to your folder. VGGNet has 16 convolutional with pooling layers from
    beginning to end and three fully connected layers followed by a `softmax` function.
    Its main objective was to show how the depth of the network gives the best performance.
    It came from **Visual Geometric Group** (**VGG**) at Oxford. Their best performing
    network is VGG16\. The Dog breed dataset is relatively small and has a little
    overlap with the `imageNet` dataset. So, we can remove the last fully connected
    layer after the convolutional layer and replace it with our own. The weights of
    the convolutional layer are kept constant. An input image is passed through the
    convolutional layer and stops at the 16th layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6d97faf-9d6c-49ee-b829-2de1dd1da654.png)'
  prefs: []
  type: TYPE_IMG
- en: VGGNet Architecture
  prefs: []
  type: TYPE_NORMAL
- en: We will use the bottleneck features of a pre-trained VGG16 network – such a
    network has already learned features from the `imageNet` dataset. Because the `imageNet`
    dataset already contains a few images of dogs, the VGG16 network model has already
    learned key features for classification. Similarly, other pre-trained CNN architectures
    can also be considered as an exercise to solve other image classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the `bottleneck_features` of VGG16 here, copy it to your own folder,
    and load it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now define the model architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile the model and train it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the model and calculate the classification accuracy on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Multi-task learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In multi-task learning, transfer learning happens to be from one pre-trained
    model to many tasks simultaneously. For example, in self-driving cars, the deep
    neural network detects traffic signs, pedestrians, and other cars in front at
    the same time. Speech recognition also benefits from multi-task learning.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a few specific cases, convolutional neural network architectures trained
    on images allow us to reuse learned features in a new network. The performance
    benefits of transferring features decrease the more dissimilar the base task and
    target task are. It is surprising to know that initializing a convolutional neural
    network with transferred features from almost any number of layers can produce
    a boost to generalization performance after fine-tuning to a new dataset.
  prefs: []
  type: TYPE_NORMAL
