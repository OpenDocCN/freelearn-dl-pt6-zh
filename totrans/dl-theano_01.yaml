- en: Chapter 1. Theano Basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter presents Theano as a compute engine and the basics for symbolic
    computing with Theano. Symbolic computing consists of building graphs of operations
    that will be optimized later on for a specific architecture, using the computation
    libraries available for this architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Although this chapter might appear to be a long way from practical applications,
    it is essential to have an understanding of the technology for the following chapters;
    what is it capable of and what value does it bring? All the following chapters
    address the applications of Theano when building all possible deep learning architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Theano may be defined as a library for scientific computing; it has been available
    since 2007 and is particularly suited to deep learning. Two important features
    are at the core of any deep learning library: tensor operations, and the capability
    to run the code on CPU or **Graphical Computation Unit** (**GPU**). These two
    features enable us to work with a massive amount of multi-dimensional data. Moreover,
    Theano proposes automatic differentiation, a very useful feature that can solve
    a wider range of numeric optimizations than deep learning problems.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Theano installation and loading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tensors and algebra
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Symbolic programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic differentiation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPU programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profiling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The need for tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Usually, input data is represented with multi-dimensional arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Images have three dimensions**: The number of channels, the width, and the
    height of the image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sounds and times series have one dimension**: The duration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Natural language sequences can be represented by two-dimensional arrays**:
    The duration and the alphabet length or the vocabulary length'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll see more examples of input data arrays in the future chapters.
  prefs: []
  type: TYPE_NORMAL
- en: In Theano, multi-dimensional arrays are implemented with an abstraction class,
    named **tensor**, with many more transformations available than traditional arrays
    in a computer language such as Python.
  prefs: []
  type: TYPE_NORMAL
- en: At each stage of a neural net, computations such as matrix multiplications involve
    multiple operations on these multi-dimensional arrays.
  prefs: []
  type: TYPE_NORMAL
- en: Classical arrays in programming languages do not have enough built-in functionalities
    to quickly and adequately address multi-dimensional computations and manipulations.
  prefs: []
  type: TYPE_NORMAL
- en: Computations on multi-dimensional arrays have a long history of optimizations,
    with tons of libraries and hardware. One of the most important gains in speed
    has been permitted by the massive parallel architecture of the GPU, with computation
    ability on a large number of cores, from a few hundred to a few thousand.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the traditional CPU, for example, a quadricore, 12-core, or 32-core
    engine, the gains with GPU can range from 5x to 100x, even if part of the code
    is still being executed on the CPU (data loading, GPU piloting, and result outputting).
    The main bottleneck with the use of GPU is usually the transfer of data between
    the memory of the CPU and the memory of the GPU, but still, when well programmed,
    the use of GPU helps bring a significant increase in speed of an order of magnitude.
    Getting results in days rather than months, or hours rather than days, is an undeniable
    benefit for experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: The Theano engine has been designed to address the challenges of multi-dimensional
    arrays and architecture abstraction from the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is another undeniable benefit of Theano for scientific computation: the
    automatic differentiation of functions of multi-dimensional arrays, a well-suited
    feature for model parameter inference via objective function minimization. Such
    a feature facilitates experimentation by releasing the pain to compute derivatives,
    which might not be very complicated, but are prone to many errors.'
  prefs: []
  type: TYPE_NORMAL
- en: Installing and loading Theano
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll install Theano, run it on the CPU and GPU devices, and
    save the configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Conda package and environment manager
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The easiest way to install Theano is to use `conda`, a cross-platform package
    and environment manager.
  prefs: []
  type: TYPE_NORMAL
- en: 'If `conda` is not already installed on your operating system, the fastest way
    to install `conda` is to download the `miniconda` installer from [https://conda.io/miniconda.html](https://conda.io/miniconda.html).
    For example, for `conda under Linux 64 bit and Python 2.7`, use this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Conda enables us to create new environments in which versions of Python (2 or
    3) and the installed packages may differ. The `conda` root environment uses the
    same version of Python as the version installed on the system on which you installed
    `conda`.
  prefs: []
  type: TYPE_NORMAL
- en: Installing and running Theano on CPU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s install Theano:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Run a Python session and try the following commands to check your configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The last command prints all the configuration of Theano. The `theano.config`
    object contains keys to many configuration options.
  prefs: []
  type: TYPE_NORMAL
- en: 'To infer the configuration options, Theano looks first at the `~/.theanorc`
    file, then at any environment variables that are available, which override the
    former options, and lastly at the variable set in the code that are first in order
    of precedence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Some of the properties might be read-only and cannot be changed in the code,
    but `floatX`, which sets the default floating point precision for floats, is among
    the properties that can be changed directly in the code.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is advised to use `float32` since GPU has a long history without `float64`.
    `float64` execution speed on GPU is slower, sometimes much slower (2x to 32x on
    latest generation Pascal hardware), and `float32` precision is enough in practice.
  prefs: []
  type: TYPE_NORMAL
- en: GPU drivers and libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Theano enables the use of GPU, units that are usually used to compute the graphics
    to display on the computer screen.
  prefs: []
  type: TYPE_NORMAL
- en: To have Theano work on the GPU as well, a GPU backend library is required on
    your system.
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA library (for NVIDIA GPU cards only) is the main choice for GPU computations.
    There is also the OpenCL standard, which is open source but far less developed,
    and much more experimental and rudimentary on Theano.
  prefs: []
  type: TYPE_NORMAL
- en: Most scientific computations still occur on NVIDIA cards at the moment. If you
    have an NVIDIA GPU card, download CUDA from the NVIDIA website, [https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads),
    and install it. The installer will install the latest version of the GPU drivers
    first, if they are not already installed. It will install the CUDA library in
    the `/usr/local/cuda` directory.
  prefs: []
  type: TYPE_NORMAL
- en: Install the cuDNN library, a library by NVIDIA, that offers faster implementations
    of some operations for the GPU. To install it, I usually copy the `/usr/local/cuda`
    directory to a new directory, `/usr/local/cuda-{CUDA_VERSION}-cudnn-{CUDNN_VERSION}`,
    so that I can choose the version of CUDA and cuDNN, depending on the deep learning
    technology I use and its compatibility.
  prefs: []
  type: TYPE_NORMAL
- en: 'In your .`bashrc` profile, add the following line to set the `$PATH` and `$LD_LIBRARY_PATH`
    variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Installing and running Theano on GPU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: N-dimensional GPU arrays have been implemented in Python in six different GPU
    libraries (`Theano/CudaNdarray,PyCUDA`/ `GPUArray,CUDAMAT`/ `CUDAMatrix`, `PYOPENCL`/`GPUArray`,
    `Clyther`, `Copperhead`), are a subset of `NumPy.ndarray`. `Libgpuarray` is a
    backend library to have them in a common interface with the same property.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install `libgpuarray` with `conda`, use this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To run Theano in GPU mode, you need to configure the `config.device` variable
    before execution since it is a read-only variable once the code is run. Run this
    command with the `THEANO_FLAGS` environment variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The first return shows that GPU device has been correctly detected, and specifies
    which GPU it uses.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, Theano activates CNMeM, a faster CUDA memory allocator. An initial
    pre-allocation can be specified with the `gpuarra.preallocate` option. At the
    end, my launch command will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The first line confirms that cuDNN is active, the second confirms memory pre-allocation.
    The third line gives the default **context name** (that is, `None` when `flag
    device=cuda` is set) and the model of GPU used, while the default context name
    for the CPU will always be `cpu`.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to specify a different GPU than the first one, setting the device
    to `cuda0`, `cuda1`,... for multi-GPU computers. It is also possible to run a
    program on multiple GPU in parallel or in sequence (when the memory of one GPU
    is not sufficient), in particular when training very deep neural nets, as for
    classification of full images as described in [Chapter 7](part0075_split_000.html#27GQ61-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 7. Classifying Images with Residual Networks"), *Classifying Images with
    Residual Networks*. In this case, the `contexts=dev0->cuda0;dev1->cuda1;dev2->cuda2;dev3->cuda3`
    flag activates multiple GPUs instead of one, and designates the context name to
    each GPU device to be used in the code. Here is an example on a 4-GPU instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: To assign computations to a specific GPU in this multi-GPU setting, the names
    we choose, `dev0`, `dev1`, `dev2`, and `dev3`, have been mapped to each device
    (`cuda0`, `cuda1`, `cuda2`, `cuda3`).
  prefs: []
  type: TYPE_NORMAL
- en: This name mapping enables to write codes that are independent of the underlying
    GPU assignments and libraries (CUDA or others).
  prefs: []
  type: TYPE_NORMAL
- en: 'To keep the current configuration flags active at every Python session or execution
    without using environment variables, save your configuration in the `~/.theanorc`
    file as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now you can simply run `python` command. You are now all set.
  prefs: []
  type: TYPE_NORMAL
- en: Tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Python, some scientific libraries such as NumPy provide multi-dimensional
    arrays. Theano doesn't replace Numpy, but it works in concert with it. NumPy is
    used for the initialization of tensors.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform the same computation on CPU and GPU, variables are symbolic and
    represented by the tensor class, an abstraction, and writing numerical expressions
    consists of building a computation graph of variable nodes and apply nodes. Depending
    on the platform on which the computation graph will be compiled, tensors are replaced
    by either of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A `TensorType` variable, which has to be on CPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `GpuArrayType` variable, which has to be on GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That way, the code can be written indifferently of the platform where it will
    be executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few tensor objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Object class | Number of dimensions | Example |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `theano.tensor.scalar` | 0-dimensional array | 1, 2.5 |'
  prefs: []
  type: TYPE_TB
- en: '| `theano.tensor.vector` | 1-dimensional array | [0,3,20] |'
  prefs: []
  type: TYPE_TB
- en: '| `theano.tensor.matrix` | 2-dimensional array | [[2,3][1,5]] |'
  prefs: []
  type: TYPE_TB
- en: '| `theano.tensor.tensor3` | 3-dimensional array | [[[2,3][1,5]],[[1,2],[3,4]]]
    |'
  prefs: []
  type: TYPE_TB
- en: 'Playing with these Theano objects in the Python shell gives us a better idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'With `i`, `l`, `f`, or `d` in front of the object name, you initiate a tensor
    of a given type, `integer32`, `integer64`, `float32`, or `float64`. For real-valued
    (floating point) data, it is advised to use the direct form `T.scalar()` instead
    of the `f` or `d` variants since the direct form will use your current configuration
    for floats:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Symbolic variables do either of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Play the role of placeholders, as a starting point to build your graph of numerical
    operations (such as addition, multiplication): they receive the flow of the incoming
    data during the evaluation once the graph has been compiled'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Represent intermediate or output results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Symbolic variables and operations are both part of a computation graph that
    will be compiled either on CPU or GPU for fast execution. Let''s write our first
    computation graph consisting of a simple addition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: First, two symbolic variables, or *variable nodes*, are created, with the names
    `x` and `y`, and an addition operation, an *apply node*, is applied between both
    of them to create a new symbolic variable, `z`, in the computation graph.
  prefs: []
  type: TYPE_NORMAL
- en: The pretty print function, `pp`, prints the expression represented by Theano
    symbolic variables. `Eval` evaluates the value of the output variable, `z`, when
    the first two variables, `x` and `y`, are initialized with two numerical 2-dimensional
    arrays.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows the difference between the variables `x` and `y`,
    and their names `x` and `y`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Without names, it is more complicated to trace the nodes in a large graph.
    When printing the computation graph, names significantly help diagnose problems,
    while variables are only used to handle the objects in the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Here, the original symbolic variable, named `x`, does not change and stays part
    of the computation graph. `x + x` creates a new symbolic variable we assign to
    the Python variable `x`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note also that with the names, the plural form initializes multiple tensors
    at the same time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's have a look at the different functions to display the graph.
  prefs: []
  type: TYPE_NORMAL
- en: Graphs and symbolic computing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take back the simple addition example and present different ways to
    display the same information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `debugprint` function prints the pre-compilation graph, the unoptimized
    graph. In this case, it is composed of two variable nodes, `x` and `y`, and an
    apply node, the elementwise addition, with the `no_inplace` option. The `inplace`
    option will be used in the optimized graph to save memory and re-use the memory
    of the input to store the result of the operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the `graphviz` and `pydot` libraries have been installed, the `pydotprint`
    command outputs a PNG image of the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![Graphs and symbolic computing](img/00002.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: You might have noticed that the `z.eval` command takes while to execute the
    first time. The reason for this delay is the time required to optimize the mathematical
    expression and compile the code for the CPU or GPU before being evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The compiled expression can be obtained explicitly and used as a function that
    behaves as a traditional Python function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The first argument in the function creation is a list of variables representing
    the input nodes of the graph. The second argument is the array of output variables.
    To print the post compilation graph, use this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![Graphs and symbolic computing](img/00003.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This case has been printed while using the GPU. During compilation, each operation
    has chosen the available GPU implementation. The main program still runs on CPU,
    where the data resides, but a `GpuFromHost` instruction performs a data transfer
    from the CPU to the GPU for input, while the opposite operation, `HostFromGpu`,
    fetches the result for the main program to display it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphs and symbolic computing](img/00004.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Theano performs some mathematical optimizations, such as grouping elementwise
    operations, adding a new value to the previous addition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The number of nodes in the graph has not increased: two additions have been
    merged into one node. Such optimizations make it more tricky to debug, so we''ll
    show you at the end of this chapter how to disable optimizations for debugging.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, let''s see a bit more about setting the initial value with NumPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing the function on the NumPy arrays throws an error related to loss
    of precision, since the NumPy arrays here have `float64` and `int64` `dtypes`,
    but `x` and `y` are `float32`. There are multiple solutions to this; the first
    is to create the NumPy arrays with the right `dtype`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, cast the NumPy arrays (in particular for `numpy.diag`, which
    does not allow us to choose the `dtype` directly):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Or we could allow downcasting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Operations on tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen how to create a computation graph composed of symbolic variables
    and operations, and compile the resulting expression for an evaluation or as a
    function, either on GPU or on CPU.
  prefs: []
  type: TYPE_NORMAL
- en: As tensors are very important to deep learning, Theano provides lots of operators
    to work with tensors. Most operators that exist in scientific computing libraries
    such as NumPy for numerical arrays have their equivalent in Theano and have a
    similar name, in order to be more familiar to NumPy's users. But contrary to NumPy,
    expressions written with Theano can be compiled either on CPU or GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'This, for example, is the case for tensor creation:'
  prefs: []
  type: TYPE_NORMAL
- en: '`T.zeros()`, `T.ones()`, `T.eye()` operators take a shape tuple as input'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`T.zeros_like()`, `T.one_like()`, `T.identity_like()` use the shape of the
    tensor argument'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`T.arange()`, `T.mgrid()`, `T.ogrid()` are used for range and mesh grid arrays'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s have a look in the Python shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Information such as the number of dimensions, `ndim`, and the type, `dtype`,
    are defined at tensor creation and cannot be modified later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Some other information, such as shape, is evaluated by the computation graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Dimension manipulation operators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first type of operator on tensor is for **dimension manipulation**. This
    type of operator takes a tensor as input and returns a new tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Operator | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `T.reshape` | Reshape the dimension of the tensor |'
  prefs: []
  type: TYPE_TB
- en: '| `T.fill` | Fill the array with the same value |'
  prefs: []
  type: TYPE_TB
- en: '| `T.flatten` | Return all elements in a 1-dimensional tensor (vector) |'
  prefs: []
  type: TYPE_TB
- en: '| `T.dimshuffle` | Change the order of the dimension, more or less like NumPy''s
    transpose method – the main difference is that it can be used to add or remove
    broadcastable dimensions (of length 1). |'
  prefs: []
  type: TYPE_TB
- en: '| `T.squeeze` | Reshape by removing dimensions equal to 1 |'
  prefs: []
  type: TYPE_TB
- en: '| `T.transpose` | Transpose |'
  prefs: []
  type: TYPE_TB
- en: '| `T.swapaxes` | Swap dimensions |'
  prefs: []
  type: TYPE_TB
- en: '| `T.sort, T.argsort` | Sort tensor, or indices of the order |'
  prefs: []
  type: TYPE_TB
- en: 'For example, the reshape operation''s output represents a new tensor, containing
    the same elements in the same order but in a different shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The operators can be chained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Notice the use of traditional `[::-1]` array access by indices in Python and
    the `.T` for `T.transpose`.
  prefs: []
  type: TYPE_NORMAL
- en: Elementwise operators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second type of operations on multi-dimensional arrays is elementwise operators.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first category of elementwise operations takes two input tensors of the
    same dimensions and applies a function, `f`, elementwise, which means on all pairs
    of elements with the same coordinates in the respective tensors `f([a,b],[c,d])
    = [ f(a,c), f(b,d)]`. For example, here''s multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The same multiplication can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '`T.add` and `T.mul` accept an arbitrary number of inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Some elementwise operators accept only one input tensor `f([a,b]) = [f(a),f(b)])`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, I would like to introduce the mechanism of **broadcasting**. When the
    input tensors do not have the same number of dimensions, the missing dimension
    will be broadcasted, meaning the tensor will be repeated along that dimension
    to match the dimension of the other tensor. For example, taking one multi-dimensional
    tensor and a scalar (0-dimensional) tensor, the scalar will be repeated in an
    array of the same shape as the multi-dimensional tensor so that the final shapes
    will match and the elementwise operation will be applied, `f([a,b], c) = [ f(a,c),
    f(b,c) ]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a list of elementwise operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Operator | Other form | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `T.add, T.sub, T.mul, T.truediv` | `+, -, *, /` | Add, subtract, multiply,
    divide |'
  prefs: []
  type: TYPE_TB
- en: '| `T.pow, T.sqrt` | `**, T.sqrt` | Power, square root |'
  prefs: []
  type: TYPE_TB
- en: '| `T.exp, T.log` |   | Exponential, logarithm |'
  prefs: []
  type: TYPE_TB
- en: '| `T.cos, T.sin, T.tan` |   | Cosine, sine, tangent |'
  prefs: []
  type: TYPE_TB
- en: '| `T.cosh, T.sinh, T.tanh` |   | Hyperbolic trigonometric functions |'
  prefs: []
  type: TYPE_TB
- en: '| `T.intdiv, T.mod` | `//, %` | Int div, modulus |'
  prefs: []
  type: TYPE_TB
- en: '| `T.floor, T.ceil, T.round` |   | Rounding operators |'
  prefs: []
  type: TYPE_TB
- en: '| `T.sgn` |   | Sign |'
  prefs: []
  type: TYPE_TB
- en: '| `T.and_, T.xor, T.or_, T.invert` | `&,^,&#124;,~` | Bitwise operators |'
  prefs: []
  type: TYPE_TB
- en: '| `T.gt, T.lt, T.ge, T.le` | `>, <, >=, <=` | Comparison operators |'
  prefs: []
  type: TYPE_TB
- en: '| `T.eq, T.neq, T.isclose` |   | Equality, inequality, or close with tolerance
    |'
  prefs: []
  type: TYPE_TB
- en: '| `T.isnan` |   | Comparison with NaN (not a number) |'
  prefs: []
  type: TYPE_TB
- en: '| `T.abs_` |   | Absolute value |'
  prefs: []
  type: TYPE_TB
- en: '| `T.minimum, T.maximum` |   | Minimum and maximum elementwise |'
  prefs: []
  type: TYPE_TB
- en: '| `T.clip` |   | Clip the values between a maximum and a minimum |'
  prefs: []
  type: TYPE_TB
- en: '| `T.switch` |   | Switch |'
  prefs: []
  type: TYPE_TB
- en: '| `T.cast` |   | Tensor type casting |'
  prefs: []
  type: TYPE_TB
- en: The elementwise operators always return an array with the same size as the input
    array. `T.switch` and `T.clip` accept three inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, `T.switch` will perform the traditional `switch` operator elementwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: At the same position where `cond` tensor is true, the result has the `x` value;
    otherwise, if it is false, it has the `y` value.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the `T.switch` operator, there is a specific equivalent, `ifelse`, that
    takes a scalar condition instead of a tensor condition. It is not an elementwise
    operation though, and supports lazy evaluation (not all elements are computed
    if the answer is known before it finishes):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Reduction operators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another type of operation on tensors is reductions, reducing all elements to
    a scalar value in most cases, and for that purpose, it is required to scan all
    the elements of the tensor to compute the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Operator | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `T.max, T.argmax, T.max_and_argmax` | Maximum, index of the maximum |'
  prefs: []
  type: TYPE_TB
- en: '| `T.min, T.argmin` | Minimum, index of the minimum |'
  prefs: []
  type: TYPE_TB
- en: '| `T.sum, T.prod` | Sum or product of elements |'
  prefs: []
  type: TYPE_TB
- en: '| `T.mean, T.var, T.std` | Mean, variance, and standard deviation |'
  prefs: []
  type: TYPE_TB
- en: '| `T.all, T.any` | AND and OR operations with all elements |'
  prefs: []
  type: TYPE_TB
- en: '| `T.ptp` | Range of elements (minimum, maximum) |'
  prefs: []
  type: TYPE_TB
- en: 'These operations are also available row-wise or column-wise by specifying an
    axis and the dimension along which the reduction is performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Linear algebra operators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A third category of operations are the linear algebra operators, such as matrix
    multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear algebra operators](img/00005.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Also called inner product for vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear algebra operators](img/00006.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '| Operator | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `T.dot` | Matrix multiplication/inner product |'
  prefs: []
  type: TYPE_TB
- en: '| `T.outer` | Outer product |'
  prefs: []
  type: TYPE_TB
- en: There are some generalized (`T.tensordot` to specify the axis), or batched (`batched_dot,
    batched_tensordot`) versions of the operators.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, a few operators remain and can be very useful, but they do not belong
    to any of the previous categories: `T.concatenate` concatenates the tensors along
    the specified dimension, `T.stack` creates a new dimension to stack the input
    tensors, and `T.stacklist` creates new patterns to stack tensors together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'An equivalent of the NumPy expressions `a[5:] = 5` and `a[5:] += 5` exists
    as two functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Unlike NumPy's syntax, the original tensor is not modified; instead, a new variable
    is created that represents the result of that modification. Therefore, the original
    variable `a` still refers to the original value, and the returned variable (here
    unassigned) represents the updated one, and the user should use that new variable
    in the rest of their computation.
  prefs: []
  type: TYPE_NORMAL
- en: Memory and variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is good practice to always cast float arrays to the `theano.config.floatX`
    type:'
  prefs: []
  type: TYPE_NORMAL
- en: Either at the array creation with `numpy.array(array, dtype=theano.config.floatX)`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Or by casting the array as `array.as_type(theano.config.floatX)` so that when
    compiling on the GPU, the correct type is used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, let''s transfer the data manually to the GPU (for which the default
    context is None), and for that purpose, we need to use `float32` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The `transfer(device)` functions, such as `transfer(''cpu'')`, enable us to
    move the data from one device to another one. It is particularly useful when parts
    of the graph have to be executed on different devices. Otherwise, Theano adds
    the transfer functions automatically to the GPU in the optimization phase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the transfer function explicitly, Theano removes the transfer back to
    CPU. Leaving the output tensor on the GPU saves a costly transfer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The default context for the CPU is `cpu`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'A hybrid concept between numerical values and symbolic variables is the shared
    variables. They can also lead to better performance on the GPU by avoiding transfers.
    Initializing a shared variable with the scalar zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Shared values are designed to be shared between functions. They can also be
    seen as an internal state. They can be used indifferently from the GPU or the
    CPU compile code. By default, shared variables are created on the default device
    (here, `cuda`), except for scalar integer values (as is the case in the previous
    example).
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to specify another context, such as `cpu`. In the case of multiple
    GPU instances, you''ll define your contexts in the Python command line, and decide
    on which context to create the shared variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Functions and automatic differentiation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous section introduced the `function` instruction to compile the expression.
    In this section, we develop some of the following arguments in its signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: We've already used the `allow_input_downcast` feature to convert data from `float64`
    to `float32`, `int64` to `int32` and so on. The `mode` and `profile` features
    are also displayed because they'll be presented in the optimization and debugging
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Input variables of a Theano function should be contained in a list, even when
    there is a single input.
  prefs: []
  type: TYPE_NORMAL
- en: 'For outputs, it is possible to use a list in the case of multiple outputs to
    be computed in parallel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The second useful attribute is the `updates` attribute, used to set new values
    to shared variables once the expression has been evaluated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Such a mechanism can be used as an internal state. The shared variable `w` has
    been defined outside the function.
  prefs: []
  type: TYPE_NORMAL
- en: With the `givens` parameter, it is possible to change the value of any symbolic
    variable in the graph, without changing the graph. The new value will then be
    used by all the other expressions that were pointing to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last and most important feature in Theano is the automatic differentiation,
    which means that Theano computes the derivatives of all previous tensor operators.
    Such a differentiation is performed via the `theano.grad` operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '![Functions and automatic differentiation](img/00007.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the optimization graph, `theano.grad` has computed the gradient of ![Functions
    and automatic differentiation](img/00008.jpeg) with respect to `a`, which is a
    symbolic expression equivalent to *2 * a*.
  prefs: []
  type: TYPE_NORMAL
- en: Note that it is only possible to take the gradient of a scalar, but the *wrt*
    variables can be arbitrary tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Loops in symbolic computing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Python `for` loop can be used outside the symbolic graph, as in a normal
    Python program. But outside the graph, a traditional Python `for` loop isn't compiled,
    so it will not be optimized with parallel and algebra libraries, cannot be automatically
    differentiated, and introduces costly data transfers if the computation subgraph
    has been optimized for GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'That''s why a symbolic operator, `T.scan`, is designed to create a `for` loop
    as an operator inside the graph. Theano will unroll the loop into the graph structure
    and the whole unrolled loop is going to be compiled on the target architecture
    as the rest of the computation graph. Its signature is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The `scan` operator is very useful to implement array loops, reductions, maps,
    multi-dimensional derivatives such as Jacobian or Hessian, and recurrences.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `scan` operator is running the `fn` function repeatedly for `n_steps`.
    If `n_steps` is `None`, the operator will find out by the length of the sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The step `fn` function is a function that builds a symbolic graph, and that
    function will only get called once. However, that graph will then be compiled
    into another Theano function that will be called repeatedly. Some users try to
    pass a compile Theano function as `fn`, which is not possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sequences are the lists of input variables to loop over. The number of steps
    will correspond to the shortest sequence in the list. Let''s have a look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The `scan` operator has been running the function against all elements in the
    input tensor, `a`, and kept the same shape as the input tensor, `(2,3)`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is a good practice to add the updates returned by `theano.scan` in the `theano.function`,
    even if these updates are empty.
  prefs: []
  type: TYPE_NORMAL
- en: 'The arguments given to the `fn` function can be much more complicated. `T.scan`
    will call the `fn` function at each step with the following argument list, in
    the following order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the following figure, three arrows are directed towards the `fn`
    step function and represent the three types of possible input at each time step
    in the loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loops in symbolic computing](img/00009.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: If specified, the `outputs_info` parameter is the initial state to use to start
    recurrence from. The parameter name does not sound very good, but the initial
    state also gives the shape information of the last state, as well as all other
    states. The initial state can be seen as the first output. The final output will
    be an array of states.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to compute the cumulative sum in a vector, with an initial state
    of the sum at `0`, use this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: When `outputs_info` is set, the first dimension of the `outputs_info` and sequence
    variables is the time step. The second dimension is the dimensionality of data
    at each time step.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, `outputs_info` has the number of previous time-steps required
    to compute the first step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the same example, but with a vector at each time step instead of a
    scalar for the input data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Twenty steps along the rows (times) have accumulated the sum of all elements.
    Note that initial state (here `0`) given by the `outputs_info` argument is not
    part of the output sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The recurrent function, `fn`, may be provided with some fixed data, independent
    of the step in the loop, thanks to the `non_sequences` scan parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: It is multiplying the prior value by `5` and adding the new element.
  prefs: []
  type: TYPE_NORMAL
- en: Note that `T.scan` in the optimized graph on GPU does not execute different
    iterations of the loop in parallel, even in the absence of recurrence.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration, profiling and debugging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For debugging purpose, Theano can print more verbose information and offers
    different optimization modes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'In order for Theano to use the `config.optimizer` value, the mode has to be
    set to `Mode`, otherwise the value in `config.mode` will be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '| config.mode / function mode | config.optimizer (*) | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `FAST_RUN` | `fast_run` | Default; best run performance, slow compilation
    |'
  prefs: []
  type: TYPE_TB
- en: '| `FAST_RUN` | `None` | Disable optimizations |'
  prefs: []
  type: TYPE_TB
- en: '| `FAST_COMPILE` | `fast_compile` | Reduce the number of optimizations, compiles
    faster |'
  prefs: []
  type: TYPE_TB
- en: '| `None` |   | Use the default mode, equivalent to `FAST_RUN`; `optimizer=None`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `NanGuardMode` |   | NaNs, Infs, and abnormally big value will raise errors
    |'
  prefs: []
  type: TYPE_TB
- en: '| `DebugMode` |   | Self-checks and assertions during compilation |'
  prefs: []
  type: TYPE_TB
- en: 'The same parameter as in `config.mode` can be used in the `Mode` parameter
    in the function compile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Disabling optimization and choosing high verbosity will help finding errors
    in the computation graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'For debugging on the GPU, you need to set a synchronous execution with the
    environment variable `CUDA_LAUNCH_BLOCKING`, since GPU execution is by default,
    fully asynchronous:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: To find out the origin of the latencies in your computation graph, Theano provides
    a profiling mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activate profiling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Activate memory profiling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Activate profiling of optimization phase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Or directly during compilation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first concept is symbolic computing, which consists in building graph, that
    can be compiled and then executed wherever we decide in the Python code. A compiled
    graph is acting as a function that can be called anywhere in the code. The purpose
    of symbolic computing is to have an abstraction of the architecture on which the
    graph will be executed, and which libraries to compile it with. As presented,
    symbolic variables are typed for the target architecture during compilation.
  prefs: []
  type: TYPE_NORMAL
- en: The second concept is the tensor, and the operators provided to manipulate tensors.
    Most of these were already available in CPU-based computation libraries, such
    as NumPy or SciPy. They have simply been ported to symbolic computing, requiring
    their equivalents on GPU. They use underlying acceleration libraries, such as
    BLAS, Nvidia Cuda, and cuDNN.
  prefs: []
  type: TYPE_NORMAL
- en: The last concept introduced by Theano is automatic differentiation—a very useful
    feature in deep learning to backpropagate errors and adjust the weights following
    the gradients, a process known as *gradient descent*. Also, the `scan` operator
    enables us to program loops (`while...`, `for...`,) on the GPU, and, as other
    operators, available through backpropagation as well, simplifying the training
    of models a lot.
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to apply this to deep learning in the next few chapters and
    have a look at this knowledge in practice.
  prefs: []
  type: TYPE_NORMAL
