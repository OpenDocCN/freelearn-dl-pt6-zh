- en: Chapter 4. Recurrent Neural Network
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '|   | *I think the brain is essentially a computer and consciousness is like
    a computer program. It will cease to run when the computer is turned off. Theoretically,
    it could be re-created on a neural network, but that would be very difficult,
    as it would require all one''s memories.* |   |'
  id: totrans-1
  prefs: []
  type: TYPE_TB
- en: '|   | --*Stephen Hawking* |'
  id: totrans-2
  prefs: []
  type: TYPE_TB
- en: To solve every problem, people do not initiate their thinking process from scratch.
    Our thoughts are non-volatile, and it is persistent just like the **Read Only
    Memory** (**ROM**) of a computer. When we read an article, we understand the meaning
    of every word from our understanding of earlier words in the sentences.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Let us take a real life example to explain this context a bit more. Let us assume
    we want to make a classification based on the events happening at every point
    in a video. As we do not have the information of the earlier events of the video,
    it would be a cumbersome task for the traditional deep neural networks to find
    some distinguishing reasons to classify those. Traditional deep neural networks
    cannot perform this operation, and hence, it has been one of the major limitations
    for them.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '**Recurrent neural networks** (**RNN**) [103] are a special type of neural
    network, which provides many enigmatic solutions for these difficult machine learning
    and deep learning problems. In the last chapter, we discussed convolutional neural
    networks, which is specialized in processing a set of values *X* (For example,
    an image). Similarly, RNNs are magical while processing a sequence of values,
    *x (0)*, *x (1)*,*x(2)*,*...*, *x(τ-1)*. To start with RNNs in this chapter, let
    us first place this network next to convolutional neural networks so that you
    can get an idea of its basic functionalities, and to basically know about this
    network.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural networks can easily scale to images with large width, height,
    and depth. Moreover, some convolutional neural networks can also process images
    with variable sizes.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, recurrent networks can readily scale to long sequences; also, most
    of those can also process variable length sequences. To process these arbitrary
    sequences of inputs, RNN uses their internal memory.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: RNNs generally operate on mini-batches of sequences, and contain vectors *x
    (t)* with the time-step index *t* ranging from *0* to *(τ-1)*. The sequence length
    *τ* can also vary for each member of the mini-batch. This time-step index should
    not always refer to the time intervals in the real world, but can also point to
    the position inside the sequence.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: A RNN, when unfolded in time, can be seen as a deep neural network with indefinite
    number of layers. However, compared to common deep neural networks, the basic
    functionalities and architecture of RNNs are somewhat different. For RNNs, the
    main function of the layers is to bring in memory, and not hierarchical processing.
    For other deep neural networks, the input is only provided in the first layer,
    and the output is produced at the final layer. However, in RNNs, the inputs are
    generally received at each time step, and the corresponding outputs are computed
    at those intervals. With every network iteration, fresh information is integrated
    into every layer, and the network can go along with this information for an indefinite
    number of network updates. However, during the training phase, the recurrent weights
    need to learn which information they should pass onwards, and what they must reject.
    This feature generates the primary motivation for a special form of RNN, called
    **Long short-term memory** (**LSTM**).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: RNNs started its journey a few decades back [104], but recently, it has significantly
    become a popular choice for modeling sequences of variable length. As of now,
    RNN has been successfully implemented in various problems such as learning word
    embedding [105], language modelling [106] [107] [108], speech recognition [109],
    and online handwritten recognition [110].
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss everything you need to know about RNN and the
    associated core components. We will introduce Long short-term memory later in
    the chapter, which is a special type of RNN.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'The topic-wise organization of this chapter is as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: What makes recurrent networks distinctive from others?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recurrent neural networks(RNNs)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backpropagation through time (BPTT)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long short-term memory (LSTM)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bi-directional RNNs
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed deep RNNs
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RNNs with Deeplearning4j
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What makes recurrent networks distinctive from others?
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might be curious to know the specialty of RNNs. This section of the chapter
    will discuss these things, and from the next section onwards, we will talk about
    the building blocks of this type of network.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: From [Chapter 3](ch03.html "Chapter 3.  Convolutional Neural Network") , *Convolutional
    Neural Network*, you have probably got a sense of the harsh limitation of convolutional
    networks and that their APIs are too constrained; the network can only take an
    input of a fixed-sized vector, and also generates a fixed-sized output. Moreover,
    these operations are performed through a predefined number of intermediate layers.
    The primary reason that makes RNNs distinctive from others is their ability to
    operate over long sequences of vectors, and produce different sequences of vectors
    as the output.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '|   | *"If training vanilla neural nets is optimization over functions, training
    recurrent nets is optimization over programs"* |   |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
- en: '|   | --*Alex Lebrun* |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
- en: 'We show different types of input-output relationships of the neural networks
    in *Figure 4.1* to portray the differences. We show five types of input-output
    relations as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '**One to one**: This input-output relationship is for traditional neural network
    processing without the involvement of a RNN. Mostly used for image classification,
    where the mapping is from fixed-sized input to fixed-sized output.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**One to many**: In this kind of relationship, the input and output maintain
    a one-to-many relationship. The model generates a sequence of outputs with one
    fixed-sized input. Often observed where the model takes an image (image captioning),
    and produces a sentence of words.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Many to one**: In this type of relationship, the model takes a sequence of
    inputs, and outputs one single observation. For example, in case of sentiment
    analysis, a sentence or reviews are provided to the model; it classifies the sentence
    as either a positive or negative sentiment.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Many to many (Variable intermedia states)**: The model receives a sequence
    of inputs, and a corresponding sequence of outputs are generated. In this type,
    the RNN reads a sentence in English, and then translates and outputs a sentence
    in German. Used in case of Machine Translation.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Many to many (Fixed number of intermedia state)**: The model receives a synced
    sequence of input, and generates a sequence of outputs. For example, in video
    classification, we might wish to classify every event of the movie.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![What makes recurrent networks distinctive from others?](img/image_04_001.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: The rectangles in the figure represent each element of the sequence
    vector, the arrows signify the functions. Input vectors are shown in red, and
    output vectors are in blue. The green color represents the intermediate RNN''s
    state. Image taken from [111].'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Operations that involve sequences are generally more powerful and appealing
    than networks with fixed-sized inputs and outputs. These models are used to build
    more intelligent systems. In the next sections, we will see how RNNs are built,
    and how the network unites the input vectors with their state vector with a defined
    function to generate a new state vector.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural networks(RNNs)
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss the architecture of the RNN. We will talk about
    how time is unfolded for the recurrence relation, and used to perform the computation
    in RNNs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Unfolding recurrent computations
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section will explain how unfolding a recurrent relation results in sharing
    of parameters across a deep network structure, and converts it into a computational
    model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us consider a simple recurrent form of a dynamical system:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![Unfolding recurrent computations](img/image_04_002-1.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, *s ^((t))* represents the state of the system at
    time *t*, and *θ* is the same parameter shared across all the iterations.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: This equation is called a recurrent equation, as the computation of *s ^((t))*
    requires the value returned by *s ^((t-1))*, the value of *s ^((t-1))* will require
    the value of *s ^((t-2))*, and so on.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a simple representation of a dynamic system for understanding purpose.
    Let us take one more example, where the dynamic system is driven by an external
    signal *x ^((t))*, and produces output *y ^((t))*:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![Unfolding recurrent computations](img/image_04_003-1.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: RNNs, ideally, follow the second type of equation, where the intermediate state
    retains the information about the whole past sequence. However, any equation that
    involves recurrence can be used to model the RNN.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, similar to the feed-forward neural networks, the state of the hidden
    (intermediate) layers of RNNs can be defined using the variable *h* at time *t*,
    as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![Unfolding recurrent computations](img/image_04_004-1.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: We will explain the functionality of this preceding equation in a RNN in the
    next part of this section. As of now, to illustrate the functionality of this
    hidden layer, *Figure 4.2* shows a simple recurrent network with no output. The
    left side of the figure shows a network whose current state influences the next
    state. The box in the middle of the loop represents the delay between two successive
    time steps.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding recurrent equation, we can unfold or unroll the hidden
    states in time. The right side of the image shows the unfolded structure of the
    recurrent network. There, the network can be converted to a feed-forward network
    by unfolding over time.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: In an unfolded network, each variable for each time step can be shown as a separate
    node of the network.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![Unfolding recurrent computations](img/image_04_005.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: The left part of the figure shows the recurrent network where the
    information passes through multiple times through the hidden layer with each time
    step. On the right, we have the unfolded structure of the same network. Each node
    of this network is associated with one timestamp.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: So, from *Figure 4.2*, the unfolding operation can be defined as an operation
    that performs the mapping of the circuit on the left-hand side to a computational
    model split into multiple states on the right-hand side.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of a model unfolded in time
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unfolding a network in time provides a few major advantages, which are listed
    as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: A model without a parameter would require many training examples for learning
    purposes. However, learning a shared single model helps to generalize the sequence
    lengths, even those that are not present in the training set. This allows the
    model to estimate the upcoming sequence data with fewer training examples.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Irrespective of the length of the sequence, the input size of the model will
    always remain the same. The input size in an unfolded model is specified in terms
    of transition from the hidden state to the other. However, for other cases, it
    is specified in terms of undefined length of the history of states.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to parameter sharing, the same transition function *f*, with the same parameter
    can be used at every time step.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory of RNNs
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As of now, you might have got some idea that the primary difference between
    a feed forward neural network and recurrent network is the feedback loop. The
    feedback loop is ingested into its own intermediate outcome as the input to the
    next state. The same task is performed for every element of the input sequence.
    Hence, the output of each hidden state depends on the previous computations. In
    a practical situation, each hidden state is not only concerned about the current
    input sequence in action, but also about what they perceived one step back in
    time. So, ideally, every hidden state must have all the information of the previous
    step's outcome.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Due to this requirement of persistent information, it is said that RNNs have
    their *own memory*. The sequential information is preserved as memory in the recurrent
    network's hidden state. This helps to handle the upcoming time steps as the network
    cascades forward to update the processing with each new sequence.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.3* shows the concept of *simple recurrent neural networks* proposed
    by Elman back in 1990 [112]; it shows the illustration of persistent memory for
    a RNN.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: In the next figure, a part of the word sequence AYSXWQF at the bottom represents
    the input example currently under consideration. Each box of this input example
    represents a pool of units. The forward arrow shows the complete set of trainable
    mapping from each sending input unit to each output unit for the next time step.
    The context unit, which can be considered as the persistent memory unit, preserves
    the output of the previous steps. The backward arrow, directed from the hidden
    layer to the context unit shows a copy operation of the output, used for evaluating
    the outcome for the next time step.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: The decision where a RNN reaches at time step *t*, depends mostly on its last
    decision of the time step at *(t-1)*. Therefore, it can be inferred that unlike
    traditional neural networks, RNNs have two sources of input.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: One is the current input unit under consideration, which is *X* in the following
    figure, and the other one is the information received from the recent past, which
    is taken from the context units in the figure. The two sources, in combination,
    decide the output of the current time step. More about this will be discussed
    in the next section.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![Memory of RNNs](img/image_04_006.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: A simple recurrent neural networks with the concept of memory of
    RNN is shown in this figure.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So, we have come to know that RNNs have their memory, which collects information
    about what has been computed so far. In this section, we will discuss the general
    architecture of RNNs and their functioning.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: A typical RNN, unfolded (or unrolled) at the time of the calculation involved
    in its forward computation is shown in *Figure 4.4*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'Unrolling or unfolding a network means to write out the network for the complete
    sequences of input. Let us take an example before we start explaining the architecture.
    If we have a sequence of 10 words, the RNN would then be unfolded into a 10-layer
    deep neural network, one layer for each word, as depicted by the following diagram:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture](img/image_04_007.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: The figure shows a RNN being unrolled or unfolded into a full network.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: The time period to reach from input *x* to output *o* is split into several
    timestamps given by *(t-1)*, *t*, *(t+1)*, and so on.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'The computational steps and formulae for an RNN are listed as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, *x[t]* is the input at time step *t*. The figure shows
    computations for three timestamps *(t-1)*, *t*, and *(t+1)*, where the inputs
    are *x[(t-1)]*, *x[t]*, and *x[(t+1)],* respectively. For example, *x[1]* and
    *x[2]* are the vectors that correspond to the second and third word of the sequence.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*s[t]* represents the hidden state at time step *t*. Conceptually, this state
    defines the memory of the neural network. Mathematically, the formulation for
    *s[t]* or the process of carrying memory can be written as follows:'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Architecture](img/image_04_008-1.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
- en: So, the hidden state is a function of the input at time step *x[t]*, multiplied
    by the weight *U*, and addition of the hidden state of the last time step *s[t-1]*,
    which is multiplied by its own hidden-state-to-hidden-state matrix *W*. This hidden-state-to-hidden-state
    is often termed as a transition matrix, and is similar to a Markov chain. The
    weight matrices behave as filters, which primarily decide the importance of both,
    the past hidden state and the current input. The error generated for the current
    state would be sent back via backpropagation to update these weights until the
    error is minimized to the desired value.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To calculate the first hidden state, we would require determining the value
    *s-1*, which is generally initialized to all zeroes.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Unlike a traditional deep neural network, where different parameters are used
    for the computation at each layer, a RNN shares the same parameters (here, *U*,
    *V*, and *W*) across all the time steps to calculate the value of the hidden layer.
    This makes the life of a neural network much easier, as we need to learn fewer
    number of parameters.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'This sum of the weight input and hidden state is squeezed by the function *f*,
    which usually is a nonlinearity such as a logistic sigmoid function, *tan h*,
    or ReLU:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'In the last figure, *o[t]* is represented as the output at time step t. The
    output at step *o[t]* is solely computed based on the memory available for the
    network at time t. Theoretically, although RNNs can persist memory for arbitrarily
    long sequences, in practice, it''s a bit complicated, and they are limited to
    looking back only a few time steps. Mathematically, this can be represented as
    follows:'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Architecture](img/image_04_009-1.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: The next section shall discuss how to train a RNN through back propagation.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation through time (BPTT)
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have already learnt that the primary requirement of RNNs is to distinctly
    classify the sequential inputs. The backpropagation of error and gradient descent
    primarily help to perform these tasks.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'In case of feed forward neural networks, backpropagation moves in the backward
    direction from the final error outputs, weights, and inputs of each hidden layer.
    Backpropagation assigns the weights responsible for generating the error, by calculating
    their partial derivatives: ![Backpropagation through time (BPTT)](img/B05883_04_16.jpg)
    where *E* denotes the error and *w* is the respective weights. The derivatives
    are applied on the learning rate, and the gradient decreases to update the weights
    so as to minimize the error rate.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: However, a RNN, without using backpropagation directly, uses an extension of
    it, termed as **backpropagation through time** (**BPTT**). In this section, we
    will discuss BPTT to explain how the training works for RNNs.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Error computation
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **backpropagation through time** (**BPTT**) learning algorithm is a natural
    extension of the traditional backpropagation method, which computes the gradient
    descent on a complete unrolled neural network.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.5* shows the errors associated with each hidden state for an unrolled
    RNN. Mathematically, the errors associated with each state can be given as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '![Error computation](img/image_04_011-1.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: where *o[t]* represents the correct output, and *ô[t]* represents the predicted
    word at time step *t*. The total error (cost function) of the whole network is
    calculated as the summation of all the intermediate errors at each time step.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'If the RNN is unfolded into multiple time steps, starting from *t[0]* to *t[n-1]*,
    the total error can be written as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![Error computation](img/image_04_012-1.jpg)![Error computation](img/image_04_013.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: The figure shows errors associated with every time step for a RNN.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: In the backpropagation through time method, unlike the traditional method, the
    gradient descent weight is updated in each time step.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'Let *w[ij]* denote the connection of weight from neuron *i* to neuron *j*.
    *η* denotes the learning rate of the network. So, mathematically, the weight update
    with gradient descent at every time step is given by the following equation:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '![Error computation](img/B05883_04.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: Long short-term memory
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss a special unit called **Long short-term memory**
    (**LSTM**), which is integrated into RNN. The main purpose of LSTM is to prevent
    a significant problem of RNN, called the vanishing gradient problem.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论一个特殊的单元，称为**长短期记忆**（**LSTM**），它被集成到RNN中。LSTM的主要目的是防止RNN的一个重要问题——梯度消失问题。
- en: Problem with deep backpropagation with time
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度反向传播在时间上的问题
- en: Unlike the traditional feed forward network, due to unrolling of a RNN with
    narrow time steps, the feed forward network generated this way could be aggressively
    deep. This sometimes makes it extremely difficult to train via backpropagation
    through the time procedure.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的前馈网络不同，由于RNN在狭窄时间步长下的展开，生成的前馈网络可能会非常深。这有时使得通过时间反向传播程序进行训练变得极其困难。
- en: In the first chapter, we discussed the vanishing gradient problem. An unfolded
    RNN suffers from the vanishing gradient problem of exploding while performing
    backpropagation through time.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们讨论了梯度消失问题。展开的RNN在进行反向传播时会遭遇梯度消失或梯度爆炸问题。
- en: Every state of a RNN depends on its input and its previous output multiplied
    by the current hidden state vector. The same operations happen to the gradient
    in the reverse direction during backpropagation through time. The layers and numerous
    time steps of the unfolded RNN relate to each other through multiplication, hence
    the derivatives are susceptible to vanish with every pass.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的每个状态都依赖于其输入和上一个输出与当前隐藏状态向量的乘积。反向传播过程中，梯度在逆向传播时执行相同的操作。展开的RNN的各层和多个时间步通过乘法相互关联，因此导数在每次传递时容易消失。
- en: On the other hand, a small gradient tends to get smaller, while a large gradient
    gets even larger while passing through every time step. This creates the vanishing
    or exploding gradient problem respectively for a RNN.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，小梯度趋向于变得更小，而大梯度在每次时间步传递时会变得更大。这分别导致了RNN的梯度消失或梯度爆炸问题。
- en: Long short-term memory
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 长短期记忆
- en: In the mid-90s, an updated version of RNNs with a special unit, called **Long
    short-term memory** (**LSTM**) units, was proposed by German researchers Sepp
    Hochreiter and Juergen Schmidhuber [116] to protect against the exploding or vanishing
    gradient problem.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在90年代中期，德国研究人员Sepp Hochreiter和Juergen Schmidhuber提出了一种更新版的RNN，加入了一个特殊单元，称为**长短期记忆**（**LSTM**）单元，以防止梯度爆炸或梯度消失问题[116]。
- en: LSTM helps to maintain a constant error, which can be propagated though time
    and through each layer of the network. This preservation of constant error allows
    the unrolled recurrent networks to learn on an aggressively deep network, even
    unrolled by a thousand time steps. This eventually opens a channel to link the
    causes of effects remotely.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM有助于保持恒定的误差，这些误差可以在时间和网络的每一层中传播。恒定误差的保持使得展开的循环神经网络能够在极深的网络中学习，甚至展开到千个时间步。这最终打开了一条通道，可以远程关联因果关系。
- en: 'The architecture of LSTM maintains a constant error flow through the internal
    state of special memory units. The following figure (*Figure 4.6*) shows a basic
    block diagram of a LSTM for easy understanding:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM的架构通过特殊的记忆单元保持恒定的误差流。下图（*图4.6*）展示了LSTM的基本框图，便于理解：
- en: '![Long short-term memory](img/image_04_015.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![长短期记忆](img/image_04_015.jpg)'
- en: 'Figure 4.6: The figure shows a basic model of the Long-short term memorys'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6：该图显示了长短期记忆的基本模型。
- en: As shown in the preceding figure, an LSTM unit is composed of a memory cell
    that primarily stores information for long periods of time. Three specialized
    gate neurons-- write gate, read gate, and forget gate-protect the access to this
    memory cell. Unlike the digital storage of computers, the gates are analog in
    nature, with a range of 0 to 1\. Analog devices have an added advantage over digital
    ones, as they are differentiable, and hence, serve the purpose for the backpropagation
    method. The gate cell of LSTM, instead of forwarding the information as inputs
    to the next neurons, sets the associated weights connecting the rest of the neural
    network to the memory cell. The memory cell is, basically, a self-connected linear
    neuron. When the forget cell is reset (turned **0**), the memory cell writes its
    content to itself and remembers the last content of the memory. For a memory write
    operation though, the forget gate and write get should be set (turned **1**).
    Also, when the forget gate outputs something close to **1**, the memory cell effectively
    forgets all the previous contents that it had stored. Now, when the write gate
    is set, it allows any information to write into its memory cell. Similarly, when
    the read gate outputs a **1**, it will allow the rest of the network to read from
    its memory cell.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'As explained earlier, the problem with computing the gradient descent for traditional
    RNNs is that the error gradient vanishes rapidly while propagating through the
    time steps in the unfolded network. Adding an LSTM unit, the error values when
    backpropagated from the output are collected in the memory cell of the LSTM units.
    This phenomenon is also known as *error carousel*. We will use the following example
    to describe how LSTM overcomes the vanishing gradient problem for RNNs:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![Long short-term memory](img/B05883_04_07-2.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: The figure shows a Long-short term memory unfolded in time. It
    also depicts how the content of the memory cell is protected with the help of
    three gates.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.7* shows a Long short-term memory unit unrolled through time. We
    will start with initializing the value of the forget gate to **1** and write gate
    to **1**. As shown in the preceding figure, this will write an information **K**
    into the memory cell. After writing, this value is retained in the memory cell
    by setting the value of the forget gate to **0**. We then set the value of the
    read gate as **1**, which reads and outputs the value **K** from the memory cell.
    From the point of loading **K** into the memory cell to the point of reading the
    same from the memory cell backpropagation through time is followed.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: The error derivatives received from the read point backpropagate through the
    network with some nominal changes, until the write point. This happens because
    of the linear nature of the memory neuron. Thus, with this operation, we can maintain
    the error derivatives over hundreds of steps without going into the trap of the
    vanishing gradient problems.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: So there are many reasons for why Long short-term memory outperforms standard
    RNNs. LSTM was able to achieve the best known result in unsegmented connected
    handwriting recognition [117]; also, it is equally successfully applied to automated
    speech recognition. As of now, the major technological companies such as Apple,
    Microsoft, Google, Baidu, and so on have started to widely use LSTM networks as
    a primary component for their latest products [118].
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Bi-directional RNNs
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section of the chapter will discuss the major limitations of RNNs and how
    bi-directional RNN, a special type of RNN helps to overcome those shortfalls.
    Bi-directional neural networks, apart from taking inputs from the past, takes
    the information from the future context for its required prediction.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Shortfalls of RNNs
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The computation power of standard or unidirectional RNNs has constraints, as
    the current state cannot reach its future input information. In many cases, the
    future input information coming up later becomes extremely useful for sequence
    prediction. For example, in speech recognition, due to linguistic dependencies,
    the appropriate interpretation of the voice as a phoneme might depend on the next
    few spoken words. The same situation might also arise in handwriting recognition.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'In some modified versions of RNN, this feature is partially attained by inserting
    some delay of a certain amount (*N*) of time steps in the output. This delay helps
    to capture the future information to predict the data. Although, theoretically,
    in order to capture most of the available future information, the value of *N*
    can be set as very large, but in a practical scenario, the prediction power of
    the model actually reduces with a large value of *N*. The paper [113] has put
    some logical explanation for this inference. As the value of *N* increases, most
    of the computational power of a RNN only focuses on remembering the input information
    for ![Shortfalls of RNNs](img/image_04_017.jpg) (from *Figure 4.8*) to predict
    the outcome, *y[tc]*. (*t[c]* in figure denotes the current time step in consideration).
    Therefore, the model will have less processing power to combine the prediction
    knowledge received from different input vectors. The following *Figure 4.8* shows
    an illustration of the amount of input information needed for different types
    of RNNs:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '![Shortfalls of RNNs](img/image_04_018-1.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: The figure shows visualizations of input information used by different
    types of RNNs. [113]'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Solutions to overcome
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To subjugate the limitations of a unidirectional RNN explained in the last section,
    **bidirectional recurrent network** (**BRNN**) was invented in 1997 [113].
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea behind bidirectional RNN is to split the hidden state of a regular
    RNN into two parts. One part is responsible for the forward states (positive time
    direction), and the other part for the backward states (negative time direction).
    Outputs generated from the forward states are not connected to the inputs of the
    backward states, and vice versa. A simple version of a bidirectional RNN, unrolled
    in three time steps is shown in *Figure 4.9*.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: With this structure, as both the time directions are taken care of, the currently
    evaluated time frame can easily use the input information from the past and the
    future. So, the objective function of the current output will eventually minimize,
    as we do not need to put further delays to include the future information. This
    was necessary for regular RNNs as stated in the last section.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '![Solutions to overcome](img/image_04_019.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.9: The figure shows the conventional structure of a bidirectional
    neural network unrolled in three time steps.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: So far, bidirectional RNNs have been found to be extremely useful in applications
    such as speech recognition [114], handwriting recognition, bioinformatics [115],
    and so on.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Distributed deep RNNs
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you now have an understanding of a RNN, its applications, features, and architecture,
    we can now move on to discuss how to use this network as distributed architecture.
    Distributing RNN is not an easy task, and hence, only a few researchers have worked
    on this in the past. Although the primary concept of data parallelism is similar
    for all the networks, distributing RNNs among multiple servers requires some brainstorming
    and a bit tedious work too.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Recently, one work from Google [119] has tried to distribute recurrent networks
    in many servers in a speech recognition task. In this section, we will discuss
    this work on distributed RNNs with the help of Hadoop.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '**Asynchronous stochastic gradient descent** (**ASGD**) can be used for large-scale
    training of a RNN. ASGD has particularly shown success in sequence discriminative
    training of the deep neural networks.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: A two-layer deep Long short-term memory RNN is used to build the Long short-term
    memory network. Each Long short-term memory consists of 800 memory cells. The
    paper uses 13 million parameters for the LSTM network. For cell input and output
    units tan h (hyperbolic tangent activation) is used, and for the write, read,
    and forget gates, the logistic sigmoid function is used.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: For training purposes, the input speech training data can be split and randomly
    shuffled across multiple DataNodes of the Hadoop framework. The Long short-term
    memory is put across all these DataNodes, and distributed training is performed
    on those datasets in parallel. Asynchronous stochastic gradient descent is used
    for this distributed training. One parameter server, dedicated for maintaining
    the current state of all model parameters, is used.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: To implement this procedure on Hadoop, each DataNode has to perform asynchronous
    stochastic gradient descent operations on the partitioned data. Each worker, running
    on each block of the DataNodes works on the partitions, one utterance at a time.
    For each utterance of the speech, the model parameter *P* is fetched from the
    parameter server mentioned earlier. The workers compute the current state of every
    frame; decipher the speech utterance to calculate the final outer gradients. The
    updated parameter is then sent back to the parameter server. The workers then
    repeatedly request the parameter server to provide the latest parameters. Backpropagation
    through time is then performed to calculate the updated parameter gradient for
    the next set of frames, which is again sent back to the parameter server.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: RNNs with Deeplearning4j
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training a RNN is not a simple task, and it can be extremely computationally
    demanding sometimes. With long sequences of training data involving many time
    steps, the training, sometimes becomes extremely difficult. As of now, you have
    got a better theoretical understanding of how and why backpropagation through
    time is primarily used for training a RNN. In this section, we will consider a
    practical example of the use of a RNN and its implementation using Deeplearning4j.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: We now take an example to give an idea of how to do the sentiment analysis of
    a movie review dataset using RNN. The main problem statement of this network is
    to take some raw text of a movie review as input, and classify that movie review
    as either positive or negative based on the contents present. Each word of the
    raw review text is converted to vectors using the Word2Vec model, and then fed
    into a RNN. The example uses a large-scale dataset of raw movie reviews taken
    from [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)
    .
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'The whole implementation of this model using DL4J can be split into the following
    few steps:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Download and extract the raw movie reviews data.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure the network configuration needed for training, and evaluate the performance.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load each review and convert the words to vectors using the Word2Vec model.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform training for multiple predefined epochs. For each epoch, the performance
    is evaluated on the test set.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To download and extract the movie reviews'' data, we need to set up the download
    configuration first. The following code snippet sets all the things needed to
    do so:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Location to save and extract the training and testing data in the local file
    path is set as follows:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Location of the local filesystem for the Google News vectors is given as follows:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following code helps to download the data from the web URL to the local
    file path:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, as we have downloaded the raw movie reviews'' data, we can now move to
    set up our RNN to perform the training of this data. The downloaded data is split
    on a number of examples used in each mini batch to work on each worker of Hadoop
    for distributed training purposes. We need to declare a variable, `batchSize`,
    for this purpose. Here, as a sample, we use each batch of 50 examples, which will
    be split across multiple blocks of Hadoop, where the workers will run in parallel:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As we set the network configuration for a RNN, we can now move on to the training
    operation as follows:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The testing of the network is performed by creating an object of the `Evaluation`
    class as follows:'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Summary
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RNNs are special compared to other traditional deep neural networks because
    of their capability to work over long sequences of vectors, and to output different
    sequences of vectors. RNNs are unfolded over time to work like a feed-forward
    neural network. The training of RNNs is performed with backpropagation of time,
    which is an extension of the traditional backpropagation algorithm. A special
    unit of RNNs, called Long short-term memory, helps to overcome the limitations
    of the backpropagation of time algorithm.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: We also talked about the bidirectional RNN, which is an updated version of the
    unidirectional RNN. Unidirectional RNNs sometimes fail to predict correctly because
    of lack of future input information. Later, we discussed distribution of deep
    RNNs and their implementation with Deeplearning4j. Asynchronous stochastic gradient
    descent can be used for the training of the distributed RNN. In the next chapter,
    we will discuss another model of deep neural network, called the Restricted Boltzmann
    machine.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
