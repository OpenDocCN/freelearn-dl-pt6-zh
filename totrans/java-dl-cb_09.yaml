- en: Using RL4J for Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning is a goal-oriented machine learning algorithm that trains
    an agent to make a sequence of decisions. In the case of deep learning models,
    we train them on existing data and apply the learning on new or unseen data. Reinforcement
    learning exhibits dynamic learning by adjusting its own actions based on continuous
    feedback in order to maximize the reward. We can introduce deep learning into
    a reinforcement learning system, which is known as deep reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'RL4J is a reinforcement learning framework integrated with DL4J. RL4J supports
    two reinforcement algorithms: deep Q-learning and A3C (short for **Asynchronous
    Actor-Critic Agents**). Q-learning is an off-policy reinforcement learning algorithm
    that seeks the best action for the given state. It learns from actions outside
    the ones mentioned in the current policy by taking random actions. In deep Q-learning,
    we use a deep neural network to find the optimal Q-value rather than value iteration
    in regular Q-learning. In this chapter, we will set up a gaming environment powered
    by reinforcement learning using Project Malmo. Project Malmo is a platform for
    reinforcement learning experiments built on top of Minecraft.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the Malmo environment and respective dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up the data requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring and training a Deep Q-Network (DQN) agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating a Malmo agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The source code for this chapter can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/09_Using_RL4J_for_Reinforcement%20learning/sourceCode/cookbookapp/src/main/java/MalmoExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/09_Using_RL4J_for_Reinforcement%20learning/sourceCode/cookbookapp/src/main/java/MalmoExample.java).'
  prefs: []
  type: TYPE_NORMAL
- en: After cloning our GitHub repository, navigate to the `Java-Deep-Learning-Cookbook/09_Using_RL4J_for_Reinforcement
    learning/sourceCode` directory. Then, import the `cookbookapp` project as a Maven
    project by importing `pom.xml`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You need to set up a Malmo client to run the source code. First, download the
    latest Project Malmo release as per your OS ([https://github.com/Microsoft/malmo/releases](https://github.com/Microsoft/malmo/releases)):'
  prefs: []
  type: TYPE_NORMAL
- en: For Linux OS, follow the installation instructions here: [https://github.com/microsoft/malmo/blob/master/doc/install_linux.md](https://github.com/microsoft/malmo/blob/master/doc/install_linux.md).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For Windows OS, follow the installation instructions here: [https://github.com/microsoft/malmo/blob/master/doc/install_windows.md](https://github.com/microsoft/malmo/blob/master/doc/install_windows.md).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For macOS, follow the installation instructions here: [https://github.com/microsoft/malmo/blob/master/doc/install_macosx.md](https://github.com/microsoft/malmo/blob/master/doc/install_macosx.md).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To launch the Minecraft client, navigate to the Minecraft directory and run
    the client script:'
  prefs: []
  type: TYPE_NORMAL
- en: Double-click on `launchClient.bat` (on Windows).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run `./launchClient.sh` on the console (either on Linux or macOS).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you're in Windows and are facing issues while launching the client, you can
    download the dependency walker here: [https://lucasg.github.io/Dependencies/](https://lucasg.github.io/Dependencies/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract and run `DependenciesGui.exe`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select `MalmoJava.dll` in the `Java_Examples` directory to see the missing
    dependencies like the ones shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/63ab736b-704c-47ac-909e-fe828aad702c.png)'
  prefs: []
  type: TYPE_IMG
- en: In the case of any issues, the missing dependencies will be marked on the list.
    You will need to add the missing dependencies in order to relaunch the client
    successfully. Any missing libraries/files should be present in the `PATH` environment
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may refer to OS-specific build instructions here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/microsoft/malmo/blob/master/doc/build_linux.md](https://github.com/microsoft/malmo/blob/master/doc/build_linux.md)
    (Linux)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/microsoft/malmo/blob/master/doc/build_windows.md](https://github.com/microsoft/malmo/blob/master/doc/build_windows.md)
    (Windows)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/microsoft/malmo/blob/master/doc/build_macosx.md](https://github.com/microsoft/malmo/blob/master/doc/build_macosx.md)
    (macOS)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If everything goes well, you should see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8c9d23f2-2f13-4e81-9e5d-4a2628282ce4.png)'
  prefs: []
  type: TYPE_IMG
- en: Additionally, you need to create a mission schema to build blocks for the gaming
    window. The complete mission schema can be found in this chapter's project directory
    at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/09_Using_RL4J_for_Reinforcement%20learning/sourceCode/cookbookapp/src/main/resources/cliff_walking_rl4j.xml](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/09_Using_RL4J_for_Reinforcement%20learning/sourceCode/cookbookapp/src/main/resources/cliff_walking_rl4j.xml).
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the Malmo environment and respective dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to set up RL4J Malmo dependencies to run the source code. Just like
    any other DL4J application, we also need to add ND4J backend dependencies as well
    depending upon your hardware (CPU/GPU). In this recipe, we will add the required
    Maven dependencies and set up the environment to run the application.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Malmo client should be up and running before we run the Malmo example source
    code. Our source code will communicate with the Malmo client in order to create
    and run the missions.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Add the RL4J core dependency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the RL4J Malmo dependency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Add a dependency for the ND4J backend:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For CPU, you can use the following:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For GPU, you can use the following:'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Add Maven dependency for `MalmoJavaJar`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In step 1, we added RL4J core dependencies to bring in RL4J DQN libraries in
    our application. RL4J Malmo dependencies are added in step 2 to construct the
    Malmo environment and build missions in RL4J.
  prefs: []
  type: TYPE_NORMAL
- en: We need to add CPU/GPU-specific ND4J backend dependencies as well (step 3).
    Finally, in step 4, we added dependencies for `MalmoJavaJar` (step 4), which acts
    as a communication interface for the Java program to interact with Malmo.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the data requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data for the Malmo reinforcement learning environment includes the image
    frames that the agent is moving in. A sample gaming window for Malmo will look
    like the following. Here, the agent dies if they step over the lava:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f9aed8d-f3a5-408c-9f95-57dbf0fa9577.png)'
  prefs: []
  type: TYPE_IMG
- en: Malmo requires developers to specify the XML schema in order to generate the
    mission. We will need to create mission data for both the agent and the server
    to create blocks in the world (that is, the gaming environment). In this recipe,
    we will create an XML schema to specify the mission data.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Define the initial conditions of the world using the `<ServerInitialConditions>`
    tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Navigate to [http://www.minecraft101.net/superflat/](http://www.minecraft101.net/superflat/)
    and create your own preset string for the super-flat world:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/fe296849-0271-413d-80e9-92aa424094ca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Generate a super-flat world with the specified preset string using the `<FlatWorldGenerator>`
    tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Draw structures in the world using the `<DrawingDecorator>` tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify a time limit for all agents using the `<ServerQuitFromTimeUp>` tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Add all mission handlers to the block using the `<ServerHandlers>` tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Add `<ServerHandlers>` and `<ServerInitialConditions>` under the `<ServerSection>` tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the agent name and starting position:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the block types using the `<ObservationFromGrid>` tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the video frames using the `<VideoProducer>` tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Mention the reward points to be received when an agent comes into contact with
    a block type using the `<RewardForTouchingBlockType>` tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Mention the reward points to issue a command to the agent using the `<RewardForSendingCommand>`
    tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify the mission endpoints for the agent using the `<AgentQuitFromTouchingBlockType>`
    tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Add all agent handler functions under the `<AgentHandlers>` tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Add all agent handlers to `<AgentSection>`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `DataManager` instance to record the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In step 1, the following configurations are added as the initial conditions
    for the world:'
  prefs: []
  type: TYPE_NORMAL
- en: '`StartTime`: This specifies the time of day at the start of the mission, in
    thousandths of an hour. 6,000 refers to noontime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AllowPassageOfTime`: If set to `false`, then it will stop the day-night cycle.
    The weather and the sun position will remain constant during the mission.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Weather`: This specifies the type of weather at the start of the mission.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AllowSpawning`: If set to `true`, then it will produce animals and hostiles
    during the mission.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *step 2*, we created a preset string to represent the super-flat type that
    is being used in step 3\. A super-flat type is nothing but the type of surface
    seen in the mission.
  prefs: []
  type: TYPE_NORMAL
- en: In step 4, we drew structures into the world using `DrawCuboid` and `DrawBlock`.
  prefs: []
  type: TYPE_NORMAL
- en: We follow three-dimensional space `(x1,y1,z1)` -> `(x2,y2,z2)` to specify the
    boundaries. The `type` attribute is used to represent block types. You may add
    any of the available 198 blocks for your experiments.
  prefs: []
  type: TYPE_NORMAL
- en: In step 6, we add all mission handlers specific to world creation under the `<ServerHandlers>`
    tag. Then, we add them to the `<ServerSection>` parent tag in step 7.
  prefs: []
  type: TYPE_NORMAL
- en: In step 8, the `<Placement>` tag is used to specify the player's starting position.
    The starting point will be chosen randomly if it is not specified.
  prefs: []
  type: TYPE_NORMAL
- en: 'In step 9, we specified the position of the floor block in the gaming window.
    In step 10, `viewpoint` sets the camera viewpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In step 13, we specify the block types in which agent movement is stopped once
    the step is over. In the end, we add all agent-specific mission handlers in the `AgentSection` tag
    at step 15\. Mission schema creation will end at step 15.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we need to store the training data from the mission. We use `DataManager`
    to handle the recording of training data. It creates the `rl4j-data` directory if
    it does not exist and stores the training data as the reinforcement learning training
    progresses. We passed `false` as an attribute while creating `DataManager` in
    step 16\. This means that we are not persisting the training data or the model.
    Pass `true` if the training data and model are to be persisted. Note that we are
    going to need the data manager instance while configuring DQN.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the following documentation to create your own custom XML schema for
    the Minecraft world:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://microsoft.github.io/malmo/0.14.0/Schemas/Mission.html](http://microsoft.github.io/malmo/0.14.0/Schemas/Mission.html)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://microsoft.github.io/malmo/0.30.0/Schemas/MissionHandlers.html](http://microsoft.github.io/malmo/0.30.0/Schemas/MissionHandlers.html)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring and training a DQN agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DQN refers to an important class of reinforcement learning, called value learning.
    Here, we use a deep neural network to learn the optimal Q-value function. For
    every iteration, the network approximates Q-value and evaluates them against the
    Bellman equation in order to measure the agent accuracy. Q-value is supposed to
    be optimized while the agent makes movements in the world. So, how we configure
    the Q-learning process is important. In this recipe, we will configure DQN for
    a Malmo mission and train the agent to achieve the task.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Basic knowledge on the following are prerequisites for this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: Q-learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DQN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q-learning basics will help while configuring the Q-learning hyperparameters
    for the DQN.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create an action space for the mission:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an observation space for the mission:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a Malmo consistency policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an MDP (short for **Markov Decision Process**) wrapper around the Malmo
    Java client:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a DQN using `DQNFactoryStdConv`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Use `HistoryProcessor` to scale the pixel image input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a Q-learning configuration by specifying hyperparameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the DQN model using `QLearningDiscreteConv` by passing MDP wrapper and `DataManager`:
    within the `QLearningDiscreteConv` constructor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the DQN:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In step 1, we defined an action space for the agent by specifying a defined
    set of Malmo actions. For example, `movenorth 1` means moving the agent one block
    north. We passed in a list of strings to `MalmoActionSpaceDiscrete` indicating
    an agent's actions on Malmo space.
  prefs: []
  type: TYPE_NORMAL
- en: In step 2, we created an observation space from the bitmap size (mentioned by
    `xSize` and `ySize`) of input images(from the Malmo space). Also, we assumed three
    color channels (R, G, B). The agent needs to know about observation space before
    they run. We used `MalmoObservationSpacePixels` because we target observation
    from pixels.
  prefs: []
  type: TYPE_NORMAL
- en: In step 3, we have created a Malmo consistency policy using `MalmoDescretePositionPolicy` to
    ensure that the upcoming observation is in a consistent state.
  prefs: []
  type: TYPE_NORMAL
- en: A MDP is an approach used in reinforcement learning in grid-world environments.
    Our mission has states in the form of grids. MDP requires a policy and the objective of
    reinforcement learning is to find the optimal policy for the MDP. `MalmoEnv` is
    an MDP wrapper around a Java client.
  prefs: []
  type: TYPE_NORMAL
- en: In step 4, we created an MDP wrapper using the mission schema, action space,
    observation space, and observation policy. Note that the observation policy is
    not the same as the policy that an agent wants to form at the end of the learning
    process.
  prefs: []
  type: TYPE_NORMAL
- en: In step 5, we used **`DQNFactoryStdConv`** to build the DQN by adding convolutional
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: In step 6, we configured `HistoryProcessor` to scale and remove pixels that
    were not needed. The actual intent of `HistoryProcessor` is to perform an experience
    replay, where the previous experience from the agent will be considered while
    deciding the action on the current state. With the use of `HistoryProcessor`,
    we can change the partial observation of states to a fully-observed state, that
    is, when the current state is an accumulation of the previous states.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the hyperparameters used in step 7 while creating Q-learning configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '`maxEpochStep`: The maximum number of steps allowed per epoch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxStep`: The maximum number of steps that are allowed. Training will finish
    when the iterations exceed the value specified for `maxStep`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`expRepMaxSize`: The maximum size of experience replay. Experience replay refers
    to the number of past transitions based on which the agent can decide on the next
    step to take.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doubleDQN`: This decides whether double DQN is enabled in the configuration
    (true if enabled).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`targetDqnUpdateFreq`: Regular Q-learning can overestimate the action values
    under certain conditions. Double Q-learning adds stability to the learning. The
    main idea of double DQN is to freeze the network after every *M* number of updates
    or smoothly average for every *M* number of updates. The value of M is referred
    to as `targetDqnUpdateFreq`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`updateStart`: The number of no-operation (do nothing) moves at the beginning
    to ensure the Malmo mission starts with a random configuration. If the agent starts
    the game in the same way every time, then the agent will memorize the sequence
    of actions, rather than learning to take the next action based on the current
    state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gamma`: This is also known as the discount factor. A discount factor is multiplied
    by future rewards to prevent the agent from being attracted to high rewards, rather
    than learning the actions. A discount factor close to 1 indicates that the rewards
    from the distant future are considered. On the other hand, a discount factor close
    to 0 indicates that the rewards from the immediate future are being considered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rewardFactor`: This is a reward-scaling factor to scale the reward for every
    single step of training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`errorClamp`: This will clip the gradient of loss function with respect to
    output during backpropagation. For `errorClamp = 1`, the gradient component is
    clipped to the range *(-1, 1)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minEpsilon`: Epsilon is the derivative of the loss function with respect to
    the output of the activation function. Gradients for every activation node for
    backpropagation are calculated from the given epsilon value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epsilonNbStep`: Th epsilon value is annealed to `minEpsilon` over an `epsilonNbStep` number
    of steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can make the mission even harder by putting lava onto the agent''s path
    after a certain number of actions are performed. First, start by creating a mission
    specification using the schema XML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, setting the lava challenge on the mission is as simple as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '`MissionSpec` is a class file included in the `MalmoJavaJar` dependency,which
    we can use to set missions in the Malmo space.'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a Malmo agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to evaluate the agent to see how well it has learned to play the game.
    We just trained our agent to navigate through the world to reach the target. In
    this recipe, we will evaluate the trained Malmo agent.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a prerequisite, we will need to persist the agent policies and reload them
    back during evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final policy (policy to make movements in Malmo space) used by the agent
    after training can be saved as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '`dql` refers to the DQN model. We retrieve the final policies and store them
    as a `DQNPolicy`. A DQN policy provides actions that have the highest Q-value
    estimated by the model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be restored later for evaluation/inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create an MDP wrapper to load the mission:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Malmo mission/world is launched in step 1\. In step 2, `MALMO_HPROC` is
    the history processor configuration. You can refer to step 6 of the previous recipe
    for the sample configuration. Once the agent is subjected to evaluation, you should
    see the results as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ea45d95-90e1-4b49-afd0-58da36176ad4.png)'
  prefs: []
  type: TYPE_IMG
- en: For every mission evaluation, we calculate the reward score. A positive reward
    score indicates that the agent has reached the target. At the end, we calculated
    the average reward score of the agent.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/763ff236-143a-4198-b2ad-8a59b3ef743d.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding screenshot, we can see that the agent has reached the target.
    This is the ideal target position, no matter how the agent decides to move across
    the block. After the training session, the agent will form a final policy, which
    the agent can use to reach the target without falling into lava. The evaluation
    process will ensure that the agent is trained enough to play the Malmo game on
    its own.
  prefs: []
  type: TYPE_NORMAL
