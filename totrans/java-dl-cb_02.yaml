- en: Data Extraction, Transformation, and Loading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s discuss the most important part of any machine learning puzzle: data
    preprocessing and normalization. *Garbage in, garbage out* would be the most appropriate
    statement for this situation. The more noise we let pass through, the more undesirable
    outputs we will receive. Therefore, you need to remove noise and keep signals
    at the same time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another challenge is handling various types of data. We need to convert raw
    datasets into a suitable format that a neural network can understand and perform
    scientific computations on. We need to convert data into a numeric vector so that
    it is understandable to the network and so that computations can be applied with
    ease. Remember that neural networks are constrained to only one type of data:
    vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: There has to be an approach regarding how data is loaded into a neural network.
    We cannot put 1 million data records onto a neural network at once – that would
    bring performance down. We are referring to training time when we mention performance
    here. To increase performance, we need to make use of data pipelines, batch training,
    and other sampling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '**DataVec** is an input/output format system that can manage everything that
    we just mentioned. It solves the biggest headaches that every deep learning puzzle
    causes. DataVec supports all types of input data, such as text, images, CSV files,
    and videos. The DataVec library manages the data pipeline in DL4J.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to perform ETL operations using DataVec.
    This is the first step in building a neural network in DL4J.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Reading and iterating through data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing schema transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serializing transforms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a transform process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing a transform process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizing data for network efficiency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concrete implementations of the use cases that will be discussed in this chapter
    can be found at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app).
  prefs: []
  type: TYPE_NORMAL
- en: After cloning our GitHub repository, navigate to the `Java-Deep-Learning-Cookbook/02_Data_Extraction_Transform_and_Loading/sourceCode`directory.
    Then, import the `cookbook-app` project as a Maven projectby importing the `pom.xml`file
    inside the `cookbook-app` directory.
  prefs: []
  type: TYPE_NORMAL
- en: The datasets that are required for this chapter are located in the `Chapter02`
    root directory (`Java-Deep-Learning-Cookbook/02_Data_Extraction_Transform_and_Loading/`).
    You may keep it in a different location, for example, your local directory, and
    then refer to it in the source code accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Reading and iterating through data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ETL is an important stage in neural network training since it involves data.
    Data extraction, transformation, and loading needs to be addressed before we proceed
    with neural network design. Bad data is a much worse situation than a less efficient
    neural network. We need to have a basic understanding of the following aspects
    as well:'
  prefs: []
  type: TYPE_NORMAL
- en: The type of data you are trying to process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: File-handling strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this recipe, we will demonstrate how to read and iterate data using DataVec.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a prerequisite, make sure that the required Maven dependencies have been
    added for DataVec in your `pom.xml` file, as we mentioned in previous chapter,
    *Configuring Maven for DL4J* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: The following is the sample `pom.xml` file: [https://github.com/rahul-raj/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/pom.xml](https://github.com/rahul-raj/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/pom.xml).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Manage a range of records using **`FileSplit`**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You can find the `FileSplit` example at [https://](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/FileSplitExample.java)[g](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/FileSplitExample.java)[ithu](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/FileSplitExample.java)[b](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/FileSplitExample.java)[.com](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/FileSplitExample.java)[/P](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/FileSplitExample.java)[acktPublishing/Java-Deep-Learnin](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/FileSplitExample.java)[g-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/FileSplitExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/FileSplitExample.java).
  prefs: []
  type: TYPE_NORMAL
- en: 'Manage the URI collection from a file using**`CollectionInputSplit`**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You can find the `CollectionInputSplit` example at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/CollectionInputSplitExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/CollectionInputSplitExample.java).
  prefs: []
  type: TYPE_NORMAL
- en: 'Use** `NumberedFileInputSplit`** to manage data with numbered file formats:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can find the `NumberedFileInputSplit` example at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/NumberedFileInputSplitExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/NumberedFileInputSplitExample.java).
  prefs: []
  type: TYPE_NORMAL
- en: 'Use** `TransformSplit`** to map the input URIs to the different output URIs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You can find the `TransformSplit` example at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/TransformSplitExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/TransformSplitExample.java).
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform URI string replacement using `TransformSplit`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the CSV data for the neural network using `CSVRecordReader`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You can find the `CSVRecordReader` example at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/CSVRecordReaderExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/CSVRecordReaderExample.java).
  prefs: []
  type: TYPE_NORMAL
- en: The dataset for this can be found at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/titanic.csv](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/titanic.csv).
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract image data for the neural network using `ImageRecordReader`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: You can find the `ImageRecordReader` example at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/ImageRecordReaderExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/ImageRecordReaderExample.java).
  prefs: []
  type: TYPE_NORMAL
- en: 'Transform and extract the data using `TransformProcessRecordReader`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: You can find the `TransformProcessRecordReader` example at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/TransformProcessRecordReaderExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/TransformProcessRecordReaderExample.java)
    The dataset for this can be found at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/transform-data.csv](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/transform-data.csv).
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the sequence data using `SequenceRecordReader` and `CodecRecordReader`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: You can find the `CodecRecordReader` example at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/CodecReaderExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/CodecReaderExample.java)[.](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/CodecReaderExample.java)
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to use `RegexSequenceRecordReader`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You can find the `RegexSequenceRecordReader` example at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/RegexSequenceRecordReaderExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/RegexSequenceRecordReaderExample.java).
  prefs: []
  type: TYPE_NORMAL
- en: The dataset for this can be found at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/logdata.zip](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/logdata.zip).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to use `CSVSequenceRecordReader`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: You can find the `CSVSequenceRecordReader` example at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data%20Extraction%2C%20Transform%20and%20Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/SequenceRecordReaderExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/SequenceRecordReaderExample.java).
  prefs: []
  type: TYPE_NORMAL
- en: The dataset for this can be found at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/dataset.zip](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/dataset.zip).
  prefs: []
  type: TYPE_NORMAL
- en: Extract the JSON/XML/YAML data using **`` JacksonLineRecordReader`:` ``**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You can find the `JacksonLineRecordReader` example at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/JacksonLineRecordReaderExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/recordreaderexamples/JacksonLineRecordReaderExample.java).
  prefs: []
  type: TYPE_NORMAL
- en: The dataset for this can be found at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/irisdata.txt](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/irisdata.txt).
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data can be spread across multiple files, subdirectories, or multiple clusters.
    We need a mechanism to extract and handle data in different ways due to various
    constraints, such as size. In distributed environments, a large amount of data
    can be stored as chunks in multiple clusters. DataVec uses `InputSplit` for this
    purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'In step 1, we looked at `FileSplit`, an `InputSplit` implementation that splits
    the root directory into files. `FileSplit` will recursively look for files inside
    the specified directory location. You can also pass an array of strings as a parameter
    to denote the allowed extensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sample input**:A directory location with files:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/9d46081e-a227-496a-b7a1-0a42637fd175.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Sample output**: A list of URIs with the filter applied:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/1e782ca9-2cda-437e-a4ef-8e439d440b05.png)'
  prefs: []
  type: TYPE_IMG
- en: In the sample output, we removed any file paths that are not in `.jpeg` format.
    `CollectionInputSplit` would be useful here if you want to extract data from a
    list of URIs, like we did in step 2\. In step 2, the `temp` directory has a list
    of files in it. We used `CollectionInputSplit` to generate a list of URIs from
    the files. While `FileSplit` is specifically for splitting the directory into
    files (a list of URIs), `CollectionInputSplit` is a simple `InputSplit` implementation
    that handles a collection of URI inputs. If we already have a list of URIs to
    process, then we can simply use `CollectionInputSplit` instead of `FileSplit`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Sample input**: A directory location with files. Refer to the following screenshot
    (directory with image files as input):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/0c1f8ed8-7480-4aca-b2ae-bf2acc32172f.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Sample output**: A list of URIs. Refer to the following list of URIs generated
    by `CollectionInputSplit` from the earlier mentioned input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/8de37b76-2610-4b79-b76f-c5c30b1c21fc.png)'
  prefs: []
  type: TYPE_IMG
- en: In step 3, `NumberedFileInputSplit` generates URIs based on the specified numbering
    format.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we need to pass an appropriate regular expression pattern to generate
    filenames in a sequential format. Otherwise, it will throw runtime errors. A regular
    expression allows us to accept inputs in various numbered formats. `NumberedFileInputSplit`
    will generate a list of URIs that you can pass down the level in order to extract
    and process data. We added the `%d` regular expression at the end of file name
    to specify that numbering is present at the trailing end.
  prefs: []
  type: TYPE_NORMAL
- en: '**Sample input**: A directory location with files in a numbered naming format,
    for example, `file1.txt`, `file2.txt`, and `file3.txt`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sample output**: A list of URIs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/7615f0ca-4974-470f-a2e6-04b1ac481993.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you need to map input URIs to different output URIs, then you will need
    `TransformSplit`. We used it in step 4 to normalize/transform the data URI into
    the required format. It will be especially helpful if features and labels are
    kept at different locations. When step 4 is executed, the `"."` string will be
    stripped from the URIs, which results in the following URIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sample input**: A collection of URIs, just like what we saw in `CollectionInputSplit`.
    However, `TransformSplit` can accept erroneous URIs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/9b26e276-e149-4fc0-ad68-2d53fe43a441.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Sample output**: A list of URIs after formatting them:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/c44c2da0-a1dc-4526-851e-53c06731b8eb.png)'
  prefs: []
  type: TYPE_IMG
- en: After executing step 5, the `-in.csv` substrings in the URIs will be replaced
    with `-out.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: '`CSVRecordReader` is a simple CSV record reader for streaming CSV data. We
    can form data stream objects based on the delimiters and specify various other
    parameters, such as the number of lines to skip from the beginning. In step 6,
    we used `CSVRecordReader` for the same.'
  prefs: []
  type: TYPE_NORMAL
- en: For the `CSVRecordReader` example, use the `titanic.csv` file that's included
    in this chapter's GitHub repository. You need to update the directory path in
    the code to be able to use it.
  prefs: []
  type: TYPE_NORMAL
- en: '`ImageRecordReader` is an image record reader that''s used for streaming image
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: In step 7, we read images from a local filesystem. Then, we scaled them and
    converted them according to a given height, width, and channels. We can also specify
    the labels that are to be tagged for the image data. In order to specify the labels
    for the image set, create a separate subdirectory under the root. Each of them
    represents a label.
  prefs: []
  type: TYPE_NORMAL
- en: In step 7, the first two parameters from the `ImageRecordReader` constructor
    represent the height and width to which images are to be scaled. We usually give
    a value of 3 for channels representing R, G, and B. `parentPathLabelGenerator`
    will define how to tag labels in images. `trainData` is the `inputSplit` we need
    in order to specify the range of records to load, while `transform` is the image
    transformation to be applied while loading images.
  prefs: []
  type: TYPE_NORMAL
- en: For the `ImageRecordReader` example, you can download some sample images from
    `ImageNet`. Each category of images will be represented by a subdirectory. For
    example, you can download dog images and put them under a subdirectory named "dog".
    You will need to provide the parent directory path where all the possible categories
    will be included.
  prefs: []
  type: TYPE_NORMAL
- en: The ImageNet website can be found at [http://www.image-net.org/](http://www.image-net.org/).
  prefs: []
  type: TYPE_NORMAL
- en: '`TransformProcessRecordReader` requires a bit of explanation when it''s used
    in the schema transformation process. `TransformProcessRecordReader` is the end
    product of applying schema transformation to a record reader. This will ensure
    that a defined transformation process is applied before it is fed to the training
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: In step 8, `transformProcess` defines an ordered list of transformations to
    be applied to the given dataset. This can be the removal of unwanted features,
    feature data type conversions, and so on. The intent is to make the data suitable
    for the neural network to process further. You will learn how to create a transformation
    process in the upcoming recipes in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: For the `TransformProcessRecordReader` example, use the `transform-data.csv` file
    that's included in this chapter's GitHub repository. You need to update the file
    path in code to be able to use it.
  prefs: []
  type: TYPE_NORMAL
- en: In step 9, we looked at some of the implementations of `SequenceRecordReader`.
    We use this record reader if we have a sequence of records to process. This record
    reader can be used locally as well as in distributed environments (such as Spark).
  prefs: []
  type: TYPE_NORMAL
- en: 'For the `SequenceRecordReader` example, you need to extract the `dataset.zip` file from
    this chapter''s GitHub repository. After the extraction, you will see two subdirectories
    underneath: `features` and `labels`. In each of them, there is a sequence of files.
    You need to provide the absolute path to these two directories in the code.'
  prefs: []
  type: TYPE_NORMAL
- en: '`CodecRecordReader` is a record reader that handle multimedia datasets and
    can be used for the following purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: H.264 (AVC) main profile decoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MP3 decoder/encoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apple ProRes decoder and encoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: H264 Baseline profile encoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matroska (MKV) demuxer and muxer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MP4 (ISO BMF, QuickTime) demuxer/muxer and tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MPEG 1/2 decoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MPEG PS/TS demuxer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java player applet parsing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VP8 encoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MXF demuxer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CodecRecordReader` makes use of jcodec as the underlying media parser.'
  prefs: []
  type: TYPE_NORMAL
- en: For the `CodecRecordReader` example, you need to provide the directory location
    of a short video file in the code. This video file will be the input for the `CodecRecordReader`
    example.
  prefs: []
  type: TYPE_NORMAL
- en: '`RegexSequenceRecordReader` will consider the entire file as a single sequence
    and will read it one line at a time. Then, it will split each of them using the
    specified regular expression. We can combine `RegexSequenceRecordReader` with `NumberedFileInputSplit`
    to read file sequences. In step 9, we used `RegexSequenceRecordReader` to read
    the transactional logs that were recorded over the time steps (time series data).
    In our dataset (`logdata.zip`), transactional logs are unsupervised data with
    no specification for features or labels.'
  prefs: []
  type: TYPE_NORMAL
- en: For the `RegexSequenceRecordReader` example, you need to extract the `logdata.zip` file
    from this chapter's GitHub repository. After the extraction, you will see a sequence
    of transactional logs with a numbered file naming format. You need to provide
    the absolute path to the extracted directory in the code.
  prefs: []
  type: TYPE_NORMAL
- en: '`CSVSequenceRecordReader` reads the sequences of data in CSV format. Each sequence
    represents a separate CSV file. Each line represents one time step.'
  prefs: []
  type: TYPE_NORMAL
- en: In step 10, `JacksonLineRecordReader` will read the JSON/XML/YAML data line
    by line. It expects a valid JSON entry for each of the lines without a separator
    at the end. This follows the Hadoop convention of ensuring that the split works
    properly in a cluster environment. If the record spans multiple lines, the split
    won't work as expected and may result in calculation errors. Unlike `JacksonRecordReader`,
    `JacksonLineRecordReader` doesn't create the labels automatically and will require
    you to mention the configuration during training.
  prefs: []
  type: TYPE_NORMAL
- en: For the `JacksonLineRecordReader`example, you need to provide the directory
    location of `irisdata.txt`, which is located in this chapter's GitHub repository.
    In the `irisdata.txt`file, each line represents a JSON object.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`JacksonRecordReader` is a record reader that uses the Jackson API. Just like `JacksonLineRecordReader`,
    it also supports JSON, XML, and YAML formats. For `JacksonRecordReader`, the user
    needs to provide a list of fields to read from the JSON/XML/YAML file. This may
    look complicated, but it allows us to parse the files under the following conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: There is no consistent schema for the JSON/XML/YAML data. The order of output
    fields can be provided using the `FieldSelection` object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are fields that are missing in some files but that can be provided using
    the `FieldSelection` object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`JacksonRecordReader` can also be used with `PathLabelGenerator` to append
    the label based on the file path.'
  prefs: []
  type: TYPE_NORMAL
- en: Performing schema transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data transformation is an important data normalization process. It's a possibility
    that bad data occurs, such as duplicates, missing values, non-numeric features,
    and so on. We need to normalize them by applying schema transformation so that
    data can be processed in a neural network. A neural network can only process numeric
    features. In this recipe, we will demonstrate the schema creation process.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Identify the outliers in the data**:For a small dataset with just a few features,
    we can spot outliers/noise via manual inspection. For a dataset with a large number
    of features, we can perform **Principal Component Analysis** (**PCA**), as shown
    in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Use a schema to define the structure of the data**: The following is an example
    of a basic schema for a customer churn dataset. You can download the dataset from [https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling/downloads/bank-customer-churn-modeling.zip/1](https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling/downloads/bank-customer-churn-modeling.zip/1):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start schema creation, we need to examine all the features in our
    dataset. Then, we need to clear all the noisy features, such as name, where it
    is fair to assume that they have no effect on the produced outcome. If some features
    are unclear to you, just keep them as such and include them in the schema. If
    you remove a feature that happens to be a signal unknowingly, then you''ll degrade
    the efficiency of the neural network. This process of removing outliers and keeping
    signals (valid features) is referred to in step 1\. **Principal Component Analysis**
    (**PCA**) would be an ideal choice, and the same has been implemented in ND4J.
    The **PCA** class can perform dimensionality reduction in the case of a dataset
    with a large number of features where you want to reduce the number of features
    to reduce the complexity. Reducing the features just means removing irrelevant
    features (outliers/noise). In step 1, we generated a PCA factor matrix by calling `pca_factor()` with
    the following arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`inputFeatures`: Input features as a matrix'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`projectedDimension`: The number of features to project from the actual set
    of features (for example, 100 important features out of 1,000)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`normalize`: A Boolean variable (true/false) indicating whether the features
    are to be normalized (zero mean)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrix multiplication is performed by calling the `mmul()` method and the end
    result. `reduced` is the feature matrix that we use after performing the dimensionality reduction
    based on the PCA factor. Note that you may need to perform multiple training sessions
    using input features (which are generated using the PCA factor) to understand
    signals.
  prefs: []
  type: TYPE_NORMAL
- en: In step 2, we used the customer churn dataset (the simple dataset that we used
    in the next chapter) to demonstrate the `Schema` creation process. The data types
    that are mentioned in the schema are for the respective features or labels. For
    example, if you want to add a schema definition for an integer feature, then it
    would be `addColumnInteger()`. Similarly, there are other `Schema` methods available
    that we can use to manage other data types.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical variables can be added using `addColumnCategorical()`, as we mentioned
    in step 2\. Here, we marked the categorical variables and the possible values
    were supplied. Even if we get a masked set of features, we can still construct
    their schema if the features are arranged in numbered format (for example, `column1`, `column2`,
    and similar).
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a nutshell, here is what you need to do to build the schema for your datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand your data well. Identify the noise and signals.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Capture features and labels. Identify categorical variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify categorical features that one-hot encoding can be applied to.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pay attention to missing or bad data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add features using type-specific methods such as `addColumnInteger()` and `addColumnsInteger()`,
    where the feature type is an integer. Apply the respective `Builder` method to
    other data types.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add categorical variables using `addColumnCategorical()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Call the `build()` method to build the schema.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that you cannot skip/ignore any features from the dataset without specifying
    them in the schema. You need to remove the outlying features from the dataset,
    create a schema from the remaining features, and then move on to the transformation
    process for further processing. Alternatively, you can keep all the features aside,
    keep all the features in the schema, and then define the outliers during the transformation
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to feature engineering/data analysis, DataVec comes up with its
    own analytic engine to perform data analysis on feature/target variables. For
    local executions, we can make use of `AnalyzeLocal` to return a data analysis
    object that holds information about each column in the dataset. Here is how you
    can create a data analysis object from a record reader object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also analyze your dataset for missing values and check whether it is
    schema-compliant by calling `analyzeQuality()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: For sequence data, you need to use `analyzeQualitySequence()` instead of `analyzeQuality()`.
    For data analysis on Spark, you can make use of the `AnalyzeSpark` utility class
    in place of `AnalyzeLocal`.
  prefs: []
  type: TYPE_NORMAL
- en: Building a transformation process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next step after schema creation is to define a data transformation process
    by adding all the required transformations. We can manage an ordered list of transformations
    using `TransformProcess`. During the schema creation process, we only defined
    a structure for the data with all its existing features and didn't really perform
    transformation. Let's look at how we can transform the features in the datasets
    from a non-numeric format into a numeric format. Neural networks cannot understand
    raw data unless it is mapped to numeric vectors. In this recipe, we will build
    a transformation process from the given schema.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Add a list of transformations to `TransformProcess`. Consider the following
    example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a record reader using `TransformProcessRecordReader` to extract and
    transform the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In step 1, we added all the transformations that are needed for the dataset.
    `TransformProcess` defines an unordered list of all the transformations that we
    want to apply to the dataset. We removed any unnecessary features by calling `removeColumns()`.
    During schema creation, we marked the categorical features in the `Schema`. Now,
    we can actually decide on what kind of transformation is required for a particular
    categorical variable. Categorical variables can be converted into integers by
    calling `categoricalToInteger()`. Categorical variables can undergo one-hot encoding
    if we call `categoricalToOneHot()`. Note that the schema needs to be created prior
    to the transformation process. We need the schema to create a **`TransformProcess`**.
  prefs: []
  type: TYPE_NORMAL
- en: In step 2, we apply the transformations that were added before with the help
    of **`TransformProcessRecordReader`**. All we need to do is create the basic record
    reader object with the raw data and pass it to **`TransformProcessRecordReader`**,
    along with the defined transformation process.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DataVec allows us to do much more within the transformation stage. Here are
    some of the other important transformation features that are available within `TransformProcess`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`addConstantColumn()`: Adds a new column in a dataset, where all the values
    in the column are identical and are as they were specified by the value. This
    method accepts three attributes: the new column name, the new column type, and
    the value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`appendStringColumnTransform()`: Appends a string to the specified column.
    This method accepts two attributes: the column to append to and the string value
    to append.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conditionalCopyValueTransform()`: Replaces the value in a column with the
    value specified in another column if a condition is satisfied. This method accepts
    three attributes: the column to replace the values, the column to refer to the
    values, and the condition to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conditionalReplaceValueTransform()`: Replaces the value in a column with the
    specified value if a condition is satisfied. This method accepts three attributes:
    the column to replace the values, the value to be used as a replacement, and the
    condition to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conditionalReplaceValueTransformWithDefault()`: Replaces the value in a column
    with the specified value if a condition is satisfied. Otherwise, it fills the
    column with another value. This method accepts four attributes: the column to
    replace the values, the value to be used if the condition is satisfied, the value
    to be used if the condition is not satisfied, and the condition to be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use built-in conditions that have been written in DataVec with the transformation
    process or data cleaning process. We can use `NaNColumnCondition` to replace `NaN` values
    and `NullWritableColumnCondition` to replace null values, respectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`stringToTimeTransform()`: Converts a string column into a time column. This
    targets date columns that are saved as a string/object in the dataset. This method
    accepts three attributes: the name of the column to be used, the time format to
    be followed, and the time zone.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reorderColumns()`: Reorders the columns using the newly defined order. We
    can provide the column names in the specified order as attributes to this method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter ()`: Defines a filter process based on the specified condition. If
    the condition is satisfied, remove the example or sequence; otherwise, keep the
    examples or sequence. This method accepts only a single attribute, which is the
    condition/filter to be applied. The `filter()` method is very useful for the data
    cleaning process. If we want to remove `NaN` values from a specified column, we
    can create a filter, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to remove null values from a specified column, we can create a filter,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`stringRemoveWhitespaceTransform()`: This method removes whitespace characters
    from the value of a column. This method accepts only a single attribute, which
    is the column from which whitespace is to be trimmed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`integerMathOp()`: This method is used to perform a mathematical operation
    on an integer column with a scalar value. Similar methods are available for types
    such as `double` and `long`. This method accepts three attributes: the integer
    column to apply the mathematical operation on, the mathematical operation itself,
    and the scalar value to be used for the mathematical operation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TransformProcess` is not just meant for data handling – it can also be used
    to overcome memory bottlenecks by a margin.'
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the DL4J API documentation to find more powerful DataVec features for
    your data analysis tasks. There are other interesting operations supported in
    `TransformPorocess`, such as `reduce()` and `convertToString()`. If you're a data
    analyst, then you should know that many of the data normalization strategies can
    be applied during this stage. You can refer to the DL4J API documentation for
    more information on the normalization strategies that are available on [https://deeplearning4j.org/docs/latest/datavec-normalization](https://deeplearning4j.org/docs/latest/datavec-normalization).
  prefs: []
  type: TYPE_NORMAL
- en: Serializing transforms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DataVec gives us the ability to serialize the transforms so that they're portable
    for production environments. In this recipe, we will serialize the transformation
    process.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Serialize the transforms into a human-readable format. We can transform to
    JSON using `TransformProcess` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can transform to YAML using `TransformProcess` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: You can find an example of this at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/SerializationExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/SerializationExample.java).
  prefs: []
  type: TYPE_NORMAL
- en: 'Deserialize the transforms for JSON to **`TransformProcess`** as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You can do the same for YAML to `TransformProcess` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In step 1, `toJson()` converts `TransformProcess` into a JSON string, while `toYaml()`
    converts `TransformProcess` into a YAML string.
  prefs: []
  type: TYPE_NORMAL
- en: Both of these methods can be used for the serialization of `TransformProcess`.
  prefs: []
  type: TYPE_NORMAL
- en: In step 2, `fromJson()` deserializes a JSON string into a `TransformProcess`,
    while `fromYaml()` deserializes a YAML string into a `TransformProcess`.
  prefs: []
  type: TYPE_NORMAL
- en: '`serializedTransformString` is the JSON/YAML string that needs to be converted
    into a `TrasformProcess`.'
  prefs: []
  type: TYPE_NORMAL
- en: This recipe is relevant while the application is being migrated to a different
    platform.
  prefs: []
  type: TYPE_NORMAL
- en: Executing a transform process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After the transformation process has been defined, we can execute it in a controlled
    pipeline. It can be executed using batch processing, or we can distribute the
    effort to a Spark cluster. Previously, we look at `TransformProcessRecordReader`,
    which automatically does the transformation in the background. We cannot feed
    and execute the data if the dataset is huge. Effort can be distributed to a Spark
    cluster for a larger dataset. You can also perform regular local execution. In
    this recipe, we will discuss how to execute a transform process locally as well
    as remotely.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load the dataset into **`RecordReader`**.Load the CSV data in the case of `CSVRecordReader`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the transforms in local using `LocalTransformExecutor`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the transforms in Spark using `SparkTransformExecutor`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In step 1, we load the dataset into a record reader object. For demonstration
    purposes, we used `CSVRecordReader`.
  prefs: []
  type: TYPE_NORMAL
- en: In step 2, the `execute()` method can only be used if `TransformProcess` returns
    non-sequential data. For local execution, it is assumed that you have loaded the
    dataset into a `RecordReader`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the `LocalTransformExecutor` example, please refer to the `LocalExecuteExample.java`
    file from this source:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/executorexamples/LocalExecuteExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/executorexamples/LocalExecuteExample.java).'
  prefs: []
  type: TYPE_NORMAL
- en: For the `LocalTransformExecutor` example, you need to provide a file path for
    `titanic.csv`. It is located in this chapter's GitHub directory.
  prefs: []
  type: TYPE_NORMAL
- en: In step 3, it is assumed that you have loaded the dataset into a JavaRDD object
    since we need to execute the DataVec transform process in a Spark cluster. Also,
    the `execute()` method can only be used if `TransformProcess` returns non-sequential data.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If `TransformProcess` returns sequential data, then use the `executeSequence()`
    method instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'If you need to join two record readers based on `joinCondition`, then you need
    the `executeJoin()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is an overview of local/Spark executor methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`execute()`: This applies the transformation to the record reader. `LocalTransformExecutor` takes
    the record reader as input, while `SparkTransformExecutor` needs the input data
    to be loaded into a JavaRDD object. This cannot be used for sequential data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`executeSequence()`: This applies the transformation to a sequence reader.
    However, the transform process should start with non-sequential data and then
    convert it into sequential data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`executeJoin()`: This method is used for joining two different input readers
    based on `joinCondition`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`executeSequenceToSeparate()`: This applies the transformation to a sequence
    reader. However, the transform process should start with sequential data and return
    non-sequential data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`executeSequenceToSequence()`: This applies the transformation to a sequence
    reader. However, the transform process should start with sequential data and return
    sequential data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizing data for network efficiency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Normalization makes a neural network's job much easier. It helps the neural
    network treat all the features the same, irrespective of their range of values.
    The main goal of normalization is to arrange the numeric values in a dataset on
    a common scale without actually disturbing the difference in the range of values.
    Not all datasets require a normalization strategy, but if they do have different
    numeric ranges, then it is a crucial step to perform normalization on the data. Normalization has
    a direct impact on the stability/accuracy of the model. ND4J has various preprocessors
    to handle normalization. In this recipe, we will normalize the data.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a dataset iterator from the data. Refer to the following demonstration
    for `RecordReaderDataSetIterator`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Apply the normalization to the dataset by calling the `fit()` method of the
    normalizer implementation. Refer to the following demonstration for the `NormalizerStandardize`preprocessor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Call `setPreprocessor()` to set the preprocessor for the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To start, you need to have an iterator to traverse and prepare the data. In
    step 1, we used the record reader data to create the dataset iterator. The purpose
    of the iterator is to have more control over the data and how it is presented
    to the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Once the appropriate normalization method has been identified (`NormalizerStandardize`,
    in step 2), we use `fit()` to apply the normalization to the dataset. `NormalizerStandardize`
    normalizes the data in such a way that feature values will have a zero mean and
    standard deviation of 1.
  prefs: []
  type: TYPE_NORMAL
- en: The example for this recipe can be found at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/NormalizationExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/02_Data_Extraction_Transform_and_Loading/sourceCode/cookbook-app/src/main/java/com/javadeeplearningcookbook/app/NormalizationExample.java).
  prefs: []
  type: TYPE_NORMAL
- en: '**Sample input**: A dataset iterator that holds feature variables (`INDArray`
    format). Iterators are created from the input data as mentioned in previous recipes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sample output**: Refer to the following snapshot for the normalized features
    (`INDArray` format) after applying normalization on the input data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/239553a8-700f-42fe-9403-1f0b2ecbb7b0.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that we can't skip step 3 while applying normalization. If we don't perform
    step 3, the dataset won't be auto-normalized.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Preprocessors normally have default range limits from `0` to `1`. If you don't
    apply normalization to a dataset with a wide range of numeric values (when feature
    values that are too low and too high are present), then the neural network will
    tend to favor the feature values that have high numeric values. Hence, the accuracy
    of the neural network could be significantly reduced.
  prefs: []
  type: TYPE_NORMAL
- en: If values are spread across symmetric intervals such as (`0`,`1`), then all
    the feature values are considered equivalent during training. Hence, it also has
    an impact on the neural network's generalization.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the preprocessors that are provided by ND4J:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NormalizerStandardize`: A preprocessor for datasets that normalizes feature
    values so that they have a *zero* mean and a standard deviation of 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MultiNormalizerStandardize`: A preprocessor for multi-datasets that normalizes
    feature values so that they have a zero mean and a standard deviation of 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NormalizerMinMaxScaler`: A preprocessor for datasets that normalizes feature
    values so that they lie between a minimum and maximum value that''s been specified.
    The default range is 0 to 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MultiNormalizerMinMaxScaler`: A preprocessor for multi-datasets that normalizes
    feature values that lie between a minimum and maximum value that''s been specified.
    The default range is 0 to 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ImagePreProcessingScaler`: A preprocessor for images with minimum and maximum
    scaling. The default ranges are (`miRange`, `maxRange`) – (`0`,`1`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VGG16ImagePreProcessor`: A preprocessor specifically for the VGG16 network
    architecture. It computes the mean RGB value and subtracts it from each pixel
    on the training set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
