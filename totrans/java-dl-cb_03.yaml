- en: Building Deep Neural Networks for Binary Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to develop a **Deep Neural Network** (**DNN**)
    using the standard feedforward network architecture. We will add components and
    changes to the application while we progress through the recipes. Make sure to
    revisit [Chapter 1](f88b350b-16e2-425b-8425-4631187c7803.xhtml), *Introduction
    to Deep Learning in Java*, and [Chapter 2](6ac5dff5-cc98-4d52-bc59-1da01b2aeded.xhtml), *Data
    Extraction, Transformation, and Loading*, if you have not already done so. This
    is to ensure better understanding of the recipes in this chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: We will take an example of a customer retention prediction for the demonstration
    of the standard feedforward network. This is a crucial real-world problem that
    every business wants to solve. Businesses would like to invest more in happy customers,
    who tend to stay customers for longer periods of time. At the same time, predictions
    of losing customers will make businesses focus more on decisions that encourage
    customers not to take their business elsewhere.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Remember that a feedforward network doesn't really give you any hints about
    the actual features that decide the outcome. It just predicts whether a customer
    continues to patronize the organization or not. The actual feature signals are
    hidden, and it is left to the neural network to decide. If you want to record
    the actual feature signals that control the prediction outcome, then you could
    use an autoencoder for the task. Let's examine how to construct a feedforward
    network for our aforementioned use case.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Extracting data from CSV input
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing anomalies from the data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying transformations to the data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing input layers for the neural network model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing hidden layers for the neural network model
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing output layers for the neural network model
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and evaluating the neural network model for CSV data
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying the neural network model and using it as an API
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Make sure the following requirements are satisfied:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: JDK 8 is installed and added to `PATH`. Source code requires JDK 8 for execution.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maven is installed/added to `PATH`. We use Maven to build the application JAR
    file afterward.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concrete implementation for the use case discussed in this chapter (Customer
    retention prediction) can be found at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CustomerRetentionPredictionExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CustomerRetentionPredictionExample.java).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: After cloning our GitHub repository, navigate to the `Java-Deep-Learning-Cookbook/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode` directory.
    Then import the `cookbookapp` project into your IDE as a Maven project by importing `pom.xml`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Dataset is already included in the `resources` directory (`Churn_Modelling.csv`)
    of the `cookbookapp` project.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: However, the dataset can be downloaded at [https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling/downloads/bank-customer-churn-modeling.zip/1](https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling/downloads/bank-customer-churn-modeling.zip/1).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Extracting data from CSV input
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**ETL** (short for **Extract, Transform and Load**) is the first stage prior
    to network training. Customer churn data is in CSV format. We need to extract
    it and put it in a record reader object to process further. In this recipe, we
    extract the data from a CSV file.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create `CSVRecordReader` to hold customer churn data:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Add data to `CSVRecordReader`:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: How it works...
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The CSV data from the dataset has 14 features. Each row represents a customer/record,
    as shown in the following screenshot:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9530d8be-7666-495f-aafc-18676893e788.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: Our dataset is a CSV file containing 10,000 customer records, where each record
    is labeled as to whether the customer left the business or not. Columns 0 to 13
    represent input features. The 14^(th) column, `Exited`, indicates the label or
    prediction outcome. We're dealing with a supervised model, and each prediction
    is labeled with `0` or `1`, where `0` indicates a happy customer, and `1` indicates
    an unhappy customer who has left the business. The first row in the dataset is
    just feature labels, and we don't need them while processing the data. So, we
    have skipped the first line while we created the record reader instance in step
    1\. In step 1, `1` is the number of rows to be skipped on the dataset. Also, we
    have mentioned a comma delimiter (`,`) because we are using a CSV file. In step
    2, we used `FileSplit` to mention the customer churn dataset file. We can also
    deal with multiple dataset files using other `InputSplit` implementations, such
    as `CollectionInputSplit`, `NumberedFileInputSplit`, and so on.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Removing anomalies from the data
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For supervised datasets, manual inspection works fine for datasets with fewer
    features. As the feature count goes high, manual inspection becomes impractical.
    We need to perform feature selection techniques, such as chi-square test, random
    forest, and so on, to deal with the volume of features. We can also use an autoencoder to
    narrow down the relevant features. Remember that each feature should have a fair
    contribution toward the prediction outcomes. So, we need to remove noise features
    from the raw dataset and keep everything else as is, including any uncertain features.
    In this recipe, we will walk through the steps to identify anomalies in the data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Leave out all the noise features before training the neural network. Remove
    noise features at the schema transformation stage:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Identify the missing values using the DataVec analysis API:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Remove null values using a schema transformation:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Remove NaN values using a schema transformation:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: How it works...
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you recall our customer churn dataset, there are 14 features:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5dee1e86-791a-445c-9737-deaf4dca7527.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: After performing step 1, you have 11 valid features remaining. The following
    marked features have zero significance on the prediction outcome. For example,
    the customer name doesn't influence whether a customer would leave the organization
    or not.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40ca2a93-1976-4b5f-93b3-d99d95d8c993.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: In the above screenshot, we have marked the features that are not required for
    the training. These features can be removed from the dataset as it doesn't have
    any impact on outcome.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: In step 1, we tagged the noise features (`RowNumber`, `Customerid`, and `Surname`)
    in our dataset for removal during the schema transformation process using the `removeColumns()` method.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: The customer churn dataset used in this chapter has only 14 features. Also,
    the feature labels are meaningful. So, a manual inspection was just enough. In
    the case of a large number of features, you might need to consider using **PCA**
    (short for **Principal Component Analysis**), as explained in the previous chapter.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'In step 2, we used the `AnalyzeLocal` utility class to find the missing values
    in the dataset by calling `analyzeQuality()`. You should see the following result
    when you print out the information in the `DataQualityAnalysis` object:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/621c3773-9d43-498e-967a-efe1e0b9a104.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: As you can see in the preceding screenshot, each of the features is analyzed
    for its quality (in terms of invalid/missing data), and the count is displayed
    for us to decide if we need to normalize it further. Since all features appeared
    to be OK, we can proceed further.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: There are two ways in which missing values can be handled. Either we remove
    the entire record or replace them with a value. In most cases, we don't remove
    records; instead, we replace them with a value to indicate absence. We can do
    it during the transformation process using `conditionalReplaceValueTransform()`
    or `conditionalReplaceValueTransformWithDefault()`. In step 3/4, we removed missing
    or invalid values from the dataset. Note that the feature needs to be known beforehand.
    We cannot check the whole set of features for this purpose. At the moment, DataVec
    doesn't support this functionality. You may perform step 2 to identify features
    that need attention.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We discussed earlier in this chapter how to use the `AnalyzeLocal` utility
    class to find out missing values. We can also perform extended data analysis using `AnalyzeLocal`.
    We can create a data analysis object that holds information on each column present
    in the dataset. It can be created by calling `analyze()`, as we discussed in the
    previous chapter. If you try to print out the information on the data analysis
    object, it will look like the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9107f6f7-a7c4-4830-9296-a068771ee108.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: It will calculate the standard deviation, mean, and the min/max values for all
    the features in the dataset. The count of features is also calculated, which will
    be helpful toward identifying missing or invalid values in features.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a761959-3786-4d70-a828-462ebf049e10.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: Both screenshots on the above indicate the data analysis results returned by
    calling `analyze()` method. For the customer churn dataset, we should have a total
    count of 10,000 for all features as the total number of records present in our
    dataset is 10,000.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Applying transformations to the data
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data transformation is a crucial data normalization procedure that must be done
    before we feed the data to a neural network. We need to transform non-numeric
    features to numeric values and handle missing values. In this recipe, we will
    perform schema transformation, and create dataset iterators after transformation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Add features and labels into the schema:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Identify and add categorical features to the schema:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Remove noise features from the dataset:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Transform categorical variables:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Apply one-hot encoding by calling `categoricalToOneHot()`:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Remove the correlation dependency on the `Geography` feature by calling `removeColumns()`:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here, we selected `France` as the correlation variable.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the data and apply the transformation using `TransformProcessRecordReader`:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Create a dataset iterator to train/test:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Normalize the dataset:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Split the main dataset iterator to train and test iterators:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Generate train/test iterators from `DataSetIteratorSplitter`:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: How it works...
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All features and labels need to be added to the schema as mentioned in step
    1 and step 2\. If we don't do that, then DataVec will throw runtime errors during
    data extraction/loading.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73912ea2-ea7a-4786-ab3b-953c6202b743.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: In the preceding screenshot, the runtime exception is thrown by DataVec because
    of unmatched count of features. This will happen if we provide a different value
    for input neurons instead of the actual count of features in the dataset.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: From the error description, it is clear that we have only added 13 features
    in the schema, which ended in a runtime error during execution. The first three
    features, named `Rownumber`, `Customerid`, and `Surname`, are to be added to the
    schema. Note that we need to tag these features in the schema, even though we
    found them to be noise features. You can also remove these features manually from
    the dataset. If you do that, you don't have to add them in the schema, and, thus,
    there is no need to handle them in the transformation stage
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从错误描述中可以看出，我们仅在模式中添加了13个特征，这导致在执行过程中发生了运行时错误。前三个特征，分别为`Rownumber`、`Customerid`和`Surname`，需要添加到模式中。请注意，尽管我们发现它们是噪声特征，但仍需要在模式中标记这些特征。你也可以手动从数据集中删除这些特征。如果你这么做，就不需要在模式中添加它们，因此也无需在转换阶段处理它们。
- en: For large datasets, you may add all features from the dataset to the schema,
    unless your analysis identifies them as noise. Similarly, we need to add the other
    feature variables such as `Age`, `Tenure`, `Balance`, `NumOfProducts`, `HasCrCard`, `IsActiveMember`, `EstimatedSalary`,
    and `Exited`. Note the variable types while adding them to schema. For example, `Balance` and `EstimatedSalary` have
    floating point precision, so consider their datatype as double and use `addColumnDouble()` to
    add them to schema.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大型数据集，除非分析结果将其识别为噪声，否则可以将数据集中的所有特征添加到模式中。同样，我们需要将其他特征变量如`Age`、`Tenure`、`Balance`、`NumOfProducts`、`HasCrCard`、`IsActiveMember`、`EstimatedSalary`和`Exited`添加到模式中。添加它们时，请注意变量类型。例如，`Balance`和`EstimatedSalary`具有浮动点精度，因此考虑将它们的数据类型设为double，并使用`addColumnDouble()`将它们添加到模式中。
- en: We have two features named gender and geography that require special treatment.
    These two features are non-numeric and their feature values represent categorical
    values compared to other fields in the dataset. Any non-numeric features need
    to transform numeric values so that the neural network can perform statistical
    computations on feature values. In step 2, we added categorical variables to the
    schema using `addColumnCategorical()`. We need to specify the categorical values
    in a list, and `addColumnCategorical()` will tag the integer values based on the
    feature values mentioned. For example, the `Male` and `Female` values in the categorical
    variable `Gender` will be tagged as `0` and `1` respectively. In step 2, we added
    the possible values for the categorical variables in a list. If your dataset has
    any other unknown value present for a categorical variable (other than the ones
    mentioned in the schema), DataVec will throw an error during execution.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个特征，分别为gender和geography，需要特别处理。这两个特征是非数字型的，它们的特征值表示类别值，而不是数据集中其他字段的数值。任何非数字特征都需要转换为数值，以便神经网络能够对特征值进行统计计算。在步骤2中，我们使用`addColumnCategorical()`将类别变量添加到模式中。我们需要在列表中指定类别值，`addColumnCategorical()`将基于指定的特征值标记整数值。例如，类别变量`Gender`中的`Male`和`Female`值将分别被标记为`0`和`1`。在步骤2中，我们将类别变量的可能值添加到列表中。如果数据集中有其他未知类别值（与模式中提到的值不同），DataVec将在执行过程中抛出错误。
- en: In step 3, we marked the noise features for removal during the transformation
    process by calling `removeColumns()`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤3中，我们通过调用`removeColumns()`标记了需要在转换过程中移除的噪声特征。
- en: In step 4, we performed one-hot encoding for the `Geography` categorical variable.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤4中，我们对`Geography`类别变量进行了独热编码。
- en: '`Geography` has three categorical values, and hence it will take the 0, 1,
    and 2 values after the transformation. The ideal way of transforming non-numeric
    values is to convert them to a value of zero (0) and one (1). It would significantly
    ease the effort of the neural network. Also, the normal integer encoding is applicable
    only if there exists an ordinal relationship between the variables. The risk here
    is we''re assuming that there exists natural ordering between the variables. Such
    an assumption can result in the neural network showing unpredictable behavior.
    So, we have removed the correlation variable in step 6\. For the demonstration,
    we picked `France` as a correlation variable in step 6\. However, you can choose
    any one among the three categorical values. This is to remove any correlation
    dependency that affects neural network performance and stability. After step 6, the
    resultant schema for the `Geography` feature will look like the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`Geography`有三个分类值，因此在转换后它将采用0、1和2的值。转换非数值型值的理想方式是将它们转换为零（0）和一（1）的值。这将显著减轻神经网络的负担。此外，普通的整数编码仅在变量之间存在序数关系时适用。这里的风险在于，我们假设变量之间存在自然的顺序关系。这种假设可能会导致神经网络出现不可预测的行为。因此，我们在第6步中删除了相关变量。为了演示，我们在第6步中选择了`France`作为相关变量。但你可以从三个分类值中选择任何一个。这是为了消除任何影响神经网络性能和稳定性的相关性依赖。第6步后，`Geography`特征的最终模式将如下所示：'
- en: '![](img/7be56c83-76d3-420b-9f58-f2a314822f5b.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7be56c83-76d3-420b-9f58-f2a314822f5b.png)'
- en: 'In step 8, we created dataset iterators from the record reader objects. Here
    are the attributes for the `RecordReaderDataSetIterator` builder method and their
    respective roles:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在第8步中，我们从记录读取器对象创建了数据集迭代器。以下是`RecordReaderDataSetIterator`构建方法的属性及其各自的作用：
- en: '`labelIndex`: The index location in the CSV data where our labels (outcomes)
    are located.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labelIndex`：在CSV数据中标签（结果）所在的索引位置。'
- en: '`numClasses`: The number of labels (outcomes) from the dataset.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numClasses`：数据集中标签（结果）的数量。'
- en: '`batchSize`: The block of data that passes through the neural network. If you
    specify a batch size of 10 and there are 10,000 records, then there will be 1,000
    batches holding 10 records each.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batchSize`：通过神经网络的数据块。如果你指定了批量大小为10且有10,000条记录，那么将会有1,000个批次，每个批次包含10条记录。'
- en: Also, we have a binary classification problem here, and so we used the `classification()` method
    to specify the label index and number of labels.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们这里有一个二分类问题，因此我们使用了`classification()`方法来指定标签索引和标签数量。
- en: For some of the features in the dataset, you might observe huge differences
    in the feature value ranges. Some of the features have small numeric values, while
    some have very large numeric values. These large/small values can be interpreted
    in the wrong way by the neural network. Neural networks can falsely assign high/low
    priority to these features and that results in wrong or fluctuating predictions.
    In order to avoid this situation, we have to normalize the dataset before feeding
    it to the neural network. Hence we performed normalization as in step 9.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据集中的某些特征，你可能会观察到特征值范围之间的巨大差异。有些特征的数值较小，而有些特征的数值非常大。这些大/小数值可能会被神经网络误解。神经网络可能会错误地为这些特征分配高/低优先级，从而导致错误或波动的预测。为了避免这种情况，我们必须在将数据集输入到神经网络之前对其进行归一化。因此，我们在第9步中执行了归一化操作。
- en: In step 10, we used `DataSetIteratorSplitter` to split the main dataset for
    a training or test purpose.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在第10步中，我们使用`DataSetIteratorSplitter`将主数据集拆分用于训练或测试。
- en: 'The following are the parameters of `DataSetIteratorSplitter`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`DataSetIteratorSplitter`的参数：
- en: '`totalNoOfBatches`: If you specify a batch size of 10 for 10,000 records, then
    you need to specify 1,000 as the total number of batches.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`totalNoOfBatches`：如果你指定了10的批量大小并且有10,000条记录，那么需要指定1,000作为批次的总数。'
- en: '`ratio`: This is the ratio at which the splitter splits the iterator set. If
    you specify 0.8, then it means 80% of data will be used for training and the remaining
    20% will be used for testing/evaluation.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ratio`：这是分割器分割迭代器集的比例。如果你指定0.8，这意味着80%的数据将用于训练，剩余的20%将用于测试/评估。'
- en: Designing input layers for the neural network model
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为神经网络模型设计输入层
- en: Input layer design requires an understanding of how the data flows into the
    system. We have CSV data as input, and we need to inspect the features to decide
    on the input attributes. Layers are core components in neural network architecture.
    In this recipe, we will configure input layers for the neural network.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层设计需要理解数据如何流入系统。我们有CSV数据作为输入，需要检查特征来决定输入属性。层是神经网络架构的核心组件。在这个示例中，我们将为神经网络配置输入层。
- en: Getting ready
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We need to decide the number of input neurons before designing the input layer. It
    can be derived from the feature shape. For instance, we have 13 input features
    (excluding the label). But after applying the transformation, we have a total
    of 11 feature columns present in the dataset. Noise features are removed and categorical
    variables are transformed during the schema transformation. So, the final transformed
    data will have 11 input features. There are no specific requirements for outgoing
    neurons from the input layer. If we assign the wrong number of incoming neurons
    at the input layer, we may end up with a runtime error:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在设计输入层之前决定输入神经元的数量。它可以通过特征形状得出。例如，我们有13个输入特征（不包括标签）。但在应用变换后，我们的数据集总共有11个特征列。噪声特征被移除，类别变量在模式转换过程中被转化。因此，最终的转换数据将有11个输入特征。输入层的输出神经元没有特定要求。如果我们为输入层分配错误数量的输入神经元，可能会导致运行时错误：
- en: '![](img/c3ddc74c-a677-46b4-a5e1-c95d2beebdc4.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c3ddc74c-a677-46b4-a5e1-c95d2beebdc4.png)'
- en: The DL4J error stack is pretty much self-explanatory as to the possible reason.
    It points out the exact layer where it needs a fix (`layer0`, in the preceding
    example).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: DL4J的错误堆栈几乎可以自解释可能的原因。它指明了需要修复的具体层（前面示例中的`layer0`）。
- en: How to do it...
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Define the neural network configuration using `MultiLayerConfiguration`:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`MultiLayerConfiguration`定义神经网络配置：
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Define the input layer configuration using `DenseLayer`:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`DenseLayer`定义输入层配置：
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: How it works...
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'We added layers to the network by calling the `layer()` method as mentioned
    in step 2\. Input layers are added using `DenseLayer`*. *Also, we need to add
    an activation function for the input layer. We specified the activation function
    by calling the `activation()` method. We discussed activation functions in [Chapter
    1](f88b350b-16e2-425b-8425-4631187c7803.xhtml), *Introduction to Deep Learning
    in Java*. You can use one of the available activation functions in DL4J to the `activation()` method.
    The most generic activation function used is `RELU`. Here are roles of other methods
    in layer design:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过调用`layer()`方法向网络中添加了层，如步骤2所述。输入层通过`DenseLayer`添加*。*此外，我们需要为输入层添加激活函数。我们通过调用`activation()`方法指定激活函数。我们在[第一章](f88b350b-16e2-425b-8425-4631187c7803.xhtml)中讨论了激活函数，*《Java深度学习简介》*。你可以使用DL4J中可用的激活函数之一来设置`activation()`方法。最常用的激活函数是`RELU`。以下是其他方法在层设计中的作用：
- en: '`nIn()`: This refers to the number of inputs for the layer. For an input layer,
    this is nothing but the number of input features.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nIn()`：这指的是该层的输入数量。对于输入层，它就是输入特征的数量。'
- en: '`nOut()`: This refers to number of outputs to next dense layer in neural network.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nOut()`：这指的是神经网络中到下一个全连接层的输出数量。'
- en: Designing hidden layers for the neural network model
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为神经网络模型设计隐藏层
- en: Hidden layers are the heart of a neural network. The actual decision process
    happens there. The design of the hidden layers is based on hitting a level beyond
    which a neural network cannot be optimized further. This level can be defined
    as the optimal number of hidden layers that produce optimal results.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层是神经网络的核心。实际的决策过程发生在那里。隐藏层的设计是基于达到某个层次，超过这个层次，神经网络就无法再优化的水平。这个水平可以定义为产生最佳结果的最优隐藏层数量。
- en: Hidden layers are the place where the neural network transforms the inputs into
    a different format that the output layer can consume and use to make predictions.
    In this recipe, we will design hidden layers for a neural network.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层是神经网络将输入转化为输出层能够使用并进行预测的不同格式的地方。在这个示例中，我们将为神经网络设计隐藏层。
- en: How to do it...
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Determine the incoming/outgoing connections. Set the following:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定输入/输出连接。设置如下：
- en: '[PRE19]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Configure hidden layers using `DenseLayer`:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`DenseLayer`配置隐藏层：
- en: '[PRE20]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: How it works...
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: For step 1, if the neural network has only single hidden layer, then the number
    of neurons (inputs) in the hidden layer should be the same as the number of outgoing
    connections from the preceding layer. If you have multiple hidden layers, you
    will also need to confirm this for the preceding hidden layers.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步，如果神经网络只有一个隐藏层，那么隐藏层中的神经元（输入）的数量应该与前一层的输出连接数相同。如果你有多个隐藏层，你还需要确认前一层隐藏层的这一点。
- en: After you make sure that the number of input neurons are the same as number
    of the outgoing neurons in the preceding layer, you can create hidden layers using
    `DenseLayer`. In step 2, we used `DenseLayer` to create hidden layers for the input
    layers. In practice, we need to evaluate the model multiple times to understand
    the network performance. There's no constant layer configuration that works well
    for all the models. Also, `RELU` is the preferred activation function for hidden
    layers, due to its nonlinear nature.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在确保输入层的神经元数量与前一层的输出神经元数量相同后，你可以使用 `DenseLayer` 创建隐藏层。在第二步中，我们使用 `DenseLayer` 为输入层创建了隐藏层。实际上，我们需要多次评估模型，以了解网络的表现。没有一种常规的层配置适用于所有模型。同时，`RELU` 是隐藏层的首选激活函数，因为它具有非线性特性。
- en: Designing output layers for the neural network model
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为神经网络模型设计输出层
- en: Output layer design requires an understanding of the expected output. We have
    CSV data as input, and the output layer relies on the number of labels in the
    dataset. Output layers are the place where the actual prediction is formed based
    on the learning process that happened in the hidden layers.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层设计需要理解期望的输出。我们的输入是CSV数据，输出层则依赖于数据集中的标签数量。输出层是根据隐藏层的学习过程形成实际预测的地方。
- en: In this recipe, we will design output layers for the neural network.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案中，我们将为神经网络设计输出层。
- en: How to do it...
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'Determine the incoming/outgoing connections. Set the following:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定输入/输出连接。设置以下内容：
- en: '[PRE21]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Configure the output layer for the neural network:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置神经网络的输出层：
- en: '[PRE22]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: How it works...
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: For step 1, we need to make sure that `nOut()` for the preceding layer should
    have the same number of neurons as `nIn()` for the output layer.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步，我们需要确保前一层的 `nOut()` 与输出层的 `nIn()` 拥有相同数量的神经元。
- en: So, `incomingConnectionCount` should be the same as `outgoingConnectionCount`
    from the preceding layer.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 所以， `incomingConnectionCount` 应该与前一层的 `outgoingConnectionCount` 相同。
- en: We discussed the `SOFTMAX` activation function earlier in [Chapter 1](f88b350b-16e2-425b-8425-4631187c7803.xhtml), *Introduction
    to Deep Learning in Java*. Our use case (customer churn) is an example for the
    binary classification model. We are looking for a probabilistic outcome, that
    is, the probability of a customer being labeled *happy* or *unhappy, *where `0`
    represents a happy customer and `1` represents an unhappy customer. This probability
    will be evaluated, and the neural network will train itself during the training
    process.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第一章](f88b350b-16e2-425b-8425-4631187c7803.xhtml)《*Java中的深度学习介绍*》中讨论过 `SOFTMAX` 激活函数。我们的使用案例（客户流失）是二分类模型的一个例子。我们希望得到一个概率性结果，也就是客户被标记为*开心*或*不开心*的概率，其中 `0` 代表开心的客户，`1` 代表不开心的客户。这个概率将被评估，神经网络将在训练过程中自我训练。
- en: The proper activation function at the output layer would be `SOFTMAX`. This
    is because we need the probability of the occurrence of labels and the probabilities
    should sum to 1\. `SOFTMAX` along with the log loss function produces good results
    for classification models. The introduction of `weightsArray` is to enforce a
    preference for a particular label among others in case of any data imbalance.
    In step 2, output layers are created using the `OutputLayer` class. The only difference
    is that `OutputLayer` expects an error function to calculate the error rate while
    making predictions. In our case, we used `LossMCXENT`, which is a multi-class
    cross entropy error function. Our customer churn example follows a binary classification
    model; however, we can still use this error function since we have two classes
    (labels) in our example. In step 2, `labelCount` would be 2.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluating the neural network model for CSV data
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During the training process, the neural network learns to perform the expected
    task. For every iteration/epoch, the neural network will evaluate its training
    knowledge. Accordingly, it will re-iterate the layers with updated gradient values
    to minimize the error produced at the output layer. Also, note that labels (`0`
    and `1` ) are not uniformly distributed across the dataset. So, we might need
    to consider adding weights to the label that appears less in the dataset. This
    is highly recommended before we proceed with the actual training session. In this
    recipe, we will train the neural network and evaluate the resultant model.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create an array to assign weights to minor labels:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Modify `OutPutLayer` to evenly balance the labels in the dataset:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Initialize the neural network and add the training listeners:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Add the DL4J UI Maven dependency to analyze the training process:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Start the UI server and add temporary storage to store the model information:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Replace `InMemoryStatsStorage` with `FileStatsStorage` (in case of memory restrictions):'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Assign the temporary storage space to the UI server:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Train the neural network by calling `fit()`:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Evaluate the model by calling `evaluate()`:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: How it works...
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A neural network increases its efficiency when it improves its generalization
    power. A neural network should not just memorize a certain decision-making process
    in favor of a particular label. If it does, our outcomes will be biased and wrong.
    So, it is good to have a dataset where the labels are uniformly distributed. If
    they're not uniformly distributed, then we might have to adjust a few things while
    calculating the error rate. For this purpose, we introduced a `weightsArray` in
    step 1 and added to `OutputLayer` in step 2.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: For `weightsArray = {0.35, 0.65}`, the network gives more priority to the outcomes
    of `1` (customer unhappy). As we discussed earlier in this chapter, the `Exited` column
    represents the label. If we observe the dataset, it is evident that outcomes labeled `0` (customer
    happy) have more records in the dataset compared to `1`. Hence, we need to assign
    additional priority to `1` to evenly balance the dataset. Unless we do that, our
    neural network may over fit and will be biased toward the `1` label.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: In step 3, we added `ScoreIterationListener` to log the training process on
    the console. Note that `iterationCount` is the number of iterations in which it
    should log the network score. Remember, `iterationCount`is not the epoch. We say
    an epoch has happened when the entire dataset has traveled back and forth (backpropagation)
    once through the whole neural network.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'In step 8, we used `dataSetIteratorSplitter` to obtain the training dataset
    iterator and trained our model on top of it. If you configured loggers properly,
    you should see the training instance is progressing as shown here:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ecf204a-abe2-4e43-b91b-322a0cc52540.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: The score referred to in the screenshot is not the success rate; it is the error
    rate calculated by the error function for each iteration.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'We configured the DL4J **user interface** (**UI**) in step 4, 5, and 6. DL4J
    provides a UI to visualize the current network status and training progress in
    your browser (real-time monitoring). This will help further tuning the neural
    network training. `StatsListener` will be responsible for triggering the UI monitoring
    while the training starts. The port number for UI server is `9000`. While the
    training is in progress, hit the UI server at `localhost:9000`. We should be able
    to see something like the following:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17bebe0a-7847-4d9a-9bd1-2b548b788b06.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: We can refer to the first graph seen in the Overview section for the Model Score analysis.
    The Iteration is plotted on the *x* axis, and the Model Score is on the *y *axis
    in the graph.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also further expand our research on how the Activations, Gradients,
    and the Updates parameters performed during the training process by inspecting
    the parameter values plotted on graphs:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c1035e0-e297-43a7-a888-ab4c9e8c935e.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: The *x* axis refers to the number of iterations in both the graphs. The *y* axis
    in the parameter update graph refers to the parameter update ratio, and the *y* axis
    in the activation/gradient graphs refers to the standard deviation.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to have layer-wise analysis. We just need to click on the Model tab
    on the left sidebar and choose the layer of choice for further analysis:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77ed7dd4-5797-4cae-b87c-fd26f1bf72a2.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: 'For analysis of memory consumption and JVM, we can navigate to the System tab
    on the left sidebar:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a7cc605-7c53-45af-8f80-4cb47ba58eec.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: 'We can also review the hardware/software metrics in detail at the same place:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/832323a4-3b9d-4ac1-bcdf-b02d2aaccb33.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
- en: This is very useful for benchmarking as well. As we can see, the memory consumption
    of the neural network is clearly marked and the JVM/off-heap memory consumption
    is mentioned in the UI to analyze how well the benchmarking is done.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'After step 8, evaluation results will be displayed on console:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/67b76595-73b8-49a8-81b5-14b956783fda.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: In the above screenshot, the console shows various evaluation metrics by which
    the model is evaluated. We cannot rely on a specific metrics in all the cases;
    hence, it is good to evaluate the model against multiple metrics.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Our model is showing an accuracy level of 85.75% at the moment. We have four
    different performance metrics, named accuracy, precision, recall, and F1 score.
    As you can see in the preceding screenshot, recall metrics are not so good, which
    means our model still has false negative cases. The F1 score is also significant
    here, since our dataset has an uneven proportion of output classes. We will not
    discuss these metrics in detail, since they are outside the scope of this book.
    Just remember that all these metrics are important for consideration, rather than
    just relying on accuracy alone. Of course, the evaluation trade-offs vary depending
    upon the problem. The current code has already been optimized. Hence, you will
    find almost stable accuracy from the evaluation metrics. For a well-trained network
    model, these performance metrics will have values close to `1`.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: It is important to check how stable our evaluation metrics are. If we notice
    unstable evaluation metrics for unseen data, then we need to reconsider changes
    in the network configuration.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions on the output layer have influence on the stability of
    the outputs. Hence, a good understanding on output requirements will definitely
    save you a lot of time choosing an appropriate output function (loss function).
    We need to ensure stable predictive power from our neural network.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning rate is one of the factors that decides the efficiency of the neural
    network. A high learning rate will diverge from the actual output, while a low
    learning rate will result in slow learning due to slow convergence. Neural network
    efficiency also depends on the weights that we assign to the neurons in every
    layer. Hence, a uniform distribution of weights during the early stages of training
    might help.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: The most commonly followed approach is to introduce dropouts to the layers.
    This forces the neural network to ignore some of the neurons during the training
    process. This will effectively prevent the neural network from memorizing the
    prediction process. How do we find out if a network has memorized the results?
    Well, we just need to expose the network to new data. If your accuracy metrics
    become worse after that, then you've got a case of overfitting.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Another possibility for increasing the efficiency of the neural network (and
    thus reducing overfitting) is to try for L1/L2 regularization in the network layers.
    When we add L1/L2 regularization to network layers, it will add an extra penalty
    term to the error function. L1 penalizes with the sum of the absolute value of
    the weights in the neurons, while L2 penalizes using the sum of squares of the
    weights. L2 regularization will give much better predictions when the output variable
    is a function of all input features. However, L1 regularization is preferred when
    the dataset has outliers and if not all the attributes are contributing to predicting
    the output variable. In most cases, the major reason for overfitting is the issue
    of memorization. Also, if we drop too many neurons, it will eventually underfit
    the data. This means we lose more useful data than we need to.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Note that the trade-off can vary depending on the different kinds of problems.
    Accuracy alone cannot ensure a good model performance every time. It is good to
    measure precision if we cannot afford the cost of a false positive prediction
    (such as in spam email detection). It is good to measure recall if we cannot afford
    the cost of a false negative prediction (such as in fraudulent transaction detection).
    The F1 score is optimal if there's an uneven distribution of the classes in the
    dataset. ROC curves are good to measure when there are approximately equal numbers
    of observations for each output class.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Once the evaluations are stable, we can check on the means to optimize the efficiency
    of the neural network. There are multiple methods to choose from. We can perform
    several training sessions to try to find out the optimal number of hidden layers,
    epochs, dropouts, and activation functions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot points to various hyper parameters that can influence
    neural network efficiency:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7069f9da-c4bb-47a3-a704-6266367bfe9b.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: Note that `dropOut(0.9)` means we ignore 10% of neurons during training.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'Other attributes/methods in the screenshot are the following:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '`weightInit()` : This is to specify how the weights are assigned neurons at
    each layer.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`updater()`: This is to specify the gradient updater configuration. `Adam`
    is a gradient update algorithm.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [Chapter 12](0db31248-e40b-4479-9939-0baccb0e11d1.xhtml), *Benchmarking and
    Neural Network Optimization*, we will walk through an example of hyperparameter
    optimization to automatically find the optimal parameters for you. It simply performs
    multiple training sessions on our behalf to find the optimal values by a single
    program execution. You may refer to [Chapter 12](0db31248-e40b-4479-9939-0baccb0e11d1.xhtml), *Benchmarking
    and Neural Network Optimization,* if you're interested in applying benchmarks
    to the application.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the neural network model and using it as an API
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After the training instance, we should be able to persist the model and then
    reuse its capabilities as an API. API access to the customer churn model will
    enable an external application to predict the customer retention. We will use
    Spring Boot, along with Thymeleaf, for the UI demonstration. We will deploy and
    run the application locally for the demonstration. In this recipe, we will create
    an API for a customer churn example.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As a prerequisite for API creation, you need to run the main example source
    code:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CustomerRetentionPredictionExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CustomerRetentionPredictionExample.java)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'DL4J has a utility class called `ModelSerializer` to save and restore models. We
    have used **`ModelSerializer`** to persist the model to disk, as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'For more information, refer to:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CustomerRetentionPredictionExample.java#L124](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CustomerRetentionPredictionExample.java#L124).'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Also, note that we need to persist the normalizer preprocessor along with the
    model. Then we can reuse the same to normalize user inputs on the go. In the previously
    mentioned code, we persisted the normalizer by calling `addNormalizerToModel()`
    from `ModelSerializer`.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'You also need to be aware of the following input attributes to the `addNormalizerToModel()` method:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '`multiLayerNetwork`: The model that the neural network was trained on'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataNormalization`: The normalizer that we used for our training'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Please refer to the following example for a concrete API implementation:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/api/CustomerRetentionPredictionApi.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/api/CustomerRetentionPredictionApi.java)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: In our API example, we restore the model file (model that was persisted before)
    to generate predictions.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a method to generate a schema for the user input:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Create a `TransformProcess` from the schema:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Load the data into a record reader instance:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Restore the model using `ModelSerializer`:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Create an iterator to traverse through the entire set of input records:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Design an API function to generate output from user input:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: For a further example, see: [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/api/CustomerRetentionPredictionApi.java ](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/api/CustomerRetentionPredictionApi.java)
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'Build a shaded JAR of your DL4J API project by running the Maven command:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Run the Spring Boot project included in the source directory. Import the Maven
    project to your IDE: [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/spring-dl4j](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/spring-dl4j).
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following VM options in under run configurations:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '`PATH-TO-MODEL-FILE` is the location where you stored the actual model file.
    It can be on your local disk or in a cloud as well.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, run the `SpringDl4jApplication.java` file:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b90ef87-fda2-44ee-a975-7dfe1effeb85.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
- en: 'Test your Spring Boot app at `http://localhost:8080/`:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/48d532f3-0671-4d86-b898-2595035b3407.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: Verify the functionality by uploading an input CSV file.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a sample CSV file to upload into the web application: **[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/resources/test.csv](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/resources/test.csv).**
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'The prediction results will be displayed as shown here:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ebc2020-8ab6-4743-baab-b22d16011c10.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
- en: How it works...
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to create an API to take the inputs from end users and generate the
    output. The end user will upload a CSV file with the inputs, and API returns the
    prediction output back to the user.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: In step 1, we added schema for the input data. User input should follow the
    schema structure in which we trained the model except that the `Exited` label
    is not added because that is the expected task for the trained model. In step
    2, we have created `TransformProcess` from `Schema` that was created in step 1.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: In step 3, we used `TransformProcess` from step 2 to create a record reader
    instance. This is to load the data from the dataset.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: We expect the end users to upload batches of inputs to generate outcomes. So,
    an iterator needs to be created as per step 5 to traverse through the entire set
    of input records. We set the preprocessor for the iterator using the pretrained
    model from step 4\. Also, we used a `batchSize` value of `1`. If you have more
    input samples, you can specify a reasonable batch size.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: In step 6, we used a file path named `modelFilePath` to represent the model
    file location. We pass this as a command-line argument from the Spring application.
    Thereby you can configure your own custom path where the model file is persisted. After
    step 7, a shaded JAR with all DL4J dependencies will be created and saved in the
    local Maven repository. You can also view the JAR file in the project target repository.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'Dependencies of customer retention API are added to the `pom.xml` file of the
    Spring Boot project, as shown here:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Once you have created a shaded JAR for the API by following step 7, the Spring
    Boot project will be able to fetch the dependencies from your local repository.
    So, you need to build the API project first before importing the Spring Boot project.
    Also, make sure to add the model file path as a VM argument, as mentioned in step
    8.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, these are the steps required to run the use case:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'Import and build the Customer Churn API project: [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/.](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/)'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the main example to train the model and persist the model file: [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CustomerRetentionPredictionExample.java.](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CustomerRetentionPredictionExample.java)
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the customer churn API project: [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/.](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp)
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the Spring Boot project by running the Starter here (with the earlier mentioned
    VM arguments): [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/spring-dl4j/src/main/java/com/springdl4j/springdl4j/SpringDl4jApplication.java.](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/spring-dl4j/src/main/java/com/springdl4j/springdl4j/SpringDl4jApplication.java)
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
