- en: Building Deep Neural Networks for Binary Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to develop a **Deep Neural Network** (**DNN**)
    using the standard feedforward network architecture. We will add components and
    changes to the application while we progress through the recipes. Make sure to
    revisit [Chapter 1](f88b350b-16e2-425b-8425-4631187c7803.xhtml), *Introduction
    to Deep Learning in Java*, and [Chapter 2](6ac5dff5-cc98-4d52-bc59-1da01b2aeded.xhtml), *Data
    Extraction, Transformation, and Loading*, if you have not already done so. This
    is to ensure better understanding of the recipes in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We will take an example of a customer retention prediction for the demonstration
    of the standard feedforward network. This is a crucial real-world problem that
    every business wants to solve. Businesses would like to invest more in happy customers,
    who tend to stay customers for longer periods of time. At the same time, predictions
    of losing customers will make businesses focus more on decisions that encourage
    customers not to take their business elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that a feedforward network doesn't really give you any hints about
    the actual features that decide the outcome. It just predicts whether a customer
    continues to patronize the organization or not. The actual feature signals are
    hidden, and it is left to the neural network to decide. If you want to record
    the actual feature signals that control the prediction outcome, then you could
    use an autoencoder for the task. Let's examine how to construct a feedforward
    network for our aforementioned use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting data from CSV input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing anomalies from the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying transformations to the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing input layers for the neural network model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing hidden layers for the neural network model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing output layers for the neural network model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and evaluating the neural network model for CSV data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying the neural network model and using it as an API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Make sure the following requirements are satisfied:'
  prefs: []
  type: TYPE_NORMAL
- en: JDK 8 is installed and added to `PATH`. Source code requires JDK 8 for execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maven is installed/added to `PATH`. We use Maven to build the application JAR
    file afterward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concrete implementation for the use case discussed in this chapter (Customer
    retention prediction) can be found at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CustomerRetentionPredictionExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CustomerRetentionPredictionExample.java).
  prefs: []
  type: TYPE_NORMAL
- en: After cloning our GitHub repository, navigate to the `Java-Deep-Learning-Cookbook/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode` directory.
    Then import the `cookbookapp` project into your IDE as a Maven project by importing `pom.xml`.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset is already included in the `resources` directory (`Churn_Modelling.csv`)
    of the `cookbookapp` project.
  prefs: []
  type: TYPE_NORMAL
- en: However, the dataset can be downloaded at [https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling/downloads/bank-customer-churn-modeling.zip/1](https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling/downloads/bank-customer-churn-modeling.zip/1).
  prefs: []
  type: TYPE_NORMAL
- en: Extracting data from CSV input
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**ETL** (short for **Extract, Transform and Load**) is the first stage prior
    to network training. Customer churn data is in CSV format. We need to extract
    it and put it in a record reader object to process further. In this recipe, we
    extract the data from a CSV file.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create `CSVRecordReader` to hold customer churn data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Add data to `CSVRecordReader`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The CSV data from the dataset has 14 features. Each row represents a customer/record,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9530d8be-7666-495f-aafc-18676893e788.png)'
  prefs: []
  type: TYPE_IMG
- en: Our dataset is a CSV file containing 10,000 customer records, where each record
    is labeled as to whether the customer left the business or not. Columns 0 to 13
    represent input features. The 14^(th) column, `Exited`, indicates the label or
    prediction outcome. We're dealing with a supervised model, and each prediction
    is labeled with `0` or `1`, where `0` indicates a happy customer, and `1` indicates
    an unhappy customer who has left the business. The first row in the dataset is
    just feature labels, and we don't need them while processing the data. So, we
    have skipped the first line while we created the record reader instance in step
    1\. In step 1, `1` is the number of rows to be skipped on the dataset. Also, we
    have mentioned a comma delimiter (`,`) because we are using a CSV file. In step
    2, we used `FileSplit` to mention the customer churn dataset file. We can also
    deal with multiple dataset files using other `InputSplit` implementations, such
    as `CollectionInputSplit`, `NumberedFileInputSplit`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Removing anomalies from the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For supervised datasets, manual inspection works fine for datasets with fewer
    features. As the feature count goes high, manual inspection becomes impractical.
    We need to perform feature selection techniques, such as chi-square test, random
    forest, and so on, to deal with the volume of features. We can also use an autoencoder to
    narrow down the relevant features. Remember that each feature should have a fair
    contribution toward the prediction outcomes. So, we need to remove noise features
    from the raw dataset and keep everything else as is, including any uncertain features.
    In this recipe, we will walk through the steps to identify anomalies in the data.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Leave out all the noise features before training the neural network. Remove
    noise features at the schema transformation stage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Identify the missing values using the DataVec analysis API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Remove null values using a schema transformation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Remove NaN values using a schema transformation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you recall our customer churn dataset, there are 14 features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5dee1e86-791a-445c-9737-deaf4dca7527.png)'
  prefs: []
  type: TYPE_IMG
- en: After performing step 1, you have 11 valid features remaining. The following
    marked features have zero significance on the prediction outcome. For example,
    the customer name doesn't influence whether a customer would leave the organization
    or not.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40ca2a93-1976-4b5f-93b3-d99d95d8c993.png)'
  prefs: []
  type: TYPE_IMG
- en: In the above screenshot, we have marked the features that are not required for
    the training. These features can be removed from the dataset as it doesn't have
    any impact on outcome.
  prefs: []
  type: TYPE_NORMAL
- en: In step 1, we tagged the noise features (`RowNumber`, `Customerid`, and `Surname`)
    in our dataset for removal during the schema transformation process using the `removeColumns()` method.
  prefs: []
  type: TYPE_NORMAL
- en: The customer churn dataset used in this chapter has only 14 features. Also,
    the feature labels are meaningful. So, a manual inspection was just enough. In
    the case of a large number of features, you might need to consider using **PCA**
    (short for **Principal Component Analysis**), as explained in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In step 2, we used the `AnalyzeLocal` utility class to find the missing values
    in the dataset by calling `analyzeQuality()`. You should see the following result
    when you print out the information in the `DataQualityAnalysis` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/621c3773-9d43-498e-967a-efe1e0b9a104.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see in the preceding screenshot, each of the features is analyzed
    for its quality (in terms of invalid/missing data), and the count is displayed
    for us to decide if we need to normalize it further. Since all features appeared
    to be OK, we can proceed further.
  prefs: []
  type: TYPE_NORMAL
- en: There are two ways in which missing values can be handled. Either we remove
    the entire record or replace them with a value. In most cases, we don't remove
    records; instead, we replace them with a value to indicate absence. We can do
    it during the transformation process using `conditionalReplaceValueTransform()`
    or `conditionalReplaceValueTransformWithDefault()`. In step 3/4, we removed missing
    or invalid values from the dataset. Note that the feature needs to be known beforehand.
    We cannot check the whole set of features for this purpose. At the moment, DataVec
    doesn't support this functionality. You may perform step 2 to identify features
    that need attention.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We discussed earlier in this chapter how to use the `AnalyzeLocal` utility
    class to find out missing values. We can also perform extended data analysis using `AnalyzeLocal`.
    We can create a data analysis object that holds information on each column present
    in the dataset. It can be created by calling `analyze()`, as we discussed in the
    previous chapter. If you try to print out the information on the data analysis
    object, it will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9107f6f7-a7c4-4830-9296-a068771ee108.png)'
  prefs: []
  type: TYPE_IMG
- en: It will calculate the standard deviation, mean, and the min/max values for all
    the features in the dataset. The count of features is also calculated, which will
    be helpful toward identifying missing or invalid values in features.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a761959-3786-4d70-a828-462ebf049e10.png)'
  prefs: []
  type: TYPE_IMG
- en: Both screenshots on the above indicate the data analysis results returned by
    calling `analyze()` method. For the customer churn dataset, we should have a total
    count of 10,000 for all features as the total number of records present in our
    dataset is 10,000.
  prefs: []
  type: TYPE_NORMAL
- en: Applying transformations to the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data transformation is a crucial data normalization procedure that must be done
    before we feed the data to a neural network. We need to transform non-numeric
    features to numeric values and handle missing values. In this recipe, we will
    perform schema transformation, and create dataset iterators after transformation.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Add features and labels into the schema:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Identify and add categorical features to the schema:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Remove noise features from the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Transform categorical variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Apply one-hot encoding by calling `categoricalToOneHot()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Remove the correlation dependency on the `Geography` feature by calling `removeColumns()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here, we selected `France` as the correlation variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the data and apply the transformation using `TransformProcessRecordReader`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a dataset iterator to train/test:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Normalize the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the main dataset iterator to train and test iterators:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate train/test iterators from `DataSetIteratorSplitter`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All features and labels need to be added to the schema as mentioned in step
    1 and step 2\. If we don't do that, then DataVec will throw runtime errors during
    data extraction/loading.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73912ea2-ea7a-4786-ab3b-953c6202b743.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding screenshot, the runtime exception is thrown by DataVec because
    of unmatched count of features. This will happen if we provide a different value
    for input neurons instead of the actual count of features in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: From the error description, it is clear that we have only added 13 features
    in the schema, which ended in a runtime error during execution. The first three
    features, named `Rownumber`, `Customerid`, and `Surname`, are to be added to the
    schema. Note that we need to tag these features in the schema, even though we
    found them to be noise features. You can also remove these features manually from
    the dataset. If you do that, you don't have to add them in the schema, and, thus,
    there is no need to handle them in the transformation stage
  prefs: []
  type: TYPE_NORMAL
- en: For large datasets, you may add all features from the dataset to the schema,
    unless your analysis identifies them as noise. Similarly, we need to add the other
    feature variables such as `Age`, `Tenure`, `Balance`, `NumOfProducts`, `HasCrCard`, `IsActiveMember`, `EstimatedSalary`,
    and `Exited`. Note the variable types while adding them to schema. For example, `Balance` and `EstimatedSalary` have
    floating point precision, so consider their datatype as double and use `addColumnDouble()` to
    add them to schema.
  prefs: []
  type: TYPE_NORMAL
- en: We have two features named gender and geography that require special treatment.
    These two features are non-numeric and their feature values represent categorical
    values compared to other fields in the dataset. Any non-numeric features need
    to transform numeric values so that the neural network can perform statistical
    computations on feature values. In step 2, we added categorical variables to the
    schema using `addColumnCategorical()`. We need to specify the categorical values
    in a list, and `addColumnCategorical()` will tag the integer values based on the
    feature values mentioned. For example, the `Male` and `Female` values in the categorical
    variable `Gender` will be tagged as `0` and `1` respectively. In step 2, we added
    the possible values for the categorical variables in a list. If your dataset has
    any other unknown value present for a categorical variable (other than the ones
    mentioned in the schema), DataVec will throw an error during execution.
  prefs: []
  type: TYPE_NORMAL
- en: In step 3, we marked the noise features for removal during the transformation
    process by calling `removeColumns()`.
  prefs: []
  type: TYPE_NORMAL
- en: In step 4, we performed one-hot encoding for the `Geography` categorical variable.
  prefs: []
  type: TYPE_NORMAL
- en: '`Geography` has three categorical values, and hence it will take the 0, 1,
    and 2 values after the transformation. The ideal way of transforming non-numeric
    values is to convert them to a value of zero (0) and one (1). It would significantly
    ease the effort of the neural network. Also, the normal integer encoding is applicable
    only if there exists an ordinal relationship between the variables. The risk here
    is we''re assuming that there exists natural ordering between the variables. Such
    an assumption can result in the neural network showing unpredictable behavior.
    So, we have removed the correlation variable in step 6\. For the demonstration,
    we picked `France` as a correlation variable in step 6\. However, you can choose
    any one among the three categorical values. This is to remove any correlation
    dependency that affects neural network performance and stability. After step 6, the
    resultant schema for the `Geography` feature will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7be56c83-76d3-420b-9f58-f2a314822f5b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In step 8, we created dataset iterators from the record reader objects. Here
    are the attributes for the `RecordReaderDataSetIterator` builder method and their
    respective roles:'
  prefs: []
  type: TYPE_NORMAL
- en: '`labelIndex`: The index location in the CSV data where our labels (outcomes)
    are located.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numClasses`: The number of labels (outcomes) from the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batchSize`: The block of data that passes through the neural network. If you
    specify a batch size of 10 and there are 10,000 records, then there will be 1,000
    batches holding 10 records each.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, we have a binary classification problem here, and so we used the `classification()` method
    to specify the label index and number of labels.
  prefs: []
  type: TYPE_NORMAL
- en: For some of the features in the dataset, you might observe huge differences
    in the feature value ranges. Some of the features have small numeric values, while
    some have very large numeric values. These large/small values can be interpreted
    in the wrong way by the neural network. Neural networks can falsely assign high/low
    priority to these features and that results in wrong or fluctuating predictions.
    In order to avoid this situation, we have to normalize the dataset before feeding
    it to the neural network. Hence we performed normalization as in step 9.
  prefs: []
  type: TYPE_NORMAL
- en: In step 10, we used `DataSetIteratorSplitter` to split the main dataset for
    a training or test purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the parameters of `DataSetIteratorSplitter`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`totalNoOfBatches`: If you specify a batch size of 10 for 10,000 records, then
    you need to specify 1,000 as the total number of batches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ratio`: This is the ratio at which the splitter splits the iterator set. If
    you specify 0.8, then it means 80% of data will be used for training and the remaining
    20% will be used for testing/evaluation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing input layers for the neural network model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Input layer design requires an understanding of how the data flows into the
    system. We have CSV data as input, and we need to inspect the features to decide
    on the input attributes. Layers are core components in neural network architecture.
    In this recipe, we will configure input layers for the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to decide the number of input neurons before designing the input layer. It
    can be derived from the feature shape. For instance, we have 13 input features
    (excluding the label). But after applying the transformation, we have a total
    of 11 feature columns present in the dataset. Noise features are removed and categorical
    variables are transformed during the schema transformation. So, the final transformed
    data will have 11 input features. There are no specific requirements for outgoing
    neurons from the input layer. If we assign the wrong number of incoming neurons
    at the input layer, we may end up with a runtime error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c3ddc74c-a677-46b4-a5e1-c95d2beebdc4.png)'
  prefs: []
  type: TYPE_IMG
- en: The DL4J error stack is pretty much self-explanatory as to the possible reason.
    It points out the exact layer where it needs a fix (`layer0`, in the preceding
    example).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Define the neural network configuration using `MultiLayerConfiguration`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the input layer configuration using `DenseLayer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We added layers to the network by calling the `layer()` method as mentioned
    in step 2\. Input layers are added using `DenseLayer`*. *Also, we need to add
    an activation function for the input layer. We specified the activation function
    by calling the `activation()` method. We discussed activation functions in [Chapter
    1](f88b350b-16e2-425b-8425-4631187c7803.xhtml), *Introduction to Deep Learning
    in Java*. You can use one of the available activation functions in DL4J to the `activation()` method.
    The most generic activation function used is `RELU`. Here are roles of other methods
    in layer design:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nIn()`: This refers to the number of inputs for the layer. For an input layer,
    this is nothing but the number of input features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nOut()`: This refers to number of outputs to next dense layer in neural network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing hidden layers for the neural network model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hidden layers are the heart of a neural network. The actual decision process
    happens there. The design of the hidden layers is based on hitting a level beyond
    which a neural network cannot be optimized further. This level can be defined
    as the optimal number of hidden layers that produce optimal results.
  prefs: []
  type: TYPE_NORMAL
- en: Hidden layers are the place where the neural network transforms the inputs into
    a different format that the output layer can consume and use to make predictions.
    In this recipe, we will design hidden layers for a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Determine the incoming/outgoing connections. Set the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure hidden layers using `DenseLayer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For step 1, if the neural network has only single hidden layer, then the number
    of neurons (inputs) in the hidden layer should be the same as the number of outgoing
    connections from the preceding layer. If you have multiple hidden layers, you
    will also need to confirm this for the preceding hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: After you make sure that the number of input neurons are the same as number
    of the outgoing neurons in the preceding layer, you can create hidden layers using
    `DenseLayer`. In step 2, we used `DenseLayer` to create hidden layers for the input
    layers. In practice, we need to evaluate the model multiple times to understand
    the network performance. There's no constant layer configuration that works well
    for all the models. Also, `RELU` is the preferred activation function for hidden
    layers, due to its nonlinear nature.
  prefs: []
  type: TYPE_NORMAL
- en: Designing output layers for the neural network model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Output layer design requires an understanding of the expected output. We have
    CSV data as input, and the output layer relies on the number of labels in the
    dataset. Output layers are the place where the actual prediction is formed based
    on the learning process that happened in the hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will design output layers for the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Determine the incoming/outgoing connections. Set the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the output layer for the neural network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For step 1, we need to make sure that `nOut()` for the preceding layer should
    have the same number of neurons as `nIn()` for the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: So, `incomingConnectionCount` should be the same as `outgoingConnectionCount`
    from the preceding layer.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed the `SOFTMAX` activation function earlier in [Chapter 1](f88b350b-16e2-425b-8425-4631187c7803.xhtml), *Introduction
    to Deep Learning in Java*. Our use case (customer churn) is an example for the
    binary classification model. We are looking for a probabilistic outcome, that
    is, the probability of a customer being labeled *happy* or *unhappy, *where `0`
    represents a happy customer and `1` represents an unhappy customer. This probability
    will be evaluated, and the neural network will train itself during the training
    process.
  prefs: []
  type: TYPE_NORMAL
- en: The proper activation function at the output layer would be `SOFTMAX`. This
    is because we need the probability of the occurrence of labels and the probabilities
    should sum to 1\. `SOFTMAX` along with the log loss function produces good results
    for classification models. The introduction of `weightsArray` is to enforce a
    preference for a particular label among others in case of any data imbalance.
    In step 2, output layers are created using the `OutputLayer` class. The only difference
    is that `OutputLayer` expects an error function to calculate the error rate while
    making predictions. In our case, we used `LossMCXENT`, which is a multi-class
    cross entropy error function. Our customer churn example follows a binary classification
    model; however, we can still use this error function since we have two classes
    (labels) in our example. In step 2, `labelCount` would be 2.
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluating the neural network model for CSV data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During the training process, the neural network learns to perform the expected
    task. For every iteration/epoch, the neural network will evaluate its training
    knowledge. Accordingly, it will re-iterate the layers with updated gradient values
    to minimize the error produced at the output layer. Also, note that labels (`0`
    and `1` ) are not uniformly distributed across the dataset. So, we might need
    to consider adding weights to the label that appears less in the dataset. This
    is highly recommended before we proceed with the actual training session. In this
    recipe, we will train the neural network and evaluate the resultant model.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create an array to assign weights to minor labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Modify `OutPutLayer` to evenly balance the labels in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the neural network and add the training listeners:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the DL4J UI Maven dependency to analyze the training process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the UI server and add temporary storage to store the model information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Replace `InMemoryStatsStorage` with `FileStatsStorage` (in case of memory restrictions):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Assign the temporary storage space to the UI server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the neural network by calling `fit()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate the model by calling `evaluate()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A neural network increases its efficiency when it improves its generalization
    power. A neural network should not just memorize a certain decision-making process
    in favor of a particular label. If it does, our outcomes will be biased and wrong.
    So, it is good to have a dataset where the labels are uniformly distributed. If
    they're not uniformly distributed, then we might have to adjust a few things while
    calculating the error rate. For this purpose, we introduced a `weightsArray` in
    step 1 and added to `OutputLayer` in step 2.
  prefs: []
  type: TYPE_NORMAL
- en: For `weightsArray = {0.35, 0.65}`, the network gives more priority to the outcomes
    of `1` (customer unhappy). As we discussed earlier in this chapter, the `Exited` column
    represents the label. If we observe the dataset, it is evident that outcomes labeled `0` (customer
    happy) have more records in the dataset compared to `1`. Hence, we need to assign
    additional priority to `1` to evenly balance the dataset. Unless we do that, our
    neural network may over fit and will be biased toward the `1` label.
  prefs: []
  type: TYPE_NORMAL
- en: In step 3, we added `ScoreIterationListener` to log the training process on
    the console. Note that `iterationCount` is the number of iterations in which it
    should log the network score. Remember, `iterationCount`is not the epoch. We say
    an epoch has happened when the entire dataset has traveled back and forth (backpropagation)
    once through the whole neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'In step 8, we used `dataSetIteratorSplitter` to obtain the training dataset
    iterator and trained our model on top of it. If you configured loggers properly,
    you should see the training instance is progressing as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ecf204a-abe2-4e43-b91b-322a0cc52540.png)'
  prefs: []
  type: TYPE_IMG
- en: The score referred to in the screenshot is not the success rate; it is the error
    rate calculated by the error function for each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'We configured the DL4J **user interface** (**UI**) in step 4, 5, and 6. DL4J
    provides a UI to visualize the current network status and training progress in
    your browser (real-time monitoring). This will help further tuning the neural
    network training. `StatsListener` will be responsible for triggering the UI monitoring
    while the training starts. The port number for UI server is `9000`. While the
    training is in progress, hit the UI server at `localhost:9000`. We should be able
    to see something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17bebe0a-7847-4d9a-9bd1-2b548b788b06.png)'
  prefs: []
  type: TYPE_IMG
- en: We can refer to the first graph seen in the Overview section for the Model Score analysis.
    The Iteration is plotted on the *x* axis, and the Model Score is on the *y *axis
    in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also further expand our research on how the Activations, Gradients,
    and the Updates parameters performed during the training process by inspecting
    the parameter values plotted on graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c1035e0-e297-43a7-a888-ab4c9e8c935e.png)'
  prefs: []
  type: TYPE_IMG
- en: The *x* axis refers to the number of iterations in both the graphs. The *y* axis
    in the parameter update graph refers to the parameter update ratio, and the *y* axis
    in the activation/gradient graphs refers to the standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to have layer-wise analysis. We just need to click on the Model tab
    on the left sidebar and choose the layer of choice for further analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77ed7dd4-5797-4cae-b87c-fd26f1bf72a2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For analysis of memory consumption and JVM, we can navigate to the System tab
    on the left sidebar:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a7cc605-7c53-45af-8f80-4cb47ba58eec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also review the hardware/software metrics in detail at the same place:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/832323a4-3b9d-4ac1-bcdf-b02d2aaccb33.png)'
  prefs: []
  type: TYPE_IMG
- en: This is very useful for benchmarking as well. As we can see, the memory consumption
    of the neural network is clearly marked and the JVM/off-heap memory consumption
    is mentioned in the UI to analyze how well the benchmarking is done.
  prefs: []
  type: TYPE_NORMAL
- en: 'After step 8, evaluation results will be displayed on console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/67b76595-73b8-49a8-81b5-14b956783fda.png)'
  prefs: []
  type: TYPE_IMG
- en: In the above screenshot, the console shows various evaluation metrics by which
    the model is evaluated. We cannot rely on a specific metrics in all the cases;
    hence, it is good to evaluate the model against multiple metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Our model is showing an accuracy level of 85.75% at the moment. We have four
    different performance metrics, named accuracy, precision, recall, and F1 score.
    As you can see in the preceding screenshot, recall metrics are not so good, which
    means our model still has false negative cases. The F1 score is also significant
    here, since our dataset has an uneven proportion of output classes. We will not
    discuss these metrics in detail, since they are outside the scope of this book.
    Just remember that all these metrics are important for consideration, rather than
    just relying on accuracy alone. Of course, the evaluation trade-offs vary depending
    upon the problem. The current code has already been optimized. Hence, you will
    find almost stable accuracy from the evaluation metrics. For a well-trained network
    model, these performance metrics will have values close to `1`.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to check how stable our evaluation metrics are. If we notice
    unstable evaluation metrics for unseen data, then we need to reconsider changes
    in the network configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions on the output layer have influence on the stability of
    the outputs. Hence, a good understanding on output requirements will definitely
    save you a lot of time choosing an appropriate output function (loss function).
    We need to ensure stable predictive power from our neural network.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning rate is one of the factors that decides the efficiency of the neural
    network. A high learning rate will diverge from the actual output, while a low
    learning rate will result in slow learning due to slow convergence. Neural network
    efficiency also depends on the weights that we assign to the neurons in every
    layer. Hence, a uniform distribution of weights during the early stages of training
    might help.
  prefs: []
  type: TYPE_NORMAL
- en: The most commonly followed approach is to introduce dropouts to the layers.
    This forces the neural network to ignore some of the neurons during the training
    process. This will effectively prevent the neural network from memorizing the
    prediction process. How do we find out if a network has memorized the results?
    Well, we just need to expose the network to new data. If your accuracy metrics
    become worse after that, then you've got a case of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Another possibility for increasing the efficiency of the neural network (and
    thus reducing overfitting) is to try for L1/L2 regularization in the network layers.
    When we add L1/L2 regularization to network layers, it will add an extra penalty
    term to the error function. L1 penalizes with the sum of the absolute value of
    the weights in the neurons, while L2 penalizes using the sum of squares of the
    weights. L2 regularization will give much better predictions when the output variable
    is a function of all input features. However, L1 regularization is preferred when
    the dataset has outliers and if not all the attributes are contributing to predicting
    the output variable. In most cases, the major reason for overfitting is the issue
    of memorization. Also, if we drop too many neurons, it will eventually underfit
    the data. This means we lose more useful data than we need to.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the trade-off can vary depending on the different kinds of problems.
    Accuracy alone cannot ensure a good model performance every time. It is good to
    measure precision if we cannot afford the cost of a false positive prediction
    (such as in spam email detection). It is good to measure recall if we cannot afford
    the cost of a false negative prediction (such as in fraudulent transaction detection).
    The F1 score is optimal if there's an uneven distribution of the classes in the
    dataset. ROC curves are good to measure when there are approximately equal numbers
    of observations for each output class.
  prefs: []
  type: TYPE_NORMAL
- en: Once the evaluations are stable, we can check on the means to optimize the efficiency
    of the neural network. There are multiple methods to choose from. We can perform
    several training sessions to try to find out the optimal number of hidden layers,
    epochs, dropouts, and activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot points to various hyper parameters that can influence
    neural network efficiency:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7069f9da-c4bb-47a3-a704-6266367bfe9b.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that `dropOut(0.9)` means we ignore 10% of neurons during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other attributes/methods in the screenshot are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`weightInit()` : This is to specify how the weights are assigned neurons at
    each layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`updater()`: This is to specify the gradient updater configuration. `Adam`
    is a gradient update algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [Chapter 12](0db31248-e40b-4479-9939-0baccb0e11d1.xhtml), *Benchmarking and
    Neural Network Optimization*, we will walk through an example of hyperparameter
    optimization to automatically find the optimal parameters for you. It simply performs
    multiple training sessions on our behalf to find the optimal values by a single
    program execution. You may refer to [Chapter 12](0db31248-e40b-4479-9939-0baccb0e11d1.xhtml), *Benchmarking
    and Neural Network Optimization,* if you're interested in applying benchmarks
    to the application.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the neural network model and using it as an API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After the training instance, we should be able to persist the model and then
    reuse its capabilities as an API. API access to the customer churn model will
    enable an external application to predict the customer retention. We will use
    Spring Boot, along with Thymeleaf, for the UI demonstration. We will deploy and
    run the application locally for the demonstration. In this recipe, we will create
    an API for a customer churn example.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As a prerequisite for API creation, you need to run the main example source
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CustomerRetentionPredictionExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CustomerRetentionPredictionExample.java)'
  prefs: []
  type: TYPE_NORMAL
- en: 'DL4J has a utility class called `ModelSerializer` to save and restore models. We
    have used **`ModelSerializer`** to persist the model to disk, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'For more information, refer to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CustomerRetentionPredictionExample.java#L124](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CustomerRetentionPredictionExample.java#L124).'
  prefs: []
  type: TYPE_NORMAL
- en: Also, note that we need to persist the normalizer preprocessor along with the
    model. Then we can reuse the same to normalize user inputs on the go. In the previously
    mentioned code, we persisted the normalizer by calling `addNormalizerToModel()`
    from `ModelSerializer`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You also need to be aware of the following input attributes to the `addNormalizerToModel()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '`multiLayerNetwork`: The model that the neural network was trained on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataNormalization`: The normalizer that we used for our training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Please refer to the following example for a concrete API implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/api/CustomerRetentionPredictionApi.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/api/CustomerRetentionPredictionApi.java)'
  prefs: []
  type: TYPE_NORMAL
- en: In our API example, we restore the model file (model that was persisted before)
    to generate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Create a method to generate a schema for the user input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `TransformProcess` from the schema:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the data into a record reader instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Restore the model using `ModelSerializer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an iterator to traverse through the entire set of input records:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Design an API function to generate output from user input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: For a further example, see: [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/api/CustomerRetentionPredictionApi.java ](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/api/CustomerRetentionPredictionApi.java)
  prefs: []
  type: TYPE_NORMAL
- en: 'Build a shaded JAR of your DL4J API project by running the Maven command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Run the Spring Boot project included in the source directory. Import the Maven
    project to your IDE: [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/spring-dl4j](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/spring-dl4j).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following VM options in under run configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '`PATH-TO-MODEL-FILE` is the location where you stored the actual model file.
    It can be on your local disk or in a cloud as well.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, run the `SpringDl4jApplication.java` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b90ef87-fda2-44ee-a975-7dfe1effeb85.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Test your Spring Boot app at `http://localhost:8080/`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/48d532f3-0671-4d86-b898-2595035b3407.png)'
  prefs: []
  type: TYPE_IMG
- en: Verify the functionality by uploading an input CSV file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a sample CSV file to upload into the web application: **[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/resources/test.csv](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/resources/test.csv).**
  prefs: []
  type: TYPE_NORMAL
- en: 'The prediction results will be displayed as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ebc2020-8ab6-4743-baab-b22d16011c10.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to create an API to take the inputs from end users and generate the
    output. The end user will upload a CSV file with the inputs, and API returns the
    prediction output back to the user.
  prefs: []
  type: TYPE_NORMAL
- en: In step 1, we added schema for the input data. User input should follow the
    schema structure in which we trained the model except that the `Exited` label
    is not added because that is the expected task for the trained model. In step
    2, we have created `TransformProcess` from `Schema` that was created in step 1.
  prefs: []
  type: TYPE_NORMAL
- en: In step 3, we used `TransformProcess` from step 2 to create a record reader
    instance. This is to load the data from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We expect the end users to upload batches of inputs to generate outcomes. So,
    an iterator needs to be created as per step 5 to traverse through the entire set
    of input records. We set the preprocessor for the iterator using the pretrained
    model from step 4\. Also, we used a `batchSize` value of `1`. If you have more
    input samples, you can specify a reasonable batch size.
  prefs: []
  type: TYPE_NORMAL
- en: In step 6, we used a file path named `modelFilePath` to represent the model
    file location. We pass this as a command-line argument from the Spring application.
    Thereby you can configure your own custom path where the model file is persisted. After
    step 7, a shaded JAR with all DL4J dependencies will be created and saved in the
    local Maven repository. You can also view the JAR file in the project target repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dependencies of customer retention API are added to the `pom.xml` file of the
    Spring Boot project, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Once you have created a shaded JAR for the API by following step 7, the Spring
    Boot project will be able to fetch the dependencies from your local repository.
    So, you need to build the API project first before importing the Spring Boot project.
    Also, make sure to add the model file path as a VM argument, as mentioned in step
    8.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, these are the steps required to run the use case:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import and build the Customer Churn API project: [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/.](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the main example to train the model and persist the model file: [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CustomerRetentionPredictionExample.java.](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CustomerRetentionPredictionExample.java)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the customer churn API project: [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/.](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the Spring Boot project by running the Starter here (with the earlier mentioned
    VM arguments): [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/spring-dl4j/src/main/java/com/springdl4j/springdl4j/SpringDl4jApplication.java.](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/spring-dl4j/src/main/java/com/springdl4j/springdl4j/SpringDl4jApplication.java)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
