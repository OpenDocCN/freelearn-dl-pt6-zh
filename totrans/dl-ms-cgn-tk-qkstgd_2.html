<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building Neural Networks with CNTK</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we talked about what deep learning is, and how neural networks work on a conceptual level. Finally, we talked about CNTK, and how to get it installed on your machine. In this chapter, we will build our first neural network with CNTK and train it.</p>
<p>We will look at building a neural network using the different functions and classes from the CNTK library. We will do this with a basic classification problem.</p>
<p>Once we have a neural network for our classification problem, we will train it with sample data obtained from an open dataset. After our neural network is trained, we will look at how to use it to make predictions. </p>
<p>At the end of this chapter, we will spend some time talking about ways to improve your model once you've trained it.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Basic neural network concepts in CNTK</li>
<li>Building your first neural network</li>
<li>Training the neural network</li>
<li>Making predictions with a neural network</li>
<li>Improving the model</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will work on a sample model, built using Python in a Jupyter notebook. Jupyter is an open source technology that allows you to create interactive web pages that contain sections of Python code, Markdown, and HTML. It makes it much easier to document your code and assumptions you made while building your deep learning model.</p>
<p>If you've installed Anaconda using the steps defined in <a href="9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml">Chapter 1</a>, <em>Getting Started with CNTK</em>, you already have Jupyter installed on your machine. Should you not have Anaconda yet, you can download it from: <a href="https://www.anaconda.com/download/" target="_blank">https://anacondacloud.com/download</a><span>.</span></p>
<p>You can get the sample code for this chapter from: <a href="https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch2">https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch2</a>. To run the sample code, run the following commands inside a Terminal in the directory where you downloaded the sample code:</p>
<pre><strong>cd ch2</strong><br/><strong>jupyter notebook</strong></pre>
<p>Look for the <kbd>Train your first model.ipynb</kbd> notebook, and click it to open up the sample code. You can execute all the code in one step by choosing <span class="packt_screen">Cell </span>| <span class="packt_screen">Run All</span><span class="packt_screen">.</span> This will execute all the steps in the notebook.</p>
<p class="mce-root">Check out the following video to see the code in action:</p>
<p><a href="http://bit.ly/2YoyNKY">http://bit.ly/2YoyNKY</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Basic neural network concepts in CNTK</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we looked at the basic concepts of a neural network. Let's map the concepts we've learned to components in the CNTK library, and discover how you can use these concepts to build your own model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building neural networks using layer functions</h1>
                </header>
            
            <article>
                
<p>Neural networks are made using several layers of neurons. In CNTK, we can model the layers of a neural network using layer functions defined in the layers module. A <kbd>layer</kbd> function in CNTK looks like a regular function. For example, you can create the most basic layer type, <kbd>Dense</kbd>, with one line of code:</p>
<pre>from cntk.layers import Dense<br/>from cntk import input_variable<br/><br/>features = input_variable(100)<br/>layer = Dense(50)(features)</pre>
<p>To Create the most basic layer type following the given steps:</p>
<ol>
<li>First, import the <kbd>Dense</kbd> layer function from the layers package</li>
<li>Next, import the <kbd>input_variable</kbd> function from the <kbd>cntk</kbd> root package</li>
<li>Create a new input variable with the name features using the <kbd>input_variable</kbd> function and give it a size of <kbd>100</kbd></li>
<li>Create a new layer using the <kbd>Dense</kbd> function providing it with the number of neurons you want</li>
<li>Invoke the configured <kbd>Dense</kbd> layer function providing the features variable to connect the <kbd>Dense</kbd> layer to the input</li>
</ol>
<p>Working with layers in CNTK has a distinct functional programming feel to it. When we look at the previous chapter, we can understand why CNTK has gone down this route. Ultimately, every layer in a neural network is a mathematical function. All the layer functions in CNTK produce a mathematical function with a set of predefined parameters. Invoke the function again, and you bind the last missing parameter, the input, to the layer.</p>
<p class="mce-root">You will typically build neural networks with this style of programming when you want to create a neural network with a complex architecture. But, for most starting developers, the functional style feels unfamiliar. CNTK provides an easier API for when you want to build a basic neural network through the <kbd>Sequential</kbd> layer function.</p>
<p>You can use the <kbd>Sequential</kbd> layer function to chain several layers together, without having to use the functional programming style, as follows: </p>
<pre>from cntk.layers import Sequential, Dense<br/>from cntk import input_variable<br/><br/>features = input_variable(7)<br/><br/>network = Sequential([<br/>  Dense(64),<br/>  Dense(32),<br/>  Dense(3)<br/>])(features)</pre>
<p>To do so, follow the given steps:</p>
<ol>
<li>First, import the layer functions you want to use from the <kbd>layers</kbd> package</li>
<li>Import the the <kbd>input_variable</kbd> function to create an input variable used to feed data into the neural network</li>
<li>Create a new input variable to feed data into the neural network</li>
<li>Create a new sequential layer block by invoking the <kbd>Sequential</kbd> function</li>
</ol>
<ol start="5">
<li>Provide the list of layers that you want to chain together to the <kbd>Sequential</kbd> function</li>
<li>Invoke the configured <kbd>Sequential</kbd> function object providing the features input variable to complete the network structure</li>
</ol>
<p>By combining the <kbd>Sequential</kbd> function and other layer functions you can create any neural network structure. In the next section, we'll take a look at how to customize layers with settings to configure things like the <kbd>activation</kbd> function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Customizing layer settings</h1>
                </header>
            
            <article>
                
<p>CNTK provides a pretty good set of defaults for building neural networks. But you'll find yourself experimenting with those settings a lot. The behavior and performance of the neural network will be different based on the <kbd>activation</kbd> function and other settings you choose. Because of this, it is good to understand what you can configure. </p>
<p>Each layer has its own unique configuration options, some of which you will use a lot, and others you will use less. When we look at the <kbd>Dense</kbd> layer, there are a few important settings that you want to define:</p>
<ul>
<li><kbd>shape</kbd>:<strong> </strong>The output shape of the layer</li>
<li><kbd>activation</kbd>: The <kbd>activation</kbd> function for the layer</li>
<li><kbd>init</kbd>: The <kbd>initialization</kbd> function of the layer</li>
</ul>
<p class="mce-root"/>
<p>The output shape of a layer determines the number of neurons in that layer. Each neuron needs to have an <kbd>activation</kbd> function defined so it can transform the input data. Finally, we need a function that will initialize the parameters of the layer when we start training the neural network. The output shape is the first parameter in each <kbd>layer</kbd> function. The <kbd>activation</kbd> and <kbd>init</kbd> arguments are supplied as keyword arguments. These parameters have default values for them, so you can omit them should you not need a custom setting. The next sample demonstrates how to configure a <kbd>Dense</kbd> layer with a custom <kbd>initializer</kbd> and <kbd>activation</kbd> function:</p>
<pre>from cntk.layers import Dense<br/>from cntk.ops import sigmoid<br/>from cntk.initializer import glorot_uniform<br/><br/>layer = Dense(128, activation=sigmoid, init=glorot_uniform)</pre>
<p>To configure a Dense layer follow the given steps:</p>
<ol>
<li>First, import the <kbd>Dense</kbd> layer from the <kbd>layers</kbd> package</li>
<li>Next, import the <kbd>sigmoid</kbd> operator from the <kbd>ops</kbd> package so we can use it to configure as an <kbd>activation</kbd> function</li>
<li>Then import the <kbd>glorot_uniform</kbd> initializer from the <kbd>initializer</kbd> package</li>
<li>Finally, create a new layer using the <kbd>Dense</kbd> layer providing the number of neurons as the first argument and provide the <kbd>sigmoid</kbd> operator as the <kbd>activation</kbd> function and the <kbd>glorot_uniform</kbd> function as the <kbd>init</kbd> function for the layer</li>
</ol>
<p class="mce-root">There are several <kbd>activation</kbd> functions to choose from; for example, you can use <strong>Rectified Linear Unit</strong> (<strong>ReLU</strong>),<strong> </strong>or <kbd>sigmoid</kbd>, as an <kbd>activation</kbd> function. All <kbd>activation</kbd> functions can be found in the <kbd>cntk.ops</kbd> package.</p>
<p class="mce-root">Each <kbd>activation</kbd> function will have a different effect on the performance of your neural network. We will go<span> into more detail regarding <kbd>activation</kbd> functions when we build a neural network later in this chapter.</span></p>
<p>Initializers determine how the parameters in the layer are initialized when we start training our neural network. You can choose from various initializers in CNTK. <kbd>Normal</kbd>, <kbd>uniform</kbd>, and <kbd>glorot_uniform</kbd> are some of the more widely used initializers in the <kbd>cntk.initializer</kbd> package. We will get into more detail about which initializer to use when we start to solve our first deep learning problem.</p>
<p>Whatever initializer function you're using from CNTK, it's important to realize that they use random number generators to generate the initial values for the parameters in the layer. This is an important technique, because it allows the neural network to learn the right parameters effectively. All initializer functions in CNTK support an extra seed setting. When you set this parameter to a fixed value, you will get the same initial values every time you train your neural network. This can be useful when you're trying to reproduce a problem, or are experimenting with different settings. </p>
<p>When you are building a neural network, you typically have to specify the same set of settings for several layers in your neural network. This can become problematic when you are experimenting with your model. To solve this, CNTK includes a <kbd>utility</kbd> function called <kbd>default_options</kbd>:</p>
<pre>from cntk import default_options<br/>from cntk.layers import Dense, Sequential<br/>from cntk.ops import sigmoid<br/><br/>with default_options(activation=sigmoid):<br/>  network = Sequential([<br/>    Dense(1024),<br/>    Dense(512),<br/>    Dense(256)<br/>  ])</pre>
<p>By using the <kbd>default_options</kbd> function, we've configured the <kbd>sigmoid</kbd> activation function for all three layers, with just one line of code. The <kbd>default_options</kbd> function accepts a standard set of settings that get applied to all layers in the scope of this function. Using the <kbd>default_options</kbd> function makes configuring the same options for a set of layers much more comfortable. You can configure quite a lot of settings this way; for example, with the following functions:</p>
<ul>
<li><kbd>activation</kbd>:<strong> </strong>The <kbd>activation</kbd> function to use</li>
<li><kbd>init</kbd>: The <kbd>initialization</kbd> function for the layers</li>
<li><kbd>bias</kbd>: Whether the layers should have a <kbd>bias</kbd> term included</li>
<li><kbd>init_bias</kbd>:<strong> </strong>The <kbd>initialization</kbd> function for the bias term</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using learners and trainers to optimize the parameters in a neural network</h1>
                </header>
            
            <article>
                
<p>In the previous sections we've seen how to create the structure for a neural network and how to configure various settings. Now let's look at how to use <kbd>learners</kbd> and <kbd>trainers</kbd> to optimize the parameters of a neural network. In CNTK, a neural network is trained using a combination of two components. The first component is the <kbd>trainer</kbd> component, which implements the backpropagation process. The second component is the <kbd>learner</kbd>. It is responsible for performing the gradient descent algorithm that we've seen in <a href="9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml">Chapter 1</a>, <em>Getting Started with CNTK.</em></p>
<p>The <kbd>trainer</kbd> passes the data through the neural network to obtain a prediction. It then uses the <kbd>learner</kbd> to obtain the new values for the parameters in the neural network. It then applies these new values, and repeats the process. This goes on until an exit <span><span>criterion </span></span>is met. The training process is stopped when a configured number of iterations is reached. This can be enhanced using custom callbacks.</p>
<p>We've discussed a very basic form of gradient descent in <a href="9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml">Chapter 1</a>, <em>Getting Started with CNTK</em>. But, in reality, there are many variations on this basic algorithm. The basic gradient descent doesn't work very well for complex cases. Often, it gets stuck in a local optimum (a bump in the hillside, if you will), so it doesn't reach a globally optimal value for the parameters in the neural network. Other algorithms, such as <span><strong>Stochastic Gradient Descent</strong> (<strong>SGD</strong>) </span>with momentum, account for local optima, and use concepts such as momentum to get past bumps in the <span><span>slope </span></span>of the loss curve.</p>
<p>Here are few interesting <kbd>learners</kbd> that are included in the CNTK library:</p>
<ul>
<li><strong>SGD</strong>: The basic stochastic gradient descent, without any extras</li>
<li><strong>MomentumSGD</strong>: Applies momentum to overcome local optima</li>
<li><strong>RMSProp</strong>: Uses decaying learning rates to control the rate of descent</li>
<li><strong>Adam</strong>: Uses decaying momentum to decrease the rate of descent over time</li>
<li><strong>Adagrad</strong>: Uses different learning rates for frequently, and infrequently, occurring features</li>
</ul>
<p>It's important to know that you can choose different <kbd>learners</kbd>, depending on the problem you want to solve. We will learn more about choosing the right optimizer when we start to solve our first machine learning problem with a neural network. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loss functions</h1>
                </header>
            
            <article>
                
<p>In order for the <kbd>trainer</kbd> and <kbd>learner</kbd> to be able to optimize the parameters of the neural network, we need to define a function that measures the loss in the neural network. The <kbd>loss</kbd> function calculates how big the difference is between the predicted output of the neural network, and the expected output that we know beforehand. </p>
<p>CNTK contains a number of <kbd>loss</kbd> functions in the <kbd>cntk.losses</kbd> module. Each <kbd>loss</kbd> function has its own use and specific characteristics. For example, when you want to measure the loss in a model that predicts a continuous value, you're going to need the <kbd>squared_error</kbd> loss function. It measures the distance between the predicted value generated by the model, and the real value that you provided when training the model.</p>
<p>For classification models, you will need a different set of <kbd>loss</kbd> functions. The <span><kbd>binary_cross_entropy</kbd> loss function can be used to measure the loss in a model that is used for binary classification jobs, such as a fraud detection model. The <kbd>cross_entropy_with_softmax</kbd> loss function is more suitable for classification models that predict multiple classes.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model metrics</h1>
                </header>
            
            <article>
                
<p>Combining a <kbd>learner</kbd> and <kbd>loss</kbd> function with a <kbd>trainer</kbd> allows us to optimize the parameters in the neural network. This should produce a good model, but in order to know that for sure we need metrics to measure model performance. A metric is a single value that tells us, for example, what percentage of samples was predicted correctly. </p>
<p>Because the <kbd>loss</kbd> function measures the difference between the actual value and the predicted value, you might think that it's a good measure of how well our model is doing. Depending on the model, it may provide some value, but often you will need to use a separate <kbd>metric</kbd> function to measure your model's performance in a meaningful way.</p>
<p>CNTK offers a number of different <kbd>metric</kbd> functions in the <kbd>cntk.metrics</kbd> package. For example, if you want to measure the performance of a classification model, you can use the <kbd>classification_error</kbd> function. This is used to measure the percentage of samples that were predicted correctly.</p>
<p>The <kbd>classification_error</kbd> function is just one example of a metric. One other important <kbd>metric</kbd> function is the <kbd>ndcg_at_1</kbd> metric. If you're working with a ranking model, then you are interested in how closely your model ranked the samples according to a predefined ranking. This is what the <kbd>ndcg_at_1</kbd> metric gives you.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building your first neural network</h1>
                </header>
            
            <article>
                
<p>Now that we've learned what concepts CNTK offers to build a neural network, we can start to apply these concepts to a real machine learning problem. In this section, we'll explore how to use a neural network to classify species of iris flowers. </p>
<p>This is not a typical task where you want to use a neural network. But, as you will soon discover, the dataset is simple enough to get a good grasp of the process of building a deep learning model. Yet it contains enough data to ensure that the model works reasonably well.</p>
<p>The iris dataset describes the physical properties of different varieties of iris flowers:</p>
<ul>
<li>Sepal length in cm</li>
<li>Sepal width in cm</li>
<li>Petal length in cm</li>
<li>Petal width in cm</li>
<li>Class (iris setosa, iris versicolor, iris virginica)</li>
</ul>
<div class="packt_infobox"><span>The code for this chapter includes the </span>iris<span> dataset, on which you need to train the deep learning model. If you're interested, you can find the original files online at: </span><a href="http://archive.ics.uci.edu/ml/datasets/Iris">http://archive.ics.uci.edu/ml/datasets/Iris</a><span>. It is also included with the sample code for this chapter.</span></div>
<p>We are going to build a deep learning model that is going to classify a flower based on the physical properties of sepal width and length, and petal width and length. We can predict three different classes as output for the model.</p>
<p>We have a total of 150 different samples to train on, which should be enough to get reasonable performance when we try to use the model to classify a flower.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the network structure</h1>
                </header>
            
            <article>
                
<p>First, we need to determine what architecture to use for our neural network. We will be building a regular neural network, which is often called a feedforward neural network.</p>
<p>We need to define the number of neurons on the input and output layers first. Then, we need to define the shape of the hidden layer in our neural network. Because the task that we're solving is a simple one, we don't need more than one layer.</p>
<p class="mce-root"/>
<p>When we look at our dataset, we can see it has four features and one label. Because we have four features, we need to make sure our neural network has an input layer with four neurons.</p>
<p>Next, we need to define the output layer for our neural network. For this, we look at the number of classes that we need to be able to predict with our model. In our case, we have three different species of flowers to choose from, so we need three neurons in the output layer.</p>
<p><span>First, we import the necessary components from the CNTK library, which are our layer types, <kbd>activation</kbd> functions, and a function that allows us to define an input variable for our network:</span></p>
<pre>from cntk import default_options, input_variable<br/>from cntk.layers import Dense, Sequential<br/>from cntk.ops import log_softmax, relu</pre>
<p><span>We then create our model using the <kbd>Sequential</kbd> function, and feed it the layers that we want. We create two distinct layers in our networkâ€”first, one with four neurons, and then, another one with three neurons: </span></p>
<pre>model = Sequential([<br/>    Dense(4, activation=relu),<br/>    Dense(3, activation=log_softmax)<br/>])</pre>
<p><span>Finally, we bind the network to the input variable, which will compile the neural network so it has an input layer with four neurons, and an output layer with three neurons, as follows:</span></p>
<pre>features = input_variable(4)<br/>z = model(features)</pre>
<p>Now, let's go back to our layer structure. Notice that we didn't model an input layer when we invoked the <kbd>Sequential</kbd> layer function. This is because the <kbd>input_variable</kbd> we created in our code is the input layer for the neural network. </p>
<p>The first layer in the sequential call is the hidden layer in the network. As a general rule of thumb, you want hidden layers that are no bigger than two times the number of neurons in the previous layer. </p>
<p>You will want to experiment with this setup in order to get the best results. Picking the right numbers of layers and neurons in your neural network requires some experience and experimentation. There are no hard rules that determine how many hidden layers you should include.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Choosing an activation function</h1>
                </header>
            
            <article>
                
<p>In the previous section, we chose the <kbd>sigmoid</kbd> activation function for our neural network. Choosing the right activation makes a big difference to how well your deep learning model will perform.</p>
<p>You will find a lot of opinions about choosing an <kbd>activation</kbd> function. That's because there's a lot to choose from, and not enough hard proof for any of the choices made by experts in the field. So, how do you pick one for your neural network?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Choosing an activation function for the output layer</h1>
                </header>
            
            <article>
                
<p>First, we need to define what kind of problem we're solving. This determines the <kbd>activation</kbd> function for the output layer of your network. For regression problems, you want to use a <kbd>linear</kbd> activation function on the output layer. For a classification problem, you will want to use <kbd>sigmoid</kbd> for binary classification, and the <kbd>softmax</kbd> function for multi-class classification problems.</p>
<p>In the model that we're building, we need to predict one of three classes, which means we need to use the <kbd>softmax</kbd> activation function on the output layer. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Choosing an activation function for the hidden layers</h1>
                </header>
            
            <article>
                
<p class="mce-root">Now, let's look at the hidden layers. Choosing an <kbd>activation</kbd> function for the hidden layers in our model is much harder. We will need to run some experiments and monitor the performance to see which <kbd>activation</kbd> function works best.</p>
<p>For classification problems, like our flower classification model, we need something that gives us probabilistic values. We need this because we need to predict the probability a sample belongs to a specific class. The <kbd>sigmoid</kbd> function helps us reach this goal. Its output is a probability, measured as a value between 0 and 1. </p>
<p>There are some problems that we have to account for with a <kbd>sigmoid</kbd> activation function. When you create larger networks, you may run into a problem called the <strong>vanishing gradient</strong>. </p>
<p>Very large input values given to a <kbd>sigmoid</kbd> function will converge to either zero or one, depending on whether they are negative or positive. <span><span>This means that, when we work with large input values for our model, we won't see a lot of difference in the output of the <kbd>sigmoid</kbd> function. A change in an already large input value will result in only a very small change in the output. </span></span>The gradient that is derived from this by the optimizer during training is also very small. Sometimes, it is so small that your computer will round it to zero, which means the optimizer can't detect which way to go with the values for the parameters. When the optimizer can't calculate gradients because of rounding problems in the CPU, we're dealing with a vanishing gradient problem.</p>
<p>To solve this problem, scientists have come up with a new activation function, <kbd>ReLU</kbd>. This activation function converts all negative values to zero, and works as a pass-through filter for positive values. It helps solve the vanishing gradient problem, because it doesn't limit the output value. </p>
<p>There are, however, two problems with the <kbd>ReLU</kbd> function. First, it converts negative input to zero. In some cases, this can lead to a situation where the optimizer sets the weight of some parameters to zero as well. This causes your network to have dead neurons. That, of course, limits what your network can do. </p>
<p>The second problem is that the <kbd>ReLU</kbd> function suffers from exploding gradients. Because the upper bound of the output of this function isn't limited, it can amplify signals in such a way that the optimizer will calculate gradients that are close to infinity. When you apply this gradient to parameters in your network, your network will start to output NaN values.</p>
<p>Choosing the correct activation function for hidden layers requires some experimentation. Again, there is no hard rule that says which activation function to use. In the example code of this chapter, we choose the <kbd>sigmoid</kbd> function, after experimenting a bit with the model. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Picking a loss function</h1>
                </header>
            
            <article>
                
<p>When we have the structure for the model, it is time to take a look at how to optimize it. For this, we need a <kbd>loss</kbd> function to minimize. There are quite a few <kbd>loss</kbd> functions to choose from. </p>
<p>The right <kbd>loss</kbd> function depends on what kind of problem you are solving. For example, in a classification model like ours, we need a <kbd>loss</kbd> function that can measure the difference between a predicted class and an actual class. It needs to do so for three classes. The <kbd>categorical cross entropy</kbd> function is a good candidate. In CNTK, this <kbd>loss</kbd> function is implemented as <kbd>cross_entropy_with_softmax</kbd>: </p>
<pre>label = input_variable(3)<br/>loss = cross_entropy_with_softmax(z, label)</pre>
<p>We need to import the <kbd>cross_entropy_with_softmax</kbd> function from the <kbd>cntk.losses</kbd> package first. After we've imported the <kbd>loss</kbd> function, we create a new input variable so we can feed the expected label into the <kbd>loss</kbd> function. Then we create a new <kbd>loss</kbd> <span>variable </span>that will hold a reference to the <kbd>loss</kbd> function. Any <kbd>loss</kbd> function in CNTK requires the output of the model and an input variable for the expected label. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recording metrics</h1>
                </header>
            
            <article>
                
<p>With the structure in place and a <kbd>loss</kbd> function, we have all the ingredients we need to start optimizing our deep learning model. But before we start to look at how to train the model, let's take a look at metrics.</p>
<p>In order for us to see how our network is doing, we need to record some metrics. Since we're building a classification model, we're going to use a <kbd>classification_error</kbd> metric. This metric produces a number between 0 and 1, which indicates the percentage of samples correctly predicted:</p>
<pre>error_rate = classification_error(z, label)</pre>
<p>Let's import <kbd>classification_error</kbd> from the <kbd>cntk.metrics</kbd> package. We then create a new <kbd>error_rate</kbd> <span>variable </span>and bind the <kbd>classification_error</kbd> function to it. The function needs the output of the network and the expected label as input. We already have those available from defining our model and <kbd>loss</kbd> function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the neural network</h1>
                </header>
            
            <article>
                
<p>Now that we have all the components for the deep learning defined, let's train it. You can train a model in CNTK using a combination of a <kbd>learner</kbd> and <kbd>trainer</kbd>. We're going to need to define those and then feed data through the trainer to train the model. Let's see how that works.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Choosing a learner and setting up training</h1>
                </header>
            
            <article>
                
<p> </p>
<p>There are several <kbd>learners</kbd> to choose from. For our first model, we are going to use the <kbd>stochastic gradient descent</kbd> learner. Let's configure the <kbd>learner</kbd> and <kbd>trainer</kbd> to train the neural network:</p>
<pre>from cntk.learners import sgd<br/>from cntk.train.trainer import Trainer<br/><br/>learner = sgd(z.parameters, 0.01)<br/><br/>trainer = Trainer(z, (loss, error_rate), [learner])</pre>
<p>To configure the <kbd>learner</kbd> and <kbd>trainer</kbd> to train the neural network, follow the given steps:</p>
<ol>
<li>First, import the <kbd>sgd</kbd> function from the <kbd>learners</kbd> package</li>
<li>Then, import the <kbd>Trainer</kbd> from the <kbd>trainer</kbd> package which is part of the <kbd>train</kbd> package</li>
</ol>
<ol start="3">
<li>Now create a <kbd>learner</kbd> by invoking the <kbd>sgd</kbd> function providing the parameters of the model and a value for the learning rate</li>
<li>Finally, initialize the <kbd>trainer</kbd> and provide it the network, the combination of the <kbd>loss</kbd> and <kbd>metric</kbd> and the <kbd>learner</kbd></li>
</ol>
<p>The learning rate that we provide to the <kbd>sgd</kbd> function controls the speed of optimization and should be a small number somewhere in the area of 0.1 to 0.001. </p>
<p>Note that every <kbd>learner</kbd> has its own parameters, so be sure to check the documentation to find out what parameters you need to configure when using a specific <kbd>learner</kbd> from the <kbd>cntk.learners</kbd> package. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feeding data into the trainer to optimize the neural network</h1>
                </header>
            
            <article>
                
<p>We spent quite a bit of time defining our model, configuring the <kbd>loss</kbd>, <kbd>metrics</kbd>, and, finally, the <kbd>learner</kbd>. Now it is time to train it on our dataset. Before we can train our model, however, we need to load the dataset.</p>
<p>The dataset in the example code is stored as a CSV file. In order to load this dataset, we need to use a data wrangling package such as <kbd>pandas</kbd>. This package is included by default in your Anaconda installation. The following sample demonstrates how to use <kbd>pandas</kbd> to load the dataset into memory:</p>
<pre>import pandas as pd<br/><br/>df_source = pd.read_csv('iris.csv', <br/>    names=['sepal_length', 'sepal_width','petal_length','petal_width', 'species'], <br/>    index_col=False)</pre>
<p>To load the dataset into memory using <kbd>pandas</kbd> follow the given steps:</p>
<ol>
<li>First, import the <kbd>pandas</kbd> package under the alias <kbd>pd</kbd></li>
<li>Then, invoke the <kbd>read_csv</kbd> function to load the <kbd>iris.csv</kbd> file from disk</li>
</ol>
<p>Because the CSV file doesn't include column headers, we need to define them ourselves. It will make it easier to refer to specific columns later on.</p>
<p>Normally, <kbd>pandas</kbd> will use the first column in the input file as the index of the dataset. The index will serve as a key by which you can identify records. We don't have an index in our dataset, so we disable its use through the <kbd>index_col</kbd> keyword argument.</p>
<p>After we have loaded the dataset, let's split it into a set of features and a label: </p>
<pre>X = df_source.iloc[:, :4].values<br/>y = df_source['species'].values</pre>
<p>To split the dataset into a set of features and label, follow the given steps:</p>
<ol>
<li>First, use the <kbd>iloc</kbd> function to select all rows and the first four columns from the dataset</li>
<li>Next, select the species column from the dataset and use the values property to access the underlying <kbd>numpy</kbd> array</li>
</ol>
<p>Our model requires numeric input values. But the species column is a string value, indicating the type of flower. We can fix this by encoding the species column to a numeric vector representation. The vector representation we're creating matches the number of output neurons of the neural network. Each element in the vector represents a species of flowers as follows:</p>
<pre>label_mapping = {<br/>    'Iris-setosa': 0,<br/>    'Iris-versicolor': 1,<br/>    'Iris-virginica': 2<br/>}</pre>
<p>To create one-hot vector representations for the species, we will use a small <kbd>utility</kbd> function:</p>
<pre>def one_hot(index, length):<br/>    result = np.zeros(length)<br/>    result[index] = 1<br/>    <br/>    return result</pre>
<p>The <kbd>one_hot</kbd> function performs the following steps:</p>
<ol>
<li>First, initialize a new array filled with zeros with the required <kbd>length</kbd></li>
<li>Next, select the element at the specified <kbd>index</kbd> and set it to <kbd>1</kbd></li>
</ol>
<p>Now that we have a dictionary mapping the species to the index, and a way to create one-hot vectors, we can turn the string values into their vector representation using one additional line of code:</p>
<pre>y = np.array([one_hot(label_mapping[v], 3) for v in y])</pre>
<p>Follow the given steps:</p>
<ol>
<li>First, create a list expression to iterate over all elements in the array</li>
<li>For each value in the array perform a look up in the <kbd>label_mapping</kbd> dictionary</li>
<li>Next, take this converted numeric value and apply the <kbd>one_hot</kbd> function to convert it to a one-hot encoded vector</li>
<li>Finally, take the converted list and turn it into a <kbd>numpy</kbd> array</li>
</ol>
<p>When you are training a deep learning model, or any machine learning model for that matter, you need to keep in mind that the computer will try to remember all the samples that you've used for training the model. At the same time, it will try to learn general rules. When the model remembers samples, but isn't able to deduce rules from the training samples, it is overfitting on your dataset.</p>
<p>To detect overfitting, you want to keep a small portion of your dataset separate from the training set. The training set is then used to train the model, while the test set is used to measure the performance of the model. </p>
<p>We can split our dataset into training and test sets using a <kbd>utility</kbd> function from the <kbd>scikit-learn</kbd> package:</p>
<pre>from sklearn.model_selection import train_test_split<br/><br/>X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, stratify=y)</pre>
<p>Follow the given steps:</p>
<ol>
<li>First, import the <kbd>train_test_split</kbd> function from the <kbd>model_selection</kbd> module in the <kbd>sklearn</kbd> package</li>
<li>Then, invoke the <kbd>train_test_split</kbd> function with the features <kbd>X</kbd> and the labels <kbd>y</kbd></li>
<li>Specify a <kbd>test_size</kbd> of <kbd>0.2</kbd> to set aside 20% of the data</li>
<li>Use the <kbd>stratify</kbd> keyword argument with the values from the labels array <kbd>y</kbd> so that we get an equal amount of samples in the training and test set for each of the species in the dataset</li>
</ol>
<p>If you don't use the <kbd>stratify</kbd> argument, you end up with a dataset that might not contain any samples for one class, while it has too many of another class. The model then doesn't learn how to classify the class that is missing in the training set, while it overfits on the other class, which has too many samples available.</p>
<p>Now that we have a training set and validation set, let's see how to feed them to our model to train it:</p>
<pre>trainer.train_minibatch({ features: X_train, label: y_train })</pre>
<p>To train the model, invoke the <kbd>train_minibatch</kbd> method on the <kbd>trainer</kbd> and give it a dictionary that maps the input data to the input variables that you used to define the neural network and its associated <kbd>loss</kbd> function. </p>
<p>We're using the <kbd>train_minibatch</kbd> method as a convenient way to feed data into the trainer. In the next chapter, we'll discuss other ways to feed data. We'll also look at what the <kbd>train_minibatch</kbd> method does in greater detail.</p>
<p>Note that you will have to call <kbd>train_minibatch</kbd> a number of times to get the network decently trained. So we'll have to write a short loop around this method call:</p>
<pre>for _epoch in range(10):<br/>    trainer.train_minibatch({ features: X_train, label: y_train })<br/><br/>    print('Loss: {}, Acc: {}'.format(<br/>        trainer.previous_minibatch_loss_average,<br/>        trainer.previous_minibatch_evaluation_average))</pre>
<p>Follow the given steps:</p>
<ol>
<li>First, create a new loop using the <kbd>for</kbd> statement and give it a range of <kbd>10</kbd> </li>
<li>Within the loop invoke the <kbd>train_minibatch</kbd> method with a mapping between the input variables and the associated data</li>
<li>Finally, print the <kbd>previous_minibatch_loss_average</kbd> and <kbd>previous_minibatch_evaluation_average</kbd> to monitor the training progress.</li>
</ol>
<p>When you invoke the <kbd>train_minibatch</kbd> method, the <kbd>trainer</kbd> will update the output of the <kbd>loss</kbd> function and the value for the <kbd>metric</kbd> function that we provided to the <kbd>trainer</kbd> and store it in the <kbd>previous_minibatch_evaluation_average</kbd>.</p>
<p>Each time the loop completes, an<span>d we've run the whole dataset through the <kbd>trainer</kbd>, we've completed one epoch of training. As we have seen in the previous chapt</span><span>er, it is normal to run several epochs before a model works well enough. As an added bonus, we're also printing the progress of our <kbd>trainer</kbd> after each epoch.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Checking the performance of the neural network</h1>
                </header>
            
            <article>
                
<p>Every time we pass data through the trainer to optimize our model, it measures the performance of the model through the metric that we configured for the trainer. The model performance measured during training is on the training set. It is useful to measure the accuracy on the training set, because it will tell you whether the model is actually learning anything from the data. </p>
<p>For a full analysis of the model performance, you need to measure the performance of the model using the test set. This can be done by invoking the <kbd>test_minibatch</kbd> method on the <kbd>trainer</kbd> as follows:</p>
<pre>trainer.test_minibatch( {features: X_test, label: y_test })</pre>
<p>This method accepts a dictionary with a mapping between the input variables and the data for the variables. The output of this method is the output of the <kbd>metric</kbd> function you've configured earlier. In our case, it's the accuracy of our model based on the data we've given as input.</p>
<p>When the accuracy on the test set is higher than the accuracy on the training set, we will have a model that is underfitting. We're dealing with overfitting when the accuracy on the test set is lower than the accuracy on the training set.</p>
<p>Both underfitting and overfitting are bad if you take them too far. The best performance is achieved when the accuracy on both test set and training set are almost the same. We'll talk more about model performance in <a href="e39df191-73e4-414f-b44b-efca6f0ad4cd.xhtml">Chapter 4</a>, <em>Validating Model Performance</em>. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Making predictions with a neural network</h1>
                </header>
            
            <article>
                
<p>One of the most satisfying things after training a deep learning model is to actually use it in an application. For now, we'll limit ourselves to using the model with a sample that we randomly pick from our test set. But, later on, in <a href="8db9f932-5716-4a33-82a7-0c5ce5fe2ed4.xhtml">Chapter 7</a>, <span><em>Deploying Models to</em> <em>Production</em>,</span> we'll look at how to save the model to disk and use it in C# or .NET to build applications with it.</p>
<p>Let's write the code to make a prediction with the neural network that we trained:</p>
<pre>sample_index = np.random.choice(X_test.shape[0])<br/>sample = X_test[sample_index]<br/><br/>inverted_mapping = {<br/>    1: 'Iris-setosa',<br/>    2: 'Iris-versicolor',<br/>    3: 'Iris-virginica'<br/>}<br/><br/>prediction = z(sample)<br/>predicted_label = inverted_mapping[np.argmax(prediction)]<br/><br/>print(predicted_label)</pre>
<p>Follow the given steps:</p>
<ol>
<li>First, pick a random item from the test set using the <kbd>np.random.choice</kbd> function</li>
<li>Then select the sample data from the test set using the generated <kbd>sample_index</kbd></li>
<li>Next, create an inverted mapping so you can convert the numeric output of the neural network to an actual label</li>
</ol>
<ol start="4">
<li>Now, use the selected <kbd>sample</kbd> data and make a prediction by invoking the neural network <kbd>z</kbd> as a function</li>
</ol>
<ol start="5">
<li>From the predicted output, take the index of the neuron that has the highest value as the predicted value using the <kbd>np.argmax</kbd> function from the <kbd>numpy</kbd> package</li>
<li>Use the <kbd>inverted_mapping</kbd> to convert the index value into the real label</li>
</ol>
<p>When you execute the code sample, you will get output similar to this:</p>
<pre>Iris-versicolor</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Improving the model</h1>
                </header>
            
            <article>
                
<p>You will quickly learn that building and training neural networks takes more than one attempt. Usually, the first version of your model will not work as well as you hope. It requires quite a bit of experimentation to come up with a great model.</p>
<p>A good neural network starts with a great dataset. In nearly all cases, better performance is achieved by using a proper dataset. Many data scientists will tell you that they spend about 80% of their time working on a good dataset. As with all computer software, if you put garbage in, you will get garbage out.</p>
<p>Even with a good dataset, you still need to spend quite some time to build and train different models before you get the performance you're after. So, let's see what you can do to improve your model after you've built it for the first time.</p>
<p>After you've trained the model for the first time, you have a couple of options to choose from in order to improve your model. </p>
<p>Take a look at the accuracy of your training and validation sets. Is the accuracy on the training set lower? Try to train the model for more epochs. Usually, this will help improve the model. </p>
<p>Does the training accuracy not improve even, if you train the model for longer? Then your model is probably unable to learn the complex relationships in your dataset. Try to change the model structure, and train the model again to see if that improves the accuracy. </p>
<p>For example, try to change the activation function or the number of neurons in your hidden layers. This will usually help the model to learn the more complex relationships in the dataset.</p>
<p>Alternatively, you can take a look at the number of layers in your model. Adding one more layer can have quite a large effect on the ability of your model to learn rules from the data you feed it.</p>
<p class="mce-root"/>
<p>Finally, when that doesn't help, take a look at the initialization of the layers in your model. In some cases, choosing a different initialization function helps the model during the initial learning steps.</p>
<p>The key to the process of experimentation is to change one thing at a time and keep track of your experiments. Using a source control solution such as Git can help you keep track of different versions of your training code.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we've built our first neural network and trained it to recognize iris flowers. While this sample is really basic, it shows how to use CNTK to build and train neural networks. </p>
<p>We've seen how to use the layer library in CNTK to our advantage to quickly define the structure for our neural network. In this chapter, we've talked about a few basic building blocks, such as the <kbd>Dense</kbd> layer and the <kbd>Sequential</kbd> layer, to chain several other layers together. In the coming chapters, we will learn other layer functions to build other types of neural networks such as convolutional networks.</p>
<p>In this chapter, we've also discussed how to use <kbd>learner</kbd> and <kbd>trainer</kbd> to build a basic algorithm to train our neural network. We've used the <kbd>train_minibatch</kbd> method, together with a basic loop, to construct our own training process. This is a pretty simple and powerful way to train our model. In the next chapter, we'll discuss other methods of training and the <kbd>train_minibatch</kbd> method in much more detail. </p>
<p>After we trained the model, we made use of the functional properties of CNTK to make a prediction with our trained model. The fact that a model is a function is quite powerful, and makes it really intuitive to use trained models in your application.</p>
<p>Finally, we've seen how to measure model performance using the <kbd>test_minibatch</kbd> method, and how to use performance metrics to check whether our model is overfitting. We later discussed how to use metrics to determine how to improve the model.</p>
<p>In the next chapter, we will look at different ways to access and feed data to CNTK models. <span>We'll also explore each method of data access in CNTK, and which are the most appropriate to use in different circumstances.</span></p>


            </article>

            
        </section>
    </body></html>