- en: Unity ML-Agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unity has embraced machine learning, and deep reinforcement learning in particular, with
    determination and vigor with the aim of producing a working **seep reinforcement
    learning** (**DRL**) SDK for game and simulation developers. Fortunately, the
    team at Unity, led by Danny Lange, has succeeded in developing a robust cutting-edge
    DRL engine capable of impressive results. This engine is the top of the line and
    outclasses the DQN model we introduced earlier in many ways. Unity uses a **proximal
    policy optimization** (**PPO**) model as the basis for its DRL engine. This model
    is significantly more complex and may differ in some ways, but, fortunately, this
    is at the start of many more chapters, and we will have plenty of time to introduce
    the concepts as we go—this is a hands-on book, after all.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we introduce the **Unity ML-Agents** tools and SDK for building
    DRL agents to play games and simulations. While this tool is both powerful and
    cutting-edge, it is also easy to use and provides a few tools to help us learn
    concepts as we go. In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing ML-Agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training an agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What's in a brain?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring training with TensorBoard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running an agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We would like to thank the team members at Unity for their great work on ML-Agents;
    here are the team members at the time of writing:'
  prefs: []
  type: TYPE_NORMAL
- en: Danny Lange ([https://arxiv.org/search/cs?searchtype=author&query=Lange%2C+D](https://arxiv.org/search/cs?searchtype=author&query=Lange%2C+D))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arthur Juliani ([https://arxiv.org/search/cs?searchtype=author&query=Juliani%2C+A](https://arxiv.org/search/cs?searchtype=author&query=Juliani%2C+A))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vincent-Pierre Berges ([https://arxiv.org/search/cs?searchtype=author&query=Berges%2C+V](https://arxiv.org/search/cs?searchtype=author&query=Berges%2C+V))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Esh Vckay ([https://arxiv.org/search/cs?searchtype=author&query=Vckay%2C+E](https://arxiv.org/search/cs?searchtype=author&query=Vckay%2C+E))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan Gao ([https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Y](https://arxiv.org/search/cs?searchtype=author&query=Gao%2C+Y))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hunter Henry ([https://arxiv.org/search/cs?searchtype=author&query=Henry%2C+H](https://arxiv.org/search/cs?searchtype=author&query=Henry%2C+H))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Marwan Mattar ([https://arxiv.org/search/cs?searchtype=author&query=Mattar%2C+M](https://arxiv.org/search/cs?searchtype=author&query=Mattar%2C+M))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adam Crespi ([https://arxiv.org/search/cs?searchtype=author&query=Crespi%2C+A](https://arxiv.org/search/cs?searchtype=author&query=Crespi%2C+A))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jonathan Harper ([https://arxiv.org/search/cs?searchtype=author&query=Harper%2C+J](https://arxiv.org/search/cs?searchtype=author&query=Harper%2C+J))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be sure you have Unity installed as per the section in [Chapter 4](a8e699ff-c668-4601-842d-4c6e06c47a61.xhtml),
    *Building a Deep Learning Gaming Chatbot,* before proceeding with this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Installing ML-Agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we cover a high-level overview of the steps you will need to
    take in order to successfully install the ML-Agents SDK. This material is still
    in beta and has already changed significantly from version to version. As such,
    if you get stuck going through these high-level steps, just go back to the most
    recent Unity docs; they are very well written.
  prefs: []
  type: TYPE_NORMAL
- en: 'Jump on your computer and follow these steps; there may be many sub steps,
    so expect this to take a while:'
  prefs: []
  type: TYPE_NORMAL
- en: Be sure you have **Git** installed on your computer; it works from the command
    line. Git is a very popular source code management system, and there is a ton
    of resources on how to install and use Git for your platform. After you have installed
    Git, just be sure it works by test cloning a repository, any repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open a command window or a regular shell. Windows users can open an Anaconda
    window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Change to a working folder where you want to place the new code, and enter
    the following command (Windows users may want to use `C:\ML-Agents`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This will clone the `ml-agents` repository onto your computer and create a
    new folder with the same name. You may want to take the extra step of also adding
    the version to the folder name. Unity, and pretty much the whole AI space, is
    in continuous transition, at least at the moment. This means new and constant
    changes are always happening. At the time of writing, we will clone to a folder
    named `ml-agents.6`, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The author of this book previously wrote a book on ML-Agents and had to rewrite
    several chapters over the course of a short time in order to accommodate the major
    changes. In fact, this chapter has had to be also rewritten a few times to account
    for more major changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new virtual environment for `ml-agents` and set it to `3.6`, like
    so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Activate the environment, again, using Anaconda:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Install TensorFlow. With Anaconda, we can do this by using the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the Python packages. On Anaconda, enter the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This will install all the required packages for the Agents SDK and may take
    several minutes. Be sure to leave this window open, as we will use it shortly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is the basic installation of TensorFlow and does not use a GPU. Consult
    the Unity documentation in order to learn how to install the GPU version. This
    may or may not have a dramatic impact on your training performance, depending
    on the power of your GPU.
  prefs: []
  type: TYPE_NORMAL
- en: This should complete the setup of the Unity Python SDK for ML-Agents. In the
    next section, we will learn how to set up and train one of the many example environments
    provided by Unity.
  prefs: []
  type: TYPE_NORMAL
- en: Training an agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For much of this book, we have spent our time looking at code and the inner
    depths of **deep learning** (**DL**) and **reinforcement learning** (**RL**).
    With that knowledge established, we can now jump in and look at examples where
    **deep reinforcement learning** (**DRL**) is put to use. Fortunately, the new
    agent''s toolkit provides several examples to demonstrate the power of the engine.
    Open up Unity or the Unity Hub and follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Click the Open project button at the top of the Project dialog.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Locate and open the `UnitySDK` project folder as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ecbdfb91-2cfb-47ef-b264-b8cfe8e45d3b.png)'
  prefs: []
  type: TYPE_IMG
- en: Opening the UnitySDK project
  prefs: []
  type: TYPE_NORMAL
- en: Wait for the project to load and then open the Project window at the bottom
    of the editor. If you are asked to update the project, just be sure to say yes
    or continue. Thus far, all of the agent code has been designed to be backward
    compatible.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Locate and open the GridWorld scene as shown in this screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3456149c-1ca2-4371-b20e-bbba7590e13f.png)'
  prefs: []
  type: TYPE_IMG
- en: Opening the GridWorld example scene
  prefs: []
  type: TYPE_NORMAL
- en: Select the GridAcademy object in the Hierarchy window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then direct your attention to the Inspector window, and beside the Brains,
    click the target icon to open the Brain selection dialog:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3dfa4068-3490-4a17-b3fb-8f968b14433f.png)'
  prefs: []
  type: TYPE_IMG
- en: Inspecting the GridWorld example environment
  prefs: []
  type: TYPE_NORMAL
- en: Select the GridWorldPlayer brain. This brain is a *player* brain, meaning that
    a player, you, can control the game. We will look at this brain concept more in
    the next section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press the Play button at the top of the editor and watch the grid environment
    form. Since the game is currently set to a player, you can use the **WASD** controls
    to move the cube. The goal is much like the FrozenPond environment we built a
    DQN for earlier. That is, you have to move the blue cube to the green + symbol
    and avoid the red X.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feel free to play the game as much as you like. Note how the game only runs
    for a certain amount of time and is not turn-based. In the next section, we will
    learn how to run this example with a DRL agent.
  prefs: []
  type: TYPE_NORMAL
- en: What's in a brain?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the brilliant aspects of the ML-Agents platform is the ability to switch
    from player control to AI/agent control very quickly and seamlessly. In order
    to do this, Unity uses the concept of a **brain**. A brain may be either player-controlled,
    a player brain, or agent-controlled, a learning brain. The brilliant part is that
    you can build a game and test it, as a player can then turn the game loose on
    an RL agent. This has the added benefit of making any game written in Unity controllable
    by an AI with very little effort. In fact, this is such a powerful workflow that
    we will spend an entire chapter, [Chapter 12](323523c2-82f9-48c4-b1b5-35d417f90558.xhtml), *Debugging/Testing
    a Game with DRL*, on testing and debugging your games with RL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training an RL agent with Unity is fairly straightforward to set up and run.
    Unity uses Python externally to build the learning brain model. Using Python makes
    far more sense, since as we have already seen, several DL libraries are built
    on top of it. Follow these steps to train an agent for the GridWorld environment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the GridAcademy again and switch the Brains from GridWorldPlayer to
    GridWorldLearning as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/574c1a5c-98dd-4d89-8d2a-5eb9a81d1eb4.png)'
  prefs: []
  type: TYPE_IMG
- en: Switching the brain to use GridWorldLearning
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to click the Control option at the end. This simple setting is what
    tells the brain it may be controlled externally. Be sure to double-check that
    the option is enabled.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select the trueAgent object in the Hierarchy window, and then, in the Inspector
    window, change the Brain property under the Grid Agent component to a GridWorldLearning
    brain:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/438357ea-b6c5-49c3-a2a2-9e6d44c7f0ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Setting the brain on the agent to GridWorldLearning
  prefs: []
  type: TYPE_NORMAL
- en: For this sample, we want to switch our Academy and Agent to use the same brain,
    GridWorldLearning. In more advanced cases we will explore later, this is not always
    the case. You could of course have a player and an agent brain running in tandem,
    or many other configurations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Be sure you have an Anaconda or Python window open and set to the `ML-Agents/ml-agents` folder
    or your versioned `ml-agents` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following command in the Anaconda or Python window using the `ml-agents`
    virtual environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This will start the Unity PPO trainer and run the agent example as configured.
    At some point, the command window will prompt you to run the Unity editor with
    the environment loaded.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Press Play in the Unity editor to run the GridWorld environment. Shortly after,
    you should see the agent training with the results being output in the Python
    script window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/28c402ce-1770-4986-ad61-3250d518b949.png)'
  prefs: []
  type: TYPE_IMG
- en: Running the GridWorld environment in training mode
  prefs: []
  type: TYPE_NORMAL
- en: Note how the `mlagents-learn` script is the Python code that builds the RL model
    to run the agent. As you can see from the output of the script, there are several
    parameters, or what we refer to as **hyper-parameters**, that need to be configured.
    Some of these parameters may sound familiar, and they should, but several may
    be unclear. Fortunately, for the rest of this chapter and this book, we will explore
    how to tune these parameters in some detail.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let the agent train for several thousand iterations and note how quickly it
    learns. The internal model here, called **PPO**, has been shown to be a very effective
    learner at multiple forms of tasks and is very well suited for game development.
    Depending on your hardware, the agent may learn to perfect this task in less than
    an hour.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keep the agent training, and we will look at more ways to inspect the agent's
    training progress in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring training with TensorBoard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Training an agent with RL, or any DL model for that matter, while enjoyable,
    is not often a simple task and requires some attention to detail. Fortunately,
    TensorFlow ships with a set of graph tools called **TensorBoard** we can use to
    monitor training progress. Follow these steps to run TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: Open an Anaconda or Python window. Activate the `ml-agents` virtual environment.
    Don't shut down the window running the trainer; we need to keep that going.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to the `ML-Agents/ml-agents` folder and run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This will run TensorBoard with its own built-in web server. You can load the
    page by using the URL that is shown after you run the previous command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enter the URL for TensorBoard as shown in the window, or use `localhost:6006`
    or `machinename:6006` in your browser. After an hour or so, you should see something
    similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/40e5492f-96ed-4f05-b12a-b74a9ddcd12c.png)'
  prefs: []
  type: TYPE_IMG
- en: The TensorBoard graph window
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding screenshot, you can see each of the various graphs denoting
    an aspect of training. Understanding each of these graphs is important to understanding
    how your agent is training, so we will break down the output from each section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Environment: This section shows how the agent is performing overall in the
    environment. A closer look at each of the graphs is shown in the following screenshot
    with their preferred trend:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f404a429-9388-4c55-8bef-433bd24e2523.png)'
  prefs: []
  type: TYPE_IMG
- en: Closer look at the Environment section plots
  prefs: []
  type: TYPE_NORMAL
- en: 'Cumulative Reward: This is the total reward the agent is maximizing. You generally
    want to see this going up, but there are reasons why it may fall. It is always
    best to maximize rewards in the range of 1 to -1\. If you see rewards outside
    this range on the graph, you also want to correct this as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Episode Length: It usually is a better sign if this value decreases. After
    all, shorter episodes mean more training. However, keep in mind that the episode
    length could increase out of need, so this one can go either way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lesson: This represents which lesson the agent is on and is intended for Curriculum
    Learning. We will learn more about Curriculum Learning in [Chapter 9](ae184eca-6c9d-456e-a72b-85274ddcc10c.xhtml),
    *Rewards and Reinforcement Learning*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Losses: This section shows graphs that represent the calculated loss or cost
    of the policy and value. Of course, we haven''t spent much time explaining PPO
    and how it uses a policy, so, at this point, just understand the preferred direction
    when training. A screenshot of this section is shown next, again with arrows showing
    the optimum preferences:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/357e6132-9381-4c6b-bde8-3b66bd378e4f.png)'
  prefs: []
  type: TYPE_IMG
- en: Losses and preferred training direction
  prefs: []
  type: TYPE_NORMAL
- en: 'Policy Loss: This determines how much the policy is changing over time. The
    policy is the piece that decides the actions, and in general this graph should
    be showing a downward trend, indicating that the policy is getting better at making
    decisions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Value Loss: This is the mean or average loss of the `value` function. It essentially
    models how well the agent is predicting the value of its next state. Initially,
    this value should increase, and then after the reward is stabilized, it should
    decrease.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Policy: PPO uses the concept of a policy rather than a model to determine the
    quality of actions. Again, we will spend more time on this in [Chapter 8](1393797c-79cd-46c3-8e43-a09a7750fc92.xhtml), *Understanding
    PPO,* where we will uncover further details about PPO. The next screenshot shows
    the policy graphs and their preferred trend:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/cc002fa5-7562-415a-93ea-f4f5ab381170.png)'
  prefs: []
  type: TYPE_IMG
- en: Policy graphs and preferred trends
  prefs: []
  type: TYPE_NORMAL
- en: 'Entropy: This represents how much the agent is exploring. You want this value
    to decrease as the agent learns more about its surroundings and needs to explore
    less.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Learning Rate: Currently, this value is set to decrease linearly over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Value Estimate: This is the mean or average value visited by all states of
    the agent. This value should increase in order to represent a growth of the agent''s
    knowledge and then stabilize.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These graphs are all designed to work with the implementation of the PPO method
    Unity is based on. Don't worry too much about understanding these new terms just
    yet. We will explore the foundations of PPO in [Chapter 7](9b7b6ff8-8daa-42bd-a80f-a7379c37c011.xhtml),
    *Agent and the Environment*.
  prefs: []
  type: TYPE_NORMAL
- en: Let the agent run to completion and keep TensorBoard running.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Go back to the Anaconda/Python window that was training the brain and run this
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: You will again be prompted to press Play in the editor; be sure to do so. Let
    the agent start the training and run for a few sessions. As you do so, monitor
    the TensorBoard window and note how the `secondRun` is shown on the graphs. Feel
    free to let this agent run to completion as well, but you can stop it now, if
    you want to.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In previous versions of ML-Agents, you needed to build a Unity executable first
    as a game-training environment and run that. The external Python brain would still
    run the same. This method made it very difficult to debug any code issues or problems
    with your game. All of these difficulties were corrected with the current method;
    however, we may need to use the old executable method later for some custom training.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen how easy it is to set up and train an agent, we will go
    through the next section to see how that agent can be run without an external
    Python brain and run directly in Unity.
  prefs: []
  type: TYPE_NORMAL
- en: Running an agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using Python to train works well, but it is not something a real game would
    ever use. Ideally, what we want to be able to do is build a TensorFlow graph and
    use it in Unity. Fortunately, a library was constructed, called TensorFlowSharp,
    that allows .NET to consume TensorFlow graphs. This allows us to build offline
    TFModels and later inject them into our game. Unfortunately, we can only use trained
    models and not train in this manner, at least not yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how this works by using the graph we just trained for the GridWorld
    environment and use it as an internal brain in Unity. Follow the exercise in the
    next section to set up and use an internal brain:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the TFSharp plugin from this link: [https://s3.amazonaws.com/unity-ml-agents/0.5/TFSharpPlugin.unitypackage](https://s3.amazonaws.com/unity-ml-agents/0.5/TFSharpPlugin.unitypackage).
    [](https://s3.amazonaws.com/unity-ml-agents/0.5/TFSharpPlugin.unitypackage)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If this link does not work, consult the Unity docs or the Asset Store for a
    new one. The current version is described as experimental and subject to change.
  prefs: []
  type: TYPE_NORMAL
- en: From the editor menu, select Assets | Import Package | Custom Package...
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Locate the asset package you just downloaded and use the import dialogs to load
    the plugin into the project. If you need help with these basic Unity tasks, there
    is plenty of help online that can guide you further.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the menu, select Edit | Project Settings. This will open the Settings window
    (new in 2018.3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Locate under the Player options the Scripting Define Symbols and set the text
    to `ENABLE_TENSORFLOW` and enable Allow Unsafe Code, as shown in this screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ab714c99-1b2d-4e27-89e4-eff8395c7f43.png)'
  prefs: []
  type: TYPE_IMG
- en: Setting the ENABLE_TENSORFLOW flag
  prefs: []
  type: TYPE_NORMAL
- en: Locate the GridWorldAcademy object in the Hierarchy window and make sure it
    is using the Brains | GridWorldLearning. Turn the Control option off under the
    Brains section of the Grid Academy script.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Locate the GridWorldLearning brain in the `Assets/Examples/GridWorld/Brains`
    folder and make sure the Model parameter is set in the Inspector window, as shown
    in this screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/321101c9-21cb-4a31-b298-dc4c3a51cc32.png)'
  prefs: []
  type: TYPE_IMG
- en: Setting the model for the brain to use
  prefs: []
  type: TYPE_NORMAL
- en: The Model should already be set to theGridWorldLearningmodel. In this example,
    we are using the TFModel that is shipped with the GridWorld example. You could
    also easily use the model we had trained from the earlier example by just importing
    it into the project and then setting it as the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press Play to run the editor and watch the agent control the cube.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Right now, we are running the environment with the pre-trained Unity brain.
    In the next section, we will look at how to use the brain we trained in the previous
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Loading a trained brain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All of the Unity samples come with pre-trained brains you can use to explore
    the samples. Of course, we want to be able to load our own TF graphs into Unity
    and run them. Follow the next steps in order to load a trained graph:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Locate the `ML-Agents/ml-agents/models/firstRun-0` folder. Inside this folder,
    you should see a file named `GridWorldLearning.bytes`. Drag this file into the
    Unity editor into the `Project/Assets/ML-Agents/Examples/GridWorld/TFModels` folder,
    as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a41646b7-b3df-4fdf-a9bc-2ca3f9370961.png)'
  prefs: []
  type: TYPE_IMG
- en: Dragging the bytes graph into Unity
  prefs: []
  type: TYPE_NORMAL
- en: This will import the graph into the Unity project as a resource and rename it
    `GridWorldLearning 1`. It does this because the default model already has the
    same name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Locate the `GridWorldLearning` from the `brains` folder and select it in the Inspector
    windows and drag the new GridWorldLearning 1 model onto the Model slot under the
    Brain Parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/21bef292-7d4d-4f6f-a20c-9243c3c4bb8e.png)'
  prefs: []
  type: TYPE_IMG
- en: Loading the Graph Model slot in the brain
  prefs: []
  type: TYPE_NORMAL
- en: We won't need to change any other parameters at this point, but pay special
    attention to how the brain is configured. The defaults will work for now.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press Play in the Unity editor and watch the agent run through the game successfully.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How long you trained the agent for will really determine how well it plays the
    game. If you let it complete the training, the agent should be equal to the already
    trained Unity agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are plenty of Unity samples that you can now run and explore on your own.
    Feel free to train several of the examples on your own or as listed in the exercises
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Use the exercises in this section to enhance and reinforce your learning. Attempt
    at least a few of these exercises on your own, and remember this is really for
    your benefit:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up and run the 3DBall example environment to train a working agent. This
    environment uses multiple games/agents to train.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the 3DBall example to let half of the games use an already trained brain
    and the other to use training or external learning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the PushBlock environment agents using external learning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the VisualPushBlock environment. Note how this example uses a visual camera
    to capture the environment state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the Hallway scene as a player and then train the scene using an external
    learning brain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the VisualHallway scene as a player and then train the scene using an external
    learning brain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the WallJump scene and then run it under training conditions. This example
    uses Curriculum Training, which we will look at further in [Chapter 9](ae184eca-6c9d-456e-a72b-85274ddcc10c.xhtml), *Rewards
    and Reinforcement Learning*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the Pyramids scene and then set it up for training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the VisualPyramids scene and set it up for training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the Bouncer scene and set it up for training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While you don't have to run all these exercises/examples, it can be helpful
    to familiarize yourself with them. They can often be the basis for creating new
    environments, as we will see in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you have learned, the workflow for training RL and DRL agents in Unity is
    much more integrated and seamless than in OpenAI Gym. We didn't have to write
    a line of code to train an agent in a grid world environment, and the visuals
    are just plain better. For this chapter, we started by installing the ML-Agents
    toolkit. Then we loaded up a GridWorld environment and set it up to train with
    an RL agent. From there, we looked at TensorBoard for monitoring agent training
    and progress. After we were done training, we first loaded up a Unity pre-trained
    brain and ran that in the GridWorld environment. Then we used a brain we just
    trained and imported that into Unity as an asset and then as the GridWorldLearning
    brain's model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore how to construct a new RL environment or
    game we can use an agent to learn and play. This will allow us to peek under the
    covers further about the various details we skimmed over in this chapter.
  prefs: []
  type: TYPE_NORMAL
