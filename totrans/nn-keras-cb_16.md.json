["```py\ndef play_game():\n     empty_board = [0,0,0]\n     move = []\n     for step in range(3):\n         index_to_choose = [i for i,j in enumerate(empty_board) if j==0]\n         samp = random.sample(range(len(index_to_choose)), 1)[0] \n         if(step%2==0):\n             empty_board[index_to_choose[samp]]=1\n             move.append(index_to_choose[samp])\n         else:\n             empty_board[index_to_choose[samp]]=2 \n     return(reward(empty_board), move[0])\n```", "```py\ndef reward(empty_board):\n     reward = 0\n     if((empty_board[0]==1 & empty_board[1]==1) | (empty_board[1]==1 & empty_board[2]==1)):\n         reward = 1\n     else:\n         reward = 0\n     return reward\n```", "```py\nrew = []\nstep = []\nfor i in range(100):\n     r, move = play_game()\n     rew.append(r)\n     step.append(move) \n```", "```py\nsub_list = [i for i,j in enumerate(step) if j==1]\nfinal_reward = 0\ncount = 0\nfor i in sub_list:\n     final_reward += rew[i]\n     count+=1\nfinal_reward/count\n```", "```py\nempty_board = [[0,0,0]\n             ,[0,0,1]]\n```", "```py\nstate_actions = {(0,0):('D','R')\n                 ,(0,1):('D','R','L')\n                 ,(0,2):('D','L')\n                 ,(1,0):('U','R')\n                 ,(1,1):('L','U','R') \n                 }\n```", "```py\ndef get_next_state(curr_state, action):\n     i,j = curr_state\n     if action=='D':\n         i = i+1\n     elif action=='U':\n         i = i-1\n     elif action=='R':\n         j = j+1\n     elif action=='L':\n         j = j-1\n     else:\n         print('unk')\n     return((i,j))\n```", "```py\ncurr_state = (0,0)\nstate_action_reward = []\nstate = []\nstate_action = []\n```", "```py\nfor m in range(100):\n     curr_state = (0,0)\n     for k in range(100):\n         reward = 0\n         action = state_actions[curr_state][random.sample(range(len(state_actions[curr_state])),1)[0]]\n         next_state = get_next_state(curr_state, action)\n```", "```py\n        state.append(curr_state)\n        empty_board[curr_state[0]][curr_state[1]] = reward + empty_board[next_state[0]][next_state[1]]*0.9 \n        empty_board[curr_state[0]][curr_state[1]])\n```", "```py\n        curr_state = next_state\n        state_action.append(action)\n\n        if(next_state==(1,2)):\n             reward+=1\n             break\n```", "```py\nstate_actions = {(0,0):('D','R')\n                 ,(0,1):('D','R')\n                 ,(1,0):('R')\n                 ,(1,1):('R') \n                 }\n```", "```py\nimport gym\nfrom gym import envs\nfrom gym.envs.registration import register\n```", "```py\nregister(\n id = 'FrozenLakeNotSlippery-v1',\n entry_point = 'gym.envs.toy_text:FrozenLakeEnv',\n kwargs = {'map_name': '4x4', 'is_slippery':False},\n max_episode_steps = 100,\n reward_threshold = 0.8196)\n```", "```py\nenv = gym.make('FrozenLakeNotSlippery-v1')\n```", "```py\nenv.render()\n```", "```py\nenv.observation_space\n```", "```py\nenv.action_space.n\n```", "```py\nenv.action_space.sample()\n```", "```py\nenv.step(action)\n```", "```py\nenv.reset()\n```", "```py\nimport numpy as np\nqtable = np.zeros((16,4))\n```", "```py\ntotal_episodes=15000\nlearning_rate=0.8\nmax_steps=99\ngamma=0.95\nepsilon=1\nmax_epsilon=1\nmin_epsilon=0.01\ndecay_rate=0.005\n```", "```py\nrewards=[]\nfor episode in range(total_episodes):\n    state=env.reset()\n    step=0\n    done=False\n    total_rewards=0\n```", "```py\n    for step in range(max_steps):\n        exp_exp_tradeoff=random.uniform(0,1)        \n        ## Exploitation:\n        if exp_exp_tradeoff>epsilon:\n            action=np.argmax(qtable[state,:])\n        else:\n            ## Exploration\n            action=env.action_space.sample()\n```", "```py\n        new_state, reward, done, _ = env.step(action)\n```", "```py\n        qtable[state,action]=qtable[state,action]+learning_rate*(reward+gamma*np.max(qtable[new_state,:])-qtable[state,action])\n        total_rewards+=reward\n        state=new_state\n```", "```py\n        if(done):\n             break\n        epsilon=min_epsilon+(max_epsilon-min_epsilon)*np.exp(decay_rate*episode)\n        rewards.append(total_rewards)\n```", "```py\nenv.reset()\n\nfor episode in range(1):\n    state=env.reset()\n    step=0\n    done=False\n    print(\"-----------------------\")\n    print(\"Episode\",episode)\n    for step in range(max_steps):\n        env.render()\n        action=np.argmax(qtable[state,:])\n        print(action)\n        new_state,reward,done,info=env.step(action)\n\n        if done:\n            #env.render()\n            print(\"Number of Steps\",step+1)\n            break\n        state=new_state\n```", "```py\nimport gym \nenv = gym.make('CartPole-v0') \nstate_size = env.observation_space.shape[0] \naction_size = env.action_space.n\n```", "```py\nimport numpy as np\nimport random\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom collections import deque\n```", "```py\nmodel=Sequential()\nmodel.add(Dense(24,input_dim=state_size,activation='relu'))\nmodel.add(Dense(24,activation='relu'))\nmodel.add(Dense(2,activation='linear'))\nmodel.compile(loss='mse',optimizer=Adam(lr=0.01))\n```", "```py\nmemory = deque(maxlen=2000)\ngamma = 0.95 # discount rate\nepsilon = 1.0 # exploration rate\nepsilon_min = 0.01\nepsilon_decay = 0.995\ndone = False\nbatch_size=32\n```", "```py\ndef replay(model, batch_size,epsilon):\n    epsilon_min = 0.01\n    epsilon_decay = 0.995\n    minibatch = random.sample(memory, batch_size)\n    for state, action, reward, next_state, done in minibatch:\n        target = reward\n        if not done:\n            target = (reward + gamma *np.amax(model.predict(next_state)[0]))\n        new_action_value = model.predict(state)\n        new_action_value[0][action] = target\n        model.fit(state,new_action_value, epochs=1, verbose=0)\n    if epsilon > epsilon_min:\n        epsilon *= epsilon_decay\n    return model,epsilon\n```", "```py\nepisodes=200\nmaxsteps=200\nscore_list = []\nfor e in range(episodes):\n    state = env.reset()\n    state = np.reshape(state, [1, state_size])\n```", "```py\n    for step in range(maxsteps):\n        if np.random.rand()<=epsilon:\n            action=env.action_space.sample()\n        else:\n            action = np.argmax(model.predict(state)[0])\n```", "```py\n        next_state, reward, done, _ = env.step(action)\n        reward = reward if not done else -10\n        next_state = np.reshape(next_state, [1, state_size])\n        memory.append((state, action, reward, next_state, done))\n```", "```py\n        state = next_state\n        if done:\n          print(\"episode: {}/{}, score: {}, exp prob: {:.2}\".format(e, episodes, step, epsilon))\n          score_list.append(step)\n          break\n        if len(memory) > batch_size:\n          model,epsilon=replay(model, batch_size,epsilon)\n```", "```py\n$ wget http://www.atarimania.com/roms/Roms.rar && unrar x Roms.rar && unzip Roms/ROMS.zip\n$ pip3 install gym-retro\n$ python3 -m retro.import ROMS/\n```", "```py\nenv=retro.make(game='SpaceInvaders-Atari2600')\nenv.observation_space\n# Box(210,160,3)\n```", "```py\ndef preprocess_frame(frame):\n     # Greyscale frame \n     gray = rgb2gray(frame)\n     # Crop the screen (remove the part below the player)\n     # [Up: Down, Left: right]\n     cropped_frame = gray[8:-12,4:-12]\n     # Normalize Pixel Values\n     normalized_frame = cropped_frame/255.0\n     # Resize\n     preprocessed_frame = transform.resize(normalized_frame, [110,84])\n     return preprocessed_frame \n```", "```py\nstack_size = 4 # We stack 4 frames\n# Initialize deque with zero-images one array for each image\nstacked_frames = deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\ndef stack_frames(stacked_frames, state, is_new_episode):\n     # Preprocess frame\n     frame = preprocess_frame(state) \n     if is_new_episode:\n         # Clear our stacked_frames\n         stacked_frames = deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4) \n         # Because we're in a new episode, copy the same frame 4x\n         stacked_frames.append(frame)\n         stacked_frames.append(frame)\n         stacked_frames.append(frame)\n         stacked_frames.append(frame) \n         # Stack the frames\n         stacked_state = np.stack(stacked_frames, axis=2) \n     else:\n         # Append frame to deque, automatically removes the oldest frame\n         stacked_frames.append(frame)\n         # Build the stacked state (first dimension specifies different frames)\n         stacked_state = np.stack(stacked_frames, axis=2) \n     return stacked_state, stacked_frames\n```", "```py\n### MODEL HYPERPARAMETERS\nstate_size = [110, 84, 4] # Our input is a stack of 4 frames hence 110x84x4 (Width, height, channels) \naction_size = env.action_space.n # 8 possible actions\nlearning_rate = 0.00025 # Alpha (aka learning rate)\n```", "```py\n### TRAINING HYPERPARAMETERS\ntotal_episodes = 50 # Total episodes for training\nmax_steps = 50000 # Max possible steps in an episode\nbatch_size = 32 # Batch size\n```", "```py\n# Exploration parameters for epsilon greedy strategy\nexplore_start = 1.0 # exploration probability at start\nexplore_stop = 0.01 # minimum exploration probability \ndecay_rate = 0.00001 # exponential decay rate for exploration prob\n```", "```py\n# Q learning hyperparameters\ngamma = 0.9 # Discounting rate\n```", "```py\n### MEMORY HYPERPARAMETERS\npretrain_length = batch_size # Number of experiences stored in the Memory when initialized for the first time\nmemory_size = 1000000 # Number of experiences the Memory can keep\n```", "```py\n### PREPROCESSING HYPERPARAMETERS\nstack_size = 4 # Number of frames stacked\n```", "```py\n### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\ntraining = False\n```", "```py\n## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\nepisode_render = False\n```", "```py\nmemory = deque(maxlen=100000)\n```", "```py\ndef sample(memory, batch_size):\n     buffer_size = len(memory)\n     index = np.random.choice(np.arange(buffer_size),\n     size = batch_size,\n     replace = False) \n     return [memory[i] for i in index]\n```", "```py\ndef predict_action(model,explore_start, explore_stop, decay_rate, decay_step, state, actions):\n     exp_exp_tradeoff = np.random.rand()\n     explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n     if (explore_probability > exp_exp_tradeoff):\n         choice = random.randint(1,len(possible_actions))-1\n         action = possible_actions[choice]\n     else:\n         Qs = model.predict(state.reshape((1, *state.shape)))\n         choice = np.argmax(Qs)\n         action = possible_actions[choice]\n     return action, explore_probability\n```", "```py\ndef replay(agent,batch_size,memory):\n     minibatch = sample(memory,batch_size)\n     for state, action, reward, next_state, done in minibatch:\n     target = reward\n     if not done:\n         target = reward + gamma*np.max(agent.predict(next_state.reshape((1,*next_state.shape)))[0])\n     target_f = agent.predict(state.reshape((1,*state.shape)))\n     target_f[0][action] = target\n     agent.fit(state.reshape((1,*state.shape)), target_f, epochs=1, verbose=0)\n return agent\n```", "```py\ndef DQNetwork():\n     model=Sequential()\n     model.add(Convolution2D(32,input_shape=(110,84,4),kernel_size=8, strides=4, padding='valid',activation='elu'))\n     model.add(Convolution2D(64, kernel_size=4, strides=2, padding='valid',activation='elu'))\n     model.add(Convolution2D(128, kernel_size=3, strides=2, padding='valid',activation='elu'))\n     model.add(Flatten())\n     model.add(Dense(units=512))\n     model.add(Dense(units=3,activation='softmax'))\n     model.compile(optimizer=Adam(0.01),loss='mse')\n     return model\n```", "```py\nagent = DQNetwork()\nagent.summary()\nrewards_list=[]\nEpisodes=200\n# Iterate the game\nfor episode in range(Episodes):\n     # reset state in the beginning of each game\n     step = 0\n     decay_step = 0\n     episode_rewards = []\n     state = env.reset()\n     state, stacked_frames = stack_frames(stacked_frames, state, True)\n     while step < max_steps:\n         step += 1\n         decay_step +=1\n         # Predict the action to take and take it\n         action, explore_probability = predict_action(agent,explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n #Perform the action and get the next_state, reward, and done information\n         next_state, reward, done, _ = env.step(action)\n         # Add the reward to total reward\n         episode_rewards.append(reward)\n     if done:\n # The episode ends so no next state\n         next_state = np.zeros((110,84), dtype=np.int)\n         next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n # Set step = max_steps to end the episode\n         step = max_steps\n # Get the total reward of the episode\n         total_reward = np.sum(episode_rewards)\n         print('Episode:{}/{} Score:{} Explore Prob:{}'.format(episode,Episodes,total_reward,explore_probability))\n         rewards_list.append((episode, total_reward))\n # Store transition <st,at,rt+1,st+1> in memory D\n         memory.append((state, action, reward, next_state, done))\n     else:\n # Stack the frame of the next_state\n         next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n # Add experience to memory\n         memory.append((state, action, reward, next_state, done))\n # st+1 is now our current state\n         state = next_state\n     env.render() \n # train the agent with the experience of the episode\n agent=replay(agent,batch_size,memory)\n```", "```py\nscore=[]\nepisode=[]\nfor e,r in rewards_list:\n     episode.append(e)\n     score.append(r)\nimport matplotlib.pyplot as plt\nplt.plot(episode,score)\n```"]