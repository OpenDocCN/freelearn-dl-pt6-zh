["```py\nwget http://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx\n```", "```py\npip install implicit\n```", "```py\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport scipy.sparse as sp\nfrom tqdm import tqdm\n```", "```py\ndf = pd.read_excel('Online Retail.xlsx')\n```", "```py\nimport pickle\nwith open('df_retail.bin', 'wb') as f_out:\n    pickle.dump(df, f_out)\n```", "```py\nwith open('df_retail.bin', 'rb') as f_in:\n    df = pickle.load(f_in)\n```", "```py\ndf.head()\n```", "```py\ndf.columns = df.columns.str.lower()\ndf = df[~df.invoiceno.astype('str').str.startswith('C')].reset_index(drop=True)\ndf.customerid = df.customerid.fillna(-1).astype('int32')\n```", "```py\nstockcode_values = df.stockcode.astype('str')\n\nstockcodes = sorted(set(stockcode_values))\nstockcodes = {c: i for (i, c) in enumerate(stockcodes)}\n\ndf.stockcode = stockcode_values.map(stockcodes).astype('int32')\n```", "```py\ndf_train = df[df.invoicedate < '2011-10-09']\ndf_val = df[(df.invoicedate >= '2011-10-09') & \n            (df.invoicedate <= '2011-11-09') ]\ndf_test = df[df.invoicedate >= '2011-11-09']\n```", "```py\ntop = df_train.stockcode.value_counts().head(5).index.values\n```", "```py\narray([3527, 3506, 1347, 2730,  180])\n```", "```py\nnum_groups = len(df_val.invoiceno.drop_duplicates())\nbaseline = np.tile(top, num_groups).reshape(-1, 5)\n```", "```py\narray([[3527, 3506, 1347, 2730,  180],\n       [3527, 3506, 1347, 2730,  180],\n       [3527, 3506, 1347, 2730,  180],\n       ...,\n       [3527, 3506, 1347, 2730,  180],\n       [3527, 3506, 1347, 2730,  180],\n       [3527, 3506, 1347, 2730,  180]])\n```", "```py\ndef group_indptr(df):\n    indptr, = np.where(df.invoiceno != df.invoiceno.shift())\n    indptr = np.append(indptr, len(df)).astype('int32')\n    return indptr\n```", "```py\nval_indptr = group_indptr(df_val)\n```", "```py\nfrom numba import njit\n\n@njit\ndef precision(group_indptr, true_items, predicted_items):\n    tp = 0\n\n    n, m = predicted_items.shape\n\n    for i in range(n):\n        group_start = group_indptr[i]\n        group_end = group_indptr[i + 1]\n        group_true_items = true_items[group_start:group_end]\n\n        for item in group_true_items:\n            for j in range(m):\n                if item == predicted_items[i, j]:\n                    tp = tp + 1\n                    continue\n\n    return tp / (n * m)\n```", "```py\nval_items = df_val.stockcode.values\nprecision(val_indptr, val_items, baseline)\n```", "```py\ndf_train_user = df_train[df_train.customerid != -1].reset_index(drop=True)\n\ncustomers = sorted(set(df_train_user.customerid))\ncustomers = {c: i for (i, c) in enumerate(customers)}\n\ndf_train_user.customerid = df_train_user.customerid.map(customers)\n```", "```py\ndf_val.customerid = df_val.customerid.apply(lambda c: customers.get(c, -1))\n```", "```py\nuid = df_train_user.customerid.values.astype('int32')\niid = df_train_user.stockcode.values.astype('int32')\nones = np.ones_like(uid, dtype='uint8')\n\nX_train = sp.csr_matrix((ones, (uid, iid)))\n```", "```py\nfrom implicit.als import AlternatingLeastSquares\n\nitem_user = X_train.T.tocsr()\nals = AlternatingLeastSquares(factors=128, regularization=0.000001)\nals.fit(item_user)\n```", "```py\nals_U = als.user_factors\nals_I = als.item_factors\n```", "```py\nuid_val = df_val.drop_duplicates(subset='invoiceno').customerid.values\nknown_mask = uid_val != -1\nuid_val = uid_val[known_mask] \n```", "```py\nimp_baseline = baseline.copy()\n\npred_all = als_U[uid_val].dot(als_I.T)\ntop_val = (-pred_all).argsort(axis=1)[:, :5]\nimp_baseline[known_mask] = top_val\n\nprevision(val_indptr, val_items, imp_baseline)\n```", "```py\ndef embed(inputs, size, dim, name=None):\n    std = np.sqrt(2 / dim)\n    emb = tf.Variable(tf.random_uniform([size, dim], -std, std), name=name)\n    lookup = tf.nn.embedding_lookup(emb, inputs)\n    return lookup\n```", "```py\n# parameters of the model\nnum_users = uid.max() + 1\nnum_items = iid.max() + 1\n\nnum_factors = 128\nlambda_user = 0.0000001\nlambda_item = 0.0000001\nK = 5\nlr = 0.005\n\ngraph = tf.Graph()\ngraph.seed = 1\n\nwith graph.as_default():\n    # this is the input to the model\n    place_user = tf.placeholder(tf.int32, shape=(None, 1))\n    place_item = tf.placeholder(tf.int32, shape=(None, 1))\n    place_y = tf.placeholder(tf.float32, shape=(None, 1))\n\n    # user features\n    user_factors = embed(place_user, num_users, num_factors, \n        \"user_factors\")\n    user_bias = embed(place_user, num_users, 1, \"user_bias\")\n    user_bias = tf.reshape(user_bias, [-1, 1])\n\n    # item features\n    item_factors = embed(place_item, num_items, num_factors, \n        \"item_factors\")\n    item_bias = embed(place_item, num_items, 1, \"item_bias\")\n    item_bias = tf.reshape(item_bias, [-1, 1])\n\n    global_bias = tf.Variable(0.0, name='global_bias')\n\n    # prediction is dot product followed by a sigmoid\n    pred = tf.reduce_sum(user_factors * item_factors, axis=2)\n    pred = tf.sigmoid(global_bias + user_bias + item_bias + pred)\n\n    reg = lambda_user * tf.reduce_sum(user_factors * user_factors) + \\\n          lambda_item * tf.reduce_sum(item_factors * item_factors)\n\n    # we have a classification model, so minimize logloss\n    loss = tf.losses.log_loss(place_y, pred)\n    loss_total = loss + reg\n\n    opt = tf.train.AdamOptimizer(learning_rate=lr)\n    step = opt.minimize(loss_total)\n\n    init = tf.global_variables_initializer()\n```", "```py\ndef prepare_batches(seq, step):\n    n = len(seq)\n    res = []\n    for i in range(0, n, step):\n        res.append(seq[i:i+step])\n    return res\n```", "```py\nsession = tf.Session(config=None, graph=graph)\nsession.run(init)\n\nnp.random.seed(0)\n\nfor i in range(10):\n    train_idx_shuffle = np.arange(uid.shape[0])\n    np.random.shuffle(train_idx_shuffle)\n    batches = prepare_batches(train_idx_shuffle, 5000)\n\n    progress = tqdm(total=len(batches))\n    for idx in batches:\n        pos_samples = len(idx)\n        neg_samples = pos_samples * K \n\n        label = np.concatenate([\n                    np.ones(pos_samples, dtype='float32'), \n                    np.zeros(neg_samples, dtype='float32')\n                ]).reshape(-1, 1)\n\n        # negative sampling\n        neg_users = np.random.randint(low=0, high=num_users, \n                                      size=neg_samples, dtype='int32')\n        neg_items = np.random.randint(low=0, high=num_items,\n                                      size=neg_samples, dtype='int32')\n\n        batch_uid = np.concatenate([uid[idx], neg_users]).reshape(-1, 1)\n        batch_iid = np.concatenate([iid[idx], neg_items]).reshape(-1, 1)\n\n        feed_dict = {\n            place_user: batch_uid,\n            place_item: batch_iid,\n            place_y: label,\n        }\n        _, l = session.run([step, loss], feed_dict)\n\n        progress.update(1)\n        progress.set_description('%.3f' % l)\n    progress.close()\n\n    val_precision = calculate_validation_precision(graph, session, uid_val)\n    print('epoch %02d: precision: %.3f' % (i+1, val_precision))\n```", "```py\ndef get_variable(graph, session, name):\n    v = graph.get_operation_by_name(name)\n    v = v.values()[0]\n    v = v.eval(session=session)\n    return v\n\ndef calculate_validation_precision(graph, session, uid):\n    U = get_variable(graph, session, 'user_factors')\n    I = get_variable(graph, session, 'item_factors')\n    bi = get_variable(graph, session, 'item_bias').reshape(-1)\n\n    pred_all = U[uid_val].dot(I.T) + bi\n    top_val = (-pred_all).argsort(axis=1)[:, :5]\n\n    imp_baseline = baseline.copy()\n    imp_baseline[known_mask] = top_val\n\n    return precision(val_indptr, val_items, imp_baseline)\n```", "```py\nepoch 01: precision: 0.064\nepoch 02: precision: 0.086\nepoch 03: precision: 0.106\nepoch 04: precision: 0.127\nepoch 05: precision: 0.138\nepoch 06: precision: 0.145\nepoch 07: precision: 0.150\nepoch 08: precision: 0.149\nepoch 09: precision: 0.151\nepoch 10: precision: 0.152\n```", "```py\ndef init_variable(size, dim, name=None):\n    std = np.sqrt(2 / dim)\n    return tf.Variable(tf.random_uniform([size, dim], -std, std), name=name)\n\ndef embed(inputs, size, dim, name=None):\n    emb = init_variable(size, dim, name)\n    return tf.nn.embedding_lookup(emb, inputs)\n```", "```py\nnum_factors = 128\nlambda_user = 0.0000001\nlambda_item = 0.0000001\nlambda_bias = 0.0000001\nlr = 0.0005\n\ngraph = tf.Graph()\ngraph.seed = 1\n\nwith graph.as_default():\n    place_user = tf.placeholder(tf.int32, shape=(None, 1))\n    place_item_pos = tf.placeholder(tf.int32, shape=(None, 1))\n    place_item_neg = tf.placeholder(tf.int32, shape=(None, 1))\n    # no place_y\n\n    user_factors = embed(place_user, num_users, num_factors,\n        \"user_factors\")\n    # no user bias anymore as well as no global bias\n\n    item_factors = init_variable(num_items, num_factors, \n        \"item_factors\")\n    item_factors_pos = tf.nn.embedding_lookup(item_factors, place_item_pos)\n    item_factors_neg = tf.nn.embedding_lookup(item_factors, place_item_neg)\n\n    item_bias = init_variable(num_items, 1, \"item_bias\")\n    item_bias_pos = tf.nn.embedding_lookup(item_bias, place_item_pos)\n    item_bias_pos = tf.reshape(item_bias_pos, [-1, 1])\n    item_bias_neg = tf.nn.embedding_lookup(item_bias, place_item_neg)\n    item_bias_neg = tf.reshape(item_bias_neg, [-1, 1])\n\n    # predictions for each item are same as previously\n    # but no user bias and global bias\n    pred_pos = item_bias_pos + \\\n        tf.reduce_sum(user_factors * item_factors_pos, axis=2)\n    pred_neg = item_bias_neg + \\\n        tf.reduce_sum(user_factors * item_factors_neg, axis=2)\n\n    pred_diff = pred_pos—pred_neg\n\n    loss_bpr =—tf.reduce_mean(tf.log(tf.sigmoid(pred_diff)))\n    loss_reg = lambda_user * tf.reduce_sum(user_factors * user_factors) +\\\n        lambda_item * tf.reduce_sum(item_factors_pos * item_factors_pos)+\\\n        lambda_item * tf.reduce_sum(item_factors_neg * item_factors_neg)+\\\n        lambda_bias * tf.reduce_sum(item_bias_pos) + \\\n        lambda_bias * tf.reduce_sum(item_bias_neg)\n\n    loss_total = loss_bpr + loss_reg\n\n    opt = tf.train.AdamOptimizer(learning_rate=lr)\n    step = opt.minimize(loss_total)\n\n    init = tf.global_variables_initializer()\n```", "```py\nsession = tf.Session(config=None, graph=graph)\nsession.run(init)\n\nsize_total = uid.shape[0]\nsize_sample = 15000\n\nnp.random.seed(0)\n\nfor i in range(75):\n    for k in range(30):\n        idx = np.random.randint(low=0, high=size_total, size=size_sample)\n\n        batch_uid = uid[idx].reshape(-1, 1)\n        batch_iid_pos = iid[idx].reshape(-1, 1)\n        batch_iid_neg = np.random.randint(\n            low=0, high=num_items, size=(size_sample, 1), dtype='int32')\n\n        feed_dict = {\n            place_user: batch_uid,\n            place_item_pos: batch_iid_pos,\n            place_item_neg: batch_iid_neg,\n        }\n        _, l = session.run([step, loss_bpr], feed_dict)\n\n    val_precision = calculate_validation_precision(graph, session, uid_val)\n    print('epoch %02d: precision: %.3f' % (i+1, val_precision))\n```", "```py\nclass LabelEncoder:\n    def fit(self, seq):\n        self.vocab = sorted(set(seq))\n        self.idx = {c: i + 1 for i, c in enumerate(self.vocab)}\n\n    def transform(self, seq):\n        n = len(seq)\n        result = np.zeros(n, dtype='int32')\n\n        for i in range(n):\n            result[i] = self.idx.get(seq[i], 0)\n\n        return result\n\n    def fit_transform(self, seq):\n        self.fit(seq)\n        return self.transform(seq)\n\n    def vocab_size(self):\n        return len(self.vocab) + 1\n```", "```py\nitem_enc = LabelEncoder()\ndf.stockcode = item_enc.fit_transform(df.stockcode.astype('str'))\ndf.stockcode = df.stockcode.astype('int32')\n```", "```py\nuser_enc = LabelEncoder()\nuser_enc.fit(df_train[df_train.customerid != -1].customerid)\n\ndf_train.customerid = user_enc.transfrom(df_train.customerid)\ndf_val.customerid = user_enc.transfrom(df_val.customerid)\n```", "```py\nfrom collections import Counter\n\ntop_train = Counter(df_train.stockcode)\n\ndef baseline(uid, indptr, items, top, k=5):\n    n_groups = len(uid)\n    n_items = len(items)\n\n    pred_all = np.zeros((n_items, k), dtype=np.int32)\n\n    for g in range(n_groups):\n        t = top.copy()\n\n        start = indptr[g]\n        end = indptr[g+1]\n\n        for i in range(start, end):\n            pred = [k for (k, c) in t.most_common(5)]\n            pred_all[i] = pred\n\n            actual = items[i]\n            if actual in t:\n                del t[actual]\n\n    return pred_all\n```", "```py\niid_val = df_val.stockcode.values\npred_baseline = baseline(uid_val, indptr_val, iid_val, top_train, k=5)\n```", "```py\narray([[3528, 3507, 1348, 2731,  181],\n       [3528, 3507, 1348, 2731,  181],\n       [3528, 3507, 1348, 2731,  181],\n       ...,\n       [1348, 2731,  181,  454, 1314],\n       [1348, 2731,  181,  454, 1314],\n       [1348, 2731,  181,  454, 1314]], dtype=int32\n```", "```py\n@njit\ndef accuracy_k(y_true, y_pred):\n    n, k = y_pred.shape\n\n    acc = 0\n    for i in range(n):\n        for j in range(k):\n            if y_pred[i, j] == y_true[i]:\n                acc = acc + 1\n                break\n\n    return acc / n\n```", "```py\naccuracy_k(iid_val, pred_baseline)\n```", "```py\ndef pack_items(users, items_indptr, items_vals):\n    n = len(items_indptr)—1\n\n    result = []\n    for i in range(n):\n        start = items_indptr[i]\n        end = items_indptr[i+1]\n        result.append(items_vals[start:end])\n\n    return result\n```", "```py\ntrain_items = pack_items(indptr_train, indptr_train, df_train.stockcode.values)\n\ndf_train_wrap = pd.DataFrame()\ndf_train_wrap['customerid'] = uid_train\ndf_train_wrap['items'] = train_items\n```", "```py\ndf_train_wrap.head()\n```", "```py\ndef pad_seq(data, num_steps):\n    data = np.pad(data, pad_width=(1, 0), mode='constant')\n\n    n = len(data)\n\n    if n <= num_steps:\n        pad_right = num_steps—n + 1\n        data = np.pad(data, pad_width=(0, pad_right), mode='constant')\n\n    return data\n\ndef prepare_train_data(data, num_steps):\n    data = pad_seq(data, num_steps)\n\n    X = []\n    Y = []\n\n    for i in range(num_steps, len(data)):\n        start = i—num_steps\n        X.append(data[start:i])\n        Y.append(data[start+1:i+1])\n\n    return X, Y\n```", "```py\ntrain_items = df_train_wrap['items']\n\nX_train = []\nY_train = []\n\nfor i in range(len(train_items)):\n    X, Y = prepare_train_data(train_items[i], 5)\n    X_train.extend(X)\n    Y_train.extend(Y)\n\nX_train = np.array(X_train, dtype='int32')\nY_train = np.array(Y_train, dtype='int32')\n```", "```py\nclass Config:\n    num_steps = 5\n\n    num_items = item_enc.vocab_size()\n    num_users = user_enc.vocab_size()\n\n    init_scale = 0.1\n    learning_rate = 1.0\n    max_grad_norm = 5\n    num_layers = 2\n    hidden_size = 200\n    embedding_size = 200\n    batch_size = 20    \n\nconfig = Config()\n```", "```py\ndef lstm_cell(hidden_size, is_training):\n    return rnn.BasicLSTMCell(hidden_size, forget_bias=0.0, \n                 state_is_tuple=True, reuse=not is_training)\n\ndef rnn_model(inputs, hidden_size, num_layers, batch_size, num_steps,\n              is_training):\n    cells = [lstm_cell(hidden_size, is_training) for \n                                     i in range(num_layers)]\n    cell = rnn.MultiRNNCell(cells, state_is_tuple=True)\n\n    initial_state = cell.zero_state(batch_size, tf.float32)\n    inputs = tf.unstack(inputs, num=num_steps, axis=1)\n    outputs, final_state = rnn.static_rnn(cell, inputs,\n                                  initial_state=initial_state)\n    output = tf.reshape(tf.concat(outputs, 1), [-1, hidden_size])\n\n    return output, initial_state, final_state\n```", "```py\ndef model(config, is_training):\n    batch_size = config.batch_size\n    num_steps = config.num_steps\n    embedding_size = config.embedding_size\n    hidden_size = config.hidden_size\n    num_items = config.num_items\n\n    place_x = tf.placeholder(shape=[batch_size, num_steps], dtype=tf.int32)\n    place_y = tf.placeholder(shape=[batch_size, num_steps], dtype=tf.int32)\n\n    embedding = tf.get_variable(\"items\", [num_items, embedding_size],\n                   dtype=tf.float32)\n    inputs = tf.nn.embedding_lookup(embedding, place_x)\n\n    output, initial_state, final_state = \\\n        rnn_model(inputs, hidden_size, config.num_layers, batch_size,\n                  num_steps, is_training)\n\n    W = tf.get_variable(\"W\", [hidden_size, num_items], dtype=tf.float32)\n    b = tf.get_variable(\"b\", [num_items], dtype=tf.float32)\n    logits = tf.nn.xw_plus_b(output, W, b)\n    logits = tf.reshape(logits, [batch_size, num_steps, num_items])\n\n    loss = tf.losses.sparse_softmax_cross_entropy(place_y, logits)\n    total_loss = tf.reduce_mean(loss)\n\n    tvars = tf.trainable_variables()\n    gradient = tf.gradients(total_loss, tvars)\n    clipped, _ = tf.clip_by_global_norm(gradient, config.max_grad_norm)\n    optimizer = tf.train.GradientDescentOptimizer(config.learning_rate)\n\n    global_step = tf.train.get_or_create_global_step()\n    train_op = optimizer.apply_gradients(zip(clipped, tvars),\n                  global_step=global_step)\n\n    out = {}\n    out['place_x'] = place_x\n    out['place_y'] = place_y\n\n    out['logits'] = logits\n    out['initial_state'] = initial_state\n    out['final_state'] = final_state\n\n    out['total_loss'] = total_loss\n    out['train_op'] = train_op\n\n    return out\n```", "```py\nconfig = Config()\nconfig_val = Config()\nconfig_val.batch_size = 1\nconfig_val.num_steps = 1\n```", "```py\n\ngraph = tf.Graph()\ngraph.seed = 1\n\nwith graph.as_default():\n    initializer = tf.random_uniform_initializer(-config.init_scale,\n                config.init_scale)\n\n    with tf.name_scope(\"Train\"):\n        with tf.variable_scope(\"Model\", reuse=None, \n                       initializer=initializer):\n            train_model = model(config, is_training=True)\n\n    with tf.name_scope(\"Valid\"):\n        with tf.variable_scope(\"Model\", reuse=True, \n                       initializer=initializer):\n            val_model = model(config_val, is_training=False)\n\n    init = tf.global_variables_initializer()\n```", "```py\ndef run_epoch(session, model, X, Y, batch_size):\n    fetches = {\n        \"total_loss\": model['total_loss'],\n        \"final_state\": model['final_state'],\n        \"eval_op\": model['train_op']\n    }\n\n    num_steps = X.shape[1]\n    all_idx = np.arange(X.shape[0])\n    np.random.shuffle(all_idx)\n    batches = prepare_batches(all_idx, batch_size)\n\n    initial_state = session.run(model['initial_state'])\n    current_state = initial_state\n\n    progress = tqdm(total=len(batches))\n    for idx in batches:\n        if len(idx) < batch_size:\n            continue\n\n        feed_dict = {}\n        for i, (c, h) in enumerate(model['initial_state']):\n            feed_dict[c] = current_state[i].c\n            feed_dict[h] = current_state[i].h\n\n        feed_dict[model['place_x']] = X[idx]\n        feed_dict[model['place_y']] = Y[idx]\n\n        vals = session.run(fetches, feed_dict)\n        loss = vals[\"total_loss\"]\n        current_state = vals[\"final_state\"]\n\n        progress.update(1)\n        progress.set_description('%.3f' % loss)\n    progress.close()\n```", "```py\nsession = tf.Session(config=None, graph=graph) \nsession.run(init)\n\nnp.random.seed(0)\nrun_epoch(session, train_model, X_train, Y_train, batch_size=config.batch_size)\n```", "```py\ndef generate_prediction(uid, indptr, items, model, k):\n    n_groups = len(uid)\n    n_items = len(items)\n\n    pred_all = np.zeros((n_items, k), dtype=np.int32)\n    initial_state = session.run(model['initial_state'])\n\n    fetches = {\n        \"logits\": model['logits'],\n        \"final_state\": model['final_state'],\n    }\n\n    for g in tqdm(range(n_groups)):    \n        start = indptr[g]\n        end = indptr[g+1]\n\n        current_state = initial_state\n\n        feed_dict = {}\n\n        for i, (c, h) in enumerate(model['initial_state']):\n            feed_dict[c] = current_state[i].c\n            feed_dict[h] = current_state[i].h\n\n        prev = np.array([[0]], dtype=np.int32)\n\n        for i in range(start, end):\n            feed_dict[model['place_x']] = prev\n\n            actual = items[i]\n            prev[0, 0] = actual\n\n            values = session.run(fetches, feed_dict)\n            current_state = values[\"final_state\"]\n\n            logits = values['logits'].reshape(-1)\n            pred = np.argpartition(-logits, k)[:k]\n            pred_all[i] = pred\n\n    return pred_all\n```", "```py\npred_lstm = generate_prediction(uid_val, indptr_val, iid_val, val_model, k=5)\naccuracy_k(iid_val, pred_lstm)\n```", "```py\nX_train = []\nU_train = []\nY_train = []\n\nfor t in df_train_wrap.itertuples():\n    X, Y = prepare_train_data(t.items, config.num_steps)\n    U_train.extend([t.customerid] * len(X))\n    X_train.extend(X)\n    Y_train.extend(Y)\n\nX_train = np.array(X_train, dtype='int32')\nY_train = np.array(Y_train, dtype='int32')\nU_train = np.array(U_train, dtype='int32')\n```", "```py\ndef user_model(config, is_training):\n    batch_size = config.batch_size\n    num_steps = config.num_steps\n    embedding_size = config.embedding_size\n    hidden_size = config.hidden_size\n    num_items = config.num_items\n    num_users = config.num_users\n\n    place_x = tf.placeholder(shape=[batch_size, num_steps], dtype=tf.int32)\n    place_u = tf.placeholder(shape=[batch_size, 1], dtype=tf.int32)\n    place_y = tf.placeholder(shape=[batch_size, num_steps], dtype=tf.int32)\n\n    item_embedding = tf.get_variable(\"items\", [num_items, embedding_size], dtype=tf.float32)\n    item_inputs = tf.nn.embedding_lookup(item_embedding, place_x)\n\n    user_embedding = tf.get_variable(\"users\", [num_items, embedding_size], dtype=tf.float32)\n    u_repeat = tf.tile(place_u, [1, num_steps])\n    user_inputs = tf.nn.embedding_lookup(user_embedding, u_repeat)\n\n    inputs = tf.concat([user_inputs, item_inputs], axis=2)\n\n    output, initial_state, final_state = \\\n        rnn_model(inputs, hidden_size, config.num_layers, batch_size, num_steps, is_training)\n\n    W = tf.get_variable(\"W\", [hidden_size, num_items], dtype=tf.float32)\n    b = tf.get_variable(\"b\", [num_items], dtype=tf.float32)\n\n    logits = tf.nn.xw_plus_b(output, W, b)\n    logits = tf.reshape(logits, [batch_size, num_steps, num_items])\n\n    loss = tf.losses.sparse_softmax_cross_entropy(place_y, logits)\n    total_loss = tf.reduce_mean(loss)\n\n    tvars = tf.trainable_variables()\n    gradient = tf.gradients(total_loss, tvars)\n    clipped, _ = tf.clip_by_global_norm(gradient, config.max_grad_norm)\n    optimizer = tf.train.GradientDescentOptimizer(config.learning_rate)\n\n    global_step = tf.train.get_or_create_global_step()\n    train_op = optimizer.apply_gradients(zip(clipped, tvars),\n                 global_step=global_step)\n\n    out = {}\n    out['place_x'] = place_x\n    out['place_u'] = place_u\n    out['place_y'] = place_y\n\n    out['logits'] = logits\n    out['initial_state'] = initial_state\n    out['final_state'] = final_state\n\n    out['total_loss'] = total_loss\n    out['train_op'] = train_op\n\n    return out\n```", "```py\ngraph = tf.Graph()\ngraph.seed = 1\n\nwith graph.as_default():\n    initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n\n    with tf.name_scope(\"Train\"):\n        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n            train_model = user_model(config, is_training=True)\n\n    with tf.name_scope(\"Valid\"):\n        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n            val_model = user_model(config_val, is_training=False)\n\n    init = tf.global_variables_initializer()\n\nsession = tf.Session(config=None, graph=graph) \nsession.run(init)\n```", "```py\ndef user_model_epoch(session, model, X, U, Y, batch_size):\n    fetches = {\n        \"total_loss\": model['total_loss'],\n        \"final_state\": model['final_state'],\n        \"eval_op\": model['train_op']\n    }\n\n    num_steps = X.shape[1]\n    all_idx = np.arange(X.shape[0])\n    np.random.shuffle(all_idx)\n    batches = prepare_batches(all_idx, batch_size)\n\n    initial_state = session.run(model['initial_state'])\n    current_state = initial_state\n\n    progress = tqdm(total=len(batches))\n    for idx in batches:\n        if len(idx) < batch_size:\n            continue\n\n        feed_dict = {}\n        for i, (c, h) in enumerate(model['initial_state']):\n            feed_dict[c] = current_state[i].c\n            feed_dict[h] = current_state[i].h\n\n        feed_dict[model['place_x']] = X[idx]\n        feed_dict[model['place_y']] = Y[idx]\n        feed_dict[model['place_u']] = U[idx].reshape(-1, 1)\n\n        vals = session.run(fetches, feed_dict)\n        loss = vals[\"total_loss\"]\n        current_state = vals[\"final_state\"]\n\n        progress.update(1)\n        progress.set_description('%.3f' % loss)\n    progress.close()\n```", "```py\nsession = tf.Session(config=None, graph=graph) \nsession.run(init)\n\nnp.random.seed(0)\n\nuser_model_epoch(session, train_model, X_train, U_train, Y_train, batch_size=config.batch_size)\n```", "```py\ndef generate_prediction_user_model(uid, indptr, items, model, k):\n    n_groups = len(uid)\n    n_items = len(items)\n\n    pred_all = np.zeros((n_items, k), dtype=np.int32)\n    initial_state = session.run(model['initial_state'])\n\n    fetches = {\n        \"logits\": model['logits'],\n        \"final_state\": model['final_state'],\n    }\n\n    for g in tqdm(range(n_groups)):    \n        start = indptr[g]\n        end = indptr[g+1]\n        u = uid[g]\n\n        current_state = initial_state\n\n        feed_dict = {}\n        feed_dict[model['place_u']] = np.array([[u]], dtype=np.int32)\n\n        for i, (c, h) in enumerate(model['initial_state']):\n            feed_dict[c] = current_state[i].c\n            feed_dict[h] = current_state[i].h\n\n        prev = np.array([[0]], dtype=np.int32)\n\n        for i in range(start, end):\n            feed_dict[model['place_x']] = prev\n\n            actual = items[i]\n            prev[0, 0] = actual\n\n            values = session.run(fetches, feed_dict)\n            current_state = values[\"final_state\"]\n\n            logits = values['logits'].reshape(-1)\n            pred = np.argpartition(-logits, k)[:k]\n            pred_all[i] = pred\n\n    return pred_all\n```", "```py\npred_lstm = generate_prediction_user_model(uid_val, indptr_val, iid_val, val_model, k=5)\naccuracy_k(iid_val, pred_lstm)\n```"]