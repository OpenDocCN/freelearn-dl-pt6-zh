<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Using RL4J for Reinforcement Learning</h1>
                </header>
            
            <article>
                
<p>Reinforcement learning is a goal-oriented machine learning algorithm that trains an agent to make a sequence of decisions. In the case of deep learning models, we train them on existing data and apply the learning on new or unseen data. Reinforcement learning exhibits dynamic learning by adjusting its own actions based on continuous feedback in order to maximize the reward. We can introduce deep learning into a reinforcement learning system, which is known as deep reinforcement learning.</p>
<p>RL4J is a reinforcement learning framework integrated with DL4J. RL4J supports two reinforcement algorithms: deep Q-learning and A3C (short for <strong>Asynchronous Actor-Critic Agents</strong>). Q-learning is an off-policy reinforcement learning algorithm that seeks the best action for the given state. It learns from actions outside the ones mentioned in the current policy by taking random actions. In deep Q-learning, we use a deep neural network to find the optimal Q-value rather than value iteration in regular Q-learning. In this chapter, we will set up a gaming environment powered by <span>reinforcement learning </span>using Project Malmo. <span>Project </span>Malmo is a platform for reinforcement learning experiments built on top of Minecraft.</p>
<p><span>In this chapter, we will cover the following recipes:</span></p>
<ul>
<li>Setting up the Malmo environment and respective dependencies</li>
<li>Setting up the data requirements</li>
<li>Configuring and training a Deep Q-Network (DQN) agent</li>
<li>Evaluating a Malmo agent</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>The source code for this chapter can be found here:<br/>
<a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/09_Using_RL4J_for_Reinforcement%20learning/sourceCode/cookbookapp/src/main/java/MalmoExample.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/09_Using_RL4J_for_Reinforcement%20learning/sourceCode/cookbookapp/src/main/java/MalmoExample.java</a>.</p>
<p>After cloning our GitHub repository, navigate to the <kbd>Java-Deep-Learning-Cookbook/09_Using_RL4J_for_Reinforcement learning/sourceCode</kbd> <span>directory.</span> Then, import the <kbd>cookbookapp</kbd> <span>project </span>as a Maven project by importing <kbd>pom.xml</kbd>.</p>
<p>You need to set up a Malmo client to run the source code. First, download the latest Project Malmo release as per your OS (<a href="https://github.com/Microsoft/malmo/releases">https://github.com/Microsoft/malmo/releases</a>):</p>
<ul>
<li>For Linux OS, follow the installation instructions here: <a href="https://github.com/microsoft/malmo/blob/master/doc/install_linux.md">https://github.com/microsoft/malmo/blob/master/doc/install_linux.md</a>.</li>
<li>For Windows OS, follow the installation instructions here: <a href="https://github.com/microsoft/malmo/blob/master/doc/install_windows.md">https://github.com/microsoft/malmo/blob/master/doc/install_windows.md</a>.</li>
<li>For macOS, follow the installation instructions here: <a href="https://github.com/microsoft/malmo/blob/master/doc/install_macosx.md">https://github.com/microsoft/malmo/blob/master/doc/install_macosx.md</a>.</li>
</ul>
<p>To launch the Minecraft client, navigate to the Minecraft directory and run the client script:</p>
<ul>
<li>Double-click on <kbd>launchClient.bat</kbd> (on Windows).</li>
<li>Run <kbd>./launchClient.sh</kbd> on the console (either on Linux or macOS). </li>
</ul>
<p>If you're in Windows and are facing issues while launching the client, you can download the dependency walker here: <a href="https://lucasg.github.io/Dependencies/">https://lucasg.github.io/Dependencies/</a>.</p>
<p>Then, follow these steps:</p>
<ol>
<li>Extract and run <kbd>DependenciesGui.exe</kbd>.</li>
<li>Select <kbd>MalmoJava.dll</kbd> in the <kbd>Java_Examples</kbd> directory to see the missing dependencies like the ones shown here:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1424 image-border" src="assets/63ab736b-704c-47ac-909e-fe828aad702c.png" style="width:67.25em;height:49.17em;"/></p>
<p class="CDPAlignLeft CDPAlign"><span>In the case of any issues, the missing dependencies will be marked on the list. You will need to add the missing dependencies in order to relaunch the client successfully. Any missing libraries/files should be present in the <kbd>PATH</kbd> environment variable.</span></p>
<p class="CDPAlignLeft CDPAlign"><span>You may refer to OS-specific build instructions here:</span></p>
<ul>
<li class="CDPAlignLeft CDPAlign"><span><a href="https://github.com/microsoft/malmo/blob/master/doc/build_linux.md">https://github.com/microsoft/malmo/blob/master/doc/build_linux.md</a> (Linux)</span></li>
<li class="CDPAlignLeft CDPAlign"><span><a href="https://github.com/microsoft/malmo/blob/master/doc/build_windows.md">https://github.com/microsoft/malmo/blob/master/doc/build_windows.md</a> (Windows)</span></li>
<li class="CDPAlignLeft CDPAlign"><a href="https://github.com/microsoft/malmo/blob/master/doc/build_macosx.md">https://github.com/microsoft/malmo/blob/master/doc/build_macosx.md</a> <span>(macOS)</span></li>
</ul>
<p class="mce-root"/>
<p>If everything goes well, you should see something like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1221 image-border" src="assets/8c9d23f2-2f13-4e81-9e5d-4a2628282ce4.png" style="width:70.92em;height:42.50em;"/></p>
<p class="CDPAlignLeft CDPAlign"><span>Additionally, you need to create a mission schema to build blocks for the gaming window. The complete mission schema can be found</span> in this chapter's project directory at<span> <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/09_Using_RL4J_for_Reinforcement%20learning/sourceCode/cookbookapp/src/main/resources/cliff_walking_rl4j.xml" target="_blank">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/09_Using_RL4J_for_Reinforcement%20learning/sourceCode/cookbookapp/src/main/resources/cliff_walking_rl4j.xml</a>.</span><span><br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the Malmo environment and respective dependencies</h1>
                </header>
            
            <article>
                
<p>We need to set up RL4J Malmo dependencies to run the source code. Just like any other DL4J application, we also need to add ND4J backend dependencies as well depending upon your hardware (CPU/GPU). In this recipe, we will add the required Maven dependencies and set up the environment to run the application.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p><span>The Malmo client should be up and running before we run the Malmo example source code. Our source code will communicate with the Malmo client in order to create and run the missions.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Add the RL4J core dependency:</li>
</ol>
<pre style="padding-left: 60px">&lt;dependency&gt;<br/> &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;<br/> &lt;artifactId&gt;rl4j-core&lt;/artifactId&gt;<br/> &lt;version&gt;1.0.0-beta3&lt;/version&gt;<br/> &lt;/dependency&gt;</pre>
<ol start="2">
<li>Add the RL4J Malmo dependency:</li>
</ol>
<pre style="padding-left: 60px">&lt;dependency&gt;<br/> &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;<br/> &lt;artifactId&gt;rl4j-malmo&lt;/artifactId&gt;<br/> &lt;version&gt;1.0.0-beta3&lt;/version&gt;<br/> &lt;/dependency&gt;</pre>
<ol start="3">
<li>Add a dependency for the ND4J backend:
<ul>
<li>For CPU, you can use the following:</li>
</ul>
</li>
</ol>
<pre style="padding-left: 120px">&lt;dependency&gt;<br/> &lt;groupId&gt;org.nd4j&lt;/groupId&gt;<br/> &lt;artifactId&gt;nd4j-native-platform&lt;/artifactId&gt;<br/> &lt;version&gt;1.0.0-beta3&lt;/version&gt;<br/> &lt;/dependency&gt;</pre>
<ul>
<li style="list-style-type: none">
<ul>
<li>For GPU, you can use the following:</li>
</ul>
</li>
</ul>
<pre style="padding-left: 120px">&lt;dependency&gt;<br/> &lt;groupId&gt;org.nd4j&lt;/groupId&gt;<br/> &lt;artifactId&gt;nd4j-cuda-10.0&lt;/artifactId&gt;<br/> &lt;version&gt;1.0.0-beta3&lt;/version&gt;<br/> &lt;/dependency&gt;</pre>
<p class="mce-root"/>
<ol start="4">
<li>Add Maven dependency for <kbd>MalmoJavaJar</kbd>:</li>
</ol>
<pre style="padding-left: 60px">&lt;dependency&gt;<br/> &lt;groupId&gt;com.microsoft.msr.malmo&lt;/groupId&gt;<br/> &lt;artifactId&gt;MalmoJavaJar&lt;/artifactId&gt;<br/> &lt;version&gt;0.30.0&lt;/version&gt;<br/> &lt;/dependency&gt;</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, we added RL4J core dependencies to bring in RL4J DQN libraries in our application. RL4J Malmo dependencies are added in step 2 to construct the Malmo environment and build missions in RL4J.</p>
<p>We need to add CPU/GPU-specific ND4J backend dependencies as well (step 3). Finally, in step 4, we added dependencies for <span><kbd>MalmoJavaJar</kbd> (</span>step 4), which <span>acts as a communication interface for the Java program to interact with Malmo.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the data requirements</h1>
                </header>
            
            <article>
                
<p>The data for the Malmo reinforcement learning environment includes the image frames that the agent is moving in. A sample gaming window for Malmo will look like the following. Here, the a<span>gent dies if they step over the lava:<br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1222 image-border" src="assets/7f9aed8d-f3a5-408c-9f95-57dbf0fa9577.png" style="width:27.00em;height:22.42em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Malmo requires developers to specify the XML schema in order to generate the mission. We will need to create mission data for both the agent and the server to create blocks in the world (that is, the gaming environment). <span>In this recipe, we will create an XML schema to specify the mission data. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Define the initial conditions of the world using the <kbd>&lt;ServerInitialConditions&gt;</kbd> tag:</li>
</ol>
<pre style="padding-left: 60px">Sample:<br/> &lt;ServerInitialConditions&gt;<br/> &lt;Time&gt;<br/> &lt;StartTime&gt;6000&lt;/StartTime&gt;<br/> &lt;AllowPassageOfTime&gt;false&lt;/AllowPassageOfTime&gt;<br/> &lt;/Time&gt;<br/> &lt;Weather&gt;clear&lt;/Weather&gt;<br/> &lt;AllowSpawning&gt;false&lt;/AllowSpawning&gt;<br/> &lt;/ServerInitialConditions&gt;</pre>
<ol start="2">
<li>Navigate to <a href="http://www.minecraft101.net/superflat/">http://www.minecraft101.net/superflat/</a> and create your own preset string for the super-flat world:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1223 image-border" src="assets/fe296849-0271-413d-80e9-92aa424094ca.png" style="width:100.67em;height:53.92em;"/></p>
<p class="mce-root"/>
<ol start="3">
<li class="CDPAlignLeft CDPAlign">Generate a super-flat world with the specified preset string using the <kbd>&lt;FlatWorldGenerator&gt;</kbd> tag:</li>
</ol>
<pre style="padding-left: 60px"><br/>&lt;FlatWorldGenerator generatorString="3;7,220*1,5*3,2;3;,biome_1"/&gt;</pre>
<ol start="4">
<li>Draw structures in the world using the <kbd>&lt;DrawingDecorator&gt;</kbd> tag:</li>
</ol>
<pre style="padding-left: 60px">Sample:<br/> &lt;DrawingDecorator&gt;<br/> &lt;!-- coordinates for cuboid are inclusive --&gt;<br/> &lt;DrawCuboid x1="-2" y1="46" z1="-2" x2="7" y2="50" z2="18" type="air" /&gt;<br/> &lt;DrawCuboid x1="-2" y1="45" z1="-2" x2="7" y2="45" z2="18" type="lava" /&gt;<br/> &lt;DrawCuboid x1="1" y1="45" z1="1" x2="3" y2="45" z2="12" type="sandstone" /&gt;<br/> &lt;DrawBlock x="4" y="45" z="1" type="cobblestone" /&gt;<br/> &lt;DrawBlock x="4" y="45" z="12" type="lapis_block" /&gt;<br/> &lt;DrawItem x="4" y="46" z="12" type="diamond" /&gt;<br/> &lt;/DrawingDecorator&gt;</pre>
<ol start="5">
<li>Specify a time limit for all agents using the <kbd>&lt;ServerQuitFromTimeUp&gt;</kbd> tag:</li>
</ol>
<pre style="padding-left: 60px">&lt;ServerQuitFromTimeUp timeLimitMs="100000000"/&gt;</pre>
<ol start="6">
<li>Add all mission handlers to the block using the <kbd>&lt;ServerHandlers&gt;</kbd> tag:</li>
</ol>
<pre style="padding-left: 60px">&lt;ServerHandlers&gt;<br/> &lt;FlatWorldGenerator&gt;{Copy from step 3}&lt;/FlatWorldGenerator&gt;<br/> &lt;DrawingDecorator&gt;{Copy from step 4}&lt;/DrawingDecorator&gt;<br/> &lt;ServerQuitFromTimeUp&gt;{Copy from step 5}&lt;/ServerQuitFromTimeUp&gt;<br/> &lt;/ServerHandlers&gt;</pre>
<ol start="7">
<li>Add <kbd>&lt;ServerHandlers&gt;</kbd> and <kbd>&lt;ServerInitialConditions&gt;</kbd> under the <kbd>&lt;ServerSection&gt;</kbd> tag:</li>
</ol>
<pre style="padding-left: 60px">&lt;ServerSection&gt;<br/> &lt;ServerInitialConditions&gt;{Copy from step 1}&lt;/ServerInitialConditions&gt;<br/> &lt;ServerHandlers&gt;{Copy from step 6}&lt;/ServerHandlers&gt;<br/> &lt;/ServerSection&gt;</pre>
<p class="mce-root"/>
<ol start="8">
<li>Define the agent name and starting position:</li>
</ol>
<pre style="padding-left: 60px">Sample:<br/> &lt;Name&gt;Cristina&lt;/Name&gt;<br/> &lt;AgentStart&gt;<br/>   &lt;Placement x="4.5" y="46.0" z="1.5" pitch="30" yaw="0"/&gt;<br/> &lt;/AgentStart&gt;</pre>
<ol start="9">
<li>Define the block types using the <kbd>&lt;ObservationFromGrid&gt;</kbd> tag:</li>
</ol>
<pre style="padding-left: 60px">Sample:<br/> &lt;ObservationFromGrid&gt;<br/>  &lt;Grid name="floor"&gt;<br/>  &lt;min x="-4" y="-1" z="-13"/&gt;<br/>  &lt;max x="4" y="-1" z="13"/&gt;<br/>  &lt;/Grid&gt;<br/> &lt;/ObservationFromGrid&gt;</pre>
<ol start="10">
<li>Configure the video frames using the <kbd>&lt;VideoProducer&gt;</kbd> tag:</li>
</ol>
<pre style="padding-left: 60px">Sample:<br/> &lt;VideoProducer viewpoint="1" want_depth="false"&gt;<br/> &lt;Width&gt;320&lt;/Width&gt;<br/> &lt;Height&gt;240&lt;/Height&gt;<br/> &lt;/VideoProducer&gt;</pre>
<ol start="11">
<li>Mention the reward points to be received when an agent comes into contact with a block type using the <kbd>&lt;RewardForTouchingBlockType&gt;</kbd> tag:</li>
</ol>
<pre style="padding-left: 60px">Sample:<br/> &lt;RewardForTouchingBlockType&gt;<br/> &lt;Block reward="-100.0" type="lava" behaviour="onceOnly"/&gt;<br/> &lt;Block reward="100.0" type="lapis_block" behaviour="onceOnly"/&gt;<br/> &lt;/RewardForTouchingBlockType&gt;</pre>
<ol start="12">
<li>Mention the reward points to issue a command to the agent using the <kbd>&lt;RewardForSendingCommand&gt;</kbd> tag:</li>
</ol>
<pre style="padding-left: 60px">Sample:<br/> &lt;RewardForSendingCommand reward="-1"/&gt;</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="13">
<li>Specify the mission endpoints for the agent using the <kbd>&lt;AgentQuitFromTouchingBlockType&gt;</kbd> tag:</li>
</ol>
<pre style="padding-left: 60px">&lt;AgentQuitFromTouchingBlockType&gt;<br/>  &lt;Block type="lava" /&gt;<br/>  &lt;Block type="lapis_block" /&gt;<br/> &lt;/AgentQuitFromTouchingBlockType&gt;</pre>
<ol start="14">
<li>Add all agent handler functions under the <kbd>&lt;AgentHandlers&gt;</kbd> tag:</li>
</ol>
<pre style="padding-left: 60px">&lt;AgentHandlers&gt;<br/>   &lt;ObservationFromGrid&gt;{Copy from step 9}&lt;/ObservationFromGrid&gt;<br/>   &lt;VideoProducer&gt;&lt;/VideoProducer&gt; // Copy from step 10<br/>   &lt;RewardForTouchingBlockType&gt;{Copy from step 11}&lt;/RewardForTouchingBlockType&gt;<br/>   &lt;RewardForSendingCommand&gt; // Copy from step 12<br/>   &lt;AgentQuitFromTouchingBlockType&gt;{Copy from step 13}  &lt;/AgentQuitFromTouchingBlockType&gt;<br/> &lt;/AgentHandlers&gt;</pre>
<ol start="15">
<li>Add all agent handlers to <kbd>&lt;AgentSection&gt;</kbd>:</li>
</ol>
<pre style="padding-left: 60px">&lt;AgentSection mode="Survival"&gt;<br/>     &lt;AgentHandlers&gt;<br/>        {Copy from step 14}<br/>     &lt;/AgentHandlers&gt;<br/> &lt;/AgentSection&gt;</pre>
<ol start="16">
<li>Create a <kbd>DataManager</kbd> instance to record the training data:</li>
</ol>
<pre style="padding-left: 60px">DataManager manager = new DataManager(false);</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, the following configurations are added as the initial conditions for the world:</p>
<ul>
<li><kbd>StartTime</kbd>: This s<span>pecifies the time of day at the start of the mission, in thousandths of an hour. 6,000 refers t</span>o noontime.</li>
<li><span><kbd>AllowPassageOfTime</kbd>: If set</span> to <kbd>false</kbd>, then <span>it will stop the day-night cycle. The weather and the sun position will remain constant during the mission.</span></li>
<li><span><kbd>Weather</kbd>: This specifies the type of weather at the start of the mission.</span></li>
<li><span><kbd>AllowSpawning</kbd>: If set</span> to <kbd>true</kbd>, the<span>n it will produce animals and hostiles during the mission.</span></li>
</ul>
<p class="mce-root"/>
<p><span>In <em>step 2</em>, we created a preset string to repres</span>ent the super-flat type that is being used in step 3. A super-flat type is nothing but the type of surface see<span><span>n in the mission. </span></span></p>
<p>In step 4, we drew structures into the world using <span><kbd>DrawCuboid</kbd> and <kbd>DrawBlock</kbd>. </span></p>
<p><span>We follow three-dimensional space <kbd>(x1,y1,z1)</kbd> -&gt; <kbd>(x2,y2,z2)</kbd> to specify the boundar</span>ies. The <kbd>type</kbd> att<span>ribute is used to represent block types. You may add any of the available 198 blocks for your experiments.</span></p>
<p>In step 6, we add all mission handlers specific to world creation under the <kbd>&lt;ServerHandlers&gt;</kbd> tag. Then, we add them to the <kbd>&lt;ServerSection&gt;</kbd> parent tag in step 7.</p>
<p>In step 8, the <kbd>&lt;Placement&gt;</kbd> tag is used to specify the player's starting position. The starting point will be chosen randomly if it is not specified. </p>
<p>In step 9, we specified the position of the floor block in the gaming window. In step 10, <kbd>viewpoint</kbd> sets the camera viewpoint: </p>
<pre class="p1">viewpoint=0 -&gt; first-person<br/> viewpoint=1 -&gt; behind<br/> viewpoint=2 -&gt; facing<span class="s1"><br/></span></pre>
<p>In step 13, we specify the block types in which agent movement is stopped once the step is over. In the end, we add all agent-specific mission handlers in the <span><kbd>AgentSection</kbd> </span>tag at step 15. Mission schema creation will end at step 15.</p>
<p>Now, we need to store the training data from the mission. We use <kbd>DataManager</kbd> to handle the recording of training data. It creates the <kbd>rl4j-data</kbd> <span>directory </span>if it does not exist and stores the training data as the reinforcement learning training progresses. We passed <kbd>false</kbd> as an attribute while creating <kbd>DataManager</kbd> in step 16. This means that we are not persisting the training data or the model. Pass <kbd>true</kbd> if the training data and model are to be persisted. Note that we are going to need the data manager instance while configuring DQN. </p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Refer to the following documentation to create your own custom XML schema for the Minecraft world:</span>
<ul>
<li><a href="http://microsoft.github.io/malmo/0.14.0/Schemas/Mission.html">http://microsoft.github.io/malmo/0.14.0/Schemas/Mission.html</a></li>
<li><a href="http://microsoft.github.io/malmo/0.30.0/Schemas/MissionHandlers.html">http://microsoft.github.io/malmo/0.30.0/Schemas/MissionHandlers.html</a></li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring and training a DQN agent</h1>
                </header>
            
            <article>
                
<p><span>DQ</span>N refers to <span>an important class of reinforcement learning, called value learning. Here, we use a deep neural network to learn the optimal Q-value function. For every iteration, the network approximates Q-value and evaluates them against the Bellman equation in order to measure the agent accuracy. Q-value is supposed to be optimized while the agent makes movements in the world. So, how we configure the Q-learning process is important. In this recipe, we will configure DQN for a Malmo mission and train the agent to achieve the task. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Basic knowledge on the following are prerequisites for this recipe:</p>
<ul>
<li>Q-learning</li>
<li>DQN</li>
</ul>
<p>Q-learning basics will help while configuring the Q-learning hyperparameters for the DQN.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Create an action space for the mission:</li>
</ol>
<pre style="padding-left: 60px">Sample:<br/> MalmoActionSpaceDiscrete actionSpace =<br/> new MalmoActionSpaceDiscrete("movenorth 1", "movesouth 1", "movewest 1", "moveeast 1");<br/> actionSpace.setRandomSeed(rndSeed);<br/> </pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<ol start="2">
<li>Create an observation space for the mission:</li>
</ol>
<pre style="padding-left: 60px">MalmoObservationSpace observationSpace = new MalmoObservationSpacePixels(xSize, ySize);</pre>
<ol start="3">
<li>Create a Malmo consistency policy:</li>
</ol>
<pre style="padding-left: 60px">MalmoDescretePositionPolicy obsPolicy = new MalmoDescretePositionPolicy();</pre>
<ol start="4">
<li>Create an MDP (short for <strong>Markov Decision Process</strong>) wrapper around the Malmo Java client:</li>
</ol>
<pre style="padding-left: 60px">Sample:<br/> MalmoEnv mdp = new MalmoEnv("cliff_walking_rl4j.xml", actionSpace, observationSpace, obsPolicy);</pre>
<ol start="5">
<li>Create a DQN using <kbd>DQNFactoryStdConv</kbd>:</li>
</ol>
<pre style="padding-left: 60px">Sample:<br/> public static DQNFactoryStdConv.Configuration MALMO_NET = new DQNFactoryStdConv.Configuration(<br/> learingRate,<br/> l2RegParam,<br/> updaters,<br/> listeners<br/> );</pre>
<ol start="6">
<li>Use <kbd>HistoryProcessor</kbd> to scale the pixel image input:</li>
</ol>
<pre style="padding-left: 60px">Sample:<br/> public static HistoryProcessor.Configuration MALMO_HPROC = new HistoryProcessor.Configuration(<br/> numOfFrames,<br/> rescaledWidth,<br/> rescaledHeight,<br/> croppingWidth,<br/> croppingHeight,<br/> offsetX,<br/> offsetY,<br/> numFramesSkip<br/> );</pre>
<ol start="7">
<li>Create a Q-learning configuration by specifying hyperparameters:</li>
</ol>
<pre style="padding-left: 60px">Sample:<br/> public static QLearning.QLConfiguration MALMO_QL = new QLearning.QLConfiguration(<br/> rndSeed,<br/> maxEpochStep,<br/> maxStep,<br/> expRepMaxSize,<br/> batchSize,<br/> targetDqnUpdateFreq,<br/> updateStart,<br/> rewardFactor,<br/> gamma,<br/> errorClamp,<br/> minEpsilon,<br/> epsilonNbStep,<br/> doubleDQN<br/> );</pre>
<ol start="8">
<li>Create the DQN model using <kbd>QLearningDiscreteConv</kbd> by passing MDP wrapper and <kbd>DataManager</kbd>: within the <kbd>QLearningDiscreteConv</kbd> constructor:</li>
</ol>
<pre style="padding-left: 60px">Sample:<br/> QLearningDiscreteConv&lt;MalmoBox&gt; dql =<br/> new QLearningDiscreteConv&lt;MalmoBox&gt;(mdp, MALMO_NET, MALMO_HPROC, MALMO_QL, manager);</pre>
<ol start="9">
<li>Train the DQN:</li>
</ol>
<pre style="padding-left: 60px">dql.train();</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, we defined an action space for the agent by specifying a defined set of Malmo actions. For example, <kbd>movenorth 1</kbd> means moving the agent one block north. We passed in a list of strings to <kbd>MalmoActionSpaceDiscrete</kbd> indicating an agent's actions on Malmo space.</p>
<p>In step 2, we created an observation space from the bitmap size (mentioned by <kbd>xSize</kbd> and <kbd>ySize</kbd>) of input images(from the Malmo space). Also, we assumed three color channels (R, G, B). The agent needs to know about observation space before they run. We used <kbd>MalmoObservationSpacePixels</kbd> because we target observation from pixels.</p>
<p>In step 3, we have created a Malmo consistency policy using <kbd>MalmoDescretePositionPolicy</kbd> <span>to ensure that the upcoming observation is in a consistent state.</span></p>
<p class="mce-root"/>
<p><span>A </span>MDP is an a<span>pproach used in reinforcement learning in grid-world environments. Our mission has states in the form of grids.</span> MDP requires a policy and the objective of reinforcement learning is to find the optimal policy for the MDP<span>. <kbd>MalmoEnv</kbd> is an</span> MDP wrapp<span>er around a Java client.</span></p>
<p><span>In</span> step 4, we <span>created</span> an MDP wrapper usi<span>ng the mission schema, action space, observation space, and observation policy. Note that the observation policy is not the same as the policy that an agent wants to form at the end of the learning process.</span></p>
<p>In step 5, we used <strong><kbd>DQNFactoryStdConv</kbd></strong> to build the DQN by adding convolutional layers.</p>
<p>In step 6, we configured <kbd>HistoryProcessor</kbd> to scale and remove pixels that were not needed. The actual intent of <span><kbd>HistoryProcessor</kbd> is to perform an experience replay, where the previous experience from the agent will be considered while deciding the action on the current state. </span>With the use of <span><kbd>HistoryProcessor</kbd>, we can change the partial observation of states to a fully-observed state, that is, when the current state is an accumulation of the previous states.</span></p>
<p>Here are the hyperparameters used in step 7 while creating Q-learning configuration:</p>
<ul>
<li><span><kbd>maxEpochStep</kbd>: The maximum number of steps allowed per epoch.</span></li>
<li><kbd>maxStep</kbd>: The maximum number of steps that are allowed. Training will finish when the iterations exceed the value specified for <kbd>maxStep</kbd>.<span><br/></span></li>
<li><kbd>expRepMaxSize</kbd>: The maximum size of experience replay. Experience replay refers to the number of past transitions based on which the agent can decide on the next step to take. </li>
<li><kbd>doubleDQN</kbd>: This decides whether double DQN is enabled in the configuration (true if enabled). </li>
<li><kbd>targetDqnUpdateFreq</kbd>: R<span>egular Q-learning can overestimate the action values under certain conditions. Double Q-learning adds stability to the learning. The main idea of double DQN is to freeze the network after e</span>very <em>M</em> <span>number of updates or smoothly average for every </span><em>M</em><span> number of updates. The value of M is referred to as <kbd>targetDqnUpdateFreq</kbd>. </span></li>
<li><kbd>updateStart</kbd>: The number of no-operation (do nothing) moves at the beginning to ensure the Malmo mission starts with a random configuration. If the agent starts the game in the same way every time, then the agent will memorize the sequence of actions, rather than learning to take the next action based on the current state. </li>
<li><kbd>gamma</kbd>: This is also known as the discount factor. A discount factor is multiplied by future rewards to prevent the agent from being attracted to high rewards, rather than learning the actions. A discount factor close to 1 indicates that the rewards from the distant future are considered. On the other hand, a discount factor close to 0 indicates that the rewards from the immediate future are being considered.</li>
<li><kbd>rewardFactor</kbd>: This is a reward-scaling factor to scale the reward for every single step of training.</li>
<li><kbd>errorClamp</kbd>: This will clip the gradient of loss function with respect to output during backpropagation. For <kbd>errorClamp = 1</kbd>, the gradient component is clipped to the range <em>(-1, 1)</em>. </li>
<li><kbd>minEpsilon</kbd>: Epsilon is the derivative of the loss function with respect to the output of the activation function. Gradients for every activation node for backpropagation are calculated from the given epsilon value. </li>
<li><kbd>epsilonNbStep</kbd>: Th epsilon value is annealed to <kbd>minEpsilon</kbd> over an <kbd>epsilonNbStep</kbd> number of steps.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>We can make the mission even harder by putting lava onto the agent's path after a certain number of actions are performed. First, start by creating a mission specification using the schema XML:</p>
<pre>MissionSpec mission = MalmoEnv.loadMissionXML("cliff_walking_rl4j.xml");</pre>
<p>Now, setting the lava challenge on the mission is as simple as follows:</p>
<pre>mission.drawBlock(xValue, yValue, zValue, "lava");"<br/> malmoEnv.setMission(mission);</pre>
<p><kbd>MissionSpec</kbd> is a class file included in the <kbd>MalmoJavaJar</kbd> dependency,<strong> </strong>which we can use to set missions in the Malmo space. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating a Malmo agent</h1>
                </header>
            
            <article>
                
<p>We need to evaluate the agent to see how well it has learned to play the game. We just trained our agent to navigate through the world to reach the target. In this recipe, we will evaluate the trained Malmo agent. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>As a prerequisite, we will need to persist the agent policies and reload them back during evaluation.</p>
<p>The final policy (policy to make movements in Malmo space) used by the agent after training can be saved as shown here:</p>
<pre>DQNPolicy&lt;MalmoBox&gt; pol = dql.getPolicy();<br/> pol.save("cliffwalk_pixel.policy");</pre>
<p><kbd>dql</kbd> refers to the DQN model. We retrieve the final policies and store them as a <kbd>DQNPolicy</kbd><span>. A DQN policy provides actions that have the highest Q-value estimated by the model. </span></p>
<p>It can be restored later for evaluation/inference:</p>
<pre>DQNPolicy&lt;MalmoBox&gt; pol = DQNPolicy.load("cliffwalk_pixel.policy");<span><br/></span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Create an MDP wrapper to load the mission:</li>
</ol>
<pre style="padding-left: 60px">Sample:<br/> MalmoEnv mdp = new MalmoEnv("cliff_walking_rl4j.xml", actionSpace, observationSpace, obsPolicy);</pre>
<ol start="2">
<li>Evaluate the agent:</li>
</ol>
<pre style="padding-left: 60px">Sample:<br/> double rewards = 0;<br/> for (int i = 0; i &lt; 10; i++) {<br/> double reward = pol.play(mdp, new HistoryProcessor(MALMO_HPROC));<br/> rewards += reward;<br/> Logger.getAnonymousLogger().info("Reward: " + reward);<br/> }</pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>The Malmo mission/world is launched</span> in step 1. In step 2,<span> <kbd>MALMO_HPROC</kbd> is the history processor configuration. You can refer t</span>o step 6 <span>of the previous recipe for the sample configuration. Once the agent is subjected to evaluation, you should see the results as shown here:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1425 image-border" src="assets/0ea45d95-90e1-4b49-afd0-58da36176ad4.png" style="width:55.75em;height:35.75em;"/></p>
<p><span><span>For every mission evaluation, we calculate the reward score. A positive reward score indicates that the agent has reached the target. At the end, we calculated the average reward score of the agent. </span></span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1226 image-border" src="assets/763ff236-143a-4198-b2ad-8a59b3ef743d.png" style="width:28.25em;height:24.00em;"/></p>
<p>In the preceding screenshot, we can see that the agent has reached the target. This is the ideal target position, no matter how the agent decides to move across the block. After the training session, the agent will form a final policy, which the agent can use to reach the target without falling into lava. The evaluation process will ensure that the agent is trained enough to play the Malmo game on its own. </p>


            </article>

            
        </section>
    </body></html>