<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Image Classification and Recognition</h1>
                </header>
            
            <article>
                
<p>In the previous chapters, we looked at the process of developing deep neural network models for classification and regression problems. In both cases, we were dealing with structured data and the models were of the supervised learning type, where target variables were available. Images or pictures belong to the unstructured category of data. In this chapter, we will illustrate the use of deep learning neural networks for image classification and recognition using the Keras package with the help of an easy-to-follow example. We will get started with a small sample size to illustrate the steps involved in developing an image-classification model. We will apply this model to a supervised learning situation involving the labeling of images or pictures. </p>
<p>Keras contains several built-<span>in</span><span> datasets for image classification, such as CIFAR10, CIFAR100, MNIST, and fashion-MNIST. CIFAR10 contains 50,000 32 x 32 color training images and 10,000 testing images with 10 label categories. CIFAR100, on the other hand, contains 50,000 32 x 32 color training images and 10,000 testing images with as many as 100 label categories. The MNIST dataset has 60,000 28 x 28 grayscale images for training and 10,000 images for testing with 10 different digits. The fashion-MNIST dataset has 60,000 28 x 28 grayscale images for training and 10,000 images for testing with 10 fashion categories. These datasets are already in a format that can be used </span><span>straightaway</span><span> to </span><span>develop deep neural network models with a minimal need for data-preparation-related steps. However, to get a better handle on dealing with image data, we will start by reading raw images from our computer into RStudio and go over all the steps needed to make image data ready for building a classification model.</span></p>
<p>The steps involved include exploring image data, resizing and reshaping images, one-hot encoding, developing a sequential model, compiling the model, fitting the model, evaluating the model, making predictions, and model-performance assessment using a confusion matrix.</p>
<p>More specifically, in this chapter, we will cover the following topics:</p>
<ul>
<li>Handling image data</li>
<li>Data preparation</li>
<li>Creating and fitting the model</li>
<li>Model evaluation and prediction</li>
<li>Performance optimization tips and best practices</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Handling image data</h1>
                </header>
            
            <article>
                
<p>In this section, we will read image data into R and explore it further to understand the various characteristics of image data. The code for reading and displaying images is as follows:</p>
<pre># Libraries<br/>library(keras)<br/>library(EBImage)<br/><br/># Reading and plotting images<br/>setwd("~/Desktop/image18")<br/>temp = list.files(pattern="*.jpg")<br/>mypic &lt;- list()<br/>for (i in 1:length(temp)) {mypic[[i]] &lt;- readImage(temp[i])}<br/>par(mfrow = c(3,6))<br/>for (i in 1:length(temp)) plot(mypic[[i]])<br/>par(mfrow = c(1,1))</pre>
<p><span>As you can see from the preceding code, we will make use of the <kbd>keras</kbd> and <kbd>EBImage</kbd> libraries. The <kbd>EBImage</kbd> library is useful for handling and exploring image data. We will start by reading 18 JPEG image files that are stored in the <kbd>image18</kbd> folder of my computer. These images </span><span>each</span><span> </span><span>contain 6 pictures of bicycles, cars, and airplanes that were downloaded from the internet. These image files are read using the </span><kbd>readImage</kbd> <span>function and are stored in</span> <kbd>mypic</kbd><span>. </span></p>
<p><span>All 18 images are shown in the following screenshot:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/44e3c243-de5b-4199-b4ae-dd2183181391.png" style="width:36.17em;height:21.00em;"/></p>
<p>From the preceding screenshot, we can see the six images of bicycles, cars, and airplanes. You might have noticed that not all of the pictures are of the same size. For example, the fifth and sixth bicycles noticeably vary in size. Similarly, the fourth and fifth airplanes are clearly of different sizes, too. Let's take a closer look at the <span>data</span><span> for the</span><span> fifth bicycle using the following code:</span></p>
<pre># Exploring 5th image data<br/>print(mypic[[5]])<br/><br/>OUTPUT<br/><strong>Image </strong><br/><strong>  colorMode    : Color </strong><br/><strong>  storage.mode : double </strong><br/><strong>  dim          : 299 169 3 </strong><br/><strong>  frames.total : 3 </strong><br/><strong>  frames.render: 1 </strong><br/><br/><strong>imageData(object)[1:5,1:6,1]</strong><br/><strong>     [,1] [,2] [,3] [,4] [,5] [,6]</strong><br/><strong>[1,]    1    1    1    1    1    1</strong><br/><strong>[2,]    1    1    1    1    1    1</strong><br/><strong>[3,]    1    1    1    1    1    1</strong><br/><strong>[4,]    1    1    1    1    1    1</strong><br/><strong>[5,]    1    1    1    1    1    1</strong><br/><br/>hist(mypic[[5]])</pre>
<p>Using the <kbd>print</kbd> function, we can look at how the image of a bicycle (unstructured data) has been converted into numbers (structured data). The dimensions for the fifth bicycle are 299 x 169 x 3, which leads to a total of 151,593 data points, or pixels, obtained by multiplying the three numbers. The first number, 299, represents the image width in terms of pixels and the second number, 169, represents the image height in terms of pixels. Note that a colored image consists of three channels representing the <span>colors </span><span>red, blue, and green. The small table extracted from the data shows the first five rows of data in the <em>x</em>-dimension, and the first six rows of data in the <em>y</em>-dimension, and the value for the <em>z</em>-dimension is one. Although all values in the body of the table are <kbd>1</kbd>, they are expected to vary between <kbd>0</kbd> and <kbd>1</kbd>.</span></p>
<div class="packt_tip">A color image has red, green, and blue channels. A grayscale image has only one channel.</div>
<p><span>These data points for the fifth bicycle are used for creating a histogram, as shown in the following screenshot:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e4f078ef-6f58-4453-80e3-65b411e56765.png" style="width:38.58em;height:23.92em;"/></p>
<p><span>The preceding histogram shows the distribution of intensity values for the fifth image's data. It can be seen that most of the data poi</span><span>nts have high-intensity values for this image.</span></p>
<p><span>Let's now look at the following histogram of data based on the 16th image (that of an airplane) for comparison:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/00dbaf85-4c53-4c46-a5d5-fcfb93c55782.png" style="width:42.00em;height:25.00em;"/></p>
<p>From the preceding histogram, we can see that this image has different intensity values for the red, green, and blue colors. In general, intensity values lie between zero and one. Data points that are closer to zero represent a darker color in the image and those closer to one indicate a brighter color in the image.</p>
<p>Let's take a look at data related to the <span>16th image, of an airplane,</span> using the following code:</p>
<pre># Exploring 16th image data<br/>print(mypic[[16]])<br/><br/>OUTPUT<br/><br/>Image <br/> colorMode : Color <br/> storage.mode : double <br/> dim : 318 159 3 <br/> frames.total : 3 <br/> frames.render: 1 <br/><br/>imageData(object)[1:5,1:6,1]<br/> [,1] [,2] [,3] [,4] [,5] [,6]<br/>[1,] 0.2549020 0.2549020 0.2549020 0.2549020 0.2549020 0.2549020<br/>[2,] 0.2549020 0.2549020 0.2549020 0.2549020 0.2549020 0.2549020<br/>[3,] 0.2549020 0.2549020 0.2549020 0.2549020 0.2549020 0.2549020<br/>[4,] 0.2588235 0.2588235 0.2588235 0.2588235 0.2588235 0.2588235<br/>[5,] 0.2588235 0.2588235 0.2588235 0.2588235 0.2588235 0.2588235</pre>
<p>From the output provided in the preceding code, we can see that the two images have different dimensions. The dimensions for the 16th image are 318 x 159 x 3, which results in a total of 151,686 data points or pixels.</p>
<p>In order to prepare this data for developing an image classification model, we will start by resizing all images to the same dimensions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data preparation</h1>
                </header>
            
            <article>
                
<p>In this section, we will go over the steps for making our image data ready for developing an image classification model. These steps will involve resizing images to obtain the same size for all images, followed by reshaping, data partitioning, and the one-hot encoding of the response variables.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Resizing and reshaping</h1>
                </header>
            
            <article>
                
<p>To prepare the data for developing a classification model, we start by resizing the dimensions of all 18 images to the same size using the following code:</p>
<pre># Resizing<br/>for (i in 1:length(temp)) {mypic[[i]] &lt;- resize(mypic[[i]], 28, 28)}</pre>
<p>As can be seen from the preceding code, all images are now resized to 28 x 28 x 3. Let's plot all the images again to see the impact of resizing using the following code:</p>
<pre># Plot images<br/>par(mfrow = c(3,6))<br/>for (i in 1:length(temp)) plot(mypic[[i]])<br/>par(mfrow = c(1,1)</pre>
<p>When we reduce the dimensions of a picture, it will lead to a lower number of pixels, which in turn will cause pictures to have lower quality, as can be seen in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0e274d67-8078-4686-97f5-f43f485b07b4.png" style="width:28.00em;height:17.25em;"/></p>
<p class="mce-root">Next, we will reshape the dimensions of 28 x 28 x 3 into a single dimension of 28 x 28 x 3 (or 2,352 vectors) using the following code:</p>
<pre># Reshape<br/>for (i in 1:length(temp)) {mypic[[i]] &lt;- array_reshape(mypic[[i]], c(28, 28,3))}<br/>str(mypic)<br/><br/>OUTPUT<br/><br/>List of 18<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 0.953 0.953 0.953 0.953 0.953 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 0.328 ...<br/> $ : num [1:28, 1:28, 1:3] 0.26 0.294 0.312 0.309 0.289 ...<br/> $ : num [1:28, 1:28, 1:3] 0.49 0.49 0.49 0.502 0.502 ...<br/> $ : num [1:28, 1:28, 1:3] 1 1 1 1 1 1 1 1 1 1 ..</pre>
<p>By observing the structure of the preceding data using <kbd>str(mypic)</kbd>, we can see that there are 18 different items in the list that correspond to the 18 images that we started with.</p>
<p>Next, we will create training, validation, and test data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training, validation, and test data</h1>
                </header>
            
            <article>
                
<p>We will use the first three images of the bicycles, cars, and airplanes respectively for training, the fourth image of each type for validation, and the remaining two images of each type for testing. Thus, the training data will have nine images, the validation data will have three images, and the test data will have six images. The following is the code to achieve this:</p>
<pre># Training Data<br/>a &lt;- c(1:3, 7:9, 13:15)<br/>trainx &lt;- NULL<br/>for (i in a) {trainx &lt;- rbind(trainx, mypic[[i]]) }<br/>str(trainx)<br/><br/>OUTPUT<br/><br/>num [1:9, 1:2352] 1 1 1 1 0.953 ...<br/><br/># Validation data<br/>b &lt;- c(4, 10, 16)<br/>validx &lt;- NULL<br/>for (i in b) {validx &lt;- rbind(validx, mypic[[i]]) }<br/>str(validx)<br/><br/>OUTPUT<br/><br/><span>num [1:3, 1:2352] 1 1 0.26 1 1 ...<br/><br/># Test Data<br/></span>c &lt;- c(5:6, 11:12, 17:18)<br/>testx &lt;- NULL<br/>for (i in c) {testx &lt;- rbind(testx, mypic[[i]])}<br/>str(testx)<br/><br/>OUTPUT<br/><br/>num [1:6, 1:2352] 1 1 1 1 0.49 ...</pre>
<p>As you can see from the preceding code, we will use the <kbd>rbind</kbd> function to combine the rows of data that we have for each image when creating training, validation, and <kbd>test</kbd> data. After combining the rows of data from the nine images, the structure of <kbd>trainx</kbd> indicates that there are 9 rows and 2,352 columns. Similarly, for the validation data, we have 3 rows and 2,352 columns, and for the test data, we have 6 rows and 2,352 columns.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">One-hot encoding</h1>
                </header>
            
            <article>
                
<p>For the one-hot encoding of the response variables, we use the following code:</p>
<pre># Labels<br/>trainy &lt;- c(0,0,0,1,1,1,2,2,2)<br/>validy &lt;- c(0,1,2)<br/>testy &lt;- c(0,0,1,1,2,2)<br/><br/># One-hot encoding<br/>trainLabels &lt;- to_categorical(trainy)<br/>validLabels &lt;- to_categorical(validy)<br/>testLabels &lt;- to_categorical(testy)<br/>trainLabels<br/><br/>OUTPUT<br/><strong><span>      [,1] [,2] [,3]
 [1,]    1    0    0
 [2,]    1    0    0
 [3,]    1    0    0
 [4,]    0    1    0
 [5,]    0    1    0
 [6,]    0    1    0
 [7,]    0    0    1
 [8,]    0    0    1
 [9,]    0    0    1</span></strong></pre>
<p>From the preceding code, we can see the following:</p>
<ul>
<li>We have stored target values for each image in <kbd>trainy</kbd> , <kbd>validy</kbd>, and <kbd>testy</kbd>, where <kbd>0</kbd>, <kbd>1</kbd>, and <kbd>2</kbd> indicate bicycle, car, and airplane images respectively.</li>
<li>We carry out one-hot encoding of <kbd>trainy</kbd> , <kbd>validy</kbd>, and <kbd>testy</kbd> by using the <kbd>to_categorical</kbd> function. One-hot encoding here helps to convert a factor variable into a combination of zeros and ones.</li>
</ul>
<p>Now we have the data in a format that can be used for developing a deep neural network classification model, and that is what we will do in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating and fitting the model</h1>
                </header>
            
            <article>
                
<p>In this section, we will develop an image classification model to classify the images of the bicycles, cars, and airplanes. We will first specify a model architecture, then we will compile the model, and then fit the model using training and validation data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Developing the model architecture</h1>
                </header>
            
            <article>
                
<p>When developing the model architecture, we start by creating a sequential model and then add various layers. The following is the code:</p>
<pre># Model architecture<br/>model &lt;- keras_model_sequential() <br/>model %&gt;% <br/>  layer_dense(units = 256, activation = 'relu', input_shape = c(2352)) %&gt;% <br/>  layer_dense(units = 128, activation = 'relu') %&gt;% <br/>  layer_dense(units = 3, activation = 'softmax')<br/>summary(model)<br/><br/>OUTPUT<br/><strong>______________________________________________________________________</strong><br/><strong>Layer (type)                   Output Shape              Param #     </strong><br/><strong>======================================================================</strong><br/><strong>dense_1 (Dense)                (None, 256)               602368      </strong><br/><strong>______________________________________________________________________</strong><br/><strong>dense_2 (Dense)                (None, 128)               32896       </strong><br/><strong>_____________________________________________________________________</strong><br/><strong>dense_3 (Dense)                (None, 3)                  387         </strong><br/><strong>======================================================================</strong><br/><strong>Total params: 635,651</strong><br/><strong>Trainable params: 635,651</strong><br/><strong>Non-trainable params: 0</strong><br/><strong>_______________________________________________________________________</strong></pre>
<p>As can be seen from the preceding code, the input layer has  <kbd>2352</kbd> units (<span>28 x 28 x 3)</span><span>. For the initial model, we use two hidden layers with 256 and 128 units respectively. For both hidden layers, we will use the </span><kbd>relu</kbd> <span>activation function. For the output layer, we will use 3 units since the target variable has 3 classes, representing a bicycle, car, and airplane. The total number of parameters for this model is 635,651.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compiling the model</h1>
                </header>
            
            <article>
                
<p>After developing the model architecture, we can compile the model using the following code:</p>
<pre># Compile model<br/>model %&gt;% compile(loss = 'categorical_crossentropy',<br/>  optimizer = 'adam',<br/>  metrics = 'accuracy')</pre>
<p>We compile the model by using <kbd>categorical_crossentropy</kbd> for loss, since we are doing multiclass classification. W<span>e have specified</span><span> </span><kbd>adam</kbd><span> </span><span>and</span><span> </span><kbd>accuracy</kbd><span> for the optimizer and metrics,</span><span> respectively.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fitting the model</h1>
                </header>
            
            <article>
                
<p>Now we are ready to train the model. The following is the code for this:</p>
<pre># Fit model<br/>model_one &lt;- model %&gt;% fit(trainx, <br/>                         trainLabels, <br/>                         epochs = 30, <br/>                         batch_size = 32, <br/>                         validation_data =  list(validx, validLabels))<br/>plot(model_one)</pre>
<p>From the preceding code, we can see the following facts:</p>
<ul>
<li>We can fit the model using <kbd>independent</kbd> variables stored in <kbd>trainx</kbd> and <kbd>target</kbd> variables stored in <kbd>trainLabels</kbd>. <span>To safeguard against overfitting, we will use</span> <kbd>validation_data</kbd><span>. </span></li>
</ul>
<div class="packt_infobox"><span>Note that, in the previous chapters, we made use of</span> <kbd>validation_split</kbd> <span>by specifying a certain percentage, such as 20%; however, if we used</span> <kbd>validation_split</kbd> <span>with a 20% rate, it would have used the last 20% of the training data (all airplane images) for validation.</span></div>
<ul>
<li><span>This would have created a situation where the training data had no sample from the airplane images and the classification model would have been based on bicycle and car images only.</span></li>
</ul>
<ul>
<li><span>Therefore, the resulting image classification model would be biased and would have performed well only with bicycle and car images. Therefore, instead of using the</span> <kbd>validation_split </kbd><span>function, in this situation, we make use of</span> <kbd>validation_data</kbd>, <span>where we have made sure that we have a sample of each type represented in both the training and validation data.</span></li>
</ul>
<p>The following graphs show the loss and accuracy for 30 epochs separately for training and validation data:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/90692e5d-7fe2-4fd2-b639-2de1f207dc26.png" style="width:36.33em;height:22.92em;"/></p>
<p><span>We can make the following observations f</span>rom the preceding plots:</p>
<ul>
<li>From the parts of the graphs dealing with accuracy, we can see that from the eighteenth epoch onward, the accuracy values for the training data attain the highest value of 1. </li>
<li>On the other hand, the accuracy based on the validation data is mainly around two thirds, or 66.7%. Since we have data from three images that is used for validation, if all three images' from validation data is correctly classified, the reported accuracy will be 1. In this case, two out of three images are correctly classified, and that leads to accuracy of 66.7%.</li>
<li>From the parts of the graphs dealing with loss, we can see that for the training data, the loss values drop significantly from about 3 to less than 1 after 8 epochs. They continue to reduce from then on; however, the rate of decrease in the loss values slows down.</li>
</ul>
<ul>
<li>An approximately similar pattern can be seen based on the validation data.</li>
<li>In addition, since the loss uses probability values in its calculation, we observe a clearer trend for the loss-related plot compared to the accuracy-related plot.</li>
</ul>
<p>Next, let's evaluate the model's image classification performance in greater detail to understand its behavior.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model evaluation and prediction</h1>
                </header>
            
            <article>
                
<p>In this section, we will carry out model evaluation and create a confusion matrix with the help of predictions, both for training and test data. Let's start by evaluating the image classification performance of the model using training data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loss, accuracy, and confusion matrices for training data</h1>
                </header>
            
            <article>
                
<p>We will now obtain loss and accuracy values for the training data and then create a confusion matrix using the following code:</p>
<pre># Model evaluation<br/>model %&gt;% evaluate(trainx, trainLabels)<br/><br/>OUTPUT<br/><strong>12/12 [==============================] - 0s 87us/step</strong><br/>$loss<br/>[1] 0.055556579307<br/><br/>$acc<br/>[1] 1<br/><br/># Confusion matrix<br/>pred &lt;- model %&gt;%   predict_classes(trainx)<br/>table(Predicted=pred, Actual=trainy)<br/><br/>OUTPUT<br/><strong>         Actual</strong><br/><strong>Predicted 0 1 2</strong><br/><strong>        0 3 0 0</strong><br/><strong>        1 0 3 0</strong><br/><strong>        2 0 0 3</strong></pre>
<p>As you can see from the preceding output, the loss and accuracy values are <kbd>0.056</kbd> and <kbd>1</kbd> respectively. The confusion matrix based on the training data indicates that all nine images are correctly classified into three categories, and therefore the resulting accuracy is 1.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Prediction probabilities for training data</h1>
                </header>
            
            <article>
                
<p>We can now look at the probabilities of the three classes for all nine images in the training data that this model provides. The following is the code:</p>
<pre># Prediction probabilities<br/>prob &lt;- model %&gt;%   predict_proba(trainx) <br/>cbind(prob, Predicted_class = pred, Actual = trainy)<br/><br/>OUTPUT                                                <br/>                                                    <strong>Predicted_class Actual
 [1,] 0.9431666135788 0.007227868307 0.049605518579          0        0
 [2,] 0.8056846261024 0.005127847660 0.189187481999          0        0
 [3,] 0.9556384682655 0.001881886506 0.042479615659          0        0
 [4,] 0.0018005876336 0.988727569580 0.009471773170          1        1
 [5,] 0.0002136278927 0.998095452785 0.001690962003          1        1
 [6,] 0.0008950306219 0.994426369667 0.004678600468          1        1
 [7,] 0.0367377623916 0.010597365908 0.952664911747          2        2
 [8,] 0.0568452328444 0.011656147428 0.931498587132          2        2
 [9,] 0.0295505002141 0.011442330666 0.959007143974          2        2</strong></pre>
<p>In the preceding output, the first three columns show the probability of an image being a bicycle, car, or airplane, and the total of these three probabilities is 1. We can make the following observations from the output:</p>
<ul>
<li>The probabilities for the first image in the training data are <kbd>0.943</kbd>, <kbd>0.007</kbd>, and <kbd>0.049</kbd> for bicycle, car, and airplane respectively. Since the highest probability is for the first class, the predicted class based on the model is <kbd>0</kbd> (for bicycle), and this is also the actual class of the image.</li>
<li>Although all 9 images are correctly classified, the probability of correct classification varies from <kbd>0.806</kbd> (image 2) to <kbd>0.998</kbd> (image 5).</li>
<li>For the car images (rows 4 to 6), the probability of correct classification ranges from <kbd>0.989</kbd> to <kbd>0.998</kbd> and is consistently high for all three images. Therefore, this classification model gives its best performance when classifying car images.</li>
<li>For bicycle images (rows 1 to 3), the probability of correct classification ranges from <kbd>0.806</kbd> to <kbd>0.956</kbd>, which indicates some difficulty in correctly classifying bicycle images.</li>
</ul>
<ul>
<li>For the second sample, which represents a bicycle image, the second-highest probability is <kbd>0.189</kbd> of being an airplane image. Clearly, this model is little bit confused when it comes to deciding whether this image is a bicycle or an airplane.</li>
<li>For the airplane images (rows 7 to 9), the probability of correct classification ranges from <kbd>0.931</kbd> to <kbd>0.959</kbd>, which is  also consistently high for all three images.</li>
</ul>
<p>Looking at prediction probabilities allows us to dig deeper into the classification performance of the model, which cannot be obtained only by looking at the accuracy value. However, while good performance with training data is necessary, it is not sufficient to arrive at a reliable image-classification model. When a classification model suffers from an overfitting problem, we have difficulty replicating good results based on training data on test data that the model has not seen. Therefore, a real test of a good classification model is when it performs well with the test data. Let's now review the image-classification performance of the model for test data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loss, accuracy, and confusion matrices for test data</h1>
                </header>
            
            <article>
                
<p>We can now obtain loss and accuracy values for the test data and then create a confusion matrix using the following code:</p>
<pre># Loss and accuracy<br/>model %&gt;% evaluate(testx, testLabels)<br/><br/>OUTPUT<br/><strong>6/6 [==============================] - 0s 194us/step</strong><br/><strong>$loss</strong><br/><strong>[1] 0.5517520905</strong><br/><br/>$acc<br/>[1] 0.8333333<br/><br/># Confusion matrix<br/>pred &lt;- model %&gt;%   predict_classes(testx)<br/>table(Predicted=pred, Actual=testy)<br/><br/>OUTPUT<br/><strong>         Actual</strong><br/><strong>Predicted 0 1 2</strong><br/><strong>        0 2 0 0</strong><br/><strong>        1 0 1 0</strong><br/><strong>        2 0 1 2</strong></pre>
<p>As you can see from the preceding output, the loss and accuracy values for the images in the test data are <kbd>0.552</kbd> and <kbd>0.833</kbd> respectively. These results are slightly inferior to the numbers that we saw for the training data; however, some amount of performance deterioration is expected when a model is assessed based on unseen data. The confusion matrix indicates one incorrectly classified image, where an image of a car is mistaken for an image of an airplane. Therefore, with five out of six correct classifications, the model accuracy based on the test data is 83.3%. Let's now look more deeply into the model's prediction performance by investigating the probability values based on images in the test data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Prediction probabilities for test data</h1>
                </header>
            
            <article>
                
<p>We can now review the probabilities for the three classes for all six images in the test data. The following is the code:</p>
<pre class="mce-root"># Prediction probabilities<br/>prob &lt;- model %&gt;%   predict_proba(testx) <br/>cbind(prob, Predicted_class = pred, Actual = testy)<br/><br/>OUTPUT                                         <br/><br/><strong>                                                Predicted_class Actual
[1,] 0.587377548218 0.02450981364 0.38811263442           0      0
[2,] 0.532718658447 0.04708640277 0.42019486427           0      0
[3,] 0.115497209132 0.18486714363 0.69963568449           2      1
[4,] 0.001700860681 0.98481327295 0.01348586939           1      1
[5,] 0.230999588966 0.03030913882 0.73869132996           2      2
[6,] 0.112148292363 0.02054920420 0.86730253696           2      2</strong></pre>
<p>Looking at these predicted probabilities, we can make the following observations:</p>
<ul>
<li>Bicycle images are predicted correctly, as shown by the first two samples. However, prediction probabilities are relatively lower at <kbd>0.587</kbd> and <kbd>0.533</kbd>.</li>
<li>The results for car images (rows 3 and 4) are mixed, with the fourth sample, correctly predicted with a high probability of <kbd>0.985</kbd>, but the third car image is misclassified as an airplane with about <kbd>0.7</kbd> probability.</li>
<li>Airplane images are represented by the fifth and sixth samples. The prediction probabilities for these two images are <kbd>0.739</kbd> and <kbd>0.867</kbd> respectively. </li>
<li>Although five out of six images are correctly classified, many prediction probabilities are relatively low when compared to the model's performance on training data. </li>
</ul>
<p>Therefore, overall, we can say that there is definitely some scope to improve the model's performance further. In the next section, we will explore improving the model's performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance optimization tips and best practices</h1>
                </header>
            
            <article>
                
<p>In this section, we will explore a deeper network for improving the performance of the image-classification model. We will look at the results for comparison.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deeper networks</h1>
                </header>
            
            <article>
                
<p>The code used for experimenting with a deeper network in this section are as follows:</p>
<pre># Model architecture<br/>model &lt;- keras_model_sequential() <br/>model %&gt;% <br/>  layer_dense(units = 512, activation = 'relu', input_shape = c(2352)) %&gt;% <br/>  layer_dropout(rate = 0.1) %&gt;%<br/>  layer_dense(units = 256, activation = 'relu') %&gt;%<br/>  layer_dropout(rate = 0.1) %&gt;%<br/>  layer_dense(units = 3, activation = 'softmax')<br/>summary(model)<br/><br/>OUTPUT<br/><strong>_______________________________________________________________________</strong><br/><strong>Layer (type)                    Output Shape             Param #        </strong><br/><strong>=======================================================================</strong><br/><strong>dense_1 (Dense)                  (None, 512)               1204736  </strong><br/><strong>_______________________________________________________________________</strong><br/><strong>dropout_1 (Dropout)              (None, 512)               0              </strong><br/><strong>_______________________________________________________________________</strong><br/><strong>dense_2 (Dense)                  (None, 256)              131328         </strong><br/><strong>_______________________________________________________________________</strong><br/><strong>dropout_2 (Dropout)              (None, 256)               0              </strong><br/><strong>_______________________________________________________________________</strong><br/><strong>dense_3 (Dense)                  (None, 3)                 771            </strong><br/><strong>=======================================================================</strong><br/><strong>Total params: 1,336,835</strong><br/><strong>Trainable params: 1,336,835</strong><br/><strong>Non-trainable params: 0</strong><br/><strong>_______________________________________________________________________</strong><br/><br/># Compile model<br/>model %&gt;% compile(loss = 'categorical_crossentropy',<br/>  optimizer = 'adam',<br/>  metrics = 'accuracy')<br/><br/># Fit model<br/>model_two &lt;- model %&gt;% fit(trainx, <br/> trainLabels, <br/> epochs = 30, <br/> batch_size = 32, <br/> validation_data = list(validx, validLabels))<br/>plot(model_two)</pre>
<p><span>From the preceding code, we can see the following:</span></p>
<ul>
<li><span>We are increasing the number of units in the first and second hidden layers to <kbd>512</kbd> and <kbd>256</kbd> respectively.</span></li>
<li><span>We are also adding dropout layers after each hidden layer with a 10% dropout rate.</span></li>
<li><span>The total number of parameters with this change has now gone up to <kbd>1336835</kbd>.</span></li>
<li><span>This time, we will also run the model for 50 epochs. We do not make any other changes to the model. </span></li>
</ul>
<p><span>The following graphs provide accuracy and loss values for the training and validation data for 50 epochs:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b84682d8-3b5d-46f7-9e11-b4a021d69b5c.png" style="width:37.00em;height:23.83em;"/></p>
<p>From the preceding graphs, we can see the following:</p>
<ul>
<li>There are some major changes observed in the accuracy and loss values compared to the earlier model.</li>
<li>The accuracy for both the training and validation data after 50 epochs is 100%.</li>
<li>In addition, the closeness of the training- and validation-related curves for loss and accuracy indicate that this image-classification model is not likely to suffer from an overfitting problem.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Results</h1>
                </header>
            
            <article>
                
<p>To further explore any changes in the image-classification performance of the model that may not be obvious from a graphical summary, let's look at some numerical summaries:</p>
<ol>
<li>We will look at the results based on the training data first, and will make use of the following code:</li>
</ol>
<pre style="padding-left: 60px"># Loos and accuracy<br/>model %&gt;% evaluate(trainx, trainLabels)<br/>OUTPUT<br/>12/12 [==============================] - 0s 198us/step<br/>$loss<br/>[1] 0.03438224643<br/><br/>$acc<br/>[1] 1<br/><br/># Confusion matrix<br/>pred &lt;- model %&gt;%   predict_classes(trainx)<br/>table(Predicted=pred, Actual=trainy)<br/><br/>OUTPUT<br/><strong>         Actual</strong><br/><strong>Predicted 0 1 2</strong><br/><strong>        0 3 0 0</strong><br/><strong>        1 0 3 0</strong><br/><strong>        2 0 0 3</strong></pre>
<p style="padding-left: 60px">From the preceding output, we can see that the loss value has now reduced to <kbd>0.034</kbd> and the accuracy is maintained at <kbd>1.0</kbd>. We obtain the same confusion matrix results <span>for the training data</span><span> as we did earlier as all nine images are correctly classified by the model, which gives an accuracy level of 100%.</span></p>
<ol start="2">
<li>To look more deeply at the classification performance of the model, we make use of the following code and output:</li>
</ol>
<pre style="padding-left: 30px"># Prediction probabilities<br/>prob &lt;- model %&gt;%   predict_proba(trainx) <br/>cbind(prob, Predicted_class = pred, Actual = trainy)<br/><br/>OUTPUT        <br/><strong>                                                   Predicted_class Actual
 [1,] 0.97638195753098 0.0071088117547 0.01650915294886     0         0
 [2,] 0.89875286817551 0.0019298568368 0.09931717067957     0         0
 [3,] 0.98671281337738 0.0004396488657 0.01284754090011     0         0
 [4,] 0.00058794603683 0.9992876648903 0.00012432398216     1         1
 [5,] 0.00005639552546 0.9999316930771 0.00001191849515     1         1
 [6,] 0.00020669832884 0.9997472167015 0.00004611289114     1         1
 [7,] 0.03771930187941 0.0022936603054 0.95998704433441     2         2
 [8,] 0.08463590592146 0.0022607713472 0.91310334205627     2         2
 [9,] 0.03016609139740 0.0019471622072 0.96788680553436     2         2</strong></pre>
<p style="padding-left: 60px">From the preceding prediction probabilities that we obtain as an output of the training data, we can make the following observations:</p>
<ul>
<li>Correct classifications are now made with higher probability values than the earlier model.</li>
<li>The lowest correct classification probability based on the second row is <kbd>0.899</kbd>.</li>
<li>Therefore, this model seems to be more sure when correctly classifying images compared to what was observed with the previous model.</li>
</ul>
<ol start="3">
<li>Now let's see whether this improvement is also seen with the test data. We will use the following codes and output:</li>
</ol>
<pre style="padding-left: 60px"># Loss and accuracy<br/>model %&gt;% evaluate(testx, testLabels)<br/><br/>OUTPUT<br/><br/><strong>6/6 [==============================] - 0s 345us/step</strong><br/><strong>$loss</strong><br/><strong>[1] 0.40148338683</strong><br/><br/>$acc<br/>[1] 0.8333333<br/><br/># Confusion matrix<br/>pred &lt;- model %&gt;%   predict_classes(testx)<br/>table(Predicted=pred, Actual=testy)<br/><br/>OUTPUT<br/><strong>         Actual</strong><br/><strong>Predicted 0 1 2</strong><br/><strong>        0 2 0 0</strong><br/><strong>        1 0 1 0</strong><br/><strong>        2 0 1 2</strong></pre>
<p style="padding-left: 60px">As indicated in the preceding output, the test data loss and accuracy values are <kbd>0.401</kbd> and <kbd>0.833</kbd> respectively. We do see some improvement in loss values; however, the accuracy value is again the same as it was earlier. Looking at the confusion matrix, we can see that this time, an image of a car is misclassified as an airplane. Therefore, we do not see any major differences based on the confusion matrix.</p>
<ol start="4">
<li>Next, let's review the prediction probabilities using the following code and its output:</li>
</ol>
<pre style="padding-left: 60px"># Prediction probabilities<br/>prob &lt;- model %&gt;%   predict_proba(testx) <br/>cbind(prob, Predicted_class = pred, Actual = testy)<br/><br/>OUTPUT<br/><strong>                                                   Predicted_class Actual
[1,] 0.7411330938339 0.015922509134 0.242944419384           0      0
[2,] 0.7733710408211 0.021422179416 0.205206796527           0      0
[3,] 0.3322730064392 0.237866103649 0.429860889912           2      1
[4,] 0.0005808877177 0.999227762222 0.000191345287           1      1
[5,] 0.2163420319557 0.009395645000 0.774262309074           2      2
[6,] 0.1447975188494 0.002772571286 0.852429926395           2      2</strong></pre>
<p>Using the prediction probabilities for the test data, we can make the following two observations:</p>
<ul>
<li>We see a consistently similar pattern to the one that we observed based on the results from the training data. This model correctly classifies images in the test data with higher probabilities (<kbd>0.74</kbd> to <kbd>0.99</kbd>) than the earlier model (<kbd>0.53</kbd> to <kbd>0.98</kbd>).</li>
</ul>
<ul>
<li>For the fourth sample in the test data, the model seems to be confused between the image of a bicycle and an airplane, when in reality, this image is of a car.</li>
</ul>
<p>Therefore, overall, we have observed that by developing a deeper neural network, we are able to improve the model's performance. The improvement in the performance was not obvious from the accuracy calculation; however, the calculation of prediction probabilities allowed us to develop better insights and compare model performance. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we explored image data and a deep neural network image-classification model. We used data from 18 images of bicycles, cars, and airplanes, and carried out appropriate data processing to make the data ready to use with the Keras library. We partitioned image data into training, validation, and test data, and subsequently developed a deep neural model using training data and evaluated its performance by looking at the loss, accuracy, confusion matrix, and probability values for both the training and test data. We also made modifications to the model to improve its classification performance. In addition, we observed that when the confusion matrix provides the same level of performance, prediction probabilities may be able to help in extracting finer differences between the two models.</p>
<p>In the next chapter, we will go over the steps to develop a deep neural network image-classification model using <strong>convolutional neural networks</strong> (<strong>CNNs</strong>), which are becoming very popular when it comes to image classification applications. CNNs are regarded as the gold standard for image-classification problems, and are very effective for large-scale image-classification applications.</p>


            </article>

            
        </section>
    </body></html>