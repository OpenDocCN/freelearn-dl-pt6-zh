<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Constructing an LSTM Neural Network for Sequence Classification</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we discussed classifying time series data for multi-variate features. In this chapter, we will create a<span> </span><strong>long short-term memory</strong> (<strong>LSTM</strong>) neural network to classify univariate time series data. Our neural network will learn how to classify a univariate time series. We will have<span> </span><strong>UCI</strong><span> (short for <strong>University of California Irvine</strong>) </span>synthetic control data on top of which the neural network will be trained. There will be 600 sequences of data, with every sequence separated by a new line to make our job easier. Every sequence will have values recorded at 60 time steps. Since it is a univariate time series, we will <span>only </span>have columns in CSV files for every example recorded. Every sequence is an example recorded. We will split these sequences of data into train/test sets to perform training and evaluation respectively. The possible categories of class/labels are as follows:</p>
<ul>
<li><span>Normal</span></li>
<li><span>Cyclic</span></li>
<li><span>Increasing trend</span></li>
<li><span>Decreasing trend</span></li>
<li><span>Upward shift</span></li>
<li><span>Downward shift<br/></span></li>
</ul>
<p class="mce-root"><span>In this chapter, we will cover the following recipes:</span></p>
<ul>
<li>Extracting time series data</li>
<li>Loading training data</li>
<li>Normalizing training data</li>
<li>Constructing input layers for the network</li>
<li>Constructing output layers for the network</li>
<li>Evaluating the LSTM network for classified output</li>
</ul>
<p>Let's begin.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>This chapter's implementation code can be found at <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/07_Constructing_LSTM_Neural_network_for_sequence_classification/sourceCode/cookbookapp/src/main/java/UciSequenceClassificationExample.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/07_Constructing_LSTM_Neural_network_for_sequence_classification/sourceCode/cookbookapp/src/main/java/UciSequenceClassificationExample.java</a>.</p>
<p>After cloning our GitHub repository, navigate to the <kbd>Java-Deep-Learning-Cookbook/07_Constructing_LSTM_Neural_network_for_sequence_classification/sourceCode</kbd> <span>directory</span>. Then import the <kbd>cookbookapp</kbd> <span>project </span>as<span> a Maven project</span><strong> </strong><span>by impo</span>rting <kbd>pom.xml</kbd>.</p>
<p><span>Download the data from this UCI website</span><span>: </span><a href="https://archive.ics.uci.edu/ml/machine-learning-databases/synthetic_control-mld/synthetic_control.data">https://archive.ics.uci.edu/ml/machine-learning-databases/synthetic_control-mld/synthetic_control.data</a>.</p>
<p>We need to create directories to store the<span> </span>train and test data. Refer to the following directory structure:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1175 image-border" src="assets/2ec9aeb2-e022-40dc-9b40-f463321fc911.png" style="width:35.50em;height:7.42em;"/></p>
<p>We need to create two separate folders for the train and test datasets and then create subdirectories for <kbd>features</kbd><span> </span>and<span> </span><kbd>labels</kbd><span> </span>respectively:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1176 image-border" src="assets/6d26b709-f15b-482c-a21c-82d26854bc27.png" style="width:36.33em;height:8.33em;"/></p>
<p class="mce-root"/>
<p><span>This folder structure is a prerequisite for the aforementioned data extraction.</span> We separate features and labels while performing the extraction.</p>
<p><span>Note that, throughout this cookbook, we are using the DL4J version </span>1.0.0-beta 3,<span> except in this chapter. You might come across the following error while executing the code that we discuss in this chapter:<br/></span></p>
<pre>Exception in thread "main" java.lang.IllegalStateException: C (result) array is not F order or is a view. Nd4j.gemm requires the result array to be F order and not a view. C (result) array: [Rank: 2,Offset: 0 Order: f Shape: [10,1], stride: [1,10]]<span><br/></span></pre>
<p>At the time of writing, a new version of DL4J has been released that resolves the issue. Hence, we will use version<span> </span>1.0.0-beta 4<strong> </strong>to run the examples in this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extracting time series data</h1>
                </header>
            
            <article>
                
<p>We are using another time series use case, but this time we are targeting time series univariate sequence classification. ETL needs to be discussed before we configure the LSTM neural network. Data extraction is the first phase in the ETL process. This recipe covers data extraction for this use case. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Categorize the sequence data programmatically:</li>
</ol>
<pre style="padding-left: 60px">// convert URI to string<br/> final String data = IOUtils.toString(new URL(url),"utf-8");<br/> // Get sequences from the raw data<br/> final String[] sequences = data.split("\n");<br/> final List&lt;Pair&lt;String,Integer&gt;&gt; contentAndLabels = new ArrayList&lt;&gt;();<br/> int lineCount = 0;<br/> for(String sequence : sequences) {<br/> // Record each time step in new line<br/> sequence = sequence.replaceAll(" +","\n");<br/> // Labels: first 100 examples (lines) are label 0, second 100 examples are label 1, and so on<br/> contentAndLabels.add(new Pair&lt;&gt;(sequence, lineCount++ / 100));<br/> }</pre>
<p class="mce-root"/>
<ol start="2">
<li>Store the features/labels in their corresponding directories by following the numbered format:</li>
</ol>
<pre style="padding-left: 60px">for(Pair&lt;String,Integer&gt; sequencePair : contentAndLabels) {<br/> if(trainCount&lt;450) {<br/> featureFile = new File(trainfeatureDir+trainCount+".csv");<br/> labelFile = new File(trainlabelDir+trainCount+".csv");<br/> trainCount++;<br/> } else {<br/> featureFile = new File(testfeatureDir+testCount+".csv");<br/> labelFile = new File(testlabelDir+testCount+".csv");<br/> testCount++;<br/> }<br/> }<br/><br/></pre>
<ol start="3">
<li>Use <kbd>FileUtils</kbd> to write the data into files:</li>
</ol>
<pre style="padding-left: 60px">FileUtils.writeStringToFile(featureFile,sequencePair.getFirst(),"utf-8");<br/>FileUtils.writeStringToFile(labelFile,sequencePair.getSecond().toString(),"utf-8");</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>When we open the synthetic control data after the download, it will look like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1177 image-border" src="assets/32b32679-b82b-4f8c-9f86-99cf9f128d91.png" style="width:69.67em;height:27.58em;"/></p>
<p class="mce-root"/>
<p><span>A single sequence is marked in the preceding screenshot. There are 600 sequences in total, and each sequence is separated by a new line. In our example, we can split the dataset in such a way that 450 sequences will be used for training and the remaining 150 sequences will be used for evaluation. We are trying to categorize a given sequence against six known classes.</span></p>
<p><span><span><span>Note that this is a univariate time series. The data that is recorded in a single sequence is spread across different time steps. We create separate files for every single sequence. A single data unit (observation) is separated by a space within the file. We will replace spaces with new line characters so that measurements for every time step in a single sequence will appear on a new line. The first 100 sequences represent category 1, and the next 100 sequences represent category 2, and so on. Since we have univariate time series data, there is only one column in the CSV files. So, one single feature is recorded over multiple time steps. </span></span></span></p>
<p><span><span><span>In step 1, t</span></span></span>he <kbd>contentAndLabels</kbd> <span>list </span><span>will have sequence-to-label mappings. Each sequence represents a label. The sequence and label together form a pair. </span></p>
<p>Now we can have two different approaches to splitting data for training/testing purposes:</p>
<ul class="ul1">
<li class="li1">Randomly shuffle the data and take 450 sequences for training and the remaining 150 sequences for evaluation/testing purposes.</li>
<li class="li1">Split the train/test data in such a way that the categories are equally distributed across the dataset. For example, we can have 420 sequences of train data with 70 samples for each of the six categories.</li>
</ul>
<p>We use randomization as a measure to increase the generalization power of the neural network. Every sequence-to-label pair was written to a separate CSV file following the numbered file naming convention.</p>
<p><span>In step 2, we mention that there are 450 samples for training, and the remaining 150 are for evaluation.</span></p>
<p><span>In step 3, we use</span><span> </span><kbd>FileUtils</kbd><span> </span><span><span>from the </span></span>Apache Commons library to wr<span><span>ite the data to a file. The final code will look like the following:</span></span></p>
<pre>for(Pair&lt;String,Integer&gt; sequencePair : contentAndLabels) {<br/>     if(trainCount&lt;traintestSplit) {<br/>       featureFile = new File(trainfeatureDir+trainCount+".csv");<br/>       labelFile = new File(trainlabelDir+trainCount+".csv");<br/>       trainCount++;<br/>     } else {<br/>       featureFile = new File(testfeatureDir+testCount+".csv");<br/>       labelFile = new File(testlabelDir+testCount+".csv");<br/>       testCount++;<br/>     }<br/>    FileUtils.writeStringToFile(featureFile,sequencePair.getFirst(),"utf-8");<br/>    FileUtils.writeStringToFile(labelFile,sequencePair.getSecond().toString(),"utf-8");<br/> }</pre>
<p><span>We fetch the sequence data and add it to the <kbd>features</kbd> directory, and each sequence will be represented by a separate CSV file. Similarly, we add the respecti</span>ve labels <span>to a separate CSV file.</span></p>
<p><kbd>1.csv</kbd> in the <kbd>label</kbd> directory will be the respective label for the <kbd>1.csv</kbd> <span>feature </span>in the <kbd>feature</kbd> dir<span>ectory.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading training data</h1>
                </header>
            
            <article>
                
<p>Data transformation is, as usual, the second phase after data extraction. The time series data we're discussing doesn't have any non-numeric fields or noise (it had already been cleaned). So we can focus on constructing the iterators from the data and loading them directly into the neural network. In this recipe, we will load univariate time series data for neural network training. <span>We have extracted the synthetic control data and stored it in a suitable format so the neural network can process it effortlessly. Every sequence is captured over 60 time steps. In this recipe, we will load the time series data into an appropriate dataset iterator, which can be fed to the neural network for further processing.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Create a <kbd>SequenceRecordReader</kbd> instance to extract and load features from the time series data:</li>
</ol>
<pre style="padding-left: 60px">SequenceRecordReader trainFeaturesSequenceReader = new CSVSequenceRecordReader();<br/> trainFeaturesSequenceReader.initialize(new NumberedFileInputSplit(new File(trainfeatureDir).getAbsolutePath()+"/%d.csv",0,449));</pre>
<ol start="2">
<li>Create a <kbd>SequenceRecordReader</kbd> instance to extract and load labels from the time series data:</li>
</ol>
<pre style="padding-left: 60px">SequenceRecordReader trainLabelsSequenceReader = new CSVSequenceRecordReader();<br/> trainLabelsSequenceReader.initialize(new NumberedFileInputSplit(new File(trainlabelDir).getAbsolutePath()+"/%d.csv",0,449));</pre>
<ol start="3">
<li>Create sequence readers for testing and evaluation:</li>
</ol>
<pre style="padding-left: 60px">SequenceRecordReader testFeaturesSequenceReader = new CSVSequenceRecordReader();<br/> testFeaturesSequenceReader.initialize(new NumberedFileInputSplit(new File(testfeatureDir).getAbsolutePath()+"/%d.csv",0,149));<br/> SequenceRecordReader testLabelsSequenceReader = new CSVSequenceRecordReader();<br/> testLabelsSequenceReader.initialize(new NumberedFileInputSplit(new File(testlabelDir).getAbsolutePath()+"/%d.csv",0,149));|</pre>
<ol start="4">
<li>Use <kbd>SequenceRecordReaderDataSetIterator</kbd> to feed the data into our neural network:</li>
</ol>
<pre style="padding-left: 60px">DataSetIterator trainIterator = new SequenceRecordReaderDataSetIterator(trainFeaturesSequenceReader,trainLabelsSequenceReader,batchSize,numOfClasses);<br/><br/>DataSetIterator testIterator = new SequenceRecordReaderDataSetIterator(testFeaturesSequenceReader,testLabelsSequenceReader,batchSize,numOfClasses);</pre>
<ol start="5">
<li>Rewrite the train<span>/test iterator (with</span> <span><kbd><span>AlignmentMode</span></kbd></span><span>) to support time series of varying lengths:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>DataSetIterator trainIterator = new SequenceRecordReaderDataSetIterator(trainFeaturesSequenceReader,trainLabelsSequenceReader,batchSize,numOfClasses,false, SequenceRecordReaderDataSetIterator.AlignmentMode.ALIGN_END);</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>We have used <kbd><span>NumberedFileInputSplit</span></kbd> in step 1. It is necessary to use <kbd>NumberedFileInputSplit</kbd> to load data from multiple files that follow a numbered file naming convention. Refer to step 1<span> in this recipe:</span></p>
<pre>SequenceRecordReader trainFeaturesSequenceReader = new CSVSequenceRecordReader();<br/> trainFeaturesSequenceReader.initialize(new NumberedFileInputSplit(new File(trainfeatureDir).getAbsolutePath()+"/%d.csv",0,449));</pre>
<p class="mce-root"/>
<p>We stored files as a sequence of numbered files in the previous recipe. There are 450 files, and each one of them represents a sequence. <span>Note that we have stored 150 files for testing as demonstrated in step 3.</span></p>
<p>In step 5,<span> </span><kbd>numOfClasses</kbd><span> specifies the number of categories against which the neural network is trying to make a prediction. In our example, it is <kbd>6</kbd>. W</span>e mentioned <kbd>AlignmentMode.ALIGN_END</kbd> <span>while creating the iterator. The a</span>lignment mode deals with input/labels of varying lengths. For example, our time series data has 60 time steps, and there's only one label at the end of the 60<sup>th</sup> time step. That's the reason why we use <kbd>AlignmentMode.ALIGN_END</kbd> <span>in the iterator definition, as follows: </span></p>
<pre>DataSetIterator trainIterator = new SequenceRecordReaderDataSetIterator(trainFeaturesSequenceReader,trainLabelsSequenceReader,batchSize,numOfClasses,false, SequenceRecordReaderDataSetIterator.AlignmentMode.ALIGN_END);</pre>
<p>We can also have time series data that produces labels at every time step. These cases refer to many-to-many input/label connections. </p>
<p><span>In step 4, we started with the regular way of creating iterators, as follows:</span></p>
<pre>DataSetIterator trainIterator = new SequenceRecordReaderDataSetIterator(trainFeaturesSequenceReader,trainLabelsSequenceReader,batchSize,numOfClasses);<br/><br/>DataSetIterator testIterator = new SequenceRecordReaderDataSetIterator(testFeaturesSequenceReader,testLabelsSequenceReader,batchSize,numOfClasses);</pre>
<p><span>Note that this is not the only way to create sequence reader iterators. There are multiple implementations available i</span>n DataVec to sup<span>port different configurations. We can also align the input/label at the last time step of the sample. For this purpose, we</span> added <kbd>AlignmentMode.ALIGN_END</kbd> <span>into the iterator definition. </span><span>If there are varying time steps, shorter time series will be padded to the length of the longest time series. So, if there are samples that have fewer than 60 time steps recorded for a sequence, then zero values will be padded to the time series data.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Normalizing training data</h1>
                </header>
            
            <article>
                
<p>Data transformation alone may not improve the neural network's efficiency. The existence of large and small ranges of values within the same dataset can lead to overfitting (the model captures noise rather than signals). To avoid these situations, we normalize the dataset, and there are multiple DL4J implementations to do this. The n<span>ormalization process converts and fits the raw time series data into a definite value range, for example, <em>(0, 1)</em>. This will help the neural network process the data with less computational effort. We also discussed normalization in previous chapters, showing that it will reduce favoritism toward any specific label in the dataset while training a neural network.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Create a standard normalizer and fit the data:</li>
</ol>
<pre style="padding-left: 60px">DataNormalization normalization = new NormalizerStandardize();<br/> normalization.fit(trainIterator);</pre>
<ol start="2">
<li>Call the <kbd>setPreprocessor()</kbd> method to normalize the data on the fly:</li>
</ol>
<pre style="padding-left: 60px">trainIterator.setPreProcessor(normalization);<br/> testIterator.setPreProcessor(normalization);<br/><strong><span> <br/></span></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, we used <kbd>NormalizerStandardize</kbd> to normalize the dataset. <kbd>NormalizerStandardize</kbd> norm<span>alizes the data (features) so they have a mean of <em>0</em> and a standard deviation of <em>1</em>. In other words, all the values in the dataset will be normalized within the range of <em>(0, 1)</em>:</span></p>
<pre>DataNormalization normalization = new NormalizerStandardize();<br/> normalization.fit(trainIterator);</pre>
<p><span>This is a standard normalizer in DL4J, although there are other normalizer implementations available in DL4J. </span>Also, note that we don't need to call <kbd>fit()</kbd> on test data because we use the scaling parameters learned during training to scale the test data.</p>
<p class="mce-root"/>
<p><span>W</span><span>e need to call the</span> <kbd>setPreprocessor()</kbd><span> </span><span>method </span><span>as we demonstrated in step 2 for both train/test iterators. </span><span>Once we have set the normalizer using</span> <kbd>setPreprocessor()</kbd><span>, the data returned by the iterator will be auto-normalized using the specified normalizer. Hence it is important to call </span><kbd>setPreprocessor()</kbd> <span>along with the</span> <kbd>fit()</kbd> <span>method. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Constructing input layers for the network</h1>
                </header>
            
            <article>
                
<p>Layer configuration is an important step in neural network configuration. We need to create input layers to receive the univariate time series data that was loaded from disk. In this recipe, we will construct an input layer for our use case. We will also add an LSTM layer as a hidden layer for the neural network. We can use either a computation graph or a regular multilayer network to build the network configuration. In most cases, a regular multilayer network is more than enough; however, we are using a computation graph for our use case. In this recipe, we will configure input layers for the network.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Configure the neural network with default configurations:</li>
</ol>
<pre style="padding-left: 60px">NeuralNetConfiguration.Builder neuralNetConfigBuilder = new NeuralNetConfiguration.Builder();<br/> neuralNetConfigBuilder.seed(123);<br/> neuralNetConfigBuilder.weightInit(WeightInit.XAVIER);<br/> neuralNetConfigBuilder.updater(new Nadam());<br/> neuralNetConfigBuilder.gradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue);<br/> neuralNetConfigBuilder.gradientNormalizationThreshold(0.5);</pre>
<ol start="2">
<li>Specify the input layer labels by calling <kbd>addInputs()</kbd>:</li>
</ol>
<pre style="padding-left: 60px">ComputationGraphConfiguration.GraphBuilder compGraphBuilder = neuralNetConfigBuilder.graphBuilder();<br/> compGraphBuilder.addInputs("trainFeatures");</pre>
<ol start="3">
<li>Add an LSTM layer using the <kbd>addLayer()</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">compGraphBuilder.addLayer("L1", new LSTM.Builder().activation(Activation.TANH).nIn(1).nOut(10).build(), "trainFeatures");</pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, w<span>e specify the default </span><kbd>seed</kbd><span> values, the initial default weights (</span><kbd>weightInit</kbd><span>), the weight </span><kbd>updater</kbd><span>, and so on. We set the gradient normalization strategy to </span><kbd>ClipElementWiseAbsoluteValue</kbd><span>. We have also set the gradient threshold</span> to <kbd>0.5</kbd><span> a</span>s an input to th<span>e</span> <kbd>gradientNormalization</kbd><span> strategy.</span></p>
<p><span>The neural network calculates the gradients across neurons at each layer. We normalized the input data earlier in the</span> <em>Normalizing training data</em> <span>reci</span><span>pe</span>, using a <span>normalizer. It makes sense to mention that we need to normalize the gradient values to achieve data preparation goals. As we can see in step 1, we have used <kbd>ClipElementWiseAbsoluteValue</kbd> gradient normalization. It works in such a way that the absolute value of the gradient cannot be greater than the threshold. For example, if the gradient threshold value is</span> 3<span>, then the value range would be</span> [-3, 3]<span>. Any gradient values that are less than</span> -5 would be treated as -3 and any gradient values that are higher than 3 would be treated as 3. Gradient values in the range [-3, 3] <span>will be unmodified. We have mentioned the gradient normalization strategy as well as the threshold in the network configuration, as shown here:</span></p>
<pre>neuralNetConfigBuilder.gradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue);<br/> neuralNetConfigBuilder.gradientNormalizationThreshold(thresholdValue);</pre>
<p>In step 3, the <kbd>trainFeatures</kbd><span> </span><span>label </span><span>is referred to the input layer label. The inputs are basically the graph vertex objects returned by the </span><kbd>graphBuilder()</kbd><span> method.</span> <span>The specified LSTM layer name (</span><kbd>L1</kbd><span> in our example) in step 2 will be used while configuring the output layer. If there's a mismatch, our program will throw an error during execution saying that the layers are configured in such a way that they are disconnected. We will discuss this in more depth in the next recipe, when we design output layers for the neural network. Note that we have yet to add output layers in the configuration.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Constructing output layers for the network</h1>
                </header>
            
            <article>
                
<p>The very next step after the input/hidden layer design is the output layer design. As we mentioned in earlier chapters, the output layer should reflect the output you want to receive from the neural network. You may need a classifier or a regression model depending on the use case. Accordingly, the output layer has to be configured. The activation function and error function need to be justified for their use in the output layer configuration. <span>This recipe assumes that the neural network configuration has been completed up to the input layer definition. This is going to be the last step in network configuration.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Use <kbd>setOutputs()</kbd> to set the output labels: </li>
</ol>
<pre style="padding-left: 60px">compGraphBuilder.setOutputs("predictSequence");</pre>
<ol start="2">
<li>Construct an output layer using the <kbd>addLayer()</kbd> method and <kbd>RnnOutputLayer</kbd>:</li>
</ol>
<pre style="padding-left: 60px">compGraphBuilder.addLayer("predictSequence", new RnnOutputLayer.Builder(LossFunctions.LossFunction.MCXENT)<br/> .activation(Activation.SOFTMAX).nIn(10).nOut(numOfClasses).build(), "L1");<strong><span><br/></span></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, we have added a <kbd>predictSequence</kbd> <span>label </span><span>for the output layer. Note that we mentioned the input layer reference when defining the output layer. In step 2, we specified it as</span> <kbd>L1</kbd><span>, which is the LSTM input layer created in the previous recipe. We need to mention this to avoid any errors during execution due to disconnection between the LSTM layer and the output layer. Also, the output layer definition should have the <span>same </span>layer name we specified in the </span><kbd>setOutput()</kbd> <span>method.</span></p>
<p>In step 2, we have used <kbd>RnnOutputLayer</kbd> to construct the output layer. This DL4J output layer implementation is used for use cases that involve recurrent neural networks. It is functionally the same as <kbd>OutputLayer</kbd> in multi-layer perceptrons, but output and label reshaping are automatically handled. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating the LSTM network for classified output</h1>
                </header>
            
            <article>
                
<p>Now that we have configured the neural network, the next step is to start the training instance, followed by evaluation. The evaluation phase is very important for the training instance. The neural network will try to optimize the gradients for optimal results. An optimal neural network will have good and stable evaluation metrics. So it is important to evaluate the neural network to direct the training process toward the desired results. We will use the test dataset to evaluate the neural network.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In the previous <span><span>chapter</span></span>, we explored a use case for time series binary classification. Now we have six labels against which to predict. We have discussed various ways to enhance the network's efficiency. We follow the same approach in the next recipe to evaluate the neural network for optimal results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Initialize the <kbd>ComputationGraph</kbd> model configuration using the <kbd>init()</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">ComputationGraphConfiguration configuration = compGraphBuilder.build();<br/>   ComputationGraph model = new ComputationGraph(configuration);<br/> model.init();</pre>
<ol start="2">
<li>Set a score listener to monitor the training process:</li>
</ol>
<pre style="padding-left: 60px">model.setListeners(new ScoreIterationListener(20), new EvaluativeListener(testIterator, 1, InvocationType.EPOCH_END));</pre>
<ol start="3">
<li>Start the training instance by calling the <kbd>fit()</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">model.fit(trainIterator,numOfEpochs);</pre>
<ol start="4">
<li>Call <kbd>evaluate()</kbd> to calculate the evaluation metrics:</li>
</ol>
<pre style="padding-left: 60px">Evaluation evaluation = model.evaluate(testIterator);<br/> System.out.println(evaluation.stats());<strong><br/> <br/></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, we used a computation graph when configuring the neural network's structure. Computation graphs are the best choice for recurrent neural networks. We get an evaluation score of approximately 78% with a multi-layer network and a whopping 94% while using a computation graph. We get better results with <kbd>ComputationGraph</kbd> than the regular multi-layer perceptron. <kbd>ComputationGraph</kbd> is meant for complex network structures and can be customized to accommodate different types of layers in various orders. <kbd>InvocationType.EPOCH_END</kbd> is used (score iteration) in step 1 to call the score iterator at the end of a test iteration.</p>
<p>Note that we're calling the score iterator for every test iteration, and not for the training set iteration. Proper listeners need to be set by calling <kbd>setListeners()</kbd> before your training event starts to log the scores for every test iteration, as shown here:</p>
<pre>model.setListeners(new ScoreIterationListener(20), new EvaluativeListener(testIterator, 1, InvocationType.EPOCH_END));</pre>
<p class="p2">In step 4, the model was evaluated by calling <kbd>evaluate()</kbd>:</p>
<pre>Evaluation evaluation = model.evaluate(testIterator);</pre>
<p>We passed the test dataset to the <kbd>evaluate()</kbd> method in the form of an iterator that was created earlier in the <em>Loading the training data</em> <span>recipe</span>. </p>
<p><span class="s1">Also, we use the </span><kbd><span class="s2">stats()</span></kbd><span class="s1"> method to display the results. For a computation graph with </span><span class="s2">100</span><span class="s1"> epochs, we get the following evaluation metrics:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1423 image-border" src="assets/fc6f1069-4d8d-4155-9feb-fd4c1c779f11.png" style="width:43.75em;height:33.75em;"/></p>
<p class="mce-root"/>
<p class="p1"><span>Now, the following are the experiments you can perform to optimize the results even better.</span></p>
<p class="p1"><span>We used </span>100 epo<span>chs in our example. Reduce the epochs from </span>100 o<span>r increase this setting to a specific value. Note the direction that gives better results. Stop when the results are optimal. We can evaluate the results once in every epoch to understand the direction in which we can proceed. Check out the following training instance logs:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1179 image-border" src="assets/5ac34bbc-fc03-4ba3-97ff-27e9597656b9.png" style="width:80.08em;height:52.00em;"/></p>
<p class="p1 CDPAlignLeft CDPAlign"><span>The accuracy declines after the previous epoch in the preceding example. Accordingly, you can decide on the optimal number of epochs. The neural network will simply memorize the results if we go for large epochs, and this leads to overfitting.</span></p>
<p class="p1 CDPAlignLeft CDPAlign"><span>Instead of randomizing the data at first, you can ensure that the six categories are uniformly distributed across the training set. For example, we can have 420 samples for training and 180 samples for testing. Then, each category will be represented by 70 samples. We can now perform randomization followed by iterator creation. Note that we had 450 samples for training in our example. In this case, the distribution of labels/categories isn't unique and we are totally relying on the randomization of data in this case.</span></p>


            </article>

            
        </section>
    </body></html>