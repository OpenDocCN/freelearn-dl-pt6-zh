<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">What's Next for Deep Learning?</h1>
                </header>
            
            <article>
                
<p class="mce-root">This final chapter will try to give an overview of what's in store for the future of <strong>deep learning</strong> (<strong>DL</strong>) and, more generally, for AI.</p>
<p class="mce-root">We will be covering the following topics in this chapter:</p>
<ul>
<li class="mce-root">DL and AI</li>
<li class="mce-root">Hot topics</li>
<li class="mce-root">Spark and <strong>Reinforcement Learning</strong> (<strong>RL</strong>)</li>
<li class="mce-root">Support for <strong>Generative Adversarial Networks</strong> (<strong>GANs</strong>) in DL4J</li>
</ul>
<p class="mce-root">The rapid advancement of technology not only speeds up the implementation of existing AI ideas, <span>it </span>creates new opportunities in this space that would have been unthinkable one or two years ago. Day by day, AI is finding new practical applications in diverse areas and is radically transforming the way we do business in them. Therefore, it would be impossible to cover all of the new scenarios, so we are going to focus on some particular contexts/areas where we have been directly or indirectly involved.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What to expect next for deep learning and AI</h1>
                </header>
            
            <article>
                
<p>As mentioned previously, there are daily advances in technology and there's a growing availability of greater, but at the same time cheaper, computational power, along with a greater availability of data, which is driving toward the implementation of deeper and more complex models. So, at the same time, the limit for both DL and AI seems to be the sky. Trying to understand what we have to expect for these fields is speculation that could help us clearly understand what would happen in a short period of time (2-3 years), but what can happen next could be less predictable, as it has been observed that any new idea in this space is bringing up other ideas and is contributing to radically transforming ways of doing business in several areas. So, what I am going to describe in this section is related to the immediate future rather than a long-term period.</p>
<p>DL has played a key role in shaping the future of AI. In some areas, such as, for example, image classification and recognition, object detection, and NLP, DL has outperformed ML, but this doesn't mean that ML algorithms became obsolete. For some particular problems, DL is probably overkill, so ML would still be enough. In some other more complex cases, a combination of algorithms (DL and non-DL) have led to significant results;<span> </span>a perfect example is the AlphaGo system (<a href="https://deepmind.com/research/alphago/">https://deepmind.com/research/alphago/</a>) by the DeepMind team, which uses a combination of the <strong>Monte Carlo tree search</strong> (<strong>MCTS</strong>)<span>: </span><a href="http://mcts.ai/about/">http://mcts.ai/about/</a>, with a DL network to quickly search for winning moves. This huge progress in DL has also led to other more complex and advanced techniques such as RL and GANs, which are discussed in the last two sections of this chapter.</p>
<p><span><span class="Linkify">However, while algorithms and models are making incredibly fast progress, there are still plenty of obstacles that require significant human intervention (and extra time) to remove them before data can be taken and turned into machine intelligence. As discussed in the paper <em>Hidden Technical Debt in Machine Learning Systems</em> (<a href="https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf">https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf</a>) by a research group at Google, in DL and ML systems the cost of data dependencies is difficult to detect and it could easily become higher than the cost of code dependencies. The following diagram, which has been taken from the same Google research paper, shows the proportion of the dependencies in ML or DL code versus the rest of the dependencies in an ML or DL system:</span></span></p>
<p> </p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="alignnone size-full wp-image-1119 image-border" src="assets/9b3e64a5-dd2b-48ef-b401-925ae338744e.png" style="width:57.08em;height:17.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 15.1: Only a small fraction (the black rectangle at the center of the image) of real-world ML/DL systems are composed of ML/DL code</div>
<p><span><span class="Linkify">As you can see from the preceding diagram, things such as data collection and the setup and maintenance of the serving infrastructure are more time and money consuming than the model's implementation and training. Therefore, I would expect significant improvements when automating these tasks.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Topics to watch for</h1>
                </header>
            
            <article>
                
<p>In the past few months, a new debate started about the so-called explainable AI, an AI which isn't a sort of black box (where we understand only the underlying mathematical principles) and whose actions or decisions can be easily understood by humans. Criticism has been also been made (in general for AI, but in particular DL) about the generated results from models not being compliant with <strong>GDPR</strong> (short for <strong>General Data Protection Regulation</strong>): <a href="https://ec.europa.eu/commission/priorities/justice-and-fundamental-rights/data-protection/2018-reform-eu-data-protection-rules_en">https://ec.europa.eu/commission/priorities/justice-and-fundamental-rights/data-protection/2018-reform-eu-data-protection-rules_en</a> for data related to EU citizens, or other data regulations that will probably be defined next in other parts of the world, which require the right to an explanation to prevent discriminatory effects based on different factors.</p>
<p>While this is a real hot and not negligible topic, and since several interesting analyze and proposals (such as <a href="https://www.academia.edu/18088836/Defeasible_Reasoning_and_Argument-Based_Systems_in_Medical_Fields_An_Informal_Overview">https://www.academia.edu/18088836/Defeasible_Reasoning_and_Argument-Based_Systems_in_Medical_Fields_An_Informal_Overview</a> from Dr. Luca Longo (<a href="https://ie.linkedin.com/in/drlucalongo">https://ie.linkedin.com/in/drlucalongo</a>) from the Dublin Institute of Technology) have been done, I (and the readers of this book most probably too) have had the chance to listen to a few others' opinions and points of view in predicting a bad future for DL in particular, where DL applications will be restricted to non-business apps and games. In this section, I am not going to make comments on that point of view, which is often based more on opinions than facts, and is sometimes done by people who are not fully involved in production or research projects in the DL or ML spaces. Instead, I would prefer to present a list of practical DL applications that should still stay valid for a while.</p>
<p>Healthcare is one of the sectors that has a higher number of practical applications of AI and DL. Optum (<a href="https://www.optum.com/">https://www.optum.com/</a>), a tech company that's part of the UnitedHealth Group, has achieved, as part of its overall strategy to transform healthcare operations, significant results when applying NLP to several of its business use cases. The ability of AI to understand both structured and unstructured data plays a critical role in medical record review (where most parts of the data are unstructured). Optum's so-called clinically intelligent NLP unlocks the unstructured content to get structured data elements, such as diagnoses, procedures, drugs, labs, and more that make up complete and accurate clinical documentation.</p>
<p>Data from unstructured sources is automatically retrieved through NLP technology that complements the structured data coming through more <em>traditional</em> clinical models and rules engines. This level of automation can accurately identify diagnoses, along with related conditions and procedures to implement the care that's provided, but it is also necessary to define the appropriate reimbursement, quality initiatives, and other critical healthcare operations. But understanding what has been documented in a record is only a part of what makes NLP so valuable in healthcare. Clinically intelligent NLP technology can also identify documentation gaps;<span> </span>it can understand not only what is in a record, but also what is missing. This way, clinicians can get valuable feedback so that they can improve documentation. Other remarkable applications of AI in Optum have been related to payment integrity, simplified population analysis, and call centers.</p>
<p>Another hot topic in AI is robotics. While, technically speaking, it is a separate branch, it has a lot of overlap with AI. Advances in DL and RL provide answers to several questions in robotics. Robots have being defined by first being able to sense, then compute the inputs of their sensors, and finally take action based on the results of those computations. AI comes into play to move them away from an industrial step-and-repeat model and make them smarter.</p>
<p>A perfect example of a successful user story in this direction is the German startup Kewazo (<a href="https://www.kewazo.com/">https://www.kewazo.com/</a>). They have implemented a smart robotic scaffolding transportation system that addresses several problems such as understaffing, efficiency, high costs, time-consuming activities, and worker's safety. AI has made it possible for them to implement a <span>robotic system that, through data about the overall scaffolding assembly process delivered in real time, allows constant control and significant optimization or tuning. AI has also helped Kewazo engineers to identify other use cases, such as roofing or solar panel installations, where their robots can work and help achieve the same results as a scaffolding assembly.<br/></span></p>
<p>The <strong>Internet of Things</strong> (<strong>IoT</strong>) is another area where AI is becoming more pervasive every day. IoT is based on the concept that daily use physical devices are connected to the internet and can communicate with each other to exchange data. The data that's collected could be processed intelligently to make devices smarter. The number of AI and IoT use cases is constantly growing due to the rapidly increasing number of connected devices (and the data that's generated by them).</p>
<p>Among these use cases, I would like to mention the potential of AI for smart buildings. The rapid growth of the Irish economy in the past 5 years, which has been driven by industries such as IT, banking, finance, and pharma, has led to a radical transformation of the area where I work at the present time, the Dublin city center between the Docklands and the Grand Canal Dock. To address the constant increasing need for office space from new or expanding companies, hundreds of new buildings have been built (and many more are coming). All of these recent buildings use some AI, combined with IoT, to become smarter. Significant results have been achieved in the following areas:</p>
<ul>
<li>Making buildings more comfortable for humans</li>
<li>Making building safer for humans</li>
<li>Improving energy savings (and helping the environment)</li>
</ul>
<p>Traditional controllers (for temperature, lights, doors, and so on) use a limited number of sensors to automatically adjust the devices to a constant end result. This paradigm used to leave out an important thing:<span> </span>buildings are occupied by humans, but they are controlled the same, regardless of whether occupants are present or not. This means that things like making the people comfortable or saving energy, just to mention a couple issues, weren't taken into account. IoT combined with AI can add this critical missing piece. Therefore, buildings can have priorities and not simply follow a rigid programming paradigm.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Another interesting real-world use case for IoT and AI is farming. The farming sector (dairy, in particular) is a significant part of the Irish GDP and not a negligible voice in Irish exports. Farming has new and old challenges (such as producing more food on the same acres, meeting strict emissions requirements, protecting plantations from pests, taking the climate into account and global climate change, controlling water flow, monitoring extensive orchards, fighting fires, monitoring soil quality, monitoring the health of animals, and so on). This means that farmers can't rely just on traditional practices. AI, IoT, and IoT-enabled sensors are now helping them in solving the challenges we mentioned previously, and many others. Lots of practical applications of smart farming are in place in Ireland (some of them were presented at the Predict 2018 conference: <a href="https://www.tssg.org/projects/precision-dairy/">https://www.tssg.org/projects/precision-dairy/</a>) and more are to be expected across 2019.</p>
<p>Speaking about AI and IoT, edge analytics is another hot topic. Edge analytics, which is an alternative to traditional big data analytics that is performed in centralized ways, is the analysis of data from some non-central point in a system, such as a connected device or sensor. Several real-world applications of edge analytics are currently in place,<span> </span>but are not restricted to it, in the industry 4.0 space (<a href="https://en.wikipedia.org/wiki/Industry_4.0">https://en.wikipedia.org/wiki/Industry_4.0</a>). Analyzing data as it is generated can decrease latency in the decision-making process on connected devices.</p>
<p>Imagine, for example, a situation where sensor data from a manufacturing system points to the probable failure of a specific part;<span> </span>rules built into a ML or DL algorithm interpreting the data at the network edge can automatically shut down the machine and send an alert to maintenance managers so that that part can be promptly replaced. This can save lot of time compared to transmitting the data to a centralize data location for processing and analysis and reduce, if not avoid, the risk of unplanned machinery downtime.</p>
<p>Edge analytics also brings benefits in terms of scalability. In those cases where the number of connected devices in an organization increases (and the amount of generated and collected data too), by pushing algorithms to sensors and network devices, it is possible to alleviate the processing strain on enterprise data management and centralized analytics systems. There are some promising open source projects in this space to keep an eye on. One is DL4J itself;<span> </span>its mobile features allow multi-layer neural network model definition, training, and inference on Android devices (there's no support for other mobile platforms, since Android is the natural choice as it's a DL4J a framework for the JVM). TensorFlow Lite (<a href="https://www.tensorflow.org/lite/">https://www.tensorflow.org/lite/</a>) enables on‑device ML inference with low latency and a small binary size on several mobile operating systems (Android, iOS, and others) and embedded devices. The latest releases of the StreamSets data collector edge (<a href="https://streamsets.com/products/sdc-edge">https://streamsets.com/products/sdc-edge</a>) allow you to trigger advanced analytics and ML (TensorFlow) in devices (Linux, Android, iOS, Windows, and MacOS are the supported operating systems for it). I would expect much more to come from the open source world on this front.</p>
<p>The rise of DL has led researchers to develop hardware chips that can directly implement neural network architectures. They are designed to mimic the human brain at the hardware level. In a traditional chip, the data needs to be transferred between CPUs and storage blocks, while in an neuromorphic chip, data is both processed and stored in the chip and can generate synapses when required. This second approach results in no time overhead and an energy saving. Therefore, the future of AI would most probably be more neuromorphic than based on CPUs or GPUs. With about 100 billion neurons densely packed into a small volume, the human brain can handle complex computations at lightning speed using very little energy. These past few years saw brain-inspired algorithms that can do things like identify faces, mimic voices, play games, and more. But software is only part of the bigger picture. Our state-of-the-art computers can't really run these powerful algorithms. That's where neuromorphic computing comes into the game.</p>
<p>The scenarios that have been presented in this section definitely confirm that, when considering GDPR or other data regulations, DL and AI definitely wouldn't be restricted to useless applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Is Spark ready for RL?</h1>
                </header>
            
            <article>
                
<p>Throughout this book, we have understood how DL can address several problems in computer vision, natural language processing, and time series forecasting. This combination of DL with RL can lead to more astonishing applications to solve more complex problems. But what is RL? It is a specific area of ML, where agents have to take action to maximize the reward in a given environment. The term reinforcement comes from the similarity of this learning process to what happens when children are incentivized by sweets;<span> </span>the RL algorithms are rewarded when making the right decision and penalized when making a wrong one. RL differs from supervised learning, where the training data brings the answer key with it and a model is then trained with the correct answer itself. In RL, the agents decide what to do to perform the given task and, if no training dataset is available, they are tied to learn only from their experience.</p>
<p>One of the principal practical applications of RL is in computer gaming (one of the best and most popular results is from AlphaGo (<a href="https://deepmind.com/research/alphago/">https://deepmind.com/research/alphago/</a>), from Alphabet's DeepMind team), but it can also be used in other areas such as robotics, industrial automation, chatbot systems, autonomous cars, data processing, and many others.</p>
<p>Let's look at the basics of RL before we understand what the availability of support for it in Apache Spark is and what it could become.</p>
<p>Here are the main concepts:</p>
<ul>
<li><strong>Agent</strong>: It is the algorithm that takes actions.</li>
<li><strong>Action</strong>: It is one of the possible moves that an agent can make.</li>
<li><strong>Discount factor</strong>: It quantifies the difference, in terms of importance, between immediate and future rewards.</li>
<li><strong>Environment</strong>: It is the world through which agents move. The environment takes the agent's current state and action as input. It returns the agent reward and next state as output.</li>
<li><strong>State</strong>: It is a concrete situation in which an agent finds itself.</li>
<li><strong>Reward</strong>: It is the feedback by which the success or failure of an agent's action (which makes a transition from one state to another) can be measured.</li>
<li><strong>Policy</strong>: It is the strategy that an agent follows to determine its next action, based on the current state.</li>
<li><strong>Value</strong>: It is the expected long-term return of the current state under a given policy.</li>
<li><strong>Q-value</strong>: It is similar to value, but it also takes into account the current action.</li>
<li><strong>Trajectory</strong>: It is a sequence of states and actions that influence them.</li>
</ul>
<p>We can summarize RL as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ebe1cc81-df0a-459d-bb4a-6b96cfe1df3c.png" style="width:48.08em;height:37.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 15.2: RL feedback loop</div>
<p>A good example to explain these concepts is the popular Pac-Man video game (<a href="https://en.wikipedia.org/wiki/Pac-Man">https://en.wikipedia.org/wiki/Pac-Man</a>); see the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6280cd9e-4e53-44ba-b03f-eeb290ac771b.png" style="width:34.75em;height:22.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 15.3: The Pac-Man video game</div>
<p>Here, the agent is the Pac-Man character, whose goal is to eat all of the food items in a maze, while avoiding some ghosts that try to kill it. The maze is the environment for the agent. It receives a reward for eating food and punishment (game over) when it gets killed by a ghost. The states are the locations of the agent in the maze. The total cumulative reward is the agent winning the game and moving to the next level. After starting its exploration, Pac-Man (agent) might find one of the four power pellets (which make it invulnerable to the ghosts) near the four corners of the maze and decide to spend all its time exploiting that discovery by continually going around that small portion of the overall maze and never going further into the rest of the environment to pursue the bigger prize. To build an optimal policy, the agent faces the dilemma of exploring new states while maximizing its reward at the same time. This way, it would then miss out on the ultimate reward (moving to the next level). This is called an exploration versus exploitation trade-off.</p>
<p>The most popular algorithms for RL are the <strong>Markov decision process</strong> (<strong>MDP</strong>): <a href="https://en.wikipedia.org/wiki/Markov_decision_process">https://en.wikipedia.org/wiki/Markov_decision_process</a>, <strong>Q-learning</strong> (<a href="https://en.wikipedia.org/wiki/Q-learning">https://en.wikipedia.org/wiki/Q-learning</a>), and <strong>A3C</strong> (<a href="https://arxiv.org/pdf/1602.01783.pdf">https://arxiv.org/pdf/1602.01783.pdf</a>).</p>
<p>Q-learning is widely use in gaming (or gaming-like) spaces. It can be summarized with the following equation (the source code is from the Wikipedia page for Q-learning):</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f16df61e-ff74-4da3-aba9-809885b22226.png" style="width:32.50em;height:1.42em;"/></p>
<p>Here, <em>s<sub>t</sub></em> is the state at time <em>t</em>, <em>a<sub>t</sub></em> is the action taken by the agent, <em>r<sub>t</sub></em> is the reward at time <em>t</em>, <em>s<sub>t+1</sub></em> is the new state (time <em>t+1</em>), <img class="fm-editor-equation" src="assets/645aec1f-4361-4b81-ac15-021c0fae6d8e.png" style="width:0.92em;height:0.83em;"/> is the learning rate (<img class="fm-editor-equation" src="assets/653aec36-6dd8-4058-b906-55647168e2d6.png" style="width:6.00em;height:1.33em;"/>), and <img class="fm-editor-equation" src="assets/d2d3c192-68f8-46c1-afc4-962f9294af48.png" style="width:0.75em;height:1.08em;"/> is the discount factor. This last one determines the importance of future rewards. If it is zero, it will make the agent short-sighted because it means that it will only consider current rewards. If its value is close to one, the agent will work hard to achieve a long-term high reward. If the discount factor value is or exceeds one, then the action values could diverge.</p>
<p>The MLLib component of Apache Spark currently doesn't have any facility for RL and it seems that there is no plan, at the time of writing this book, to implement support for it in future Spark releases. However, there are some open source stable initiatives for RL that integrate with Spark.</p>
<p>The DL4J framework provides a specific module for RL, RL4J, which was originally a separate project. As for all of the other DL4J components, it is fully integrated with Apache Spark. It implements the DQN (Deep Q Learning with double DQN) and AC3 RL algorithms.</p>
<p>Interesting implementations have been done by Yuhao Yang (<a href="https://www.linkedin.com/in/yuhao-yang-8a150232">https://www.linkedin.com/in/yuhao-yang-8a150232</a>) from Intel, which led to the analytics zoo initiative (<a href="https://github.com/intel-analytics/analytics-zoo">https://github.com/intel-analytics/analytics-zoo</a>). Here's the link to the presentation he did at the Spark-AI summit 2018 (<a href="https://databricks.com/session/building-deep-reinforcement-learning-applications-on-apache-spark-using-bigdl">https://databricks.com/session/building-deep-reinforcement-learning-applications-on-apache-spark-using-bigdl</a>). Analytics zoo provides a unified analytics and AI platform that seamlessly puts the Spark, TensorFlow, Keras, and BigDL programs into an integrated pipeline that can scale out to a large Spark cluster for distributed training or inference.</p>
<p>While RL4J, as part of DL4J, provides APIs for the JVM languages (including Scala) and BigDL provides APIs for both Python and Scala, a Python-only, end-to-end, open source platform for large-scale RL is available from Facebook. The name of this platform is Horizon (<a href="https://github.com/facebookresearch/Horizon">https://github.com/facebookresearch/Horizon</a>). It is used by Facebook itself in production to optimize systems in large-scale environments. It supports the discrete-action DQN, parametric-action DQN, double DQN, DDPG (<a href="https://arxiv.org/abs/1509.02971">https://arxiv.org/abs/1509.02971</a>), and SAC (<a href="https://arxiv.org/abs/1801.01290">https://arxiv.org/abs/1801.01290</a>) algorithms. The workflows and algorithms included in this platform have been built on open source frameworks (PyTorch 1.0, Caffe2, and Apache Spark). There's currently no support for their use with other popular Python ML frameworks such as TensorFlow and Keras.</p>
<p>The Ray framework (<a href="https://ray-project.github.io/">https://ray-project.github.io/</a>) by RISELab (<a href="https://rise.cs.berkeley.edu/">https://rise.cs.berkeley.edu/</a>) deserves a special mention. While DL4J and the other frameworks that we mentioned previously work in a distributed mode on top of Apache Spark, in the mind of the Berkley researchers, Ray is a replacement for Spark itself, which is seen by them as more general purpose and not the perfect fit for some real-world AI applications. Ray has been implemented in Python; it is fully compatible with the most popular Python DL frameworks, including TensorFlow and PyTorch; and it allows us to use a combination of more than one of them in the same application.</p>
<p>In the specific case of RL, the Ray framework also provides a dedicated library, RLLib (<a href="https://ray.readthedocs.io/en/latest/rllib.html">https://ray.readthedocs.io/en/latest/rllib.html</a>), which implements the AC3, DQN, evolution strategy (<a href="https://en.wikipedia.org/wiki/Evolution_strategy">https://en.wikipedia.org/wiki/Evolution_strategy</a>), and PPO (<a href="https://blog.openai.com/openai-baselines-ppo/">https://blog.openai.com/openai-baselines-ppo/</a>) algorithms. At the time of writing this book, I am not aware of any real-world AI applications that are using this framework, but I believe it is worth following how it is going to evolve and its level of adoption by the industry.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DeepLearning4J future support for GANs</h1>
                </header>
            
            <article>
                
<p><strong>Generative Adversarial Networks</strong> (<strong>GANs</strong>) are deep neural network architectures that include two nets that are pitted against each other (that's the reason for the <em>adversarial</em> adjective in the name). GAN algorithms are used in unsupervised machine learning. The main focus for GANs is to generate data from scratch. Among the most popular use cases of GANs, there's image generation from text, image-to-image-translation, increasing image resolution to make more realistic pictures, and doing predictions on the next frames of videos.</p>
<p>As we mentioned previously, a GAN is made up of two deep networks, the <strong>generator</strong> and the <strong>discriminator</strong>;<span> </span>the first one generates candidates, while the second one evaluates them. Let's see how generative and discriminative algorithms work at a very high level. Discriminative algorithms try to classify the input data. Therefore, they predict a label or category to which that input data belongs. Their only concern is to map features to labels. Generative algorithms, instead of predicting a label when given certain features, attempt to predict features when given a certain label. Essentially, they do the opposite thing from what the discriminative algorithms do.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Here's how a GAN works. The generator generates new data instances, while the discriminator evaluates them to assess their authenticity. Using the same MNIST dataset (<a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>) that has been considered to illustrate more than one code example throughout this book, let's think of a scenario to make it clear what happens in GANs. Suppose we have the generator generating an MNIST dataset like hand-written numerals and then we're passing them to the discriminator. The goal of the generator is to generate passable hand-written digits without being caught, while the goal of the discriminator is to identify those images coming from the generator as fake hand-written digits. With reference to the following diagram, these are the steps that this GAN takes:</p>
<ol>
<li>The generator net takes some random numbers as input and then returns an image.</li>
<li>The generated image is used to feed the discriminator net alongside a stream of other images that have been taken from the training dataset.</li>
<li>While taking in both real and fake images, the discriminator returns probabilities, which are numbers between zero and one. Zero represents a prediction of fake, while one represents a prediction of authenticity:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ab766101-1770-4c23-a226-532ce41bf581.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 15.4: The typical flow of the MNIST example GAN</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In terms of implementation, the <strong>Discriminator Net</strong> is a standard CNN that can categorize the images fed to it, while the <strong>Generator Net</strong> is an inverse CNN. Both nets try to optimize a different and opposing loss function in a zero-sum game. This model is essentially an actor-critic model (<a href="https://cs.wmich.edu/~trenary/files/cs5300/RLBook/node66.html">https://cs.wmich.edu/~trenary/files/cs5300/RLBook/node66.html</a>), whereas the <strong>Discriminator Net</strong> changes its behavior, so does the generator net, and vice versa.</p>
<p>At the time of writing this book, DL4J doesn't provide any direct API for GANs, but it allows you to import existing Keras (like those you can find it at <a href="https://github.com/eriklindernoren/Keras-GAN">https://github.com/eriklindernoren/Keras-GAN</a>, which is our GitHub repository) or TensorFlow (like this one: <a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/gan.py">https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/gan.py</a>) GAN models and then retrain them and/or make predictions using the DL4J API in a JVM environment (which can include Spark), as explained in <a href="1066b0d4-c2f3-44f9-9cc4-d38469d72c3f.xhtml" target="_blank">Chapter 10</a>, <em>Deploying on a Distributed System</em>, and <a href="0b58f375-cfc1-4b9e-89d1-437ce6eff839.xhtml" target="_blank">Chapter 14</a>, <em>Image Classification</em>. No direct capabilities for GANs are in the immediate plan for DL4J, but the Python model's import is a valid way to train and make inference with them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter wraps up this book. In this book, we got familiar with Apache Spark and its components, and then we moved on to discover the fundamentals of DL before getting practical. We started our Scala hands-on journey with the DL4J framework by understanding how to ingest training and testing data from diverse data sources (in both batch and streaming modes) and transform it into vectors through the DataVec library. The journey then moved on to exploring the details of CNNs and RNNs the implementation of those network models through DL4J, how to train them in a distributed and Spark-based environment, how to get useful insights by monitoring them using the visual facilities of DL4J, and how to evaluate their efficiency and do inference.</p>
<p>We also learned some tips and best practices that we should use when configuring a production environment for training, and how it is possible to import Python models that have been implemented in Keras and/or TensorFlow and make them run (or be retrained) in a JVM-based environment. In the last part of this book, we applied what we learned previously to implementing NLP use cases with DL first and then an end-to-end image classification application.</p>
<p>I hope that all of the readers who went through all of the chapters of this book have reached my initial goal:<span> </span>they have all of the building blocks to start tackling their own specific DL use case scenarios in Scala and/or Python, in a distributed system such as Apache Spark.</p>


            </article>

            
        </section>
    </body></html>