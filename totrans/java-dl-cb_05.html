<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Implementing Natural Language Processing</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we will discuss word vectors (Word2Vec) and paragraph vectors (<span>Doc2Vec</span>) in DL4J. We will develop a complete running example step by step, covering all the stages, such as ETL, model configuration, training, and evaluation. Word2Vec and Doc2Vec are <strong>natural language processing</strong> (<strong>NLP</strong>) implementations in DL4J. <span>It is worth mentioning a lit</span>tle about the bag-of-words algorithm before we talk about Word2Vec.<span><br/></span></p>
<p class="mce-root"><span><strong>Bag-of-words</strong> is an algorithm that counts the instances of words in documents. This will allow us to perform document classification. Bag of words a</span>nd Word2Vec are just t<span>wo different types of text classification. </span><span><strong>Word2Vec</strong> can use </span><span>a bag of words extracted from a documen</span><span>t</span><span> to create vectors</span><span>.</span> In addition to these text classification methods, <span><strong>term frequency–inverse document frequency</strong> (<strong>TF-IDF</strong>) can be used to judge the topic/context of the document.</span> <span>In the case of TF-IDF, a score will be calculated for all the w</span><span>ords, and word counts will be replaced with this score. TF-IDF is a simple scoring scheme, but word embeddings may be a better choice, as the semantic similarity can be captured by word embedding. Also, if your dataset is small an</span><span>d the context is domain-specific, then bag of words may be a better choice than Word2Vec.</span></p>
<p class="mce-root">Word2Vec is a two-layer neural network that processes text. It converts the text corpus to vectors.</p>
<div class="mce-root packt_infobox">Note that Word2Vec is not a <strong>deep neural network</strong> (<strong>DNN</strong>). It transforms text data into a numerical format that a DNN can understand, making customization possible.</div>
<p class="mce-root">We can even combine <span>Word2Vec</span><span> </span>with DNNs to serve this purpose. It doesn't train the input words through reconstruction; instead, it trains words using the neighboring words in the corpus.</p>
<p class="mce-root"/>
<p class="mce-root"><span>Doc2Vec</span> (paragraph vectors) associates documents with labels, and is an extension of Word2Vec. Word2Vec tries to correlate words with words, while <span>Doc2Vec</span> (paragraph vectors) correlates words with labels. Once we represent documents in vector formats, we can then use these formats as an input to a supervised learning algorithm to map these vectors to labels.</p>
<p class="mce-root"><span>In this chapter, we will cover the following recipes:</span></p>
<ul>
<li class="mce-root">Reading and loading text data</li>
<li class="mce-root">Tokenizing data and training the model</li>
<li class="mce-root">Evaluating the model</li>
<li class="mce-root"><span>Generating plots from the model</span></li>
<li class="mce-root">Saving and reloading the model</li>
<li class="mce-root">Importing Google News vectors</li>
<li class="mce-root">Troubleshooting and tuning Word2Vec models</li>
<li class="mce-root">Using Word2Vec for sentence classification using CNNs</li>
<li class="mce-root">Using Doc2Vec for document classification</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p><span>The examples discussed in this chapter can be found at </span><a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples</a>.<a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples"/></p>
<p><span>After cloning our GitHub repository, navigate to the directory called </span><kbd>Java-Deep-Learning-Cookbook/<span>05_Implementing_NLP</span>/sourceCode</kbd>.<strong> </strong><span>Then, import the</span> <kbd>cookbookapp</kbd><span> </span><span>project</span><span> </span><span>as a Maven project</span><strong> </strong><span>by importing </span><kbd>pom.xml</kbd><span>.</span></p>
<p class="mce-root">To get started with NLP in DL4J, add the following Maven dependency in<span> </span><kbd>pom.xml</kbd>:</p>
<pre class="mce-root">&lt;dependency&gt;<br/> &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;<br/> &lt;artifactId&gt;deeplearning4j-nlp&lt;/artifactId&gt;<br/> &lt;version&gt;1.0.0-beta3&lt;/version&gt;<br/> &lt;/dependency&gt;</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data requirements</h1>
                </header>
            
            <article>
                
<p>The project directory has a <kbd>resource</kbd> folder with the required data for the <kbd>LineIterator</kbd> examples:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1430 image-border" src="assets/afff3d5b-69da-41a6-8309-79bab888a6c6.png" style="width:14.25em;height:13.50em;"/></p>
<p class="CDPAlignLeft CDPAlign"><span>For <kbd>CnnWord2VecSentenceClassificationExample</kbd> or <kbd>GoogleNewsVectorExampleYou</kbd>, you can download datasets from the following URLs:</span></p>
<ul>
<li><span><strong>Google News vector</strong>: <a href="https://deeplearning4jblob.blob.core.windows.net/resources/wordvectors/GoogleNews-vectors-negative300.bin.gz">https://deeplearning4jblob.blob.core.windows.net/resources/wordvectors/GoogleNews-vectors-negative300.bin.gz</a><br/></span></li>
<li><span><strong>IMDB review data</strong>: <a href="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz">http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz</a><br/></span></li>
</ul>
<div class="packt_infobox">Note that IMDB review data needs to be extracted twice in order to get the actual dataset folder.</div>
<p>For the <strong>t-Distributed Stochastic Neighbor Embedding</strong> (<strong>t-SNE</strong>) visualization example, the required data (<kbd>words.txt</kbd>) can be located in the project root directory itself.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading and loading text data</h1>
                </header>
            
            <article>
                
<p><span>We need to load raw sentences in text format and iterate them using an underlined iterator that serves the purpose. A text corpus can also be subjected to preprocessing, such as lowercase conversion. Stop words can be mentioned while configuring the </span>Word2Vec model. In this recipe, we will extract and load text data from various data-input scenarios.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Select an iterator approach from step 1 to step 5 depending on what kind of data you're looking for and how you want to load it. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Create a sentence iterator using <kbd>BasicLineIterator</kbd>:</li>
</ol>
<pre style="padding-left: 60px">File file = new File("raw_sentences.txt");<br/>SentenceIterator iterator = new BasicLineIterator(file);</pre>
<p style="padding-left: 60px">For an example, go to <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/BasicLineIteratorExample.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/BasicLineIteratorExample.java</a><span>.</span></p>
<ol start="2">
<li>Create a sentence iterator using <kbd>LineSentenceIterator</kbd>:</li>
</ol>
<pre style="padding-left: 60px">File file = new File("raw_sentences.txt");<br/>SentenceIterator iterator = new LineSentenceIterator(file);</pre>
<p style="padding-left: 60px">For an example, go to <strong><a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/LineSentenceIteratorExample.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/LineSentenceIteratorExample.java</a></strong><span>.</span><strong><br/></strong></p>
<ol start="3">
<li>Create a sentence iterator using <kbd>CollectionSentenceIterator</kbd>:<span><br/></span></li>
</ol>
<pre style="padding-left: 60px">List&lt;String&gt; sentences= Arrays.asList("sample text", "sample text", "sample text");<br/>SentenceIterator iter = new CollectionSentenceIterator(sentences);<span> </span></pre>
<p style="padding-left: 60px">For an example, go to<span> <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CollectionSentenceIteratorExample.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CollectionSentenceIteratorExample.java</a>.<br/></span></p>
<ol start="4">
<li>Create a sentence iterator using <kbd>FileSentenceIterator</kbd>:</li>
</ol>
<pre style="padding-left: 60px">SentenceIterator iter = new FileSentenceIterator(new File("/home/downloads/sentences.txt"));</pre>
<p style="padding-left: 60px">For an example, go to<strong> <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/FileSentenceIteratorExample.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/FileSentenceIteratorExample.java</a><span>.</span><a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/FileSentenceIteratorExample.java"><br/></a></strong></p>
<ol start="5">
<li>Create a sentence iterator using <kbd>UimaSentenceIterator</kbd>.</li>
</ol>
<p style="padding-left: 60px">Add the following Maven dependency:</p>
<pre style="padding-left: 60px">&lt;dependency&gt;<br/> &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;<br/> &lt;artifactId&gt;deeplearning4j-nlp-uima&lt;/artifactId&gt;<br/> &lt;version&gt;1.0.0-beta3&lt;/version&gt;<br/> &lt;/dependency&gt;</pre>
<p style="padding-left: 60px">Then use the iterator, as shown here:</p>
<pre style="padding-left: 60px">SentenceIterator iterator = UimaSentenceIterator.create("path/to/your/text/documents"); </pre>
<p style="padding-left: 60px">You can also use it like this:</p>
<pre style="padding-left: 60px">SentenceIterator iter = UimaSentenceIterator.create("path/to/your/text/documents");</pre>
<p style="padding-left: 60px">For an example, go to <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/UimaSentenceIteratorExample.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/UimaSentenceIteratorExample.java</a><span>.</span></p>
<ol start="6">
<li>Apply the preprocessor to the text corpus:</li>
</ol>
<pre style="padding-left: 60px">iterator.setPreProcessor(new SentencePreProcessor() {<br/> @Override<br/> public String preProcess(String sentence) {<br/> return sentence.toLowerCase();<br/> }<br/> });</pre>
<p style="padding-left: 60px">For an example, go to <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/SentenceDataPreProcessor.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/SentenceDataPreProcessor.java</a><span>.</span><a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/SentenceDataPreProcessor.java"/></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, we used <kbd>BasicLineIterator</kbd>, which is a <span>basic, single-line sentence iterator without any customization involved.</span></p>
<p>In step 2, we used <span><kbd>LineSentenceIterator</kbd> to iterate through multi-sentence text data. Each line is considered a sentence here. We can use them for multiple lines of text.</span></p>
<p><span>I</span>n step 3,<span> <strong><kbd>CollectionSentenceIterator</kbd></strong> will accept a list of strings as text input where each string represents a sentence (document). This can be a list of tweets or articles.</span></p>
<p>In step 4, <span><strong><kbd>FileSentenceIterator</kbd></strong></span><span class="s1"> processes sentences in a file/directory. Sentences will be processed line by line from each file.</span></p>
<p>For anything complex, we recommend that you use <strong><kbd><span>UimaSentenceIterator</span></kbd></strong>, which is a proper machine learning level pipeline. <span>It iterates over a set of files and segments the sentences. The <kbd>UimaSentenceIterator</kbd> pipeline can perform tokenization, lemmatization, and part-of-speech tagging. The behavior can be</span> customized<span> based on the analysis engines that are passed on. This iterator is the best fit for complex data, such as data returned from the Twitter API. An analysis engine is a text-processing pipeline.</span></p>
<div class="packt_infobox"><span>You need to use the </span><kbd>reset()</kbd><span> method if you want to begin the iterator traversal from the beginning after traversing once.<br/>
<br/></span></div>
<p>We can normalize the data and remove anomalies by defining a preprocessor on the data iterator. Hence, we defined a normalizer (preprocessor) in step 5. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>We can also create a sentence iterator using <kbd>UimaSentenceIterator</kbd> by passing an analysis engine, as shown in the following code:</p>
<pre>SentenceIterator iterator = new UimaSentenceIterator(path,AnalysisEngineFactory.createEngine( AnalysisEngineFactory.createEngineDescription(TokenizerAnnotator.getDescription(), SentenceAnnotator.getDescription())));</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The concept of an analysis engine is borrowed from UIMA's text-processing pipeline. DL4J has standard analysis engines available for common tasks that enable further text customization and decide how sentences are defined. Analysis engines are thread safe compared to OpenNLP text-processing pipelines. ClearTK-based pipelines are also used to handle common text-processing tasks in DL4J. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><strong>UIMA</strong>: <a href="http://uima.apache.org/">http://uima.apache.org/</a></li>
<li><strong>OpenNLP</strong>: <a href="http://opennlp.apache.org/">http://opennlp.apache.org/</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tokenizing data and training the model</h1>
                </header>
            
            <article>
                
<p>We need to perform tokenization in order to build the Word2Vec models. The context of a sentence (document) is determined by the words in it. Word2Vec models require words rather than sentences (documents) to feed in, so we need to break the sentence into atomic units and create a token each time a white space is hit. DL4J has a tokenizer factory that is responsible for creating the tokenizer. The <kbd>TokenizerFactory</kbd> generates a tokenizer for the given string. In this recipe, we will tokenize the text data and train the Word2Vec model on top of them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Create a tokenizer factory and set the token preprocessor:</li>
</ol>
<pre style="padding-left: 60px">TokenizerFactory tokenFactory = new DefaultTokenizerFactory();<br/>tokenFactory.setTokenPreProcessor(new CommonPreprocessor());<span><br/></span></pre>
<ol start="2">
<li>Add the tokenizer factory to the Word2Vec model configuration:<span><br/></span></li>
</ol>
<pre style="padding-left: 60px">Word2Vec model = new Word2Vec.Builder()<br/> .minWordFrequency(wordFrequency)<br/> .layerSize(numFeatures)<br/> .seed(seed)<br/> .epochs(numEpochs)<br/> .windowSize(windowSize)<br/> .iterate(iterator)<br/> .tokenizerFactory(tokenFactory)<br/> .build();<span><br/></span></pre>
<p class="mce-root"/>
<ol start="3">
<li>Train the Word2Vec model:</li>
</ol>
<pre style="padding-left: 60px">model.fit();</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, we used <span><kbd>DefaultTokenizerFactory()</kbd> to create the tokenizer factory to tokenize the words. </span><span>This is the default tokenizer f</span>or Word2Vec and it is base<span>d on a string tokenizer, or stream tokenizer. We</span><span> a</span><span>lso used <kbd>CommonPreprocessor</kbd> as the token preprocessor. A preprocessor will remove anomalies from the text corpus. The </span><span class="s1"><kbd>CommonPreprocessor</kbd></span><span class="s2"> is a token preprocessor implementation that removes punctuation marks and converts the text to lowercase. It</span> <span class="s3">uses the </span><kbd>toLowerCase(String)</kbd><span> </span><span class="s3">method and its behavior depends on the default locale.</span></p>
<p><span class="s3">Here are the configurations that we made i</span>n step 2:<span class="s3"><br/></span></p>
<ul class="ul1">
<li class="li1"><kbd>minWordFrequency()</kbd>: <span class="s3">This is the minimum number of times in which a word must exist in the text corpora. In our example, if a word appears fewer than five</span><span class="s3"> times, then it is not learned. Words should occur multiple times in text corpora in order for the model to learn useful features about them. In very large text corpora, it's reasonable to raise the minimum value of word occurrences.</span></li>
<li class="li1"><kbd><span class="s2">layerSize()</span></kbd><span class="s3">: This defines the number of features in a word vector. This is equivalent to the number of dimensions in the feature space. Words represented by </span><span class="s2">100</span><span class="s3"> features become points in a 100-dimensional space.</span></li>
<li class="li1"><kbd><span class="s2">iterate()</span></kbd><span class="s3">: This specifies the batch on which the training is taking place. We can pass in an iterator to convert to word vectors. In our case, we passed in a sentence iterator.</span></li>
<li class="li1"><kbd><span class="s2">epochs()</span></kbd><span class="s3">: This specifies the number of iterations over the training corpus as a whole.<br/></span></li>
<li><kbd>windowSize():</kbd> This defines the context window size.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p><span class="s1">The following are the other tokenizer factory implementations available in DL4J Word2Vec to generate tokenizers for </span><span class="s2">the given input:</span></p>
<ul>
<li><span><kbd><span class="s5">NGramTokenizerFactory</span></kbd></span><span class="s1">: This is the tokenizer factory that creates a tokenizer</span> based on the <em>n</em>-gram model. <em>N</em>-grams are a combination of contiguous words or letters of length <em>n</em> that are present in the text corpus.</li>
<li><kbd><span class="s5">PosUimaTokenizerFactory</span></kbd><span class="s1">: This creates a tokenizer that filters part of the speech tags.</span></li>
<li><kbd><span class="s5">UimaTokenizerFactory</span></kbd><span class="s3">: This creates a tokenizer that uses the UIMA analysis engine for tokenization. </span><span class="s2">The analysis engine</span><span class="s3"> performs an inspection of unstructured information, makes a discovery, and represents semantic content. Unstructured information is included, but is not restricted to text documents.<br/></span></li>
</ul>
<p class="p1"><span class="s2">Here are the inbuilt token</span><span class="s3"> preprocessors (not including <kbd><span>CommonPreprocessor</span></kbd>) available in DL4J:</span></p>
<ul class="ul1">
<li class="li1"><kbd><span class="s5">EndingPreProcessor</span></kbd><span class="s3">: This is a preprocessor that gets rid of word endings in the text corpus—f</span><span class="s3">or example, i</span>t removes <em>s</em>, <em>ed</em>, <em>.</em>, <em>ly</em>, and <em>ing</em> fro<span class="s3">m the text.</span></li>
<li class="li1"><kbd><span class="s5">LowCasePreProcessor</span></kbd><span class="s3">: This is a preprocessor that converts text to lowercase format.</span></li>
<li class="li1"><kbd><span class="s5">StemmingPreprocessor</span></kbd><span class="s3">: This tokenizer preprocessor implements basic cleaning inherited from </span><kbd><span class="s5">CommonPreprocessor</span></kbd><span class="s3"> and performs English porter stemming on tokens.</span></li>
<li class="li1"><kbd><span class="s5">CustomStemmingPreprocessor</span></kbd><span class="s3">: This is the stemming preprocessor that is</span> compatible with different stemming processors defined as lucene/tartarus <kbd>SnowballProgram</kbd>, such as <kbd>RussianStemmer</kbd>, <kbd>DutchStemmer</kbd>, and <kbd>FrenchStemmer</kbd>. This m<span class="s3">eans that it is suitable for</span> <span class="s1">multilanguage stemming.</span></li>
<li class="li3"><kbd><span class="s5">EmbeddedStemmingPreprocessor</span></kbd><span class="s1">: This tokenizer preprocessor uses a given preprocessor and performs English porter stemming on tokens on top of it.</span></li>
</ul>
<p class="p1"><span class="s1">We can also implement our own token preprocessor—for example, a preprocessor to remove all stop words from the tokens.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating the model</h1>
                </header>
            
            <article>
                
<p>We need to check the feature vector quality during the evaluation process. This will give us an idea of the quality of the Word2Vec model that was generated. In this recipe, we will follow two different approaches to evaluate the Word2Vec model.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Find similar words to a given word:</li>
</ol>
<pre style="padding-left: 60px">Collection&lt;String&gt; words = model.wordsNearest("season",10); </pre>
<p style="padding-left: 60px">You will see an <em>n</em> output similar to the following:</p>
<pre style="padding-left: 60px">week<br/>game<br/>team<br/>year<br/>world<br/>night<br/>time<br/>country<br/>last<br/>group</pre>
<ol start="2">
<li>Find the cosine similarity of the given two words:</li>
</ol>
<pre style="padding-left: 60px">double cosSimilarity = model.similarity("season","program");<br/>System.out.println(cosSimilarity);<br/> </pre>
<p style="padding-left: 60px">For the preceding example, the cosine similarity is calculated as follows:</p>
<pre style="padding-left: 60px">0.2720930874347687<span><br/></span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, we found the top <em>n</em> similar words (similar in context) to a given word by calling <kbd>wordsNearest()</kbd>, providing both the input and count <kbd>n</kbd>. The <kbd>n</kbd> count is the number of words that we want to list. <br/>
<br/>
In step 2, we tried to find the similarity of two given words. To do this, we actually calculated the <strong>cosine similarity</strong> between the two <span>given</span><span> </span><span>words. The cosine similarity is one of the useful metrics that we can use to find the similarity between words/documents. </span><span>We converted input words into vectors using our trained model.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p><span>Cosine similarity is the similarity between two nonzero vectors measured by the cosine of the angle between them. This metric measures the orientation instead of the magnitude because cosine similarity calculates the angle between document vectors instead of the word count. If the angle is zero, then the cosine value reaches 1, indicating that they are very similar. If the cosine similarity is near zero, then this indicates that there's less</span> similarity between documents, and the document vectors will be orthogonal (perpendicular) to each other. Also, the documents that are dissimilar to each other will yield a negative cosine similarity. For such documents, cosine similarity can go up to -1, indicating an angle of 1,800 between document vectors.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating plots from the model</h1>
                </header>
            
            <article>
                
<p><span>We have mentioned that we have been using a layer size of</span> <kbd>100</kbd> <span>while training the</span> Word2Vec model. This means that there can be 100 features and, eventually, a 100-dimensional feature space. It is impossible to plot a 100-dimensional space, and therefore we rely on t-SNE to perform dimensionality reduction. In this recipe, we will generate 2D plots from the Word2Vec model.</p>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p><span>For this recipe, r</span>efer to the t-SNE visualization example found at: <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/TSNEVisualizationExample.java">//github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/TSNEVisualizationExample.java</a><span>.</span></p>
<p>The example generates t-SNE plots in a CSV file.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Add the following snippet (at the beginning of the source code) to set the data type for the current JVM runtime:</li>
</ol>
<pre style="padding-left: 60px">Nd4j.setDataType(DataBuffer.Type.DOUBLE);</pre>
<p class="mce-root"/>
<ol start="2">
<li>Write word vectors into a file:</li>
</ol>
<pre style="padding-left: 60px">WordVectorSerializer.writeWordVectors(model.lookupTable(),new File("words.txt"));</pre>
<ol start="3">
<li>Separate the weights of the unique words into their own list using <kbd>WordVectorSerializer</kbd>:</li>
</ol>
<pre style="padding-left: 60px">Pair&lt;InMemoryLookupTable,VocabCache&gt; vectors = WordVectorSerializer.loadTxt(new File("words.txt"));<br/>VocabCache cache = vectors.getSecond();<br/>INDArray weights = vectors.getFirst().getSyn0();<span> <br/></span></pre>
<ol start="4">
<li>Create a list to add all unique words:</li>
</ol>
<pre style="padding-left: 60px"> List&lt;String&gt; cacheList = new ArrayList&lt;&gt;();<br/> for(int i=0;i&lt;cache.numWords();i++){<br/> cacheList.add(cache.wordAtIndex(i));<br/> }</pre>
<ol start="5">
<li>Build a dual-tree t-SNE model for dimensionality reduction using <kbd>BarnesHutTsne</kbd>: <span><br/></span></li>
</ol>
<pre style="padding-left: 60px">BarnesHutTsne tsne = new BarnesHutTsne.Builder()<br/> .setMaxIter(100)<br/> .theta(0.5)<br/> .normalize(false)<br/> .learningRate(500)<br/> .useAdaGrad(false)<br/> .build();<span><br/></span></pre>
<ol start="6">
<li>Establish the t-SNE values and save them to a file:</li>
</ol>
<pre style="padding-left: 60px">tsne.fit(weights);<br/>tsne.saveAsFile(cacheList,"tsne-standard-coords.csv");</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 2, word vectors from the trained model are saved to your local machine for further processing. </p>
<p>In step 3, we extracted data from all the unique word vectors by using <kbd>WordVectorSerializer</kbd>. Basically, this will load an in-memory VocabCache from the mentioned input words. But it doesn't <span>load whole vocab/lookup tables into the memory, so it is capable of processing large vocabularies served over the network.</span></p>
<p>A <kbd>VocabCache</kbd> manages the storage of information required for the Word2Vec lookup table. We need to pass the labels to the t-SNE model, and labels are nothing but the words represented by word vectors.</p>
<p>In step 4, we created a list to add all unique words.</p>
<p>The <kbd>BarnesHutTsne</kbd> phrase is the DL4J implementation class for the dual-tree t-SNE model. <span>The </span><span>Barnes–Hut algorithm takes a dual-tree approximation strategy. It is recommended that you reduce the dimension by up to 50 using another method, such as</span> <strong>principal component analysis</strong><span> (</span><strong>PCA</strong><span>) or similar.</span></p>
<p>In step 5, we used <kbd><span>BarnesHutTsne</span></kbd> to design a t-SNE model for the purpose. This model contained the following components:</p>
<ul>
<li><kbd>theta()</kbd>: This is the Barnes–Hut trade-off parameter.</li>
<li><kbd>useAdaGrad()</kbd>: This is the legacy AdaGrad implementation for use in NLP applications.</li>
</ul>
<p>Once the t-SNE model is designed, we can fit it with weights loaded from words. We can then save the feature plots to an Excel file, as demonstrated in step 6. </p>
<p>The feature coordinates will look like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1206 image-border" src="assets/f566cd82-b2d0-4d28-abf4-e46bf33e8092.png" style="width:21.17em;height:44.08em;"/></p>
<p>We can plot these coordinates using gnuplot or any other third-party libraries. DL4J also supports JFrame-based visualizations.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Saving and reloading the model</h1>
                </header>
            
            <article>
                
<p>Model persistence is a key topic, especially while operating with different platforms. We can also reuse the model for further training (transfer learning) or performing tasks. </p>
<p>In this recipe, we will persist (<span>save and reload)</span> the Word2Vec models. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Save the Word2Vec model using <kbd>WordVectorSerializer</kbd>:<strong> <br/></strong></li>
</ol>
<pre style="padding-left: 60px">WordVectorSerializer.writeWord2VecModel(model, "model.zip");<strong><br/></strong></pre>
<ol start="2">
<li>Reload the Word2Vec model using <kbd>WordVectorSerializer</kbd>: </li>
</ol>
<pre style="padding-left: 60px">Word2Vec word2Vec = WordVectorSerializer.readWord2VecModel("model.zip");</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, the <kbd>writeWord2VecModel()</kbd> method sa<span>ves</span> <span>the Word2Vec model int</span><span>o a compressed ZIP file and sends it to the output stream. It saves the full model, including <kbd>Syn0</kbd> and <kbd>Syn1</kbd>. The <kbd>Syn0</kbd> is the array that holds raw word vectors and is a projection layer that can convert one-hot encoding of a word into a dense embedding vector of the right dimension. The <kbd>Syn1</kbd> array represents the model's internal hidden weights to process the input/output.</span></p>
<p><span>I</span>n step 2, <span>the <kbd>readWord2VecModel()</kbd></span><span>method </span><span>loads the models that are in the following format:</span></p>
<ul>
<li class="mce-root"><span>Binary model, either compressed or not compressed</span></li>
<li>Popular CSV/Word2Vec<span> </span>text format</li>
<li>DL4J compressed format</li>
</ul>
<div class="packt_infobox">Note that only weights will be loaded by this method.</div>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Importing Google News vectors</h1>
                </header>
            
            <article>
                
<p>Google provides a large, pretrained Word2Vec model with around 3 millio<span>n 300-dimension English word vectors</span><span>. </span>It is large enough, and pretrained to display promising results. <span>We will use Google vectors as our input word vectors for the evaluation. You will need at least 8 GB of RAM to run this example. </span>In this recipe, we will import the Google News vectors and then perform an evaluation. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Import the Google News vectors:</li>
</ol>
<pre style="padding-left: 60px">File file = new File("GoogleNews-vectors-negative300.bin.gz");<br/>Word2Vec model = WordVectorSerializer.readWord2VecModel(file);</pre>
<ol start="2">
<li>Run an evaluation on the Google News vectors:</li>
</ol>
<pre style="padding-left: 60px">model.wordsNearest("season",10))</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, the <kbd>readWord2VecModel()</kbd> <span>method </span>is used to load the pretrained Google News vector that was saved in compressed file format.</p>
<p>In step 2, the <kbd>wordsNearest()</kbd> <span>method </span>is used to find the nearest words to the given word based on positive/negative scores.</p>
<p>After performing step 2, we should see the following results:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1207 image-border" src="assets/8e2f1a8c-a69b-4cec-82f7-cc6513924b93.png" style="width:53.58em;height:10.25em;"/></p>
<p><span>You can try this technique using your own inputs to see different results.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p><span>The Google News vector's compressed model file is sized at 1.6 GB. It can take a while to load and evaluate the model. You might observe an </span><kbd><span>OutOfMemoryError</span></kbd><span> error if you're running the code for the first time:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1417 image-border" src="assets/93bb0b0b-11ae-405f-9f12-b31a8ae5f1f8.png" style="width:53.83em;height:15.50em;"/></p>
<p><span>We now need to adjust the <span class="packt_screen">VM options</span> to accommodate more memory for the application. You can adjust the</span> <span class="packt_screen"><span>V</span><span>M options</span></span> <span>in IntelliJ IDE, as shown in the following screenshot. You just need to make sure that you assign enough memory value and restart the application:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1209 image-border" src="assets/ff3fbe8d-7414-42ef-a681-9eb114dc1ba4.png" style="width:50.58em;height:27.17em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Troubleshooting and tuning Word2Vec models</h1>
                </header>
            
            <article>
                
<p>Word2Vec models can be tuned further to produce better results. Runtime errors can happen in situations where there is high memory demand and less resource availability. We need to troubleshoot them to understand why they are happening and take preventative measures. In this recipe, we will troubleshoot Word2Vec models and tune them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Monitor <kbd>OutOfMemoryError</kbd> in the application console/logs to check whether the heap space needs to be increased.</li>
<li>Check your IDE console for out-of-memory errors. If there are out-of-memory errors, then add <span>VM options to your IDE to increase the Java memory heap. </span></li>
<li>Monitor <kbd>StackOverflowError</kbd> while running Word2Vec models. Watch out for the following error:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1418 image-border" src="assets/5b2382d7-5db2-4dae-98d2-795986783447.png" style="width:45.83em;height:13.83em;"/></p>
<p style="padding-left: 60px"><span>This error can happen because of unwanted temporary files in a project.</span><strong><span><br/></span></strong></p>
<ol start="4">
<li>Perform hyperparameter tuning for Word2Vec models. <span>You might need to perform multiple training sessions with different values for the hyperparameters, such as</span> <kbd>layeSize</kbd>, <kbd>windowSize</kbd>, and so on.<span><br/></span></li>
<li>Derive the memory consumption at the code level. Calculate the memory consumption based on the data types used in the code and how much data is being consumed by them. </li>
</ol>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>Out-of-memory errors</span><strong><span><em> </em></span></strong><span>are an indication that </span>VM options<span> need to be adjusted. How you adjust these parameters will depend on the RAM capacity of the hardware. Fo</span>r step 1, if <span>you're using an IDE</span> such as IntelliJ, you can provi<span>de the VM options using VM attr</span>ibutes such as <kbd>-Xmx</kbd>, <kbd>-Xms</kbd>, and so on. VM <span>options can also be used from the command line. <br/></span></p>
<p><span>For example, to increase the maximum memory consumption to 8 GB, you will need to add the </span><kbd><span>-Xmx8G </span></kbd><span><kbd>VM</kbd> argument to your IDE. </span></p>
<p>To mitigate <kbd>StackOverflowError</kbd> mentioned in step 2, we need to delete the temporary files created under the project directory where our Java program is executed. These temporary files should look like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1211 image-border" src="assets/d0394d7e-63de-493b-bd02-525542a51f6e.png" style="width:26.58em;height:7.67em;"/></p>
<p>With regard to step 3, if y<span>ou observ</span>e that your Word2Vec model doesn't h<span>old all the words from the raw text data, then you might be interested in increasing the</span><span> layer size </span><span>of</span> the Word2Vec mod<span>el. This</span><span> </span><kbd>layerSize</kbd><span> </span><span>is nothing but the output vector dimension or the feature space dimension. For example, we had</span> <kbd>layerSize</kbd><span> </span><span>of <kbd>100</kbd> in our code. This means that we can increase it to a larger value, say <kbd>200</kbd>, as a workaround:<br/></span></p>
<pre>Word2Vec model = new Word2Vec.Builder()<br/> .iterate(iterator)<br/> .tokenizerFactory(tokenizerFactory)<br/> .minWordFrequency(5)<br/> .layerSize(200)<br/> .seed(42)<br/> .windowSize(5)<br/> .build();</pre>
<p>If you have a GPU-powered machine, you can use this to accelerate the Word2Vec training time. Just make sure that the dependencies for the DL4J and ND4J backend are added as usual. If the results still don't look right, then make sure there are no normalization issues.</p>
<div class="packt_infobox">Tasks such as <kbd>wordsNearest()</kbd> use normalized weights by default, and others require weights without normalization applied.</div>
<p class="mce-root"/>
<p>With regard to step 4, we can use the conventional approach. The weights matrix<span> </span><span>has the most memory consumption i</span>n Word2Vec. It is calculated as follows: </p>
<p class="CDPAlignCenter CDPAlign"><em>NumberOfWords * NumberOfDimensions * 2 * DataType memory footprint</em></p>
<p><span>For example, if o</span>ur Word2Vec model with 100,000 words uses <kbd>long</kbd> as t<span>he data type, and 100 dimensions, the memory footprint wil</span>l be 100,000 * 100 * 2 * 8 (lo<span>ng data type size) = 160 MB RAM, just for the weights matrix. <br/></span></p>
<p><span>Note that DL4J UI will only provide a high-level overview of memory consumption. <br/>
<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Refer to the official DL4J documentation at <a href="https://deeplearning4j.org/docs/latest/deeplearning4j-config-memory">https://deeplearning4j.org/docs/latest/deeplearning4j-config-memory </a><span>to learn more about memory management</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using Word2Vec for sentence classification using CNNs</h1>
                </header>
            
            <article>
                
<p>Neural networks require numerical inputs to perform their operations as expected. For text inputs, we cannot directly feed text data into a neural network. Since Word2Vec converts text data to vectors, it is possible to exploit Word2Vec so that we can use it <span>with neural networks. We will use a pretrained Google News vector model as a reference and train a CNN network on top of it. At the end of this process, we will develop an IMDB review classifier to classify reviews as positive or negative. As per the paper found at </span><span><a href="https://arxiv.org/abs/1408.5882">https://arxiv.org/abs/1408.5882</a>, combin</span>ing a pretrained Word2Vec model with a CNN will give us <span>better results.</span></p>
<p><span>We will employ custom CNN architecture along with the pretrained word vector model as suggested by Yoon Kim in his 2014 publication, <a href="https://arxiv.org/abs/1408.5882">https://arxiv.org/abs/1408.5882</a>. The architecture is slightly more advanced than standard CNN models. We will also be using two huge datasets, and so the application might require a fair amount of RAM and performance benchmarks to ensure a reliable training duratio</span>n and no <kbd>OutOfMemory</kbd> errors.<span><br/></span></p>
<p>In this recipe, we will perform sentence classification using both Word2Vec and a CNN.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Use the example found at <a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CnnWord2VecSentenceClassificationExample.java">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/05_Implementing_NLP/sourceCode/cookbookapp/src/main/java/com/javadeeplearningcookbook/examples/CnnWord2VecSentenceClassificationExample.java</a> <span>for reference.</span></p>
<p>You should also make sure that you add more Java heap space through changing the VM options—for example, if you have 8 GB of RAM, then you may set <kbd>-Xmx2G -Xmx6G</kbd> as VM arguments.</p>
<p class="CDPAlignLeft CDPAlign">We will extract the IMDB data to start with in step 1. The file structure will look like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1212 image-border" src="assets/e06d569b-8da9-447e-995c-5018afb8c890.png" style="width:42.75em;height:8.92em;"/></p>
<p class="CDPAlignCenter CDPAlign"><span>If we further navigate to the dataset directories, you will see them labeled as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1213 image-border" src="assets/6169b9f6-fc75-4f34-9af6-9a8641177fde.png" style="width:43.42em;height:14.42em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Load the word vector model using <kbd>WordVectorSerializer</kbd>:<span><br/></span></li>
</ol>
<pre style="padding-left: 60px">WordVectors wordVectors = WordVectorSerializer.loadStaticModel(new File(WORD_VECTORS_PATH));<span><br/></span></pre>
<ol start="2">
<li>Create a sentence provider using <kbd>FileLabeledSentenceProvider</kbd>:</li>
</ol>
<pre style="padding-left: 60px"> Map&lt;String,List&lt;File&gt;&gt; reviewFilesMap = new HashMap&lt;&gt;();<br/> reviewFilesMap.put("Positive", Arrays.asList(filePositive.listFiles()));<br/> reviewFilesMap.put("Negative", Arrays.asList(fileNegative.listFiles()));<br/> LabeledSentenceProvider sentenceProvider = new FileLabeledSentenceProvider(reviewFilesMap, rndSeed); </pre>
<ol start="3">
<li>Create train iterators or test iterators using <kbd>CnnSentenceDataSetIterator</kbd> to load the IMDB review data:</li>
</ol>
<pre style="padding-left: 60px">CnnSentenceDataSetIterator iterator = new CnnSentenceDataSetIterator.Builder(CnnSentenceDataSetIterator.Format.CNN2D)<br/> .sentenceProvider(sentenceProvider)<br/> .wordVectors(wordVectors) //we mention word vectors here<br/> .minibatchSize(minibatchSize)<br/> .maxSentenceLength(maxSentenceLength) //words with length greater than this will be ignored.<br/> .useNormalizedWordVectors(false)<br/> .build();<br/> </pre>
<ol start="4">
<li>Create a <kbd>ComputationGraph</kbd> configuration by adding default hyperparameters:</li>
</ol>
<pre style="padding-left: 60px">ComputationGraphConfiguration.GraphBuilder builder = new NeuralNetConfiguration.Builder()<br/> .weightInit(WeightInit.RELU)<br/> .activation(Activation.LEAKYRELU)<br/> .updater(new Adam(0.01))<br/> .convolutionMode(ConvolutionMode.Same) //This is important so we can 'stack' the results later<br/> .l2(0.0001).graphBuilder();<span><br/></span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="5">
<li>Configure layers for <kbd>ComputationGraph</kbd> using the <kbd>addLayer()</kbd> method:<strong><br/></strong></li>
</ol>
<pre style="padding-left: 60px">builder.addLayer("cnn3", new ConvolutionLayer.Builder()<br/> .kernelSize(3,vectorSize) //vectorSize=300 for google vectors<br/> .stride(1,vectorSize)<br/> .nOut(100)<br/> .build(), "input");<br/> builder.addLayer("cnn4", new ConvolutionLayer.Builder()<br/> .kernelSize(4,vectorSize)<br/> .stride(1,vectorSize)<br/> .nOut(100)<br/> .build(), "input");<br/> builder.addLayer("cnn5", new ConvolutionLayer.Builder()<br/> .kernelSize(5,vectorSize)<br/> .stride(1,vectorSize)<br/> .nOut(100)<br/> .build(), "input");</pre>
<ol start="6">
<li>Set the convolution mode to stack the results later:</li>
</ol>
<pre style="padding-left: 60px">builder.addVertex("merge", new MergeVertex(), "cnn3", "cnn4", "cnn5")<span><br/></span></pre>
<ol start="7">
<li>Create a <kbd>ComputationGraph</kbd> model and initialize it:<strong><span><span><br/></span></span></strong></li>
</ol>
<pre style="padding-left: 60px">ComputationGraphConfiguration config = builder.build();<br/> ComputationGraph net = new ComputationGraph(config);<br/>  net.init();</pre>
<ol start="8">
<li>Perform the training using the <kbd>fit()</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">for (int i = 0; i &lt; numEpochs; i++) {<br/> net.fit(trainIterator);<br/> }</pre>
<ol start="9">
<li>Evaluate the results:</li>
</ol>
<pre style="padding-left: 60px">Evaluation evaluation = net.evaluate(testIter);<br/>System.out.println(evaluation.stats());</pre>
<ol start="10">
<li>Retrieve predictions for the IMDB reviews data:</li>
</ol>
<pre style="padding-left: 60px">INDArray features = ((CnnSentenceDataSetIterator)testIterator).loadSingleSentence(contents);<br/> INDArray predictions = net.outputSingle(features);<br/> List&lt;String&gt; labels = testIterator.getLabels();<br/> System.out.println("\n\nPredictions for first negative review:");<br/> for( int i=0; i&lt;labels.size(); i++ ){<br/> System.out.println("P(" + labels.get(i) + ") = " + predictions.getDouble(i));<br/> }</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">In step 1, we used <span><kbd>loadStaticModel()</kbd> to load the model from the given path; however, you can also use <kbd>readWord2VecModel()</kbd>. Unlike <kbd>readWord2VecModel()</kbd>, <kbd>loadStaticModel()</kbd> utilizes host memory.<br/></span></p>
<p class="CDPAlignLeft CDPAlign"><span>I</span>n step 2,<span> <kbd>FileLabeledSentenceProvider</kbd> is used as a data source to load the sentences/documents from the files. We created <kbd>CnnSentenceDataSetIterator</kbd> using the same. </span><kbd>CnnSentenceDataSetIterator</kbd><span> </span>handles the conversion of sentences to training data for CNNs, where each word is encoded using the word vector from the specified word vector model. Sentences and labels are provided by a <kbd>LabeledSentenceProvider</kbd> interface. Different implementations of <kbd>LabeledSentenceProvider</kbd> provide different ways of loading the sentence/documents with labels.</p>
<p class="CDPAlignLeft CDPAlign"><span>In</span> step 3, <span>we created <kbd>CnnSentenceDataSetIterator</kbd> to create train/test dataset iterators. The parameters we configured here are as follows:<br/></span></p>
<ul>
<li class="CDPAlignLeft CDPAlign"><span><kbd>sentenceProvider()</kbd>: Adds a sentence provider (data source) </span>to <kbd>CnnSentenceDataSetIterator</kbd><span><br/></span></li>
<li class="CDPAlignLeft CDPAlign"><span><kbd>wordVectors()</kbd>: Adds a word vector reference to the dataset iterator—for example, the Google News vectors<br/></span></li>
<li class="CDPAlignLeft CDPAlign"><span><kbd>useNormalizedWordVectors()</kbd>: Sets whether normalized word vectors can be used</span></li>
</ul>
<p><span>I</span>n step 5, <span>we created layers for a <kbd>ComputationGraph</kbd> model.</span></p>
<p><span>The <kbd>ComputationGraph</kbd> configuration is a configuration object for neural networks with an arbitrary connection structure. It is analogous to multilayer configuration, but allows considerably greater flexibility for the network architecture.</span></p>
<p><span>We also created multiple convolution layers stacked together with multiple filter widths and feature maps.<br/></span></p>
<p><span>In</span> step 6,<span> </span><kbd><span class="s1">MergeVertex</span></kbd><span class="s2"> performs in-depth concatenation on activation of these three convolution layers.<br/></span></p>
<p><span class="s2">Once all steps </span><span>u</span>p to step 8 are c<span>ompleted</span><span>, w</span><span>e should see the following evaluation metrics:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1214 image-border" src="assets/e363c5d4-89d7-4980-97d2-e1b856408dd5.png" style="width:52.75em;height:28.00em;"/></p>
<p class="CDPAlignLeft CDPAlign">In step 10, <span class="s1"><kbd>contents</kbd> refers to the</span><span class="s2"> content from a single-sentence document in string format.<br/></span></p>
<p class="CDPAlignLeft CDPAlign">For negative review content, we would see the following result after step 9:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1215 image-border" src="assets/424f42b6-b2c7-42a8-8dd9-91e56cf9a770.png" style="width:24.17em;height:6.08em;"/></p>
<p>This means that the document has a 77.8% probability of having a negative sentiment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Initializing word vectors with those retrieved from pretrained unsupervised models is a known method for increasing performance. If you can recall what we have done in this recipe, you will remember that we used pretrained Google News vectors for the same purpose. For a CNN, when applied to text instead of images, we will be dealing with one-dimensional array vectors that represent the text. We perform the same steps, such as convolution and max pooling with feature maps, as discussed in<span> </span><a href="4a688ef9-2dd8-47de-abaf-456fa88bcfc2.xhtml">Chapter 4</a>,<span> </span><em>Building Convolutional Neural Networks</em>. The only difference is that instead of image pixels, we use vectors that represent text. CNN architectures have subsequently shown great results against NLP tasks. The paper found at <a href="https://www.aclweb.org/anthology/D14-1181">https://www.aclweb.org/anthology/D14-1181</a> will contain further insights on this.</p>
<p><span>The network architecture of a computation graph is a directed acyclic graph, where each vertex in the graph is a graph vertex. A graph vertex can be a layer or a vertex that defines a random forward/backward pass functionality. Computation graphs can have a random number of inputs and outputs. We needed to stack multiple convolution layers, which was not possible in the case of a normal CNN architecture.<br/></span></p>
<p><kbd>ComputaionGraph</kbd><span> </span><span>has an option to set the configuration known a</span>s <kbd>convolutionMode</kbd>. <kbd>convolutionMode</kbd> determines the network <span>configuration and </span><span>how the convolution operations should be performed for convolutional and</span> subsampling layers (for a given input size). Network configurations such as <kbd>stride</kbd>/<kbd>padding</kbd>/<kbd>kernelSize</kbd> are a<span>pplicable for a given convolution mode</span><span>. We are setting the convolution mode us</span>ing <kbd>convolutionMode</kbd> because <span>we want to stack the results of all three convolution layers as one and generate the prediction.</span></p>
<p><span>The output sizes for convolutional and subsampling layers are calculated in each dimension as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><em>outputSize = (inputSize - kernelSize + 2*padding) / stride + 1</em><span><br/></span></p>
<p><span>If</span><span> </span><kbd>outputSize</kbd><span> </span><span>is not an integer, an exception will be thrown during the network initialization or forward pass. </span>We have discussed <kbd>MergeVertex</kbd>, which was used to combine the activations of two or more layers. We used <kbd>MergeVertex</kbd><span> to perform the same operation with our convolution layers. The merge will depend on the type of inputs—for example, if we wanted to merge two convolution layers with a sample size (</span><kbd>batchSize</kbd><span>) of <kbd>100</kbd>, and <kbd>depth</kbd> of <kbd>depth1</kbd> and <kbd>depth2</kbd> respectively, then <kbd>merge</kbd> will stack the results where the following applies: </span></p>
<p class="CDPAlignCenter CDPAlign"><em>depth = depth1 + depth2</em></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using Doc2Vec for document classification</h1>
                </header>
            
            <article>
                
<p>Word2Vec correlates words with words, while the purpose of Doc2Vec (also known as paragraph vectors) is to correlate labels with words. We will discuss Doc2Vec in this recipe. Documents are labeled in such a way that the subdirectories under the document's root represent document labels. For example, all finance-related data should be placed under the <kbd>finance</kbd> subdirectory. <span>In this recipe, we will perform document classification using</span> Doc2Vec.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Extract and load the data using <kbd>FileLabelAwareIterator</kbd>:</li>
</ol>
<pre style="padding-left: 60px">LabelAwareIterator labelAwareIterator = new FileLabelAwareIterator.Builder()<br/> .addSourceFolder(new ClassPathResource("label").getFile()).build();<span>  </span></pre>
<ol start="2">
<li>Create a tokenizer using <kbd>TokenizerFactory</kbd>:<span><br/></span></li>
</ol>
<pre style="padding-left: 60px">TokenizerFactory tokenizerFactory = new DefaultTokenizerFactory();<br/>tokenizerFactory.setTokenPreProcessor(new CommonPreprocessor());<span> <br/></span></pre>
<ol start="3">
<li>Create a <kbd>ParagraphVector</kbd> model definition:</li>
</ol>
<pre style="padding-left: 60px">ParagraphVectors paragraphVectors = new ParagraphVectors.Builder()<br/> .learningRate(learningRate)<br/> .minLearningRate(minLearningRate)<br/> .batchSize(batchSize)<br/> .epochs(epochs)<br/> .iterate(labelAwareIterator)<br/> .trainWordVectors(true)<br/> .tokenizerFactory(tokenizerFactory)<br/> .build();<br/> </pre>
<ol start="4">
<li>Train <kbd>ParagraphVectors</kbd> by calling the <kbd>fit()</kbd> method:<span><span><br/></span></span></li>
</ol>
<pre style="padding-left: 60px">paragraphVectors.fit();<span><br/></span></pre>
<ol start="5">
<li>Assign labels to unlabeled data and evaluate the results:</li>
</ol>
<pre style="padding-left: 60px">ClassPathResource unClassifiedResource = new ClassPathResource("unlabeled");<br/> FileLabelAwareIterator unClassifiedIterator = new FileLabelAwareIterator.Builder()<br/> .addSourceFolder(unClassifiedResource.getFile())<br/> .build();</pre>
<ol start="6">
<li>Store the weight lookup table:</li>
</ol>
<pre style="padding-left: 60px">InMemoryLookupTable&lt;VocabWord&gt; lookupTable = (InMemoryLookupTable&lt;VocabWord&gt;)paragraphVectors.getLookupTable();<span><br/></span></pre>
<ol start="7">
<li>Predict labels for every unclassified document, as shown in the following pseudocode:<span><br/></span></li>
</ol>
<pre style="padding-left: 60px">while (unClassifiedIterator.hasNextDocument()) {<br/>//Calculate the domain vector of each document.<br/>//Calculate the cosine similarity of the domain vector with all <br/>//the given labels<br/> //Display the results<br/> }<span><br/></span></pre>
<ol start="8">
<li>Create the tokens from the document and use the iterator to retrieve the document instance:<span><br/></span></li>
</ol>
<pre style="padding-left: 60px">LabelledDocument labelledDocument = unClassifiedIterator.nextDocument();<br/> List&lt;String&gt; documentAsTokens = tokenizerFactory.create(labelledDocument.getContent()).getTokens();<span><br/></span></pre>
<ol start="9">
<li>Use the lookup table to get the vocabulary information (<kbd>VocabCache</kbd>):<span><br/></span></li>
</ol>
<pre style="padding-left: 60px">VocabCache vocabCache = lookupTable.getVocab();<span><br/></span></pre>
<ol start="10">
<li>Count all the instances where the words are matched in <kbd>VocabCache</kbd>:<span><br/></span></li>
</ol>
<pre style="padding-left: 60px">AtomicInteger cnt = new AtomicInteger(0);<br/> for (String word: documentAsTokens) {<br/> if (vocabCache.containsWord(word)){<br/> cnt.incrementAndGet();<br/> }<br/> }<br/> INDArray allWords = Nd4j.create(cnt.get(), lookupTable.layerSize());<span><br/></span></pre>
<ol start="11">
<li>Store word vectors of the matching words in the vocab:</li>
</ol>
<pre style="padding-left: 60px">cnt.set(0);<br/> for (String word: documentAsTokens) {<br/> if (vocabCache.containsWord(word))<br/> allWords.putRow(cnt.getAndIncrement(), lookupTable.vector(word));<br/> }<span><br/></span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<ol start="12">
<li>Calculate the domain vector by calculating the mean of the word embeddings:<span><br/></span></li>
</ol>
<pre style="padding-left: 60px">INDArray documentVector = allWords.mean(0);<span><br/></span></pre>
<ol start="13">
<li>Check the cosine similarity of the document vector with labeled word vectors:</li>
</ol>
<pre style="padding-left: 60px">List&lt;String&gt; labels = labelAwareIterator.getLabelsSource().getLabels();<br/> List&lt;Pair&lt;String, Double&gt;&gt; result = new ArrayList&lt;&gt;();<br/> for (String label: labels) {<br/> INDArray vecLabel = lookupTable.vector(label);<br/> if (vecLabel == null){<br/> throw new IllegalStateException("Label '"+ label+"' has no known vector!");<br/> }<br/> double sim = Transforms.cosineSim(documentVector, vecLabel);<br/> result.add(new Pair&lt;String, Double&gt;(label, sim));<br/> }</pre>
<ol start="14">
<li>Display the results:</li>
</ol>
<pre style="padding-left: 60px"> for (Pair&lt;String, Double&gt; score: result) {<br/> log.info(" " + score.getFirst() + ": " + score.getSecond());<br/> }</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, we created a dataset iterator using <span><kbd>FileLabelAwareIterator</kbd>.</span></p>
<p><span>The </span><kbd>FileLabelAwareIterator</kbd> is a simple filesystem-based <kbd>LabelAwareIterator</kbd> interfa<span>ce. It assumes that you have one or more folders organized in the following way:</span></p>
<ul>
<li><strong>First-level subfolder</strong>: Label name</li>
<li><strong>Second-level subfolder</strong>: The documents for that label</li>
</ul>
<p>Look at the following screenshot for an example of this data structure:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1216 image-border" src="assets/d4d9286f-50f4-431b-b705-d6beb0cb8d00.png" style="width:10.33em;height:6.92em;"/></p>
<p class="mce-root"/>
<p>In step 3, we created <strong><kbd>ParagraphVector</kbd></strong> by adding all required hyperparameters. <span>The purpose of </span><span>paragraph vectors </span><span>is to associate arbitrary documents with labels. Paragrap</span>h vectors are an extension to Word2Vec that learn to correlate labels and words, while Word2Vec corr<span>elates words with other words. We need to define labels for the paragraph vectors to work.</span></p>
<p>For more information on what we did in step 5, refer to the following directory structure (under the <kbd>unlabeled</kbd> directory in the project):</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1217 image-border" src="assets/37bfbc65-3bb3-4cad-acba-462579d829b5.png" style="width:14.25em;height:8.67em;"/></p>
<p>The directory names can be random and no specific labels are required. Our task is to find the proper labels (document classifications) for these documents. <span class="s1">Word embeddings are stored in the lookup table. For any given word, a word vector of numbers will be returned. </span></p>
<p><span>Word embeddings are stored in the lookup table. For any given word, a word vector will be returned from the lookup table.</span></p>
<p><span>I</span>n step 6, <span>we created <kbd>InMemoryLookupTable</kbd> from paragraph </span>vectors. <kbd>InMemoryLookupTable</kbd> is the default word lookup table in DL4J. Basically, the <span>lookup table operates as the hidden layer and the word/document vectors refer to the output. <br/></span></p>
<p>Step 8 to step 12 are solely used for the calculation of the domain vector of each document. </p>
<p>In step 8, we created tokens for the document using the tokenizer that was created in step 2. In step 9, we used the lookup table that was created in step 6 to obtain <kbd>VocabCache</kbd>. <kbd>VocabCache</kbd> stores the <span>information needed to operate the lookup table. We can look up words in the lookup table using <kbd>VocabCache</kbd>. <br/></span></p>
<p><span>I</span>n step 11, <span>we store the word vectors along with the occurrence of a particular word in an </span>INDArray. <span><br/></span></p>
<p>In step 12, we <span>calculated the mean of this </span>INDArray to get the docum<span>ent vector.<br/></span></p>
<div class="packt_infobox">The mean across the zero dimension means that it is calculated across all dimensions.<span> </span></div>
<ol start="5"/>
<p>In step 13, the <span>cosine similarity is calculated by calling the </span><kbd>cosineSim()</kbd><span> method provided by ND4J. We use cosine similarity to calculate the similarity of document vectors. ND4J provides a functional interface to calculate the cosine similarity of two domain vectors. </span><kbd><span>vecLabel</span></kbd><span> represents the document vector for the labels from classified documents. Then, we compared <kbd>vecLabel</kbd> with our unlabeled document vector, </span><kbd>documentVector</kbd><span>. </span></p>
<p>After step 14, you should see an output similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1218 image-border" src="assets/adff8732-4749-4183-9cb0-006c7a32869e.png" style="width:94.50em;height:21.08em;"/></p>
<p><span>We can choose the label that has the higher cosine similarity value. From the preceding screenshots, we can infer that the first document is more likely finance-related content with a 69.7% probability. The second document is more likely health-related content with a 53.2% probability.</span></p>


            </article>

            
        </section>
    </body></html>