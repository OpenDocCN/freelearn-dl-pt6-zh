["```py\ncase class QLAction(from: Int, to: Int) {\n    require(from >= 0, s\"QLAction found from: \n    $from required: >=0\")require(to >= 0, s\"QLAction found to: \n    $to required: >=0\")\n\noverride def toString: String = s\"n\n    Action: state \n    $from => state $to\"\n}\n```", "```py\ncase class QLState[T](id: Int, actions: Seq[QLAction] = List.empty, instance: T) {\n import QLState._check(id)\n final def isGoal: Boolean = actions.nonEmpty\n override def toString: String =s\"state: $id ${actions.mkString(\" \")\n        }\n    nInstance: ${instance.toString}\"\n}\n```", "```py\nprivate[scalaml] class QLSpace[T] protected (states: Seq[QLState[T]], goalIds: Array[Int]) {\n import QLSpace._check(states, goalIds)\n```", "```py\nprivate[this] val statesMap: immutable.Map[Int, QLState[T]] = states.map(st => (st.id, st)).toMap\n```", "```py\nfinal def maxQ(state: QLState[T], policy: QLPolicy): Double = {\n val best=states.filter(_ != state).maxBy(st=>policy.EQ(state.id, st.id))policy.EQ(state.id, best.id)\n    }\n```", "```py\nfinal def getNumStates: Int = states.size\n```", "```py\ndef init(state0: Int): QLState[T] =\n if (state0 < 0) {\n val r = new Random(System.currentTimeMillis \n                + Random.nextLong)states(r.nextInt(states.size - 1))\n        } \n else states(state0)\n```", "```py\nfinal def nextStates(st: QLState[T]): Seq[QLState[T]] =\n if (st.actions.isEmpty)Seq.empty[QLState[T]]\n else st.actions.flatMap(ac => statesMap.get(ac.to))\n```", "```py\nfinal def isGoal(state: QLState[T]): Boolean = goalStates.contains(state.id)\n```", "```py\ndef apply[T](goal: Int,instances: Seq[T],constraints: Option[Int => List[Int]]): QLSpace[T] =             \n    apply(Array[Int](goal), instances, constraints)\n```", "```py\nfinal private[scalaml] class QLData(\n val reward: Double, \n val probability: Double = 1.0) {\n\n import QLDataVar._\n var value: Double = 0.0\n    @inline final def estimate: Double = value * probability\n\n final def value(varType: QLDataVar): Double = varType \n match {\n case REWARD => reward\n case PROB => probability\n case VALUE => value\n            }\noverride def toString: String = s\"nValue= $value Reward= $reward Probability= $probability\"}\n```", "```py\ncase class QLInput(from: Int, to: Int, reward: Double = 1.0, prob: Double = 1.0)\n```", "```py\nfinal private[scalaml] class QLPolicy(val input: Seq[QLInput]) {\n import QLDataVar._QLPolicy.check(input)\n private[this] val qlData = input.map(qlIn => new QLData(qlIn.reward, qlIn.prob))\n private[this] val numStates = Math.sqrt(input.size).toInt\n\n def setQ(from: Int, to: Int, value: Double): Unit = \n        {check(from, to, \"setQ\")qlData(from * numStates + to).value = value}\n\n final def get(from: Int, to: Int, varType: QLDataVar): String\n    {f\"${qlData(from * numStates + to).value(varType)}%2.2f\"}\n\n final def Q(from: Int, to: Int): Double = {check(from, to, \"Q\") qlData(from * numStates + to).value}\n final def EQ(from: Int, to: Int): Double = {check(from, to, \"EQ\") qlData(from * numStates + to).estimate}\n final def R(from: Int, to: Int): Double = {check(from, to, \"R\") qlData(from * numStates + to).reward}\n final def P(from: Int, to: Int): Double = {check(from, to, \"P\") qlData(from * numStates + to).probability}\n\n final def minMaxQ: DblPair = {\n val r = Range(0, numStates)\n val _min = r.minBy(from => r.minBy(Q(from, _)))\n val _max = r.maxBy(from => r.maxBy(Q(from, _)))(_min, _max)}\n\n final def EQ: Vector[DblPair] = {\n import scala.collection.mutable.ArrayBuffer\n val r = Range(0, numStates)r.flatMap(from =>r.map(to => (from, to, Q(from, to)))).map { \n case (i, j, q) => \n if (q > 0.0) (i.toDouble, j.toDouble) \n else (0.0, 0.0) }.toVector}\n\noverride def toString: String = s\"Rewardn${toString(REWARD)}\"\n\ndef toString(varType: QLDataVar): String = {\n val r = Range(1, numStates)r.map(i => r.map(get(i, _, varType)).mkString(\",\")).mkString(\"n\")}\n private def check(from: Int, to: Int, meth: String): Unit = {require(from >= 0 && from <                         numStates,s\"QLPolicy.\n            $meth Found from:\n            $from required >= 0 and < \n            $numStates\")require(to >= 0 && to < numStates,s\"QLPolicy.\n            $meth Found to: $to required >= 0 and < $numStates\")\n}\n```", "```py\nfinal class QLearning[T](conf: QLConfig,qlSpace: QLSpace[T],qlPolicy: QLPolicy)\n extends ETransform[QLState[T], QLState[T]](conf) with Monitor[Double]\n```", "```py\nprivate def train: Option[QLModel] = Try {\n val completions = Range(0, conf.numEpisodes).map(epoch => \n if (heavyLiftingTrain (-1)) 1 else 0)\n        .sum\n        completions.toDouble / conf.numEpisodes\n        }\n    .filter(_ > conf.minCoverage).map(new QLModel(qlPolicy, _)).toOption;\n```", "```py\nprivate def heavyLiftingTrain(state0: Int): Boolean = {\n    @scala.annotation.tailrec\n def search(iSt: QLIndexedState[T]): QLIndexedState[T] = {\n val states = qlSpace.nextStates(iSt.state)\n if (states.isEmpty || iSt.iter >= conf.episodeLength)\n            QLIndexedState(iSt.state, -1)\n else {\n val state = states.maxBy(s => qlPolicy.EQ(iSt.state.id, s.id))\n if (qlSpace.isGoal(state))\n                QLIndexedState(state, iSt.iter)\n\n else {\n val fromId = iSt.state.id\n val r = qlPolicy.R(fromId, state.id)\n val q = qlPolicy.Q(fromId, state.id)\n val nq = q + conf.alpha * (r + conf.gamma * qlSpace.maxQ(state, qlPolicy) - q)\n                count(QVALUE_COUNTER, nq)\n                qlPolicy.setQ(fromId, state.id, nq)\n                search(QLIndexedState(state, iSt.iter + 1))\n                }\n            }\n        }\n\nval finalState = search(QLIndexedState(qlSpace.init(state0), 0))\nif (finalState.iter == -1)\n false else\n    qlSpace.isGoal(finalState.state)\n    }\n}\n```", "```py\nprivate[this] val model: Option[QLModel] = train\n```", "```py\ndef getInput: Seq[QLInput] = qlPolicy.input\n```", "```py\n@inline\nfinaldef getModel: Option[QLModel] = model\n```", "```py\n@inline\nfinaldef isModel: Boolean = model.isDefined\noverride def toString: String = qlPolicy.toString + qlSpace.toString\n```", "```py\n@scala.annotation.tailrec\nprivate def nextState(iSt: QLIndexedState[T]): QLIndexedState[T] = {\n val states = qlSpace.nextStates(iSt.state)\n if (states.isEmpty || iSt.iter >= conf.episodeLength)\n                iSt\n else {\n val fromId = iSt.state.id\n val qState = states.maxBy(s => model.map(_.bestPolicy.EQ(fromId, s.id)).getOrElse(-1.0))\n                nextState(QLIndexedState[T](qState, iSt.iter + 1))\n        }\n}\n```", "```py\ncase class QLConfig(alpha: Double,gamma: Double,episodeLength: Int,numEpisodes: Int,minCoverage: Double) \nextends Config {\nimport QLConfig._check(alpha, gamma, episodeLength, numEpisodes, minCoverage)}\n```", "```py\nprivate[scalaml] object QLConfig {\n private val NO_MIN_COVERAGE = 0.0\n private val MAX_EPISODES = 1000\n\n private def check(alpha: Double,gamma: Double,\n                          episodeLength: Int,numEpisodes: Int,\n                          minCoverage: Double): Unit = {\n                    require(alpha > 0.0 && alpha < 1.0,s\"QLConfig found alpha: $alpha required \n                            > 0.0 and < 1.0\")\n                    require(gamma > 0.0 && gamma < 1.0,s\"QLConfig found gamma $gamma required \n                           > 0.0 and < 1.0\")\n                    require(numEpisodes > 2 && numEpisodes < MAX_EPISODES,s\"QLConfig found \n                            $numEpisodes $numEpisodes required > 2 and < $MAX_EPISODES\")\n                    require(minCoverage >= 0.0 && minCoverage <= 1.0,s\"QLConfig found $minCoverage \n                            $minCoverage required > 0 and <= 1.0\")\n        }\n```", "```py\ndef validateConstraints(numStates: Int, constraint: Int => List[Int]): Boolean = {require(numStates > 1,         s\"QLearning validateConstraints found $numStates states should be >1\")!Range(0,                 \n        numStates).exists(constraint(_).isEmpty)\n}\n```", "```py\noverride def |> : PartialFunction[QLState[T], Try[QLState[T]]] = {\n case st: QLState[T] \n if isModel =>\n            Try(\n if (st.isGoal) st \n else nextState(QLIndexedState[T](st, 0)).state)\n    }\n```", "```py\nclass OptionProperty(timeToExp: Double,volatility: Double,vltyByVol: Double,priceToStrike: Double) {\n val toArray = Array[Double](timeToExp, volatility, vltyByVol, priceToStrike)\n        require(timeToExp > 0.01, s\"OptionProperty time to expiration found $timeToExp required 0.01\")\n    }\n```", "```py\nclass OptionModel(\n    symbol: String,\n    strikePrice: Double,\n    src: DataSource,\n    minExpT: Int,\n    nSteps: Int\n    )\n```", "```py\ncheck(strikePrice, minExpT, nSteps)\n```", "```py\ndef check(strikePrice: Double, minExpT: Int, nSteps: Int): Unit = {\n    require(strikePrice > 0.0, s\"OptionModel.check price found $strikePrice required > 0\")\n    require(minExpT > 2 && minExpT < 16,s\"OptionModel.check Minimum expiration time found $minExpT                     required ]2, 16[\")\n    require(nSteps > 1,s\"OptionModel.check, number of steps found $nSteps required > 1\")\n    }\n```", "```py\nval propsList = (for {\n    price <- src.get(adjClose)\n    volatility <- src.get(volatility)\n    nVolatility <- normalize[Double](volatility)\n    vltyByVol <- src.get(volatilityByVol)\n    nVltyByVol <- normalize[Double](vltyByVol)\n    priceToStrike <- normalize[Double](price.map(p => 1.0 - strikePrice / p))\n    } \n yield {\n        nVolatility.zipWithIndex./:(List[OptionProperty]()) {\n case (xs, (v, n)) =>\n val normDecay = (n + minExpT).toDouble / (price.size + minExpT)\n new OptionProperty(normDecay, v, nVltyByVol(n), priceToStrike(n)) :: xs\n        }\n     .drop(2).reverse\n    }).get\n```", "```py\ndef quantize(o: Array[Double]): Map[Array[Int], Double] = {\n val mapper = new mutable.HashMap[Int, Array[Int]]\n val acc: NumericAccumulator[Int] = propsList.view.map(_.toArray)\n    map(toArrayInt(_)).map(ar => {\n val enc = encode(ar)\n        mapper.put(enc, ar)\n        enc\n            })\n    .zip(o)./:(\n new NumericAccumulator[Int]) {\n case (_acc, (t, y)) => _acc += (t, y); _acc\n            }\n        acc.map {\n case (k, (v, w)) => (k, v / w) }\n            .map { \n case (k, v) => (mapper(k), v) }.toMap\n    }\n```", "```py\nprivate def encode(arr: Array[Int]): Int = arr./:((1, 0)) { \n case ((s, t), n) => (s * nSteps, t + s * n) }._2\n private def toArrayInt(feature: Array[Double]): Array[Int] = feature.map(x => (nSteps *         \n            x).floor.toInt)\n\nfinal class NumericAccumulator[T] \n extends mutable.HashMap[T, (Int, Double)] {\n def +=(key: T, x: Double): Option[(Int, Double)] = {\n val newValue = \n if (contains(key)) (get(key).get._1 + 1, get(key).get._2 + x) \n else (1, x)\n super.put(key, newValue)\n    }\n}\n```", "```py\nval name: String = \"Q-learning\"// Files containing the historical prices for the stock and option\nval STOCK_PRICES = \"/static/IBM.csv\"\nval OPTION_PRICES = \"/static/IBM_O.csv\"// Run configuration parameters\nval STRIKE_PRICE = 190.0 // Option strike price\nval MIN_TIME_EXPIRATION = 6 // Min expiration time for option recorded\nval QUANTIZATION_STEP = 32 // Quantization step (Double => Int)\nval ALPHA = 0.2 // Learning rate\nval DISCOUNT = 0.6 // Discount rate used in Q-Value update equation\nval MAX_EPISODE_LEN = 128 // Max number of iteration for an episode\nval NUM_EPISODES = 20 // Number of episodes used for training.\nval NUM_NEIGHBHBOR_STATES = 3 // No. of states from any other state\n```", "```py\nprivate def run(rewardType: String,quantizeR: Int,alpha: Double,gamma: Double): Int = {\n val sPath = getClass.getResource(STOCK_PRICES).getPath\n val src = DataSource(sPath, false, false, 1).get\n val option = createOptionModel(src, quantizeR)\n\n val oPricesSrc = DataSource(OPTION_PRICES, false, false, 1).get\n val oPrices = oPricesSrc.extract.get\n\n val model = createModel(option, oPrices, alpha, gamma)model.map(m => {if (rewardType != \"Random\")\n    display(m.bestPolicy.EQ,m.toString,s\"$rewardType with quantization order             \n            $quantizeR\")1}).getOrElse(-1)\n}\n```", "```py\nprivate def createOptionModel(src: DataSource, quantizeR: Int): OptionModel =\n new OptionModel(\"IBM\", STRIKE_PRICE, src, MIN_TIME_EXPIRATION, quantizeR)\n```", "```py\ndef createModel(ibmOption: OptionModel,oPrice: Seq[Double],alpha: Double,gamma: Double): Try[QLModel] = {\n val qPriceMap = ibmOption.quantize(oPrice.toArray)\n val numStates = qPriceMap.size\n val neighbors = (n: Int) => {\ndef getProximity(idx: Int, radius: Int): List[Int] = {\n val idx_max =\n if (idx + radius >= numStates) numStates - 1 \n else idx + radius\n val idx_min = \n if (idx < radius) 0 \n else idx - radiusRange(idx_min, idx_max + 1).filter(_ != idx)./:(List[Int]())((xs, n) => n :: xs)}getProximity(n, NUM_NEIGHBHBOR_STATES)\n        }\n val qPrice: DblVec = qPriceMap.values.toVector\n val profit: DblVec = normalize(zipWithShift(qPrice, 1).map {\n case (x, y) => y - x}).get\n val maxProfitIndex = profit.zipWithIndex.maxBy(_._1)._2\n val reward = (x: Double, y: Double) => Math.exp(30.0 * (y - x))\n val probabilities = (x: Double, y: Double) => \n if (y < 0.3 * x) 0.0 \n else 1.0println(s\"$name Goal state index: $maxProfitIndex\")\n if (!QLearning.validateConstraints(profit.size, neighbors))\n thrownew IllegalStateException(\"QLearningEval Incorrect states transition constraint\")\n val instances = qPriceMap.keySet.toSeq.drop(1)\n val config = QLConfig(alpha, gamma, MAX_EPISODE_LEN, NUM_EPISODES, 0.1)\n val qLearning = QLearning[Array[Int]](config,Array[Int](maxProfitIndex),profit,reward,probabilities,instances,Some(neighbors))    val modelO = qLearning.getModel\n if (modelO.isDefined) {\n val numTransitions = numStates * (numStates - 1)println(s\"$name Coverage ${modelO.get.coverage} for $numStates states and $numTransitions transitions\")\n val profile = qLearning.dumpprintln(s\"$name Execution profilen$profile\")display(qLearning)Success(modelO.get)} \n else Failure(new IllegalStateException(s\"$name model undefined\"))\n}\n```", "```py\nval config = QLConfig(alpha, gamma, MAX_EPISODE_LEN, NUM_EPISODES, 0.0)\n```", "```py\nprivate def display(eq: Vector[DblPair],results: String,params: String): Unit = {\n import org.scalaml.plots.{ScatterPlot, BlackPlotTheme, Legend}\n val labels = Legend(name, s\"Q-learning config: $params\", \"States\", \"States\")ScatterPlot.display(eq, \n        labels, new BlackPlotTheme)\n}\n```", "```py\ndef main(args: Array[String]): Unit = {\n run(\"Maximum reward\",QUANTIZATION_STEP, ALPHA, DISCOUNT)\n } \n>>> \nAction: state 71 => state 74\nAction: state 71 => state 73\nAction: state 71 => state 72\nAction: state 71 => state 70\nAction: state 71 => state 69\nAction: state 71 => state 68...Instance: [I@1f021e6c - state: 124\nAction: state 124 => state 125\nAction: state 124 => state 123\nAction: state 124 => state 122\nAction: state 124 => state 121Q-learning Coverage 0.1 for 126 states and 15750 transitions\nQ-learning Execution profile\nQ-Value -> 5.572310105096295, 0.013869013819834967, 4.5746487300071825, 0.4037703812585325, 0.17606260549479869, 0.09205272504875522, 0.023205692430068765, 0.06363082458984902, 50.405283888218435... 6.5530411130514015\nModel: Success(Optimal policy: Reward - 1.00,204.28,115.57,6.05,637.58,71.99,12.34,0.10,4939.71,521.30,402.73, with coverage: 0.1)\n```", "```py\nname := \"PlayML\"version := \"1.0\"\nlazy val `playml` = (project in file(\".\")).enablePlugins(PlayScala)\nresolvers += \"scalaz-bintray\" \nscalaVersion := \"2.11.11\"\nlibraryDependencies ++= Seq(filters, cache, ws, \"org.apache.commons\" % \"commons-math3\" %                 \n        \"3.6\",\"com.typesafe.play\" %% \"play-json\" % \"2.5\",\n        \"org.jfree\" % \"jfreechart\" % \"1.0.17\",\n        \"com.typesafe.akka\" %% \"akka-actor\" % \"2.3.8\",\n        \"org.apache.spark\" %% \"spark-core\" % \"2.1.0\",\n        \"org.apache.spark\" %% \"spark-mllib\" % \"2.1.0\",\n        \"org.apache.spark\" %% \"spark-streaming\" % \"2.1.0\")\n```", "```py\nimport java.nio.file.Paths\nimport org.codehaus.janino.Java\nimport ml.stats.TSeries.{normalize, zipWithShift}\nimport ml.workflow.data.DataSource\nimport ml.trading.OptionModel\nimport ml.Predef.{DblPair, DblVec}\nimport ml.reinforcement.qlearning.{QLConfig, QLModel, QLearning}\nimport scala.util.{Failure, Success, Try}\nimport play.api._\nimport play.api.data.Form\nimport play.api.libs.json._\nimport play.api.mvc._\nimport scala.util.{Failure, Success, Try}\n\nclass API extends Controller {\n protected val name: String = \"Q-learning\"\n private var sPath = Paths.get((s\"${\"public/data/IBM.csv\"}\")).toAbsolutePath.toString\n private var oPath = Paths.get((s\"${\"public/data/IBM_O.csv\"}\")).toAbsolutePath.toString\n\n   // Run configuration parameters\n private var STRIKE_PRICE = 190.0 // Option strike price\n private var MIN_TIME_EXPIRATION = 6 // Minimum expiration time for the option recorded\n private var QUANTIZATION_STEP = 32 // Quantization step (Double => Int)\n private var ALPHA = 0.2 // Learning rate\n private var DISCOUNT = 0.6 // Discount rate used in the Q-Value update equation\n private var MAX_EPISODE_LEN = 128 // Maximum number of iteration for an episode\n private var NUM_EPISODES = 20 // Number of episodes used for training.\n private var MIN_COVERAGE = 0.1\n private var NUM_NEIGHBOR_STATES = 3 // Number of states accessible from any other state\n private var REWARD_TYPE = \"Maximum reward\"\n private var ret = JsObject(Seq())\n private var retry = 0\n\n private def run(REWARD_TYPE: String,quantizeR: Int,alpha: Double,gamma: Double) = {\n val maybeModel = createModel(createOptionModel(DataSource(sPath, false, false, 1).get, quantizeR),             DataSource(oPath, false, false, 1).get.extract.get, alpha, gamma)\n if (maybeModel != None) {\n val model = maybeModel.get\n if (REWARD_TYPE != \"Random\") {\n var value = JsArray(Seq())\n var x = model.bestPolicy.EQ.distinct.map(x => {value = value.append(JsObject(Seq(\"x\" ->                     JsNumber(x._1), \"y\" -> JsNumber(x._2))))})ret = ret.+(\"OPTIMAL\", value)\n                }\n            }\n        }\n/** Create an option model for a given stock with default strike and minimum expiration time parameters.\n*/\n privatedef createOptionModel(src: DataSource, quantizeR: Int): OptionModel =\n new OptionModel(\"IBM\", STRIKE_PRICE, src, MIN_TIME_EXPIRATION, quantizeR)\n/** Create a model for the profit and loss on an option given\n* the underlying security. The profit and loss is adjusted to\n* produce positive values.\n*/\n privatedef createModel(ibmOption: OptionModel,oPrice: Seq[Double],alpha: Double,gamma: Double): Option[QLModel] = {\n val qPriceMap = ibmOption.quantize(oPrice.toArray)\n val numStates = qPriceMap.size\n val neighbors = (n: Int) => {\n def getProximity(idx: Int, radius: Int): List[Int] = {\n val idx_max = if (idx + radius >= numStates) numStates - 1\n            else idx + radius\n val idx_min = if (idx < radius) 0 \n                        else idx - radiusscala.collection.immutable.Range(idx_min, idx_max + 1)\n                            .filter(_ != idx)./:(List[Int]())((xs, n) => n :: xs)\n                        }\n                getProximity(n, NUM_NEIGHBOR_STATES)\n            }\n       // Compute the minimum value for the profit, loss so the maximum loss is converted to a null profit\n val qPrice: DblVec = qPriceMap.values.toVector\n val profit: DblVec = normalize(zipWithShift(qPrice, 1).map {\n        case (x, y) => y - x }).get\n val maxProfitIndex = profit.zipWithIndex.maxBy(_._1)._2\n val reward = (x: Double, y: Double) => Math.exp(30.0 * (y - x))\n\n val probabilities = (x: Double, y: Double) =>\n             if (y < 0.3 * x) 0.0 else 1.0ret = ret.+(\"GOAL_STATE_INDEX\", JsNumber(maxProfitIndex))\n if (!QLearning.validateConstraints(profit.size, neighbors)) {ret = ret.+(\"error\",                             JsString(\"QLearningEval Incorrect states transition constraint\"))\n\n thrownew IllegalStateException(\"QLearningEval Incorrect states transition constraint\")}\n\n val instances = qPriceMap.keySet.toSeq.drop(1)\n val config = QLConfig(alpha, gamma, MAX_EPISODE_LEN, NUM_EPISODES, MIN_COVERAGE)\n val qLearning = QLearning[Array[Int]](config,Array[Int]                \n                (maxProfitIndex),profit,reward,probabilities,instances,Some(neighbors))    \n            val modelO = qLearning.getModel\n\n if (modelO.isDefined) {\n val numTransitions = numStates * (numStates - 1)ret = ret.+(\"COVERAGE\",             \n                JsNumber(modelO.get.coverage))ret = ret.+(\"COVERAGE_STATES\", JsNumber(numStates))\n                ret = ret.+(\"COVERAGE_TRANSITIONS\", JsNumber(numTransitions))\n var value = JsArray()\n var x = qLearning._counters.last._2.distinct.map(x => {value = value.append(JsNumber(x))\n                })    \n                ret = ret.+(\"Q_VALUE\", value)modelO\n                } \n else {\n                if (retry > 5) {ret = ret.+(\"error\", JsString(s\"$name model undefined\"))\n return None\n                 }\n                retry += 1Thread.sleep(500)\n return createModel(ibmOption,oPrice,alpha,gamma)\n            }        \n        }\ndef compute = Action(parse.anyContent) { request =>\n try {\n        if (request.body.asMultipartFormData != None) {\n val formData = request.body.asMultipartFormData.get\n if (formData.file(\"STOCK_PRICES\").nonEmpty && formData.file(\"STOCK_PRICES\").get.filename.nonEmpty)sPath = formData.file(\"STOCK_PRICES\").get.ref.file.toString\n if (formData.file(\"OPTION_PRICES\").nonEmpty && formData.file(\"OPTION_PRICES\").get.filename.nonEmpty)oPath = formData.file(\"OPTION_PRICES\").get.ref.file.toString\n val parts = formData.dataParts\n if (parts.get(\"STRIKE_PRICE\") != None)STRIKE_PRICE = parts.get(\"STRIKE_PRICE\").get.mkString(\"\").toDouble\n if (parts.get(\"MIN_TIME_EXPIRATION\") != None)MIN_TIME_EXPIRATION = parts.get(\"MIN_TIME_EXPIRATION\").get.mkString(\"\").toInt\n if (parts.get(\"QUANTIZATION_STEP\") != None)QUANTIZATION_STEP = parts.get(\"QUANTIZATION_STEP\").get.mkString(\"\").toInt\n if (parts.get(\"ALPHA\") != None)ALPHA = parts.get(\"ALPHA\").get.mkString(\"\").toDouble\n if (parts.get(\"DISCOUNT\") != None)DISCOUNT = parts.get(\"DISCOUNT\").get.mkString(\"\").toDouble\n if (parts.get(\"MAX_EPISODE_LEN\") != None)MAX_EPISODE_LEN = parts.get(\"MAX_EPISODE_LEN\").get.mkString(\"\").toInt\n if (parts.get(\"NUM_EPISODES\") != None)NUM_EPISODES = parts.get(\"NUM_EPISODES\").get.mkString(\"\").toInt\n if (parts.get(\"MIN_COVERAGE\") != None)MIN_COVERAGE = parts.get(\"MIN_COVERAGE\").get.mkString(\"\").toDouble\n if (parts.get(\"NUM_NEIGHBOR_STATES\") != None)NUM_NEIGHBOR_STATES = parts.get(\"NUM_NEIGHBOR_STATES\").get.mkString(\"\").toInt\n if (parts.get(\"REWARD_TYPE\") != None)REWARD_TYPE = parts.get(\"REWARD_TYPE\").get.mkString(\"\")\n            }\n        ret = JsObject(Seq(\"STRIKE_PRICE\" ->\n        JsNumber(STRIKE_PRICE),\"MIN_TIME_EXPIRATION\" -> JsNumber(MIN_TIME_EXPIRATION),\n        \"QUANTIZATION_STEP\" -> \nJsNumber(QUANTIZATION_STEP),\n        \"ALPHA\" -> JsNumber(ALPHA),\n        \"DISCOUNT\" -> JsNumber(DISCOUNT),\n        \"MAX_EPISODE_LEN\" -> \nJsNumber(MAX_EPISODE_LEN),\n        \"NUM_EPISODES\" -> JsNumber(NUM_EPISODES),\n        \"MIN_COVERAGE\" -> JsNumber(MIN_COVERAGE),\n        \"NUM_NEIGHBOR_STATES\" -> \nJsNumber(NUM_NEIGHBOR_STATES),\n        \"REWARD_TYPE\" -> JsString(REWARD_TYPE)))\n        run(REWARD_TYPE, QUANTIZATION_STEP, ALPHA, DISCOUNT)\n    }\n catch {\n        case e: Exception => {\n            ret = ret.+(\"exception\", JsString(e.toString))\n                }\n            }\n```", "```py\n       Ok(ret)\n    }\n}\n```", "```py\nangular.module(\"App\", ['chart.js']).controller(\"Ctrl\", ['$scope', '$http', function ($scope, $http) {\n// First we initialize the form:\n$scope.form = {REWARD_TYPE: \"Maximum reward\",NUM_NEIGHBOR_STATES: 3,STRIKE_PRICE: 190.0,MIN_TIME_EXPIRATION: 6,QUANTIZATION_STEP: 32,ALPHA: 0.2,DISCOUNT: 0.6,MAX_EPISODE_LEN: 128,NUM_EPISODES: 20,MIN_COVERAGE: 0.1\n};\n```", "```py\n$scope.run = function () {\n    var formData = new FormData(document.getElementById('form'));\n    $http.post('/api/compute', formData, {\n    headers: {'Content-Type': undefined}}).then(function successCallback(response) {\n    $scope.result = response.data;\n    $('#canvasContainer').html('');\n\n    if (response.data.OPTIMAL) {\n        $('#canvasContainer').append('<canvas id=\"optimalCanvas\"></canvas>')\n        Chart.Scatter(document.getElementById(\"optimalCanvas\").getContext(\"2d\"), {data: { datasets:             [{data: response.data.OPTIMAL}] }, options: {...}});}if (response.data.Q_VALUE) {\n        $('#canvasContainer').append('<canvas id=\"valuesCanvas\"></canvas>')\n        Chart.Line(document.getElementById(\"valuesCanvas\").getContext(\"2d\"), {\n        data: { labels: new Array(response.data.Q_VALUE.length), datasets: [{\n        data: response.data.Q_VALUE }] }, options: {...}});}});}}]\n    );\n```", "```py\n$ /path/to/bin/<project-name> -Dhttp.port=9000\n```", "```py\n$ unzip APP-NAME-SNAPSHOT.zip$ APP-NAME-SNAPSHOT /bin/ APP-NAME -Dhttp.port=9000\n```", "```py\n$ /path/to/bin/<project-name> -Dhttp.port=9000\n```"]