["```py\n    from gensim.models import word2vec\n    ```", "```py\n    sentences = word2vec.Text8Corpus('text8')\n    ```", "```py\n    model = word2vec.Word2Vec(sentences, size = 200)\n    ```", "```py\n    model.most_similar(['man'])\n    ```", "```py\n    model.most_similar(['girl', 'father'], ['boy'], topn=3)\n    ```", "```py\n    import nltk\n    nltk.download('treebank')\n    tagged_sentences = nltk.corpus.treebank.tagged_sents()\n    print(tagged_sentences[0])\n    print(\"Tagged sentences: \", len(tagged_sentences))\n    print (\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))\n    ```", "```py\n    def features(sentence, index):\n        \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n        return {\n            'word': sentence[index],\n            'is_first': index == 0,\n            'is_last': index == len(sentence) - 1,\n            'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n            'is_all_caps': sentence[index].upper() == sentence[index],\n            'is_all_lower': sentence[index].lower() == sentence[index],\n            'prefix-1': sentence[index][0],\n            'prefix-2': sentence[index][:2],\n            'prefix-3': sentence[index][:3],\n            'suffix-1': sentence[index][-1],\n            'suffix-2': sentence[index][-2:],\n            'suffix-3': sentence[index][-3:],\n            'prev_word': '' if index == 0 else sentence[index - 1],\n            'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n            'has_hyphen': '-' in sentence[index],\n            'is_numeric': sentence[index].isdigit(),\n            'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n        }\n    import pprint \n    pprint.pprint(features(['This', 'is', 'a', 'sentence'], 2))\n\n    {'capitals_inside': False,\n     'has_hyphen': False,\n     'is_all_caps': False,\n     'is_all_lower': True,\n     'is_capitalized': False,\n     'is_first': False,\n     'is_last': False,\n     'is_numeric': False,\n     'next_word': 'sentence',\n     'prefix-1': 'a',\n     'prefix-2': 'a',\n     'prefix-3': 'a',\n     'prev_word': 'is',\n     'suffix-1': 'a',\n     'suffix-2': 'a',\n     'suffix-3': 'a',\n     'word': 'a'}\n    ```", "```py\n    def untag(tagged_sentence):\n        return [w for w, t in tagged_sentence]\n    ```", "```py\n    # Split the dataset for training and testing\n    cutoff = int(.75 * len(tagged_sentences))\n    training_sentences = tagged_sentences[:cutoff]\n    test_sentences = tagged_sentences[cutoff:]\n\n    print(len(training_sentences))   # 2935\n    print(len(test_sentences))      # 979\n     and create a function to assign the features to 'X' and append the POS tags to 'Y'.\n    def transform_to_dataset(tagged_sentences):\n        X, y = [], []\n\n        for tagged in tagged_sentences:\n            for index in range(len(tagged)):\n                X.append(features(untag(tagged), index))\n                y.append(tagged[index][1])\n\n        return X, y\n\n    X, y = transform_to_dataset(training_sentences)\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.feature_extraction import DictVectorizer\n    from sklearn.pipeline import Pipeline\n    ```", "```py\n    clf = Pipeline([\n        ('vectorizer', DictVectorizer(sparse=False)),\n        ('classifier', DecisionTreeClassifier(criterion='entropy'))\n    ])\n\n    clf.fit(X[:10000], y[:10000])   # Use only the first 10K samples if you're running it multiple times. It takes a fair bit :)\n\n    print('Training completed')\n\n    X_test, y_test = transform_to_dataset(test_sentences)\n\n    print(\"Accuracy:\", clf.score(X_test, y_test))\n    ```", "```py\n    import nltk\n    nltk.download('treebank')\n    nltk.download('maxent_ne_chunker')\n    nltk.download('words')\n    ```", "```py\n    nltk.corpus.treebank.tagged_sents()\n    sent = nltk.corpus.treebank.tagged_sents()[0]\n    print(nltk.ne_chunk(sent, binary=True))\n    ```", "```py\n    sent = nltk.corpus.treebank.tagged_sents()[1]\n    ```", "```py\n    print(nltk.ne_chunk(sent, binary=False))\n    sent = nltk.corpus.treebank.tagged_sents()[2]\n    rint(nltk.ne_chunk(sent))\n    ```", "```py\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    dataset = pd.read_csv('train_comment_small_100.csv', sep=',')\n    ```", "```py\n    import re\n    import nltk\n    nltk.download('stopwords')\n    from nltk.corpus import stopwords\n    from nltk.stem.porter import PorterStemmer\n    corpus = []\n    ```", "```py\n    for i in range(0, dataset.shape[0]-1):\n        review = re.sub('[^a-zA-Z]', ' ', dataset['comment_text'][i])\n        review = review.lower()\n        review = review.split()\n    ps = PorterStemmer()\n        review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n        review = ' '.join(review)\n        corpus.append(review)\n    ```", "```py\n    from sklearn.feature_extraction.text import CountVectorizer\n    cv = CountVectorizer(max_features = 20)\n    ```", "```py\n    X = cv.fit_transform(corpus).toarray()\n    y = dataset.iloc[:,0]\n    y1 = y[:99]\n    y1\n    ```", "```py\n    from sklearn import preprocessing\n    labelencoder_y = preprocessing.LabelEncoder()\n    y = labelencoder_y.fit_transform(y1)\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n    ```", "```py\n    from sklearn.preprocessing import StandardScaler\n    sc = StandardScaler()\n    X_train = sc.fit_transform(X_train)\n    X_test = sc.transform(X_test)\n    ```", "```py\n    import tensorflow\n    import keras\n    from keras.models import Sequential\n    from keras.layers import Dense\n    ```", "```py\n    classifier = Sequential()\n    classifier.add(Dense(output_dim = 20, init = 'uniform', activation = 'relu', input_dim = 20))\n    classifier.add(Dense(output_dim =20, init = 'uniform', activation = 'relu'))\n    classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'softmax'))\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    ```", "```py\n    classifier.fit(X_train, y_train, batch_size = 3, nb_epoch = 5)\n    X_test\n    ```", "```py\n    y_pred = classifier.predict(X_test)\n    scores = classifier.evaluate(X_test, y_pred, verbose=1)\n    print(\"Accuracy:\", scores[1])\n    ```", "```py\n    from sklearn.metrics import confusion_matrix\n    cm = confusion_matrix(y_test, y_pred)\n    scores\n    ```", "```py\n    from keras.preprocessing.text import Tokenizer\n    from keras.models import Sequential\n    from keras import layers\n    from keras.preprocessing.sequence import pad_sequences\n    import numpy as np\n    import pandas as pd\n    ```", "```py\n    epochs = 20\n    maxlen = 100\n    embedding_dim = 50\n    num_filters = 64\n    kernel_size = 5\n    batch_size = 32\n    ```", "```py\n    data = pd.read_csv('data/sentiment labelled sentences/yelp_labelled.txt',names=['sentence', 'label'], sep='\\t')\n    data.head()\n    ```", "```py\n    sentences=data['sentence'].values\n    labels=data['label'].values\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(\n        sentences, labels, test_size=0.30, random_state=1000)\n    ```", "```py\n    tokenizer = Tokenizer(num_words=5000)\n    tokenizer.fit_on_texts(X_train)\n    X_train = tokenizer.texts_to_sequences(X_train)\n    X_test = tokenizer.texts_to_sequences(X_test)\n    vocab_size = len(tokenizer.word_index) + 1 #The vocabulary size has an additional 1 due to the 0 reserved index\n    ```", "```py\n    X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n    X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n    ```", "```py\n    model = Sequential()\n    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n    model.add(layers.GlobalMaxPooling1D())\n    model.add(layers.Dense(10, activation='relu'))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    model.summary()\n    ```", "```py\n    model.fit(X_train, y_train,\n                        epochs=epochs,\n                        verbose=False,\n                        validation_data=(X_test, y_test),\n                        batch_size=batch_size)\n    loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n    print(\"Training Accuracy: {:.4f}\".format(accuracy))\n    loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n    print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n    ```", "```py\n    import numpy as np\n    import os\n    from sklearn.model_selection import train_test_split\n    # Classes for A/B/Unknown\n    A = 0\n    B = 1\n    UNKNOWN = -1\n    def preprocess_text(file_path):\n        with open(file_path, 'r') as f:\n            lines = f.readlines()\n            text = ' '.join(lines[1:]).replace(\"\\n\", ' ').replace('  ',' ').lower().replace('hamilton','').replace('madison', '')\n            text = ' '.join(text.split())\n            return text\n    # Concatenate all the papers known to be written by A/B into a single long text\n    all_authorA, all_authorB = '',''\n    for x in os.listdir('./papers/A/'):\n        all_authorA += preprocess_text('./papers/A/' + x)\n    for x in os.listdir('./papers/B/'):\n        all_authorB += preprocess_text('./papers/B/' + x)\n\n    # Print lengths of the large texts\n    print(\"AuthorA text length: {}\".format(len(all_authorA)))\n    print(\"AuthorB text length: {}\".format(len(all_authorB)))\n    ```", "```py\n    from keras.preprocessing.text import Tokenizer\n    # Hyperparameter - sequence length to use for the model\n    SEQ_LEN = 30\n    def make_subsequences(long_sequence, label, sequence_length=SEQ_LEN):\n        len_sequences = len(long_sequence)\n        X = np.zeros(((len_sequences - sequence_length)+1, sequence_length))\n        y = np.zeros((X.shape[0], 1))\n        for i in range(X.shape[0]):\n            X[i] = long_sequence[i:i+sequence_length]\n            y[i] = label\n        return X,y\n\n    # We use the Tokenizer class from Keras to convert the long texts into a sequence of characters (not words)\n    tokenizer = Tokenizer(char_level=True)\n    # Make sure to fit all characters in texts from both authors\n    tokenizer.fit_on_texts(all_authorA + all_authorB)\n    authorA_long_sequence = tokenizer.texts_to_sequences([all_authorA])[0]\n    authorB_long_sequence = tokenizer.texts_to_sequences([all_authorB])[0]\n    # Convert the long sequences into sequence and label pairs\n    X_authorA, y_authorA = make_subsequences(authorA_long_sequence, A)\n    X_authorB, y_authorB = make_subsequences(authorB_long_sequence, B)\n    # Print sizes of available data\n    print(\"Number of characters: {}\".format(len(tokenizer.word_index)))\n    print('author A sequences: {}'.format(X_authorA.shape))\n    print('author B sequences: {}'.format(X_authorB.shape))\n    ```", "```py\n    # Calculate the number of unique words in the text\n    word_tokenizer = Tokenizer()\n    word_tokenizer.fit_on_texts([all_authorA, all_authorB])\n    print(\"Total word count: \", len((all_authorA + ' ' + all_authorB).split(' ')))\n    print(\"Total number of unique words: \", len(word_tokenizer.word_index))\n    ```", "```py\n    # Take equal amounts of sequences from both authors\n    X = np.vstack((X_authorA, X_authorB))\n    y = np.vstack((y_authorA, y_authorB))\n    # Break data into train and test sets\n    X_train, X_val, y_train, y_val = train_test_split(X,y, train_size=0.8)\n    # Data is to be fed into RNN - ensure that the actual data is of size [batch size, sequence length]\n    X_train = X_train.reshape(-1, SEQ_LEN)\n    X_val =  X_val.reshape(-1, SEQ_LEN) \n    # Print the shapes of the train, validation and test sets\n    print(\"X_train shape: {}\".format(X_train.shape))\n    print(\"y_train shape: {}\".format(y_train.shape))\n    print(\"X_validate shape: {}\".format(X_val.shape))\n    print(\"y_validate shape: {}\".format(y_val.shape))\n    ```", "```py\n    from keras.layers import SimpleRNN, Embedding, Dense\n    from keras.models import Sequential\n    from keras.optimizers import SGD, Adadelta, Adam\n    Embedding_size = 100\n    RNN_size = 256\n    model = Sequential()\n    model.add(Embedding(len(tokenizer.word_index)+1, Embedding_size, input_length=30))\n    model.add(SimpleRNN(RNN_size, return_sequences=False))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\n    model.summary()\n    ```", "```py\n    Batch_size = 4096\n    Epochs = 20\n    model.fit(X_train, y_train, batch_size=Batch_size, epochs=Epochs, validation_data=(X_val, y_val))\n    ```", "```py\n    for x in os.listdir('./papers/Unknown/'):\n        unknown = preprocess_text('./papers/Unknown/' + x)\n        unknown_long_sequences = tokenizer.texts_to_sequences([unknown])[0]\n        X_sequences, _ = make_subsequences(unknown_long_sequences, UNKNOWN)\n        X_sequences = X_sequences.reshape((-1,SEQ_LEN))\n\n        votes_for_authorA = 0\n        votes_for_authorB = 0\n\n        y = model.predict(X_sequences)\n        y = y>0.5\n        votes_for_authorA = np.sum(y==0)\n        votes_for_authorB = np.sum(y==1)\n\n        print(\"Paper {} is predicted to have been written by {}, {} to {}\".format(\n                    x.replace('paper_','').replace('.txt',''), \n                    (\"Author A\" if votes_for_authorA > votes_for_authorB else \"Author B\"),\n                    max(votes_for_authorA, votes_for_authorB), min(votes_for_authorA, votes_for_authorB)))\n    ```", "```py\n    from keras.datasets import imdb\n    max_features = 10000\n    maxlen = 500\n\n    (train_data, y_train), (test_data, y_test) = imdb.load_data(num_words=max_features)\n    print('Number of train sequences: ', len(train_data))\n    print('Number of test sequences: ', len(test_data))\n    ```", "```py\n    from keras.preprocessing import sequence\n    train_data = sequence.pad_sequences(train_data, maxlen=maxlen)\n    test_data = sequence.pad_sequences(test_data, maxlen=maxlen)\n    ```", "```py\n    from keras.models import Sequential\n    from keras.layers import Embedding\n    from keras.layers import Dense\n    from keras.layers import GRU\n    from keras.layers import SimpleRNN\n    model = Sequential()\n    model.add(Embedding(max_features, 32))\n    model.add(SimpleRNN(32))\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.compile(optimizer='rmsprop',\n                  loss='binary_crossentropy',\n                  metrics=['acc'])\n\n    history = model.fit(train_data, y_train,\n                        epochs=10,\n                        batch_size=128,\n                        validation_split=0.2)\n    ```", "```py\n    import matplotlib.pyplot as plt\n\n    def plot_results(history):\n        acc = history.history['acc']\n        val_acc = history.history['val_acc']\n        loss = history.history['loss']\n        val_loss = history.history['val_loss']\n\n        epochs = range(1, len(acc) + 1)\n        plt.plot(epochs, acc, 'bo', label='Training Accuracy')\n        plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n\n        plt.title('Training and validation Accuracy')\n        plt.legend()\n        plt.figure()\n        plt.plot(epochs, loss, 'bo', label='Training Loss')\n        plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n        plt.title('Training and validation Loss')\n        plt.legend()\n        plt.show()\n    ```", "```py\n    plot_results(history)\n    ```", "```py\n    import sys\n    import random\n    import string\n    import numpy as np\n    from keras.models import Sequential\n    from keras.layers import Dense\n    from keras.layers import LSTM, GRU\n    from keras.optimizers import RMSprop\n    from keras.models import load_model\n    # load text\n    def load_text(filename):\n        with open(filename, 'r') as f:\n            text = f.read()\n        return text\n    in_filename = 'drive/shakespeare_poems.txt' # Add your own text file here\n    text = load_text(in_filename)\n    print(text[:200])\n    ```", "```py\n    chars = sorted(list(set(text)))\n    print('Number of distinct characters:', len(chars))\n    char_indices = dict((c, i) for i, c in enumerate(chars))\n    indices_char = dict((i, c) for i, c in enumerate(chars))\n    ```", "```py\n    max_len_chars = 40\n    step = 3\n    sentences = []\n    next_chars = []\n    for i in range(0, len(text) - max_len_chars, step):\n        sentences.append(text[i: i + max_len_chars])\n        next_chars.append(text[i + max_len_chars])\n    print('nb sequences:', len(sentences))\n    ```", "```py\n    x = np.zeros((len(sentences), max_len_chars, len(chars)), dtype=np.bool)\n    y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n    for i, sentence in enumerate(sentences):\n        for t, char in enumerate(sentence):\n            x[i, t, char_indices[char]] = 1\n        y[i, char_indices[next_chars[i]]] = 1\n    ```", "```py\n    print('Build model...')\n    model = Sequential()\n    model.add(GRU(128, input_shape=(max_len_chars, len(chars))))\n    model.add(Dense(len(chars), activation='softmax'))\n    optimizer = RMSprop(lr=0.01)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n    model.fit(x, y,batch_size=128,epochs=10)\n    model.save(\"poem_gen_model.h5\")\n    ```", "```py\n    def sample(preds, temperature=1.0):\n        # helper function to sample an index from a probability array\n        preds = np.asarray(preds).astype('float64')\n        preds = np.log(preds) / temperature\n        exp_preds = np.exp(preds)\n        preds = exp_preds / np.sum(exp_preds)\n        probas = np.random.multinomial(1, preds, 1)\n        return np.argmax(probas)\n    ```", "```py\n    from keras.models import load_model\n    model_loaded = load_model('poem_gen_model.h5')\n    def generate_poem(model, num_chars_to_generate=400):\n        start_index = random.randint(0, len(text) - max_len_chars - 1)\n        generated = ''\n        sentence = text[start_index: start_index + max_len_chars]\n        generated += sentence\n        print(\"Seed sentence: {}\".format(generated))\n        for i in range(num_chars_to_generate):\n            x_pred = np.zeros((1, max_len_chars, len(chars)))\n            for t, char in enumerate(sentence):\n                x_pred[0, t, char_indices[char]] = 1.\n\n            preds = model.predict(x_pred, verbose=0)[0]\n            next_index = sample(preds, 1)\n            next_char = indices_char[next_index]\n            generated += next_char\n            sentence = sentence[1:] + next_char\n        return generated\n    generate_poem(model_loaded, 100)\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    from keras.models import Model, Sequential\n    from keras.layers import SimpleRNN, Dense,Embedding\n    from keras.preprocessing.text import Tokenizer\n    from keras.preprocessing import sequence\n    ```", "```py\n    df = pd.read_csv(\"drive/spam.csv\", encoding=\"latin\")\n    df.head()\n    ```", "```py\n    df = df[[\"v1\",\"v2\"]]\n    df.head()\n    ```", "```py\n    df[\"v1\"].value_counts()\n    ```", "```py\n    lab_map = {\"ham\":0, \"spam\":1}\n    X = df[\"v2\"].values\n    Y = df[\"v1\"].map(lab_map).values\n    ```", "```py\n    max_words = 100\n    mytokenizer = Tokenizer(nb_words=max_words,lower=True, split=\" \")\n    mytokenizer.fit_on_texts(X)\n    text_tokenized = mytokenizer.texts_to_sequences(X)\n    text_tokenized\n    ```", "```py\n    max_len = 50\n    sequences = sequence.pad_sequences(text_tokenized,maxlen=max_len)\n    sequences\n    ```", "```py\n    model = Sequential()\n    model.add(Embedding(max_words, 20, input_length=max_len))\n    model.add(SimpleRNN(64))\n    model.add(Dense(1, activation=\"sigmoid\"))\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    model.fit(sequences,Y,batch_size=128,epochs=10,\n              validation_split=0.2)\n    ```", "```py\n    inp_test_seq = \"WINNER! U win a 500 prize reward & free entry to FA cup final tickets! Text FA to 34212 to receive award\"\n    test_sequences = mytokenizer.texts_to_sequences(np.array([inp_test_seq]))\n    test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n    model.predict(test_sequences_matrix)\n    ```", "```py\n    import os\n    import re\n    import numpy as np\n    ```", "```py\n    with open(\"fra.txt\", 'r', encoding='utf-8') as f:\n        lines = f.read().split('\\n')\n    num_samples = 20000 # Using only 20000 pairs for this example\n    lines_to_use = lines[: min(num_samples, len(lines) - 1)]\n    ```", "```py\n    for l in range(len(lines_to_use)):\n        lines_to_use[l] = re.sub(\"\\u202f\", \"\", lines_to_use[l])\n    for l in range(len(lines_to_use)):\n        lines_to_use[l] = re.sub(\"\\d\", \" NUMBER_PRESENT \", lines_to_use[l])\n    ```", "```py\n    input_texts = []\n    target_texts = []\n    input_words = set()\n    target_words = set()\n    for line in lines_to_use:\n        target_text, input_text = line.split('\\t')\n        target_text = 'BEGIN_ ' + target_text + ' _END'\n        input_texts.append(input_text)\n        target_texts.append(target_text)\n        for word in input_text.split():\n            if word not in input_words:\n                input_words.add(word)\n        for word in target_text.split():\n            if word not in target_words:\n                target_words.add(word)\n    max_input_seq_length = max([len(i.split()) for i in input_texts])\n    max_target_seq_length = max([len(i.split()) for i in target_texts])\n    input_words = sorted(list(input_words))\n    target_words = sorted(list(target_words))\n    num_encoder_tokens = len(input_words)\n    num_decoder_tokens = len(target_words)\n    ```", "```py\n    input_token_index = dict(\n        [(word, i) for i, word in enumerate(input_words)])\n    target_token_index = dict(\n        [(word, i) for i, word in enumerate(target_words)])\n    encoder_input_data = np.zeros(\n        (len(input_texts), max_input_seq_length),\n        dtype='float32')\n    decoder_input_data = np.zeros(\n        (len(target_texts), max_target_seq_length),\n        dtype='float32')\n    decoder_target_data = np.zeros(\n        (len(target_texts), max_target_seq_length, num_decoder_tokens),\n        dtype='float32')\n    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n        for t, word in enumerate(input_text.split()):\n            encoder_input_data[i, t] = input_token_index[word]\n        for t, word in enumerate(target_text.split()):\n            decoder_input_data[i, t] = target_token_index[word]\n            if t > 0:\n                # decoder_target_data is ahead of decoder_input_data #by one timestep\n                decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n    ```", "```py\n    from keras.layers import Input, LSTM, Embedding, Dense\n    from keras.models import Model\n    embedding_size = 50\n    ```", "```py\n    encoder_inputs = Input(shape=(None,))\n    encoder_after_embedding =  Embedding(num_encoder_tokens, embedding_size)(encoder_inputs)\n    encoder_lstm = LSTM(50, return_state=True)_, \n    state_h, state_c = encoder_lstm(encoder_after_embedding)\n    encoder_states = [state_h, state_c]\n    ```", "```py\n    decoder_inputs = Input(shape=(None,))\n    decoder_after_embedding = Embedding(num_decoder_tokens, embedding_size)(decoder_inputs)\n    decoder_lstm = LSTM(50, return_sequences=True, return_state=True)\n    decoder_outputs, _, _ = decoder_lstm(decoder_after_embedding,\n                                         initial_state=encoder_states)\n    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n    decoder_outputs = decoder_dense(decoder_outputs)\n    ```", "```py\n    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n    model.fit([encoder_input_data, decoder_input_data], \n              decoder_target_data,\n              batch_size=128,\n              epochs=20,\n              validation_split=0.05)\n    ```", "```py\n    # encoder part\n    encoder_model = Model(encoder_inputs, encoder_states)\n    # decoder part\n    decoder_state_input_h = Input(shape=(50,))\n    decoder_state_input_c = Input(shape=(50,))\n    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n    decoder_outputs_inf, state_h_inf, state_c_inf = decoder_lstm(decoder_after_embedding, initial_state=decoder_states_inputs)\n    decoder_states_inf = [state_h_inf, state_c_inf]\n    decoder_outputs_inf = decoder_dense(decoder_outputs_inf)\n    decoder_model = Model(\n        [decoder_inputs] + decoder_states_inputs,\n        [decoder_outputs_inf] + decoder_states_inf)\n    ```", "```py\n    reverse_input_word_index = dict(\n        (i, word) for word, i in input_token_index.items())\n    reverse_target_word_index = dict(\n        (i, word) for word, i in target_token_index.items())\n    def decode_sequence(input_seq):\n    ```", "```py\n        states_value = encoder_model.predict(input_seq)\n    ```", "```py\n        target_seq = np.zeros((1,1))\n    ```", "```py\n        target_seq[0, 0] = target_token_index['BEGIN_']\n    ```", "```py\n    stop_condition = False\n        decoded_sentence = ''\n\n        while not stop_condition:\n            output_tokens, h, c = decoder_model.predict(\n                [target_seq] + states_value)\n    ```", "```py\n            sampled_token_index = np.argmax(output_tokens)\n            sampled_word = reverse_target_word_index[sampled_token_index]\n            decoded_sentence += ' ' + sampled_word\n    ```", "```py\n            if (sampled_word == '_END' or\n               len(decoded_sentence) > 60):\n                stop_condition = True\n    ```", "```py\n            target_seq = np.zeros((1,1))\n            target_seq[0, 0] = sampled_token_index\n    ```", "```py\n            states_value = [h, c]\n\n        return decoded_sentence\n    ```", "```py\n    text_to_translate = \"Où est ma voiture??\"\n    encoder_input_to_translate = np.zeros(\n        (1, max_input_seq_length),\n        dtype='float32')\n    for t, word in enumerate(text_to_translate.split()):\n        encoder_input_to_translate[0, t] = input_token_index[word]\n    decode_sequence(encoder_input_to_translate)\n    ```", "```py\n    import os\n    import re\n    import pdb\n    import string\n    import numpy as np\n    import pandas as pd\n    from keras.utils import to_categorical\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    ```", "```py\n    path_data = \"news_summary_small.csv\"\n    df_text_file = pd.read_csv(path_data)\n    df_text_file.headlines = df_text_file.headlines.str.lower()\n    df_text_file.text = df_text_file.text.str.lower()\n    lengths_text = df_text_file.text.apply(len)\n    dataset = list(zip(df_text_file.text.values, df_text_file.headlines.values))\n    ```", "```py\n    input_texts = []\n    target_texts = []\n    input_chars = set()\n    target_chars = set()\n    for line in dataset:\n        input_text, target_text = list(line[0]), list(line[1])\n        target_text = ['BEGIN_'] + target_text + ['_END']\n        input_texts.append(input_text)\n        target_texts.append(target_text)\n\n        for character in input_text:\n            if character not in input_chars:\n                input_chars.add(character)\n        for character in target_text:\n            if character not in target_chars:\n                target_chars.add(character)\n    input_chars.add(\"<unk>\")\n    input_chars.add(\"<pad>\")\n    target_chars.add(\"<pad>\")\n    input_chars = sorted(input_chars)\n    target_chars = sorted(target_chars)\n    human_vocab = dict(zip(input_chars, range(len(input_chars))))\n    machine_vocab = dict(zip(target_chars, range(len(target_chars))))\n    inv_machine_vocab = dict(enumerate(sorted(machine_vocab)))\n    def string_to_int(string_in, length, vocab):\n        \"\"\"\n        Converts all strings in the vocabulary into a list of integers representing the positions of the\n        input string's characters in the \"vocab\"\n        Arguments:\n        string -- input string\n        length -- the number of time steps you'd like, determines if the output will be padded or cut\n        vocab -- vocabulary, dictionary used to index every character of your \"string\"\n        Returns:\n        rep -- list of integers (or '<unk>') (size = length) representing the position of the string's character in the vocabulary\n        \"\"\"\n    ```", "```py\n        string_in = string_in.lower()\n        string_in = string_in.replace(',','')\n        if len(string_in) > length:\n            string_in = string_in[:length]\n        rep = list(map(lambda x: vocab.get(x, '<unk>'), string_in))\n        if len(string_in) < length:\n            rep += [vocab['<pad>']] * (length - len(string_in))\n\n        return rep\n    def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n        X, Y = zip(*dataset)\n        X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\n        Y = [string_to_int(t, Ty, machine_vocab) for t in Y]\n        print(\"X shape from preprocess: {}\".format(X.shape))\n        Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n        Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n        return X, np.array(Y), Xoh, Yoh\n    def softmax(x, axis=1):\n        \"\"\"Softmax activation function.\n        # Arguments\n            x : Tensor.\n            axis: Integer, axis along which the softmax normalization is applied.\n        # Returns\n            Tensor, output of softmax transformation.\n        # Raises\n            ValueError: In case 'dim(x) == 1'.\n        \"\"\"\n        ndim = K.ndim(x)\n        if ndim == 2:\n            return K.softmax(x)\n        elif ndim > 2:\n            e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n            s = K.sum(e, axis=axis, keepdims=True)\n            return e / s\n        else:\n            raise ValueError('Cannot apply softmax to a tensor that is 1D')\n    ```", "```py\n    Tx = 460\n    Ty = 75\n    X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n    Define the model functions (Repeator, Concatenate, Densors, Dotor)\n    # Defined shared layers as global variables\n    repeator = RepeatVector(Tx)\n    concatenator = Concatenate(axis=-1)\n    densor1 = Dense(10, activation = \"tanh\")\n    densor2 = Dense(1, activation = \"relu\")\n    activator = Activation(softmax, name='attention_weights')\n    dotor = Dot(axes = 1)\n    Define one-step-attention function:\n    def one_step_attention(h, s_prev):\n        \"\"\"\n        Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n        \"alphas\" and the hidden states \"h\" of the Bi-LSTM.\n\n        Arguments:\n        h -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_h)\n        s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n\n        Returns:\n        context -- context vector, input of the next (post-attetion) LSTM cell\n        \"\"\"  \n    ```", "```py\n        s_prev = repeator(s_prev)\n    ```", "```py\n        concat = concatenator([h, s_prev])\n    ```", "```py\n        e = densor1(concat)\n    ```", "```py\n        energies = densor2(e)  \n    ```", "```py\n        alphas = activator(energies)\n    ```", "```py\n        context = dotor([alphas, h])\n\n        return context\n    Define the number of hidden states for decoder and encoder.\n    n_h = 32\n    n_s = 64\n    post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n    output_layer = Dense(len(machine_vocab), activation=softmax)\n    Define the model architecture and run it to obtain a model.\n    def model(Tx, Ty, n_h, n_s, human_vocab_size, machine_vocab_size):\n        \"\"\"\n        Arguments:\n        Tx -- length of the input sequence\n        Ty -- length of the output sequence\n        n_h -- hidden state size of the Bi-LSTM\n        n_s -- hidden state size of the post-attention LSTM\n        human_vocab_size -- size of the python dictionary \"human_vocab\"\n        machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n        Returns:\n        model -- Keras model instance\n        \"\"\"\n    ```", "```py\n        X = Input(shape=(Tx, human_vocab_size), name=\"input_first\")\n        s0 = Input(shape=(n_s,), name='s0')\n        c0 = Input(shape=(n_s,), name='c0')\n        s = s0\n        c = c0\n    ```", "```py\n        outputs = []\n    ```", "```py\n        a = Bidirectional(LSTM(n_h, return_sequences=True))(X)\n\n        # Iterate for Ty steps\n        for t in range(Ty):\n\n            # Perform one step of the attention mechanism to get back the context vector at step t\n            context = one_step_attention(h, s)      \n    ```", "```py\n            # Pass: initial_state = [hidden state, cell state]\n            s, _, c = post_activation_LSTM_cell(context, initial_state = [s,c])  \n    ```", "```py\n            out = output_layer(s)    \n    ```", "```py\n            outputs.append(out)\n    ```", "```py\n        model = Model(inputs=[X, s0, c0], outputs=outputs)\n\n        return model\n    model = model(Tx, Ty, n_h, n_s, len(human_vocab), len(machine_vocab))\n    #Define model loss functions and other hyperparameters. Also #initialize decoder state vectors.\n    opt = Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay = 0.01)\n    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n    s0 = np.zeros((10000, n_s))\n    c0 = np.zeros((10000, n_s))\n    outputs = list(Yoh.swapaxes(0,1))\n    Fit the model to our data:\n    model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)\n    #Run inference step for the new text.\n    EXAMPLES = [\"Last night a meteorite was seen flying near the earth's moon.\"]\n    for example in EXAMPLES:\n\n        source = string_to_int(example, Tx, human_vocab)\n        source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n        source = source[np.newaxis, :]\n        prediction = model.predict([source, s0, c0])\n        prediction = np.argmax(prediction, axis = -1)\n        output = [inv_machine_vocab[int(i)] for i in prediction]\n\n        print(\"source:\", example)\n        print(\"output:\", ''.join(output))\n    ```", "```py\n    import tensorflow as tf\n    tf.test.gpu_device_name()\n    ```", "```py\n    from google.colab import drive\n    drive.mount('/content/gdrive')\n    # Run the below command in a new cell\n    cd /content/gdrive/My Drive/Lesson-9/\n    # Run the below command in a new cell\n    !unzip data.csv.zip\n    ```", "```py\n    import os\n    import re\n    import pickle\n    import pandas as pd\n    from keras.preprocessing.text import Tokenizer\n    from keras.preprocessing.sequence import pad_sequences\n    from keras.models import Sequential\n    from keras.layers import Dense, Embedding, LSTM\n    ```", "```py\n    def preprocess_data(data_file_path):\n        data = pd.read_csv(data_file_path, header=None) # read the csv\n        data.columns = ['rating', 'title', 'review'] # add column names\n        data['review'] = data['review'].apply(lambda x: x.lower()) # change all text to lower\n        data['review'] = data['review'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x))) # remove all numbers\n        return data\n    df = preprocess_data('data.csv')\n    ```", "```py\n    max_features = 2000\n    maxlength = 250\n    tokenizer = Tokenizer(num_words=max_features, split=' ')\n    ```", "```py\n    tokenizer.fit_on_texts(df['review'].values)\n    X = tokenizer.texts_to_sequences(df['review'].values)\n    ```", "```py\n    X = pad_sequences(X, maxlen=maxlength)\n    ```", "```py\n    y_train = pd.get_dummies(df.rating).values\n    embed_dim = 128\n    hidden_units = 100\n    n_classes = 5\n    model = Sequential()\n    model.add(Embedding(max_features, embed_dim, input_length = X.shape[1]))\n    model.add(LSTM(hidden_units))\n    model.add(Dense(n_classes, activation='softmax'))\n    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n    print(model.summary())\n    ```", "```py\n    model.fit(X[:100000, :], y_train[:100000, :], batch_size = 128, epochs=15, validation_split=0.2)\n    ```", "```py\n    model.save('trained_model.h5')  # creates a HDF5 file 'trained_model.h5'\n    with open('trained_tokenizer.pkl', 'wb') as f: # creates a pickle file 'trained_tokenizer.pkl'\n        pickle.dump(tokenizer, f)\n    from google.colab import files\n    files.download('trained_model.h5')\n    files.download('trained_tokenizer.pkl')\n    ```", "```py\n    import re\n    import pickle\n    import numpy as np\n    from flask import Flask, request, jsonify\n    from keras.models import load_model\n    from keras.preprocessing.sequence import pad_sequences\n    ```", "```py\n    def load_variables():\n        global model, tokenizer\n        model = load_model('trained_model.h5')\n        model._make_predict_function()  # https://github.com/keras-team/keras/issues/6462\n        with open('trained_tokenizer.pkl',  'rb') as f:\n            tokenizer = pickle.load(f)\n    ```", "```py\n    def do_preprocessing(reviews):\n        processed_reviews = []\n        for review in reviews:\n            review = review.lower()\n            processed_reviews.append(re.sub('[^a-zA-z0-9\\s]', '', review))\n        processed_reviews = tokenizer.texts_to_sequences(np.array(processed_reviews))\n        processed_reviews = pad_sequences(processed_reviews, maxlen=250)\n        return processed_reviews\n    ```", "```py\n    app = Flask(__name__)\n    ```", "```py\n    @app.route('/')\n    def home_routine():\n        return 'Hello World!'\n    ```", "```py\n    @app.route('/prediction', methods=['POST'])\n    def get_prediction():\n      # get incoming text\n      # run the model\n        if request.method == 'POST':\n            data = request.get_json()\n        data = do_preprocessing(data)\n        predicted_sentiment_prob = model.predict(data)\n        predicted_sentiment = np.argmax(predicted_sentiment_prob, axis=-1)\n        return str(predicted_sentiment)\n    ```", "```py\n    if __name__ == '__main__':\n      # load model\n      load_variables()\n      app.run(debug=True)\n    ```", "```py\n    python app.py\n    ```"]