["```py\nimport numpy as np\n# Write a sigmoid function to be used later in the program\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\nn_x = 5 # Dimensionality of input vector\nn_h = 3 # Number of hidden units\n# Define an input at time 't' having a dimensionality of n_x\nx_t = np.random.randn(n_x, 1)\n# Define W_z, U_z and h_prev (last time step activation)\nW_z = np.random.randn(n_h,  n_x) # n_h = 3, n_x=5\nU_z = np.random.randn(n_h, n_h) # n_h = 3\nh_prev = np.random.randn(n_h, 1)\n```", "```py\n# Calculate expression for update gate\nz_t = sigmoid(np.matmul(W_z, x_t) + np.matmul(U_z, h_prev))\n```", "```py\n# Define W_r, U_r\nW_r = np.random.randn(n_h,  n_x) # n_h = 3, n_x=5\nU_r = np.random.randn(n_h, n_h) # n_h = 3\n# Calculate expression for update gate\nr_t = sigmoid(np.matmul(W_r, x_t) + np.matmul(U_r, h_prev))\n```", "```py\n# Define W, U\nW = np.random.randn(n_h,  n_x) # n_h = 3, n_x=5\nU = np.random.randn(n_h, n_h) # n_h = 3\n# Calculate h_candidate\nh_candidate = np.tanh(np.matmul(W, x_t) + np.matmul(U,np.multiply(r_t, h_prev)))\n```", "```py\n# Calculate h_new\nh_new = np.multiply(z_t, h_prev) + np.multiply((1-z_t), h_candidate)\n```", "```py\nfrom keras.datasets import imdb\n```", "```py\n    max_features = 10000\n    maxlen = 500\n    ```", "```py\n    (train_data, y_train), (test_data, y_test) = imdb.load_data(num_words=max_features)\n    print('Number of train sequences: ', len(train_data))\n    print('Number of test sequences: ', len(test_data))\n    ```", "```py\n    from keras.preprocessing import sequence\n    train_data = sequence.pad_sequences(train_data, maxlen=maxlen)\n    test_data = sequence.pad_sequences(test_data, maxlen=maxlen)\n    ```", "```py\n    print('train_data shape:', train_data.shape)\n    print('test_data shape:', test_data.shape)\n    ```", "```py\n    from keras.models import Sequential\n    from keras.layers import Embedding\n    from keras.layers import Dense\n    from keras.layers import GRU\n    ```", "```py\n    model = Sequential()\n    model.add(Embedding(max_features, 32))\n    ```", "```py\n    model.add(GRU(32))\n    model.add(Dense(1, activation='sigmoid'))\n    ```", "```py\n    model.compile(optimizer='rmsprop',\n                  loss='binary_crossentropy',\n                  metrics=['acc'])\n    ```", "```py\n    history = model.fit(train_data, y_train,\n                        epochs=10,\n                        batch_size=128,\n                        validation_split=0.2\n    ```", "```py\n    import matplotlib.pyplot as plt\n    def plot_results(history):\n        acc = history.history['acc']\n        val_acc = history.history['val_acc']\n        loss = history.history['loss']\n        val_loss = history.history['val_loss']\n\n        epochs = range(1, len(acc) + 1)\n        plt.plot(epochs, acc, 'bo', label='Training Accuracy')\n        plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n        plt.title('Training and validation Accuracy')\n        plt.legend()\n        plt.figure()\n        plt.plot(epochs, loss, 'bo', label='Training Loss')\n        plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n        plt.title('Training and validation Loss')\n        plt.legend()\n        plt.show()\n    ```", "```py\n    plot_results(history)\n    ```", "```py\n    import io\n    import sys\n    import random\n    import string\n    import numpy as np\n    from keras.models import Sequential\n    from keras.layers import Dense\n    from keras.layers import GRU\n    from keras.optimizers import RMSprop\n    ```", "```py\n    def load_text(filename):\n        with open(filename, 'r') as f:\n            text = f.read()\n        return text\n    file_poem = 'shakespeare_poems.txt' # Path of the file\n    text = load_text(file_poem)\n    print(text[:200])\n    ```", "```py\n    chars = sorted(list(set(text)))\n    print('Number of distinct characters:', len(chars))\n    char_indices = dict((c, i) for i, c in enumerate(chars))\n    indices_char = dict((i, c) for i, c in enumerate(chars))\n    ```", "```py\nmax_len_chars = 40\nstep = 3\nsentences = []\nnext_chars = []\nfor i in range(0, len(text) - max_len_chars, step):\n    sentences.append(text[i: i + max_len_chars])\n    next_chars.append(text[i + max_len_chars])\nprint('nb sequences:', len(sentences))\n```", "```py\nx = np.zeros((len(sentences), max_len_chars, len(chars)), dtype=np.bool)\ny = np.zeros((len(sentences), len(chars)), dtype=np.bool)\nfor i, sentence in enumerate(sentences):\n    for t, char in enumerate(sentence):\n        x[i, t, char_indices[char]] = 1\n    y[i, char_indices[next_chars[i]]] = 1\n```", "```py\n    model = Sequential()\n    model.add(GRU(128, input_shape=(max_len_chars, len(chars))))\n    model.add(Dense(len(chars), activation='softmax'))\n    optimizer = RMSprop(lr=0.01)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n    ```", "```py\n    model.fit(x, y,batch_size=128,epochs=10)\n    model.save(\"poem_gen_model.h5\")\n    ```", "```py\n    from keras.models import load_model\n    model_loaded = load_model('poem_gen_model.h5')\n    ```", "```py\n    def sample(preds, temperature=1.0):\n        # helper function to sample an index from a probability array\n        preds = np.asarray(preds).astype('float64')\n        preds = np.log(preds) / temperature\n        exp_preds = np.exp(preds)\n        preds = exp_preds / np.sum(exp_preds)\n        probas = np.random.multinomial(1, preds, 1)\n        return np.argmax(probas)\n    ```", "```py\n    def generate_poem(model, num_chars_to_generate=400):\n        start_index = random.randint(0, len(text) - max_len_chars - 1)\n        generated = ''\n        sentence = text[start_index: start_index + max_len_chars]\n        generated += sentence\n        print(\"Seed sentence: {}\".format(generated))\n        for i in range(num_chars_to_generate):\n            x_pred = np.zeros((1, max_len_chars, len(chars)))\n            for t, char in enumerate(sentence):\n                x_pred[0, t, char_indices[char]] = 1.\n\n            preds = model.predict(x_pred, verbose=0)[0]\n            next_index = sample(preds, 1)\n            next_char = indices_char[next_index]\n            generated += next_char\n            sentence = sentence[1:] + next_char\n        return generated\n    ```", "```py\n    generate_poem(model_loaded, 100)\n    ```"]