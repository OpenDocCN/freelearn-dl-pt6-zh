- en: Clients Subscription Assessment for Bank Telemarketing using Deep Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will see two examples of how to build very robust and accurate
    predictive models for predictive analytics using H2O on a bank marketing dataset.
    The data is related to the direct marketing campaigns of a Portuguese banking
    institution. The marketing campaigns were based on phone calls. The goal of this
    end-to-end project is to predict that the client will subscribe to a term deposit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout this project, the following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Client subscription assessment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset description
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploratory analysis of the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Client subscription assessment using H2O
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Client subscription assessment through telemarketing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some time ago, due to the global financial crisis, getting credit in international
    markets became more restricted for banks. This turned attention to internal customers
    and their deposits to gather funds. This led to a demand for information about
    a client's behavior for their deposits and their response to telemarketing campaigns
    conducted by the banks periodically. Often, more than one contact to the same
    client is required in order to assess whether the product (bank term deposit)
    will be (**yes**) or will be (**no**) subscribed.
  prefs: []
  type: TYPE_NORMAL
- en: The aim of this project is to implement an ML model that predicts that the client
    will subscribe to a term deposit (variable `y`). In short, this is a binary classification
    problem. Now, before we start implementing our application, we need to know about
    the dataset. Then we will see an explanatory analysis of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset description
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two sources that I would like to acknowledge. This dataset was used
    in a research paper published by Moro et al, *A Data-Driven Approach to Predict
    the Success of Bank Telemarketing*, Decision Support Systems, Elsevier, June 2014\.
    Later on, it was donated to the UCI Machine Learning repository and can be downloaded
    from [https://archive.ics.uci.edu/ml/datasets/bank+marketing](https://archive.ics.uci.edu/ml/datasets/bank+marketing).
    According to the dataset description, there are four datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '`bank-additional-full.csv`: This includes all examples (41,188) and 20 inputs,
    ordered by date (from May 2008 to November 2010), very close to the data analyzed
    by Moro et al, 2014'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bank-additional.csv`: This includes 10% of the examples (4,119), randomly
    selected from 1 and 20 inputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bank-full.csv`: This includes all the examples and 17 inputs, ordered by date
    (an older version of this dataset with fewer inputs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bank.csv`: This includes 10% of the examples and 17 inputs randomly selected
    from three (an older version of this dataset with fewer inputs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are 21 attributes in the dataset. The independent variables, that is,
    features, can be further categorized as bank-client-related data (attributes 1
    to 7), related to the last contact with the current campaign (attributes 8 to
    11), other attributes (attributes 12 to 15), and social and economic context attributes
    (attributes 16 to 20). The dependent variable is specified by `y`, the last attribute
    (21):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **ID** | **Attribute** | **Explanation** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | `age` | Age in numbers. |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | `job` | This is the type of job in a categorical format, with these possible
    values: `admin`, `blue-collar`, `entrepreneur`, `housemaid`, `management`, `retired`,
    `self-employed`, `services`, `student`, `technician`, `unemployed`, and `unknown`.
    |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | `marital` | This is the marital status in a categorical format, with
    these possible values: `divorced`, `married`, `single`, and `unknown`. Here, `divorced`
    means divorced or widowed. |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | `education` | This is the educational background in categorical format,
    with possible values as follows: `basic.4y`, `basic.6y`, `basic.9y`, `high.school`, `illiterate`, `professional.course`, `university.degree`,
    and `unknown`. |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | `default` | This is a categorical format with possible values in credit
    in default as `no`, `yes`, and `unknown`. |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | `housing` | Does the customer have a housing loan? |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | `loan` | The personal loan in a categorical format, with possible values
    as `no`, `yes`, and `unknown`. |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | `contact` | This is the contact communication type in a categorical format.
    The possible values are `cellular` and `telephone`. |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | `month` | This is the last contact month of the year in a categorical
    format with possible values `jan`, `feb`, `mar`, ..., `nov`, and `dec`. |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | `day_of_week` | This is the last contact day of the week in a categorical
    format with possible values `mon`, `tue`, `wed`, `thu`, and `fri`. |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | `duration` | This is the last contact duration, in seconds (numerical
    value). This attribute highly affects the output target (for example, if `duration=0`,
    then `y=no`). Yet, the duration is not known before a call is performed. Also,
    after the end of the call, `y` is obviously known. Thus, this input should only
    be included for benchmark purposes and should be discarded if the intention is
    to have a realistic predictive model. |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | `campaign` | This is the number of contacts performed during this campaign
    and for this client. |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | `pdays` | This is the number of days that passed by after the client
    was last contacted by a previous campaign (numeric; 999 means client was not previously
    contacted). |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | `previous` | This is the number of contacts performed before this campaign
    and for this client (numeric). |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | `poutcome` | The outcome of the previous marketing campaign (categorical:
    `failure`, `nonexistent`, and `success`). |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | `emp.var.rate` | Employment variation rate—quarterly indicator (numeric).
    |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | `cons.price.idx` | Consumer price index—monthly indicator (numeric).
    |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | `cons.conf.idx` | Consumer confidence index—monthly indicator (numeric).
    |'
  prefs: []
  type: TYPE_TB
- en: '| 19 | `euribor3m` | Euribor 3 month rate—daily indicator (numeric). |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | `nr.employed` | Number of employees—quarterly indicator (numeric). |'
  prefs: []
  type: TYPE_TB
- en: '| 21 | `y` | Signifies whether the client has subscribed a term deposit. It
    has possible binary (`yes` and `no`) values. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Description of the bank marketing dataset'
  prefs: []
  type: TYPE_NORMAL
- en: For the exploratory analysis of the dataset, we will be using Apache Zeppelin
    and Spark. We'll start by visualizing the distributions of the categorical features,
    and then the numeric features. At the end, we'll compute some statistics that
    describe numeric features. But before that, let's configure Zeppelin.
  prefs: []
  type: TYPE_NORMAL
- en: Installing and getting started with Apache Zeppelin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Zeppelin is a web-based notebook that enables you to do data analytics
    in an interactive way. Using Zeppelin, you can make beautiful, data-driven, interactive,
    and collaborative documents with SQL, Scala, and more. The Apache Zeppelin interpreter
    concept allows any language/data processing backend to be plugged into Zeppelin.
    Currently, Apache Zeppelin supports many interpreters such as Apache Spark, Python,
    JDBC, Markdown, and Shell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apache Zeppelin is a relatively newer technology from Apache Software Foundation
    that enables the data scientist, engineer, and practitioner to do data exploration,
    visualization, sharing, and collaboration with multiple programming language backends
    (such as Python, Scala, Hive, SparkSQL, Shell, Markdown, and more). Since using
    other interpreters is not the goal of this book, we''ll be using Spark on Zeppelin,
    and all the codes will be written using Scala. In this section, therefore, we
    will show you how to configure Zeppelin using a binary package that contains only
    the Spark interpreter. Apache Zeppelin officially supports, and is tested in,
    the following environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Requirements** | **Value/Version** |'
  prefs: []
  type: TYPE_TB
- en: '| Oracle JDK | 1.7+ (set `JAVA_HOME`) |'
  prefs: []
  type: TYPE_TB
- en: '| OS | Mac OS X Ubuntu 14.X+'
  prefs: []
  type: TYPE_NORMAL
- en: CentOS 6.X+
  prefs: []
  type: TYPE_NORMAL
- en: Windows 7 Pro SP1+ |
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the preceding table, Java is required to execute Spark code on
    Zeppelin. Therefore, if it is not set up, install and set up java on any of the
    platforms mentioned earlier. The latest release of Apache Zeppelin can be downloaded
    from [https://zeppelin.apache.org/download.html](https://zeppelin.apache.org/download.html).
    Each release has three options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Binary package with all interpreters**: Contains all the support for many
    interpreters. For example, Spark, JDBC, Pig, Beam, Scio, BigQuery, Python, Livy,
    HDFS, Alluxio, Hbase, Scalding, Elasticsearch, Angular, Markdown, Shell, Flink,
    Hive, Tajo, Cassandra, Geode, Ignite, Kylin, Lens, Phoenix, and PostgreSQL are
    currently supported in Zeppelin.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Binary package with Spark interpreter**: Usually, this contains only the Spark
    interpreter. It also contains an interpreter net-install script.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Source**: You can also build Zeppelin with all the latest changes from the
    GitHub repository (more on this later). To show you how to install and configure
    Zeppelin, we have downloaded the binary package from this site''s mirror. Once
    you have downloaded it, unzip it somewhere on your machine. Suppose the path where
    you have unzipped the file is `/home/Zeppelin/`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building from the source
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can also build Zeppelin with all the latest changes from the GitHub repository.
    If you want to build from the source, you must first install the following dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Git**: Any version'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maven**: 3.1.x or higher'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**JDK**: 1.7 or higher'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you haven't installed Git and Maven yet, check out the Build requirements
    at [http://zeppelin.apache.org/docs/latest/install/build.html#build-requirements](http://zeppelin.apache.org/docs/latest/install/build.html#build-requirements).
    Due to page limitations, we have not discussed all the steps in detail. Interested
    readers should refer to this URL more details on Apache Zeppelin website at [http://zeppelin.apache.org/](http://zeppelin.apache.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Starting and stopping Apache Zeppelin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'On all UNIX-like platforms (such as Ubuntu, Mac, and so on), use the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If the preceding command is successfully executed, you should observe the following
    logs on the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2e1f12f-b74a-47d1-93bb-e67096f361ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Starting Zeppelin from the Ubuntu terminal'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are on Windows, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After Zeppelin has started successfully, go to `http://localhost:8080` with
    your web browser and you will see that Zeppelin is running. More specifically,
    you''ll see this on your browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/25a4ebe1-b037-46ed-8396-897842d096bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Zeppelin is running on http://localhost:8080'
  prefs: []
  type: TYPE_NORMAL
- en: 'Congratulations! You have successfully installed Apache Zeppelin! Now let''s
    access Zeppelin on the browser at `http://localhost:8080/` and get started on
    our data analytics once we have configured the preferred interpreter. Now, to
    stop Zeppelin from the command line, issue this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Creating notebooks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once you are on `http://localhost:8080/`, you can explore different options
    and menus that help you understand how to get familiar with Zeppelin. For more
    information on Zeppelin and its user-friendly UI, interested readers can refer
    to [http://zeppelin.apache.org/docs/latest/](http://zeppelin.apache.org/docs/latest/).
    Now let''s first create a sample notebook and get started. As shown in the following
    figure, you can create a new notebook by clicking on the Create new note option
    in *Figure 2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c900fcff-1938-4c1b-9163-da82e72ff41b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Creating a sample Zeppelin notebook'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 3*, the default interpreter is selected as Spark. In the
    drop-down list, you will see only Spark since you have downloaded the Spark-only
    binary package for Zeppelin.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory analysis of the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Well done! We have been able to install, configure, and get started with Zeppelin.
    Now let''s get going. We will see how the variables are correlated with the label.
    We start by loading the dataset in Apache, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Label distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see the class distribution. We will use the SQL interpreter for this.
    Just execute the following SQL query on the Zeppelin notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/f96956b3-12cf-4c1e-9024-0f97366e8cf1.png)'
  prefs: []
  type: TYPE_IMG
- en: Job distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s see whether the job titles have any correlation with the subscription
    decision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/532371f6-b5a0-4848-9ae2-d887205ec555.png)'
  prefs: []
  type: TYPE_IMG
- en: From the chart we can see that most of the clients have jobs as admin, blue-collar,
    or technician, while students and retired clients have the biggest *count(y) /
    count (n)* ratio.
  prefs: []
  type: TYPE_NORMAL
- en: Marital distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Does marital status have a correlation with the subscription decision? Let''s
    see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/1ff14701-0f9b-44b3-8397-eb053becfa63.png)'
  prefs: []
  type: TYPE_IMG
- en: The distribution shows that the subscriptions are proportional to the number
    of instances regardless of the marital status of the client.
  prefs: []
  type: TYPE_NORMAL
- en: Education distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s see whether educational status has a correlation with the subscription
    decision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/37719635-692d-47e2-bc3a-2975491b4102.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, similar to marital status, the education level gives no clue about the
    subscriptions. Now let's keep exploring other variables.
  prefs: []
  type: TYPE_NORMAL
- en: Default distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s check whether default credit has a correlation with the subscription
    decision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/fae02864-02b6-43f0-a1a9-3ab41e0559ff.png)'
  prefs: []
  type: TYPE_IMG
- en: This chart shows there are almost no clients with default credit and clients
    with no default credit have a slight subscription ratio.
  prefs: []
  type: TYPE_NORMAL
- en: Housing distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s see whether having a house has an interesting correlation with the
    subscription decision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/41a17ae7-7cc5-470e-9c42-04bf60185643.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding figures say that housing also gives no clue about subscription.
  prefs: []
  type: TYPE_NORMAL
- en: Loan distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s look at loan distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/0952fe25-77cb-45e5-a45a-59606d69a87f.png)'
  prefs: []
  type: TYPE_IMG
- en: The chart shows that most of the clients have no personal loan and loans have
    no effect on the subscription ratio.
  prefs: []
  type: TYPE_NORMAL
- en: Contact distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s check whether the medium of contact has a significant correlation
    with the subscription decision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/91093805-6701-40b3-8197-760d5d51be4d.png)'
  prefs: []
  type: TYPE_IMG
- en: Month distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It might sound funny, but the month of telemarketing can have a significant
    correlation with the subscription decision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/3d762f6c-9761-4bba-931f-1a56c2264df1.png)'
  prefs: []
  type: TYPE_IMG
- en: So, the preceding chart shows that the highest subscription ratios are obtained
    for months that have fewer instances (for example, December, March, October, and
    September).
  prefs: []
  type: TYPE_NORMAL
- en: Day distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, what about the day of the week and its correlation with the subscription
    decision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/83409632-61b3-45f3-bfca-d347404f5ce9.png)'
  prefs: []
  type: TYPE_IMG
- en: The day feature has a uniform distribution, so it's not that significant.
  prefs: []
  type: TYPE_NORMAL
- en: Previous outcome distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What about the previous outcome and its correlation with the subscription decision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/63c6ed5f-083c-42c5-8f2b-15a6fa28cca1.png)'
  prefs: []
  type: TYPE_IMG
- en: The distribution shows that clients with a successful outcome from the previous
    marketing campaign will most likely subscribe. At the same time, those clients
    represent the minority of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Age feature
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let us see how age correlates with the subscription decision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/131b8d6e-2689-49ce-b137-cfbfc515e96c.png)'
  prefs: []
  type: TYPE_IMG
- en: The normalized chart shows that most of the clients are aged between **25**
    and **60**.
  prefs: []
  type: TYPE_NORMAL
- en: The following chart shows that the bank gets a high subscription ratio with
    clients in the age interval *(25, 60)*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/edd13366-b56b-4710-8695-f48b18c45f3f.png)'
  prefs: []
  type: TYPE_IMG
- en: Duration distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let us now take a look at how the duration of calls is related to subscription:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/ac6d0053-9163-4cdc-81da-7eab79b18b99.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The charts show that most of the calls are short and the subscription ratio
    is proportional to the call duration. The expanded version provides better insight:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43a919d0-cf55-4db3-9fa9-26f3b90b2d42.png)'
  prefs: []
  type: TYPE_IMG
- en: Campaign distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we will see how campaign distribution is correlated to subscription:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/57baabd2-03b5-41e0-972c-b4cbd34b151f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The charts show that most of the clients are contacted less than five times
    and the more a client is contacted, the less he/she is likely to subscribe. Now
    the expanded version provides better insight:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a935ae2-27e5-4156-a391-ceb1c3e7bfc0.png)'
  prefs: []
  type: TYPE_IMG
- en: Pdays distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Lets us now take a look at how the `pdays` distribution is correlated to subscription:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/23830f8d-4edf-4be9-a0ce-d255e0d5be8f.png)'
  prefs: []
  type: TYPE_IMG
- en: The chart shows that most of the clients were not previously contacted.
  prefs: []
  type: TYPE_NORMAL
- en: Previous distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following command, we can see how the previous distribution affects
    the subscription:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/c9251b02-5d01-4058-a0ea-4e682fd557d5.png)'
  prefs: []
  type: TYPE_IMG
- en: Like the previous chart, this one confirms that most of the clients were not
    previously contacted for this campaign.
  prefs: []
  type: TYPE_NORMAL
- en: emp_var_rate distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following command shows how `emp_var_rate` distributions correlate with
    the subscription:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/c1bdd55d-b703-423e-8cfc-2f1b003b66c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The charts show that clients with a less common employment variation rate will
    be more likely to subscribe than other clients. Now the expanded version provides
    a better insight:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/810f87cd-3a6a-4004-a436-784f78bb8091.png)'
  prefs: []
  type: TYPE_IMG
- en: cons_price_idx features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The correlation between `con_price_idx` features and subscription can be computed
    by the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/6cac30e0-83dc-4271-be8e-f136816db9a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The charts show that clients with a less common consumer price index are more
    likely to subscribe compared to other clients. Now, the expanded version provides
    better insight:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e1158e07-d282-4269-b3de-bb6fcfbe9133.png)'
  prefs: []
  type: TYPE_IMG
- en: cons_conf_idx distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The correlation between `cons_conf_idx` distribution and subscription can be
    computed by the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/d57b72e9-5e2d-4b6a-97e7-64d876d2f5c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Clients with less common consumer confidence index are more likely to subscribe
    compared to other clients.
  prefs: []
  type: TYPE_NORMAL
- en: Euribor3m distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let us see how `euribor3m` distribution is correlated to subscription:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/08a74d07-cf57-43a2-8d3c-f289b2137fd7.png)'
  prefs: []
  type: TYPE_IMG
- en: This chart shows that the euribor 3-month rate has a large range and most of
    the clients cluster around four or five values of that feature.
  prefs: []
  type: TYPE_NORMAL
- en: nr_employed distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The correlation between `nr_employed` distribution and subscription can be
    seen with the help of the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/2345643f-0898-408b-98b3-0b9c37b36312.png)'
  prefs: []
  type: TYPE_IMG
- en: The chart shows that the subscription rate is inversely proportional to the
    number of employees.
  prefs: []
  type: TYPE_NORMAL
- en: Statistics of numeric features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now take a look at the statistics of numeric features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '| `summary` | `age` | `duration` | `campaign` | `pdays` | `previous` |'
  prefs: []
  type: TYPE_TB
- en: '| `count` | 41188.00 | 41188.00 | 41188.00 | 41188.00 | 41188.00 |'
  prefs: []
  type: TYPE_TB
- en: '| `mean` | 40.02 | 258.29 | 2.57 | 962.48 | 0.17 |'
  prefs: []
  type: TYPE_TB
- en: '| `stddev` | 10.42 | 259.28 | 2.77 | 186.91 | 0.49 |'
  prefs: []
  type: TYPE_TB
- en: '| `min` | 17.00 | 0.00 | 1.00 | 0.00 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| `max` | 98.00 | 4918.00 | 56.00 | 999.00 | 7.00 |'
  prefs: []
  type: TYPE_TB
- en: '| `q1` | 32.00 | 102.00 | 1.00 | 999.00 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| `median` | 38.00 | 180.00 | 2.00 | 999.00 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '| `q3` | 47.00 | 319.00 | 3.00 | 999.00 | 0.00 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| `summary` | `emp_var_rate` | `cons_price_idx` | `cons_conf_idx` | `euribor3m`
    | `nr_employed` |'
  prefs: []
  type: TYPE_TB
- en: '| `count` | 41188.00 | 41188.00 | 41188.00 | 41188.00 | 41188.00 |'
  prefs: []
  type: TYPE_TB
- en: '| `mean` | 0.08 | 93.58 | -40.50 | 3.62 | 5167.04 |'
  prefs: []
  type: TYPE_TB
- en: '| `stddev` | 1.57 | 0.58 | 4.63 | 1.73 | 72.25 |'
  prefs: []
  type: TYPE_TB
- en: '| `min` | -3.40 | 92.20 | -50.80 | 0.63 | 4963.60 |'
  prefs: []
  type: TYPE_TB
- en: '| `max` | 1.40 | 94.77 | -26.90 | 5.05 | 5228.10 |'
  prefs: []
  type: TYPE_TB
- en: '| `q1` | -1.80 | 93.08 | -42.70 | 1.34 | 5099.10 |'
  prefs: []
  type: TYPE_TB
- en: '| `median` | 1.10 | 93.75 | -41.80 | 4.86 | 5191.00 |'
  prefs: []
  type: TYPE_TB
- en: '| `q3` | 1.40 | 93.99 | -36.40 | 4.96 | 5228.10 |'
  prefs: []
  type: TYPE_TB
- en: Implementing a client subscription assessment model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To predict a client subscription assessment, we use the deep learning classifier
    implementation from H2O. First, we set up and create a Spark session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we load the dataset as a data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Although there are categorical features in this dataset, there is no need to
    use a `StringIndexer` since the categorical features have small domains. By indexing
    them, an order relationship that does not exist is introduced. Thus, a better
    solution is to use One Hot Encodng, and it turns out that H2O, by default, uses
    this encoding strategy for enumerations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the dataset description, I have already stated that the `duration` feature
    is only available after the label is known. So it can''t be used for prediction.
    Therefore, we should drop it as unavailable before calling the client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'So far, we have used Sparks built-in methods for loading the dataset and dropping
    unwanted features, but now we need to set up `h2o` and import its implicits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We then shuffle the training dataset and transform it into an H2O frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'String features are then converted into categorical (type "2 Byte" stands for
    the String type of H2O):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding line of code, `toCategorical()` is a user-defined function
    used to transform a string feature into a categorical feature. Here''s the signature
    of the method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it''s time to split the dataset into 60% training, 20% validation, and
    20% test datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we train the deep learning model using the training set and validate the
    training using the validation set, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding line, `buildDLModel()` is a user-defined function that sets
    up a deep learning model and trains it using the train and validation data frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code, we have instantiated a deep learning (that is, MLP) network of
    three hidden layers, L1 regularization and that intended to iterate the training
    for only 10 times. Note that these are hyperparameters and nothing is tuned. So
    feel free to change this and see the performance to get a set of the most optimized
    parameters. Once the training phase is completed, we print the training metrics
    (that is, AUC):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'About 81% accuracy does not seem good. We now evaluate the model on the test
    set. We predict the labels of the testing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we add the original labels to the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Transform the result into a Spark DataFrame and print the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/82558f0e-ad15-4fa7-bdd4-36432e211d52.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, the preceding confusion matrix can be represented by the following plot
    using Vegas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/c8fe633b-61e1-4663-aaca-6ec8241b7655.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Graphical representation of the confusion matrix—normalized (left)
    versus un-normalized (right)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s see the overall performance summary on the test set—that is, test
    AUC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/f70bf664-ff77-4418-8449-e73fd44992dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So the test accuracy in terms of AUC is 76%, which is not that great. But why
    don''t we iterate the training an additional number of times (say, 1,000 times)?
    Well, I leave it up to you. But still, we can visually inspect the precision-recall
    curve to see how the evaluation phase went:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/f0e403c2-05c7-459c-b50d-34ab5e5340fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Precision-recall curve'
  prefs: []
  type: TYPE_NORMAL
- en: 'We then compute and draw the sensitivity specificity curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/0c6c8a6a-1879-4928-af84-cd455a2b3eec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Sensitivity specificity curve'
  prefs: []
  type: TYPE_NORMAL
- en: Now the sensitivity specificity curve tells us the relationship between correctly
    predicted classes from both labels. For example, if we have 100% correctly predicted
    fraud cases, there will be no correctly classified non-fraud cases and vice versa.
    Finally, it would be great to take a closer look at this a little differently,
    by manually going through different prediction thresholds and calculating how
    many cases were correctly classified in the two classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, we can visually inspect true positives, false positives,
    true negatives, and false negatives over different prediction thresholds, say
    **0.0** to **1.0**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'First, let''s draw the true positive one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/651bd9cd-c73d-453f-9ee9-3b8a3cff2c1c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7: True positives across different prediction thresholds in [0.0, 1.0]
  prefs: []
  type: TYPE_NORMAL
- en: 'Secondly, let''s draw the false positive one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/92ccd496-3fee-4774-b0ef-bfd227041d9d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: False positives across different prediction thresholds in [0.0, 1.0]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, it''s the turn of the true negative ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/bc6a1193-63f9-428f-9637-c077bc88c951.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: False positives across different prediction thresholds in [0.0, 1.0]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s draw the false negative ones as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/721dfd31-3f89-4e42-9afa-9001575bec49.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: False positives across different prediction thresholds in [0.0,
    1.0]'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the preceding plots tell us that we can increase the number of correctly
    classified non-fraud cases without losing correctly classified fraud cases when
    we increase the prediction threshold from the default **0.5** to **0.6**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from these two auxiliary methods, I have defined three Scala case classes
    for computing `precision`, `recall`, `sensitivity`, `specificity`, true positives
    (`tp`), true negatives (`tn`), false positives (`fp`), false negatives (`fn`),
    and so on. The signature is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, stop the Spark session and H2O context. The `stop()` method invocation
    will shut down the H2O context and Spark cluster respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The first one especially is more important; otherwise, it sometimes does not
    stop the H2O flow but still holds the computing resources.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning and feature selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The flexibility of neural networks is also one of their main drawbacks: there
    are many hyperparameters to tweak. Even in a simple MLP, you can change the number
    of layers, the number of neurons per layer, the type of activation function to
    use in each layer, the number of epochs, the learning rate, weight initialization
    logic, drop-out keep probability, and so on. How do you know what combination
    of hyperparameters is best for your task?'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, you can use grid search with cross-validation to find the right hyperparameters
    for linear machine learning models, but for deep learning models, there are many
    hyperparameters to tune. And since training a neural network on a large dataset
    takes a lot of time, you will only be able to explore a tiny part of the hyperparameter
    space in a reasonable amount of time. Here are some useful insights.
  prefs: []
  type: TYPE_NORMAL
- en: Number of hidden layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For many problems, you can start with one or two hidden layers and it will work
    just fine using two hidden layers with the same total amount of neurons, in roughly
    the same amount of training time. For more complex problems, you can gradually
    ramp up the number of hidden layers until you start overfitting the training set.
    Very complex tasks, such as large image classification or speech recognition,
    typically require networks with dozens of layers and they need a large amount
    of training data.
  prefs: []
  type: TYPE_NORMAL
- en: Number of neurons per hidden layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Obviously, the number of neurons in the input and output layers is determined
    by the type of input and output your task requires. For example, if your dataset
    has a shape of 28 x 28, it should expect to have input neurons of size 784, and
    the output neurons should be equal to the number of classes to be predicted.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen in this project how it works in practice in the next example using
    MLP, where we set 256 neurons, four each for the hidden layers; that's just one
    hyperparameter to tune instead of one per layer. Just like the number of layers,
    you can try increasing the number of neurons gradually until the network starts
    overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In most cases, you can use the ReLU activation function in the hidden layers.
    It is a bit faster to compute than other activation functions, and gradient descent
    does not get stuck as much on plateaus compared to the logistic function or the
    hyperbolic tangent function, which usually saturated at one.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the output layer, the softmax activation function is generally a good choice
    for classification tasks. For regression tasks, you can simply use no activation
    function. Other activation functions include Sigmoid and Tanh. The current implementation
    of the H2O-based deep learning model supports the following activation functions:'
  prefs: []
  type: TYPE_NORMAL
- en: ExpRectifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ExpRectifierWithDropout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maxout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MaxoutWithDropout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rectifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RectifierWthDropout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tanh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TanhWithDropout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from Tanh (the default one in H2O), I have not tried any other activation
    functions for this project. However, you should definitely try.
  prefs: []
  type: TYPE_NORMAL
- en: Weight and bias initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Initializing the weight and biases for the hidden layers is an important hyperparameter
    to be taken care of:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Do not do all-zero initialization**: A reasonable-sounding idea might be
    to set all the initial weights to zero, but it does not work in practice because
    if every neuron in the network computes the same output, there will be no source
    of asymmetry between neurons as their weights are initialized to be the same.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Small random numbers**: It is also possible to initialize the weights of
    the neurons to small numbers but not identically zero. Alternatively, it is also
    possible to use small numbers drawn from a uniform distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Initializing the biases**: It is possible, and common, to initialize the
    biases to zero since the asymmetry breaking is provided by small random numbers
    in the weights. Setting the biases to a small constant value, such as 0.01 for
    all biases, ensures that all ReLU units can propagate a gradient. However, it
    neither performs well nor does consistent improvement. Therefore, sticking to
    zero is recommended.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several ways of controlling the training of neural networks to prevent
    overfitting in the training phase, for example, L2/L1 regularization, max-norm
    constraints, and dropout:'
  prefs: []
  type: TYPE_NORMAL
- en: '**L2 regularization**: This is probably the most common form of regularization.
    Using the gradient descent parameter update, L2 regularization signifies that
    every weight will be decayed linearly towards zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**L1 regularization**: For each weight *w*, we add the term λ∣w∣ to the objective.
    However, it is also possible to combine L1 and L2 regularization *to achieve*
    elastic net regularization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Max-norm constraints**: Used to enforce an absolute upper bound on the magnitude
    of the weight vector for each hidden layer neuron. Projected gradient descent
    can then be used further to enforce the constraint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dropout**: When working with a neural network, we need another placeholder
    for dropout, which is a hyperparameter to be tuned and the training time but not
    the test time. It is implemented by only keeping a neuron active with some probability,
    say *p<1.0*, or setting it to zero otherwise. The idea is to use a single neural
    net at test time without dropout. The weights of this network are scaled-down
    versions of the trained weights. If a unit is retained with `dropout_keep_prob`
    *< 1.0* during training, the outgoing weights of that unit are multiplied by *p*
    at test time (*Figure 17*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from these hyperparameters, another advantage of using H2O-based deep
    learning algorithms is that we can take the relative variable/feature importance.
    In previous chapters, we saw that by using the random forest algorithm in Spark,
    it is also possible to compute the variable importance.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the idea is that if your model does not perform well, it would be worth
    dropping the less important features and doing the training again. Now, it is
    possible to find the feature importance during supervised training. I have observed
    this feature importance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39aee4cf-5a16-4e45-80de-72b7c9e6ca4e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 25: Relative variable importance'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the question would be: why don''t you drop them and try training again
    to see if the accuracy has increased or not? Well, I leave it up to the readers.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how to develop a **machine learning** (**ML**) project
    using H2O on a bank marketing dataset for predictive analytics. We were able to
    predict that the client would subscribe to a term deposit with an accuracy of
    80%. Furthermore, we saw how to tune typical neural network hyperparameters. Considering
    the fact that this small-scale dataset, final improvement suggestion would be
    using Spark based Random Forest, Decision trees or gradient boosted trees for
    better accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will use a dataset having more than 284,807 instances
    of credit card use, where only 0.172% of transactions are fraudulent—that is,
    highly unbalanced data. So it would make sense to use autoencoders to pretrain
    a classification model and apply anomaly detection to predict possible fraud transaction—that
    is, we expect our fraud cases to be anomalies within the whole dataset.
  prefs: []
  type: TYPE_NORMAL
