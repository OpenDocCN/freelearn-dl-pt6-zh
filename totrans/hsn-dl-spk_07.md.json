["```py\nobject Nd4JScalaSample {\n  def main (args: Array[String]) {\n\n      // Create arrays using the numpy syntax\n      var arr1 = Nd4j.create(4)\n      val arr2 = Nd4j.linspace(1, 10, 10)\n\n      // Fill an array with the value 5 (equivalent to fill method in numpy)\n      println(arr1.assign(5) + \"Assigned value of 5 to the array\")\n\n      // Basic stats methods\n      println(Nd4j.mean(arr1) + \"Calculate mean of array\")\n      println(Nd4j.std(arr2) + \"Calculate standard deviation of array\")\n      println(Nd4j.`var`(arr2), \"Calculate variance\")\n\n     ...\n```", "```py\nval channels = 1\n val outputNum = 10\n val conf = new NeuralNetConfiguration.Builder()\n     .seed(seed)\n     .iterations(iterations)\n     .regularization(true)\n     .l2(0.0005)\n     .learningRate(.01)\n     .weightInit(WeightInit.XAVIER)\n     .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n     .updater(Updater.NESTEROVS)\n     .momentum(0.9)\n     .list\n     .layer(0, new ConvolutionLayer.Builder(5, 5)\n         .nIn(channels)\n         .stride(1, 1)\n         .nOut(20)\n         .activation(Activation.IDENTITY)\n         .build)\n     .layer(1, new\n     SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)\n         .kernelSize(2, 2)\n         .stride(2, 2)\n         .build)\n     .layer(2, new ConvolutionLayer.Builder(5, 5)\n         .stride(1, 1)\n         .nOut(50)\n         .activation(Activation.IDENTITY)\n         .build)\n     .layer(3, new\n     SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)\n         .kernelSize(2, 2)\n         .stride(2, 2)\n         .build)\n     .layer(4, new DenseLayer.Builder()\n         .activation(Activation.RELU)\n         .nOut(500)\n         .build)\n     .layer(5, new\n     OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)\n         .nOut(outputNum)\n         .activation(Activation.SOFTMAX).build)\n         .setInputType(InputType.convolutionalFlat(28, 28, 1))\n         .backprop(true).pretrain(false).build\n```", "```py\nval sparkConf = new SparkConf\n sparkConf.setMaster(master)\n     .setAppName(\"DL4J Spark MNIST Example\")\n val sc = new JavaSparkContext(sparkConf)\n```", "```py\nval trainDataList = mutable.ArrayBuffer.empty[DataSet]\n while (trainIter.hasNext) {\n     trainDataList += trainIter.next\n }\n\n val paralleltrainData = sc.parallelize(trainDataList)\n```", "```py\nvar batchSizePerWorker: Int = 16\n val tm = new ParameterAveragingTrainingMaster.Builder(batchSizePerWorker)\n     .averagingFrequency(5)\n     .workerPrefetchNumBatches(2)      \n     .batchSizePerWorker(batchSizePerWorker)\n     .build\n```", "```py\nval sparkNet = new SparkDl4jMultiLayer(sc, conf, tm)\n```", "```py\nvar numEpochs: Int = 15\n var i: Int = 0\n     for (i <- 0 until numEpochs) {\n     sparkNet.fit(paralleltrainData)\n     println(\"Completed Epoch {}\", i)\n }\n```", "```py\nval rng = new Random(12345)\n val lstmLayerSize: Int = 200\n val tbpttLength: Int = 50\n val nSamplesToGenerate: Int = 4\n val nCharactersToSample: Int = 300\n val generationInitialization: String = null\n val conf = new NeuralNetConfiguration.Builder()\n     .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n     .iterations(1)\n     .learningRate(0.1)\n     .rmsDecay(0.95)\n     .seed(12345)\n     .regularization(true)\n     .l2(0.001)\n     .weightInit(WeightInit.XAVIER)\n     .updater(Updater.RMSPROP)\n     .list\n     .layer(0, new GravesLSTM.Builder().nIn(SparkLSTMCharacterExample.CHAR_TO_INT.size).nOut(lstmLayerSize).activation(Activation.TANH).build())\n     .layer(1, new GravesLSTM.Builder().nIn(lstmLayerSize).nOut(lstmLayerSize).activation(Activation.TANH).build())\n     .layer(2, new RnnOutputLayer.Builder(LossFunction.MCXENT).activation(Activation.SOFTMAX)\n       .nIn(lstmLayerSize).nOut(SparkLSTMCharacterExample.nOut).build) //MCXENT + softmax for classification\n     .backpropType(BackpropType.TruncatedBPTT).tBPTTForwardLength(tbpttLength).tBPTTBackwardLength(tbpttLength)\n     .pretrain(false).backprop(true)\n     .build\n```", "```py\nval sparkNetwork: SparkDl4jMultiLayer = new SparkDl4jMultiLayer(sc, conf, tm)\n sparkNetwork.setListeners(Collections.singletonList[IterationListener](new ScoreIterationListener(1)))\n```", "```py\nval numEpochs: Int = 10\n```", "```py\n(0 until numEpochs).foreach { i =>\n     //Perform one epoch of training. At the end of each epoch, we are returned a copy of the trained network\n     val net = sparkNetwork.fit(trainingData)\n\n     //Sample some characters from the network (done locally)\n     println(\"Sampling characters from network given initialization \\\"\" +\n       (if (generationInitialization == null) \"\" else generationInitialization) + \"\\\"\")\n     val samples = ... // Implement your own sampling method\n\n     samples.indices.foreach { j =>\n       println(\"----- Sample \" + j + \" -----\")\n       println(samples(j))\n     }\n }\n```", "```py\ntm.deleteTempFiles(sc)\n```", "```py\n<dependency>\n   <groupId>org.nd4j</groupId>\n   <artifactId>nd4j-kryo_2.11</artifactId>\n   <version>0.9.1</version>\n </dependency>\n```", "```py\nval sparkConf = new SparkConf\n sparkConf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n sparkConf.set(\"spark.kryo.registrator\", \"org.nd4j.Nd4jRegistrator\")\n```", "```py\ngroupId: org.deeplearning4j\n artifactId: arbiter-deeplearning4j\n version: 0.9.1\n```", "```py\nval learningRateHyperparam = new ContinuousParameterSpace(0.0001, 0.1)  \nval layerSizeHyperparam = new IntegerParameterSpace(16, 256)\n```", "```py\nval hyperparameterSpace = new MultiLayerSpace.Builder\n    .weightInit(WeightInit.XAVIER)\n    .l2(0.0001)\n    .updater(new SgdSpace(learningRateHyperparam))\n    .addLayer(new DenseLayerSpace.Builder\n        .nIn(784)\n        .activation(Activation.LEAKYRELU)\n        .nOut(layerSizeHyperparam)\n        .build())\n    .addLayer(new OutputLayerSpace.Builder\n        .nOut(10)\n        .activation(Activation.SOFTMAX)\n        .lossFunction(LossFunctions.LossFunction.MCXENT)\n        .build)\n    .numEpochs(2)\n    .build\n```", "```py\nval candidateGenerator:CandidateGenerator = new RandomSearchGenerator(hyperparameterSpace, null)\n```", "```py\nval baseSaveDirectory = \"arbiterOutput/\"\nval file = new File(baseSaveDirectory)\nif (file.exists) file.delete\nfile.mkdir\nval modelSaver: ResultSaver = new FileModelSaver(baseSaveDirectory)\n```", "```py\nval scoreFunction:ScoreFunction = new EvaluationScoreFunction(Evaluation.Metric.ACCURACY)\n```", "```py\nval terminationConditions = Array(new MaxTimeCondition(15, TimeUnit.MINUTES), new MaxCandidatesCondition(20))\n```", "```py\nval configuration: OptimizationConfiguration = new OptimizationConfiguration.Builder\n    .candidateGenerator(candidateGenerator)\n    .dataSource(dataSourceClass,dataSourceProperties)\n    .modelSaver(modelSaver)\n    .scoreFunction(scoreFunction)\n    .terminationConditions(terminationConditions)\n    .build\n```", "```py\nval runner = new LocalOptimizationRunner(configuration, new MultiLayerNetworkTaskCreator())\nrunner.execute\n```", "```py\nval indexOfBestResult: Int = runner.bestScoreCandidateIndex\nval allResults = runner.getResults\n\nval bestResult = allResults.get(indexOfBestResult).getResult\nval bestModel = bestResult.getResult\n\nprintln(\"Configuration of the best model:\\n\")\nprintln(bestModel.getLayerWiseConfigurations.toJson)\n```", "```py\ngroupId: org.deeplearning4j\n artifactId: arbiter-ui_2.11\n version: 1.0.0-beta3\n```", "```py\nval ss: StatsStorage = new FileStatsStorage(new File(\"arbiterUiStats.dl4j\"))\nrunner.addListeners(new ArbiterStatusListener(ss))\nUIServer.getInstance.attach(ss)\n```", "```py\nhttp://:9000/arbiter\n```"]