<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Unity ML-Agents</h1>
                </header>
            
            <article>
                
<p class="mce-root"> Unity has embraced machine learning, and deep reinforcement learning <span>in particular, </span>with determination and vigor with the aim of producing a working <strong>seep reinforcement learning</strong> (<strong>DRL</strong>) SDK for game and simulation developers. Fortunately, the team at Unity, led by Danny Lange, has succeeded in developing a robust cutting-edge DRL engine capable of impressive results. This engine is the top of the line and outclasses the DQN model we introduced earlier in many ways. Unity uses a <strong>proximal policy optimization</strong> (<strong>PPO</strong>) model as the basis for its DRL engine. This model is significantly more complex and may differ in some ways, but, fortunately, this is at the start of many more chapters, and we will have plenty of time to introduce the concepts as we go—this is a hands-on book, after all.</p>
<p>In this chapter, we introduce the <strong>Unity ML-Agents</strong> tools and SDK for building DRL agents to play games and simulations. While this tool is both powerful and cutting-edge, it is also easy to use and provides a few tools to help us learn concepts as we go. In this chapter, we will cover the following topics:</p>
<ul>
<li>Installing ML-Agents</li>
<li>Training an agent</li>
<li>What's in a brain?</li>
<li>Monitoring training with TensorBoard</li>
<li>Running an agent</li>
</ul>
<div class="packt_infobox">We would like to thank the team members at Unity for their great work on ML-Agents; here are the team members at the time of writing:<br/>
<ul>
<li>Danny Lange (<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lange%2C+D" target="_blank">https://arxiv.org/search/cs?searchtype=author&amp;query=Lange%2C+D</a>)</li>
<li>Arthur Juliani (<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Juliani%2C+A" target="_blank">https://arxiv.org/search/cs?searchtype=author&amp;query=Juliani%2C+A</a>)</li>
<li>Vincent-Pierre Berges (<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Berges%2C+V" target="_blank">https://arxiv.org/search/cs?searchtype=author&amp;query=Berges%2C+V</a><span>)</span></li>
<li>Esh Vckay (<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Vckay%2C+E" target="_blank">https://arxiv.org/search/cs?searchtype=author&amp;query=Vckay%2C+E</a><span>)</span></li>
<li>Yuan Gao (<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gao%2C+Y" target="_blank">https://arxiv.org/search/cs?searchtype=author&amp;query=Gao%2C+Y</a><span>)</span></li>
<li>Hunter Henry (<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Henry%2C+H" target="_blank">https://arxiv.org/search/cs?searchtype=author&amp;query=Henry%2C+H</a><span>)</span></li>
<li>Marwan Mattar (<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mattar%2C+M" target="_blank">https://arxiv.org/search/cs?searchtype=author&amp;query=Mattar%2C+M</a><span>)</span></li>
<li>Adam Crespi (<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Crespi%2C+A" target="_blank">https://arxiv.org/search/cs?searchtype=author&amp;query=Crespi%2C+A</a><span>)</span></li>
<li>Jonathan Harper (<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Harper%2C+J" target="_blank">https://arxiv.org/search/cs?searchtype=author&amp;query=Harper%2C+J</a><span>)</span></li>
</ul>
</div>
<p>Be sure you have Unity installed as per the section in <a href="a8e699ff-c668-4601-842d-4c6e06c47a61.xhtml">Chapter 4</a>, <em>Building a Deep Learning Gaming Chatbot,</em> before proceeding with this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing ML-Agents</h1>
                </header>
            
            <article>
                
<p>In this section, we cover a high-level overview of the steps you will need to take in order to successfully install the ML-Agents SDK. This material is still in beta and has already changed significantly from version to version. As such, if you get stuck going through these high-level steps, just go back to the most recent Unity docs; they are very well written.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Jump on your computer and follow these steps; there may be many sub steps, so expect this to take a while:</p>
<ol>
<li>Be sure you have <strong>Git</strong> installed on your computer; it works from the command line. Git is a very popular source code management system, and there is a ton of resources on how to install and use Git for your platform. After you have installed Git, just be sure it works by test cloning a repository, any repository.</li>
<li>Open a command window or a regular shell. Windows users can open an Anaconda window.</li>
<li class="mce-root"><span>Change to a working folder where you want to place the new code, and enter the following command (Windows users may want to use</span> <kbd>C:\ML-Agents</kbd><span>):</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>git clone</strong> <strong>https://github.com/Unity-Technologies/ml-agents</strong></pre>
<ol start="4">
<li class="mce-root"><span>This will clone the</span> <kbd>ml-agents</kbd> repository onto your computer and create a new folder with the same name. You may want to take the extra step of also adding the version to the folder name. Unity, and pretty much the whole AI space, is in continuous transition, at least at the moment. This means new and constant changes are always happening. At the time of writing, we will clone to a folder named <kbd>ml-agents.6</kbd>, like so:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>git clone https://github.com/Unity-Technologies/ml-agents ml-agents.6</strong></pre>
<div class="packt_infobox">The author of this book previously wrote a book on ML-Agents and had to rewrite several chapters over the course of a short time in order to accommodate the major changes. In fact, this chapter has had to be also rewritten a few times to account for more major changes.</div>
<ol start="5">
<li>Create a new virtual environment for <kbd>ml-agents</kbd> and set it to <kbd>3.6</kbd>, like so:</li>
</ol>
<pre style="color: black;padding-left: 60px">#Windows <br/><strong>conda create -n ml-agents python=3.6</strong><br/><br/>#Mac<br/>Use the documentation for your preferred environment</pre>
<ol start="6">
<li>Activate the environment, again, using Anaconda:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>activate ml-agents</strong></pre>
<ol start="7">
<li>Install TensorFlow. With Anaconda, we can do this by using the following:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>pip install tensorflow==1.7.1</strong></pre>
<p class="mce-root"/>
<ol start="8">
<li>Install the Python packages. On Anaconda, enter the following:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>cd ML-Agents </strong>#from root folder<br/><strong>cd ml-agents</strong> or<strong> cd ml-agents.6  </strong>#for example<br/><strong>cd ml-agents</strong><br/><strong>pip install -e .</strong> or<strong> pip3 install -e .</strong></pre>
<ol start="9">
<li>This will install all the required packages for the Agents SDK and may take several minutes. Be sure to leave this window open, as we will use it shortly.</li>
</ol>
<div class="packt_tip">This is the basic installation of TensorFlow and does not use a GPU. Consult the Unity documentation in order to learn how to install the GPU version. This may or may not have a dramatic impact on your training performance, depending on the power of your GPU.</div>
<p>This should complete the setup of the Unity Python SDK for ML-Agents. In the next section, we will learn how to set up and train one of the many example environments provided by Unity.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training an agent</h1>
                </header>
            
            <article>
                
<p>For much of this book, we have spent our time looking at code and the inner depths of <strong>deep learning</strong> (<strong>DL</strong>) and <strong>reinforcement learning</strong> (<strong>RL</strong>). With that knowledge established, we can now jump in and look at examples where <strong>deep reinforcement learning</strong> (<strong>DRL</strong>) is put to use. Fortunately, the new agent's toolkit provides several examples to demonstrate the power of the engine. Open up Unity or the Unity Hub and follow these steps:</p>
<ol>
<li>Click the <span class="packt_screen">Open</span> project button at the top of the <span class="packt_screen">Project</span> dialog.</li>
<li>Locate and open the <kbd>UnitySDK</kbd> project folder as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign" style="color: black;font-size: 1em"><img src="assets/ecbdfb91-2cfb-47ef-b264-b8cfe8e45d3b.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Opening the UnitySDK project</div>
<ol start="3">
<li>Wait for the project to load and then open the <span class="packt_screen">Project</span> window at the bottom of the editor. If you are asked to update the project, just be sure to say <span class="packt_screen">yes</span> or <span class="packt_screen">continue</span>. Thus far, all of the agent code has been designed to be backward compatible.</li>
</ol>
<p class="mce-root"/>
<ol start="4">
<li>Locate and open the <span class="packt_screen">GridWorld</span> scene as shown in this screenshot:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign" style="color: black"><img src="assets/3456149c-1ca2-4371-b20e-bbba7590e13f.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign" style="color: black">Opening the GridWorld example scene</div>
<ol start="5">
<li>Select the <span class="packt_screen">GridAcademy</span> object in the <span class="packt_screen">Hierarchy</span> window. </li>
</ol>
<ol start="6">
<li class="mce-root"><span>Then direct your attention to the</span> <span class="packt_screen">Inspector</span> window, and beside the Brains, click the target icon to open the Brain selection dialog:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3dfa4068-3490-4a17-b3fb-8f968b14433f.png" style="width:38.00em;height:48.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign" style="color: black">Inspecting the GridWorld example environment</div>
<ol start="7">
<li>Select the <span class="packt_screen">GridWorldPlayer</span> brain. This brain is a <em>player</em> brain, meaning that a player, you, can control the game. We will look at this brain concept more in the next section.</li>
<li>Press the <span class="packt_screen">Play</span> button at the top of the editor and watch the grid environment form. Since the game is currently set to a player, you can use the <strong>WASD</strong> controls to move the cube. The goal is much like the FrozenPond environment we built a DQN for earlier. That is, you have to move the blue cube to the green <span class="packt_screen">+</span> symbol and avoid the red X.</li>
</ol>
<p>Feel free to play the game as much as you like. Note how the game only runs for a certain amount of time and is not turn-based. In the next section, we will learn how to run this example with a DRL agent.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What's in a brain?</h1>
                </header>
            
            <article>
                
<p>One of the brilliant aspects of the ML-Agents platform is the ability to switch from player control to AI/agent control very quickly and seamlessly. In order to do this, Unity uses the concept of a <strong>brain</strong>. A brain may be either player-controlled, a player brain, or agent-controlled, a learning brain. The brilliant part is that you can build a game and test it, as a player can then turn the game loose on an RL agent. This has the added benefit of making any game written in Unity controllable by an AI with very little effort. In fact, this is such a powerful workflow that we will spend an entire chapter, <span><span><a href="323523c2-82f9-48c4-b1b5-35d417f90558.xhtml">Chapter 12</a>, <em>Debugging/Testing a Game with DRL</em>,</span></span> on testing and debugging your games with RL.</p>
<p>Training an RL agent with Unity is fairly straightforward to set up and run. Unity uses Python externally to build the learning brain model. Using Python makes far more sense, since as we have already seen, several DL libraries are built on top of it. Follow these steps to train an agent for the GridWorld environment:</p>
<ol>
<li class="mce-root"><span>Select the</span> <span class="packt_screen">GridAcademy</span> <span>again and switch the</span> <span class="packt_screen">Brains</span> <span>from</span> <span class="packt_screen">GridWorldPlayer</span> <span>to</span> <span class="packt_screen">GridWorldLearning</span> <span>as shown:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/574c1a5c-98dd-4d89-8d2a-5eb9a81d1eb4.png" style="width:38.25em;height:38.92em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign" style="color: black">Switching the brain to use GridWorldLearning</div>
<ol start="2">
<li>Make sure to click the <span class="packt_screen">Control</span> option at the end. This simple setting is what tells the brain it may be controlled externally. Be sure to double-check that the option is enabled.</li>
<li class="mce-root"><span>Select the</span> <span class="packt_screen">trueAgent</span> <span>object in the <span class="packt_screen">Hierarchy</span> window, and then, in the</span> <span class="packt_screen">Inspector</span> <span>window, change the</span> <span class="packt_screen">Brain</span> <span>property under the</span> <span class="packt_screen">Grid Agent</span> <span>component to a</span> <span class="packt_screen">GridWorldLearning</span> <span>brain:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/438357ea-b6c5-49c3-a2a2-9e6d44c7f0ee.png" style="width:30.25em;height:42.58em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign" style="color: black">Setting the brain on the agent to GridWorldLearning</div>
<ol start="4">
<li>For this sample, we want to switch our <span class="packt_screen">Academy</span> and <span class="packt_screen">Agent</span> to use the same brain, <span class="packt_screen">GridWorldLearning</span>. In more advanced cases we will explore later, this is not always the case. You could of course have a player and an agent brain running in tandem, or many other configurations.</li>
<li>Be sure you have an Anaconda or Python window open and set to the <kbd>ML-Agents/ml-agents</kbd> folder or your versioned <kbd>ml-agents</kbd> folder. </li>
<li class="mce-root"><span>Run the following command in the Anaconda or Python window using the</span> <kbd>ml-agents</kbd> virtual environment:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=firstRun --train</strong></pre>
<ol start="7">
<li>This will start the Unity PPO trainer and run the agent example as configured. At some point, the command window will prompt you to run the Unity editor with the environment loaded.</li>
<li>Press <span class="packt_screen">Play</span> in the Unity editor to run the <span class="packt_screen">GridWorld</span> environment. Shortly after, you should see the agent training with the results being output in the Python script window:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign" style="text-align: center;color: black"><img src="assets/28c402ce-1770-4986-ad61-3250d518b949.png"/></div>
<div class="packt_figref" style="text-align: center;color: black">Running the GridWorld environment in training mode</div>
<ol start="9">
<li>Note how the <kbd>mlagents-learn</kbd> script is the Python code that builds the RL model to run the agent. As you can see from the output of the script, there are several parameters, or what we refer to as <strong>hyper-parameters</strong>, that need to be configured. Some of these parameters may sound familiar, and they should, but several may be unclear. Fortunately, for the rest of this chapter and this book, we will explore how to tune these parameters in some detail.</li>
<li>Let the agent train for several thousand iterations and note how quickly it learns. The internal model here, called <strong>PPO</strong>, has been shown to be a very effective learner at multiple forms of tasks and is very well suited for game development. Depending on your hardware, the agent may learn to perfect this task in less than an hour.</li>
</ol>
<p>Keep the agent training, and we will look at more ways to inspect the agent's training progress in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monitoring training with TensorBoard</h1>
                </header>
            
            <article>
                
<p>Training an agent with RL, or any DL model for that matter, while enjoyable, is not often a simple task and requires some attention to detail. Fortunately, TensorFlow ships with a set of graph tools called <strong>TensorBoard</strong> we can use to monitor training progress. Follow these steps to run TensorBoard:</p>
<ol>
<li>Open an Anaconda or Python window. Activate the <kbd>ml-agents</kbd> virtual environment. Don't shut down the window running the trainer; we need to keep that going.</li>
<li class="mce-root"><span>Navigate to the <kbd>ML-Agents/ml-agents</kbd> folder and run the following command:</span></li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>tensorboard --logdir=summaries</strong></pre>
<ol start="3">
<li>This will run TensorBoard with its own built-in web server. You can load the page by using the URL that is shown after you run the previous command.</li>
<li>Enter the URL for TensorBoard as shown in the window, or use <kbd>localhost:6006</kbd> or <kbd>machinename:6006</kbd> in your browser. After an hour or so, you should see something similar to the following:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/40e5492f-96ed-4f05-b12a-b74a9ddcd12c.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign" style="color: black">The TensorBoard graph window</div>
<ol start="5">
<li>In the preceding screenshot, you can see each of the various graphs denoting an aspect of training. Understanding each of these graphs is important to understanding how your agent is training, so we will break down the output from each section:
<ul>
<li><span class="packt_screen">Environment</span>: This section shows how the agent is performing overall in the environment. A closer look at each of the graphs is shown in the following screenshot with their preferred trend:</li>
</ul>
</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f404a429-9388-4c55-8bef-433bd24e2523.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Closer look at the Environment section plots</div>
<ul>
<li style="padding-left: 60px"><span class="packt_screen">Cumulative Reward</span>: This is the total reward the agent is maximizing. You generally want to see this going up, but there are reasons why it may fall. It is always best to maximize rewards in the range of 1 to -1. If you see rewards outside this range on the graph, you also want to correct this as well.</li>
<li style="padding-left: 60px"><span class="packt_screen">Episode Length</span>: I<span>t usually is a better sign if this value decreases. After all, shorter episodes mean more training. However, keep in mind that the episode length could increase out of need, so this one can go either way.</span></li>
<li style="padding-left: 60px"><span class="packt_screen">Lesson</span>: This represents which lesson the agent is on and is intended for Curriculum Learning. We will learn more about Curriculum Learning in <a href="ae184eca-6c9d-456e-a72b-85274ddcc10c.xhtml">Chapter 9</a>, <em>Rewards and Reinforcement Learning</em>. </li>
<li style="padding-left: 60px"><span class="packt_screen">Losses</span>: This section shows graphs that represent the calculated loss or cost of the policy and value. Of course, we haven't spent much time explaining PPO and how it uses a policy, so, at this point, just understand the preferred direction when training. A screenshot of this section is shown next, again with arrows showing the optimum preferences:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img src="assets/357e6132-9381-4c6b-bde8-3b66bd378e4f.png" style="width:48.42em;height:22.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign" style="color: black">Losses and preferred training direction</div>
<ul>
<li style="padding-left: 60px"><span class="packt_screen">Policy Loss</span>: This determines how much the policy is changing over time. The policy is the piece that decides the actions, and in general this graph should be showing a downward trend, indicating that the policy is getting better at making decisions.</li>
<li style="padding-left: 60px"><span class="packt_screen">Value Loss</span>: This is the mean or average loss of the <kbd>value</kbd> function. It essentially models how well the agent is predicting the value of its next state. Initially, this value should increase, and then after the reward is stabilized, it should decrease.</li>
<li style="padding-left: 60px"><span class="packt_screen">Policy</span><span>: PPO uses the concept of a policy rather than a model to determine the quality of actions. Again, we will spend more time on this in <a href="1393797c-79cd-46c3-8e43-a09a7750fc92.xhtml">Chapter 8</a>, <em>Understanding PPO,</em> where we will uncover further details about PPO. The next screenshot shows the policy graphs and their preferred trend:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="assets/cc002fa5-7562-415a-93ea-f4f5ab381170.png" style="color: #333333;font-size: 1em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign" style="color: black">Policy graphs and preferred trends</div>
<ul>
<li style="padding-left: 60px"><span class="packt_screen">Entropy</span>: This represents how much the agent is exploring. You want this value to decrease as the agent learns more about its surroundings and needs to explore less.</li>
<li style="padding-left: 60px"><span class="packt_screen">Learning Rate</span>: Currently, this value is set to decrease linearly over time.</li>
<li style="padding-left: 60px"><span class="packt_screen">Value Estimate</span>: This is the mean or average value visited by all states of the agent. This value should increase in order to represent a growth of the agent's knowledge and then stabilize.</li>
</ul>
<div class="packt_infobox">These graphs are all designed to work with the implementation of the PPO method Unity is based on. Don't worry too much about understanding these new terms just yet. We will explore the foundations of PPO in <a href="9b7b6ff8-8daa-42bd-a80f-a7379c37c011.xhtml">Chapter 7</a>, <em>Agent and the Environment</em>.</div>
<ol start="6">
<li>Let the agent run to completion and keep TensorBoard running.</li>
<li>Go back to the Anaconda/Python window that was training the brain and run this command:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=secondRun --train</strong></pre>
<ol start="8">
<li>You will again be prompted to press <span class="packt_screen">Play</span> in the editor; be sure to do so. Let the agent start the training and run for a few sessions. As you do so, monitor the TensorBoard window and note how the <kbd>secondRun</kbd> is shown on the graphs. Feel free to let this agent run to completion as well, but you can stop it now, if you want to.</li>
</ol>
<p>In previous versions of ML-Agents, you needed to build a Unity executable first as a game-training environment and run that. The external Python brain would still run the same. This method made it very difficult to debug any code issues or problems with your game. All of these difficulties were corrected with the current method; however, we may need to use the old executable method later for some custom training.</p>
<p>Now that we have seen how easy it is to set up and train an agent, we will go through the next section to see how that agent can be run without an external Python brain and run directly in Unity.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running an agent</h1>
                </header>
            
            <article>
                
<p>Using Python to train works well, but it is not something a real game would ever use. Ideally, what we want to be able to do is build a TensorFlow graph and use it in Unity. Fortunately, a library was constructed, called TensorFlowSharp, that allows .NET to consume TensorFlow graphs. This allows us to build offline TFModels and later inject them into our game. Unfortunately, we can only use trained models and not train in this manner, at least not yet.</p>
<p>Let's see how this works by using the graph we just trained for the <span class="packt_screen">GridWorld</span> environment and use it as an internal brain in Unity. Follow the exercise in the next section to set up and use an internal brain:</p>
<ol>
<li>Download the TFSharp plugin from this link: <a href="https://s3.amazonaws.com/unity-ml-agents/0.5/TFSharpPlugin.unitypackage">https://s3.amazonaws.com/unity-ml-agents/0.5/TFSharpPlugin.unitypackage</a>.<a href="https://s3.amazonaws.com/unity-ml-agents/0.5/TFSharpPlugin.unitypackage"><br/></a></li>
</ol>
<div class="packt_tip">If this link does not work, consult the Unity docs or the Asset Store for a new one. The current version is described as experimental and subject to change.</div>
<ol start="2">
<li>From the editor menu, select <span class="packt_screen">Assets</span> | <span class="packt_screen">Import Package</span> | <span class="packt_screen">Custom Package... </span></li>
<li>Locate the asset package you just downloaded and use the import dialogs to load the plugin into the project. If you need help with these basic Unity tasks, there is plenty of help online that can guide you further.</li>
</ol>
<ol start="4">
<li>From the menu, select <span class="packt_screen">Edit</span> | <span class="packt_screen">Project Settings</span>. This will open the <span class="packt_screen">Settings</span> window (new in 2018.3)</li>
<li class="mce-root"><span>Locate under the <span class="packt_screen">Player</span> options the</span> <span class="packt_screen">Scripting Define Symbols</span><span> and set the text to</span> <kbd>ENABLE_TENSORFLOW</kbd><span class="packt_screen"> </span><span>and enable</span> <span class="packt_screen">Allow Unsafe Code</span>, as sho<span>wn in this screenshot:</span></li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign" style="color: black"><img src="assets/ab714c99-1b2d-4e27-89e4-eff8395c7f43.png" style="width:39.42em;height:46.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign" style="color: black">Setting the ENABLE_TENSORFLOW flag</div>
<ol start="6">
<li>Locate the <span class="packt_screen">GridWorldAcademy</span> object in the <span class="packt_screen">Hierarchy</span> window and make sure it is using the <span class="packt_screen">Brains </span>| <span class="packt_screen">GridWorldLearning</span>. Turn the <span class="packt_screen">Control</span> option off under the <span class="packt_screen">Brains</span> section of the <span class="packt_screen">Grid Academy</span> script.</li>
<li>Locate the <span class="packt_screen">GridWorldLearning</span> brain in the <kbd>Assets/Examples/GridWorld/Brains</kbd> folder and make sure the <span class="packt_screen">Model</span> parameter is set in the <span class="packt_screen">Inspector</span> window, as shown in this screenshot:</li>
</ol>
<div class="CDPAlignCenter packt_figref CDPAlign" style="color: black;font-size: 1em"><img src="assets/321101c9-21cb-4a31-b298-dc4c3a51cc32.png" style="width:30.50em;height:43.00em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Setting the model for the brain to use</div>
<ol start="8">
<li>The<span class="packt_screen"><span> </span>Model</span><span> </span>should already be set to the<strong><span class="packt_screen"><span> </span></span></strong><span class="packt_screen">GridWorldLearning</span><strong><span class="packt_screen"><span> </span></span></strong>model. In this example, we are using the TFModel that is shipped with the <span class="packt_screen">GridWorld</span> example. You could also easily use the model we had trained from the earlier example by just importing it into the project and then setting it as the model.</li>
<li>Press <span class="packt_screen">Play</span> to run the editor and watch the agent control the cube.</li>
</ol>
<p>Right now, we are running the environment with the pre-trained Unity brain. In the next section, we will look at how to use the brain we trained in the previous section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading a trained brain</h1>
                </header>
            
            <article>
                
<p>All of the Unity samples come with pre-trained brains you can use to explore the samples. Of course, we want to be able to load our own TF graphs into Unity and run them. Follow the next steps in order to load a trained graph:</p>
<ol>
<li class="mce-root"><span>Locate the</span> <kbd>ML-Agents/ml-agents/models/firstRun-0</kbd> <span>folder. Inside this folder, you should see a file named</span> <kbd>GridWorldLearning.bytes</kbd><span>. Drag this file into the Unity editor into the</span> <kbd>Project/Assets/ML-Agents/Examples/GridWorld/TFModels</kbd> <span>folder, as shown:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a41646b7-b3df-4fdf-a9bc-2ca3f9370961.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign" style="color: black">Dragging the bytes graph into Unity</div>
<ol start="2">
<li>This will import the graph into the Unity project as a resource and rename it <kbd>GridWorldLearning 1</kbd>. It does this because the default model already has the same name.</li>
</ol>
<ol start="3">
<li><span>Locat</span><span>e the</span> <kbd>GridWorldLearning</kbd> from the <kbd>brains</kbd> folder and select it in the <span class="packt_screen">Inspector</span> windows and drag the new <span class="packt_screen">GridWorldLearning 1</span> model onto the <span class="packt_screen">Model</span> slot under the <span class="packt_screen">Brain Parameters</span>:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign" style="color: black"><img src="assets/21bef292-7d4d-4f6f-a20c-9243c3c4bb8e.png" style="width:34.17em;height:47.00em;"/></div>
<p class="mce-root"/>
<div class="packt_figref CDPAlignCenter CDPAlign" style="color: black">Loading the Graph Model slot in the brain</div>
<ol start="4">
<li>We won't need to change any other parameters at this point, but pay special attention to how the brain is configured. The defaults will work for now.</li>
<li>Press <span class="packt_screen">Play</span> in the Unity editor and watch the agent run through the game successfully.</li>
<li>How long you trained the agent for will really determine how well it plays the game. If you let it complete the training, the agent should be equal to the already trained Unity agent.</li>
</ol>
<p>There are plenty of Unity samples that you can now run and explore on your own. Feel free to train several of the examples on your own or as listed in the exercises in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<p>Use the exercises in this section to enhance and reinforce your learning. Attempt at least a few of these exercises on your own, and remember this is really for your benefit:</p>
<ol>
<li>Set up and run the <span class="packt_screen">3DBall</span> example environment to train a working agent. This environment uses multiple games/agents to train.</li>
<li>Set the <span class="packt_screen">3DBall</span> example to let half of the games use an already trained brain and the other to use training or external learning.</li>
<li>Train the <span class="packt_screen">PushBlock</span> environment agents using external learning.</li>
<li>Train the <span class="packt_screen">VisualPushBlock</span> environment. Note how this example uses a visual camera to capture the environment state.</li>
<li>Run the <span class="packt_screen">Hallway</span> scene as a player and then train the scene using an external learning brain.</li>
<li>Run the <span class="packt_screen">VisualHallway</span> scene as a player and then train the scene using an external learning brain.</li>
<li>Run the <span class="packt_screen">WallJump</span> scene and then run it under training conditions. This example uses Curriculum Training, which we will look at further in <a href="ae184eca-6c9d-456e-a72b-85274ddcc10c.xhtml">Chapter 9</a>, <em>Rewards and Reinforcement Learning</em>.</li>
<li>Run the <span class="packt_screen">Pyramids</span> scene and then set it up for training.</li>
<li>Run the <span class="packt_screen">VisualPyramids</span> scene and set it up for training.</li>
<li>Run the <span class="packt_screen">Bouncer</span> scene and set it up for training.</li>
</ol>
<p>While you don't have to run all these exercises/examples, it can be helpful to familiarize yourself with them. They can often be the basis for creating new environments, as we will see in the next chapter.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>As you have learned, the workflow for training RL and DRL agents in Unity is much more integrated and seamless than in OpenAI Gym. We didn't have to write a line of code to train an agent in a grid world environment, and the visuals are just plain better. For this chapter, we started by installing the ML-Agents toolkit. Then we loaded up a <span class="packt_screen">GridWorld</span> environment and set it up to train with an RL agent. From there, we looked at TensorBoard for monitoring agent training and progress. After we were done training, we first loaded up a Unity pre-trained brain and ran that in the <span class="packt_screen">GridWorld</span> environment. Then we used a brain we just trained and imported that into Unity as an asset and then as the <span class="packt_screen">GridWorldLearning</span> brain's model.</p>
<p>In the next chapter, we will explore how to construct a new RL environment or game we can use an agent to learn and play. This will allow us to peek under the covers further about the various details we skimmed over in this chapter.</p>


            </article>

            
        </section>
    </body></html>