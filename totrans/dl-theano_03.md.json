["```py\nwget http://mattmahoney.net/dc/text8.zip -O /sharedfiles/text8.gz\ngzip -d /sharedfiles/text8.gz -f\n```", "```py\n    words = []\n    with open('data/text8') as fin:\n      for line in fin:\n        words += [w for w in line.strip().lower().split()]\n\n    data_size = len(words)  \n    print('Data size:', data_size)\n    ```", "```py\n    unkown_token = '<UNK>'\n    pad_token = '<PAD>' # for padding the context\n    max_df = 5 # maximum number of freq\n    word_freq = [[unkown_token, -1], [pad_token, 0]]\n    word_freq.extend(Counter(words).most_common())\n    word_freq = OrderedDict(word_freq)\n    word2idx = {unkown_token: 0, pad_token: 1}\n    idx2word = {0: unkown_token, 1: pad_token}\n    idx = 2\n    for w in word_freq:\n      f = word_freq[w]\n      if f >= max_df:\n        word2idx[w] = idx\n        idx2word[idx] = w\n        idx += 1\n      else:\n        word2idx[w] = 0 # map the rare word into the unkwon token\n        word_freq[unkown_token] += 1 # increment the number of unknown tokens\n\n    data = [word2idx[w] for w in words]\n\n    del words # for reduce mem use\n\n    vocabulary_size = len(word_freq)\n    most_common_words = list(word_freq.items())[:5]\n    print('Most common words (+UNK):', most_common_words)\n    print('Sample data:', data[:10], [idx2word[i] for i in data[:10]])\n\n    *Data size: 17005207*\n    *Most common words (+UNK): [('<UNK>', 182564), ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]*\n    *Sample data: [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']*\n\n    ```", "```py\n    def get_sample(data, data_size, word_idx, pad_token, c = 1):\n\n      idx = max(0, word_idx - c)\n      context = data[idx:word_idx]\n      if word_idx + 1 < data_size:\n        context += data[word_idx + 1 : min(data_size, word_idx + c + 1)]\n      target = data[word_idx]\n      context = [w for w in context if w != target]\n      if len(context) > 0:\n        return target, context + (2 * c - len(context)) * [pad_token]\n      return None, None\n\n    def get_data_set(data, data_size, pad_token, c=1):\n      contexts = []\n      targets = []\n      for i in xrange(data_size):\n        target, context =  get_sample(data, data_size, i, pad_token, c)\n        if not target is None:\n          contexts.append(context)\n          targets.append(target)\n\n      return np.array(contexts, dtype='int32'), np.array(targets, dtype='int32')\n    ```", "```py\nimport theano\nimport theano.tensor as T\nimport numpy as np\nimport math\ncontext = T.imatrix(name='context')\ntarget = T.ivector('target')\n```", "```py\nvocab_size = len(idx2word)\nemb_size = 128\nW_in_values = np.asarray(np.random.uniform(-1.0, 1.0, \n\t(vocab_size, emb_size)),\ndtype=theano.config.floatX)\n\nW_out_values = np.asarray(np.random.normal(\n\tscale=1.0 / math.sqrt(emb_size),\n   size=(emb_size, vocab_size)),\n   dtype=theano.config.floatX)\n\nW_in = theano.shared(value=W_in_values,\n                      name='W_in',\n                      borrow=True)\n\nW_out = theano.shared(value=W_out_values,\n                      name='W_out',\n                      borrow=True)\n\nparams = [W_in, W_out]\n```", "```py\nh = T.mean(W_in[context], axis=1) \n\nFor the hidden -> output layer (eq. 2)\nuj = T.dot(h, W_out) \n```", "```py\np_target_given_contex = T.nnet.softmax(uj).dimshuffle(1, 0)\n```", "```py\nloss = -T.mean(T.log(p_target_given_contex)[T.arange(target.shape[0]), target]) \n```", "```py\ng_params = T.grad(cost=loss, wrt=params)\nupdates = [\n        (param, param - learning_rate * gparam)\n        for param, gparam in zip(params, g_params)\n]\n```", "```py\ncontexts, targets = get_data_set(data, data_size, word2idx[pad_token], c=2)\n\ncontexts = theano.shared(contexts)\ntargets = theano.shared(targets)\n\nindex = T.lscalar('index')\n\ntrain_model = theano.function(\n    inputs=[index],\n    outputs=[loss],\n    updates=updates,\n    givens={\n        context: contexts[index * batch_size: (index + 1) * batch_size],\n        target: targets[index * batch_size: (index + 1) * batch_size]\n    }\n)\n```", "```py\nvalid_samples = T.ivector('valid_samples') \n```", "```py\nembeddings = params[0]\nnorm = T.sqrt(T.sum(T.sqr(embeddings), axis=1, keepdims=True))\nnormalized_embeddings = W_in / norm\n\nvalid_embeddings = normalized_embeddings[valid_samples]\n```", "```py\nsimilarity = theano.function([valid_samples], T.dot(valid_embeddings, normalized_embeddings.T))\n```", "```py\n    valid_size = 16     # Random set of words to evaluate similarity on.\n    valid_window = 100  # Only pick dev samples in the head of the distribution.\n    valid_examples = np.array(np.random.choice(valid_window, valid_size, replace=False), dtype='int32')\n\n    n_epochs = 100\n    n_train_batches = data_size // batch_size\n    n_iters = n_epochs * n_train_batches\n    train_loss = np.zeros(n_iters)\n    average_loss = 0\n\n    for epoch in range(n_epochs):\n        for minibatch_index in range(n_train_batches):\n\n            iteration = minibatch_index + n_train_batches * epoch\n            loss = train_model(minibatch_index)\n            train_loss[iteration] = loss\n            average_loss += loss\n\n            if iteration % 2000 == 0:\n\n              if iteration > 0:\n                average_loss /= 2000\n                # The average loss is an estimate of the loss over the last 2000 batches.\n                print(\"Average loss at step \", iteration, \": \", average_loss)\n                average_loss = 0  \n\n            # Note that this is expensive (~20% slowdown if computed every 500 steps)\n            if iteration % 10000 == 0:\n\n              sim = similarity(valid_examples)\n              for i in xrange(valid_size):\n                  valid_word = idx2word[valid_examples[i]]\n                  top_k = 8 # number of nearest neighbors\n                  nearest = (-sim[i, :]).argsort()[1:top_k+1]\n                  log_str = \"Nearest to %s:\" % valid_word\n                  for k in xrange(top_k):\n                      close_word = idx2word[nearest[k]]\n                      log_str = \"%s %s,\" % (log_str, close_word)\n                  print(log_str)\n    ```", "```py\n    def save_params(outfile, params):\n        l = []\n        for param in params:\n            l = l + [ param.get_value() ]\n        numpy.savez(outfile, *l)\n        print(\"Saved model parameters to {}.npz\".format(outfile))\n\n    def load_params(path, params):\n        npzfile = numpy.load(path+\".npz\")\n        for i, param in enumerate(params):\n            param.set_value( npzfile[\"arr_\" +str(i)] )\n        print(\"Loaded model parameters from {}.npz\".format(path))\n    ```", "```py\n    *Using gpu device 1: Tesla K80 (CNMeM is enabled with initial size: 80.0% of memory, cuDNN 5105)*\n    *Data size 17005207*\n    *Most common words (+UNK) [('<UNK>', 182565), ('<PAD>', 0), ('the', 1061396), ('of', 593677), ('and', 416629)]*\n    *Sample data [5240, 3085, 13, 7, 196, 3, 3138, 47, 60, 157] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']*\n    *Average loss at step  0 :  11.2959747314*\n    *Average loss at step  2000 :  8.81626828802*\n    *Average loss at step  4000 :  7.63789177912*\n    *Average loss at step  6000 :  7.40699760973*\n    *Average loss at step  8000 :  7.20080085599*\n    *Average loss at step  10000 :  6.85602856147*\n    *Average loss at step  12000 :  6.88123817992*\n    *Average loss at step  14000 :  6.96217652643*\n    *Average loss at step  16000 :  6.53794862854*\n    *...*\n\n    *Average loss at step  26552000 :  4.52319500107*\n    *Average loss at step  26554000 :  4.55709513521*\n    *Average loss at step  26556000 :  4.62755958384*\n    *Average loss at step  26558000 :  4.6266620369*\n    *Average loss at step  26560000 :  4.82731778347*\n    *Nearest to system: systems, network, device, unit, controller, schemes, vessel, scheme,*\n    *Nearest to up: off, out, alight, forth, upwards, down, ordered, ups,*\n    *Nearest to first: earliest, last, second, next, oldest, fourth, third, newest,*\n    *Nearest to nine: apq, nineteenth, poz, jyutping, afd, apod, eurocents, zoolander,*\n    *Nearest to between: across, within, involving, with, among, concerning, through, from,*\n    *Nearest to state: states, provincial, government, nation, gaeltachta, reservation, infirmity, slates,*\n    *Nearest to are: were, is, aren, was, include, have, weren, contain,*\n    *Nearest to may: might, should, must, can, could, would, will, cannot,*\n    *Nearest to zero: hundred, pounders, hadza, cest, bureaus, eight, rang, osr,*\n    *Nearest to that: which, where, aurea, kessai, however, unless, but, although,*\n    *Nearest to can: could, must, cannot, should, may, will, might, would,*\n    *Nearest to s: his, whose, its, castletown, cide, codepoint, onizuka, brooklands,*\n    *Nearest to will: would, must, should, could, can, might, shall, may,*\n    *Nearest to their: its, his, your, our, her, my, the, whose,*\n    *Nearest to but: however, though, although, which, while, whereas, moreover, unfortunately,*\n    *Nearest to not: never, indeed, rarely, seldom, almost, hardly, unable, gallaecia,*\n    *Saved model parameters to model.npz*\n\n    ```", "```py\n    def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n      assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n      plt.figure(figsize=(18, 18))  #in inches\n      for i, label in enumerate(labels):\n        x, y = low_dim_embs[i,:]\n        plt.scatter(x, y)\n        plt.annotate(label,\n                     xy=(x, y),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n\n      plt.savefig(filename)\n\n    from sklearn.manifold import TSNE\n    import matplotlib.pyplot as plt\n\n    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n    plot_only = 500\n    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only,:])\n    labels = [idx2word[i] for i in xrange(plot_only)]\n    plot_with_labels(low_dim_embs, labels)\n    ```", "```py\nanalogy_a = T.ivector('analogy_a')  \nanalogy_b = T.ivector('analogy_b')  \nanalogy_c = T.ivector('analogy_c')\n```", "```py\na_emb = embeddings[analogy_a]  # a's embs\nb_emb = embeddings[analogy_b]  # b's embs\nc_emb = embeddings[analogy_c]  # c's embs\n```", "```py\n dist = T.dot(target, embeddings.T)\n```", "```py\npred_idx = T.argsort(dist, axis=1)[:, -4:]\nprediction = theano.function([analogy_a, analogy_b, analogy_c], pred_idx)\n```", "```py\nAthens Greece Baghdad Iraq\nAthens Greece Bangkok Thailand\nAthens Greece Beijing China\n```", "```py\ndef read_analogies(fname, word2idx):\n    \"\"\"Reads through the analogy question file.\n    Returns:\n      questions: a [n, 4] numpy array containing the analogy question's\n                 word ids.\n      questions_skipped: questions skipped due to unknown words.\n    \"\"\"\n    questions = []\n    questions_skipped = 0\n    with open(fname, \"r\") as analogy_f:\n      for line in analogy_f:\n        if line.startswith(\":\"):  # Skip comments.\n          continue\n        words = line.strip().lower().split(\" \")\n        ids = [word2idx.get(w.strip()) for w in words]\n        if None in ids or len(ids) != 4:\n          questions_skipped += 1\n        else:\n          questions.append(np.array(ids))\n    print(\"Eval analogy file: \", fname)\n    print(\"Questions: \", len(questions))\n    print(\"Skipped: \", questions_skipped)\n\n    return np.array(questions, dtype=np.int32)\n```", "```py\n  \"\"\"Evaluate analogy questions and reports accuracy.\"\"\"\n\n  # How many questions we get right at precision@1.\n  correct = 0\n  analogy_data = read_analogies(args.eval_data, word2idx)\n  analogy_questions = analogy_data[:, :3]\n  answers = analogy_data[:, 3]\n  del analogy_data\n  total = analogy_questions.shape[0]\n  start = 0\n\n  while start < total:\n    limit = start + 200\n    sub_questions = analogy_questions[start:limit, :]\n    sub_answers = answers[start:limit]\n    idx = prediction(sub_questions[:,0], sub_questions[:,1], sub_questions[:,2])\n\n    start = limit\n    for question in xrange(sub_questions.shape[0]):\n      for j in xrange(4):\n        if idx[question, j] == sub_answers[question]:\n          # Bingo! We predicted correctly. E.g., [italy, rome, france, paris].\n          correct += 1\n          break\n        elif idx[question, j] in sub_questions[question]:\n          # We need to skip words already in the question.\n          continue\n        else:\n          # The correct label is not the precision@1\n          break\n  print()\n  print(\"Eval %4d/%d accuracy = %4.1f%%\" % (correct, total,\n                                            correct * 100.0 / total))\n```", "```py\n*Eval analogy file:  questions-words.txt*\n*Questions:  17827*\n*Skipped:  1717*\n*Eval   831/17827 accuracy =  4.7%*\n\n```"]