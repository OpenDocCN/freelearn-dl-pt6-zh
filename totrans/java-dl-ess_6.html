<html><head></head><body><div class="chapter" title="Chapter&#xA0;6.&#xA0;Approaches to Practical Applications &#x2013; Recurrent Neural Networks and More"><div class="titlepage"><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Approaches to Practical Applications – Recurrent Neural Networks and More</h1></div></div></div><p>In the previous chapters, you learned quite a lot about deep learning. You should now understand the fundamentals of the concepts, theories, and implementations of deep neural networks. You also learned that you can experiment with deep learning algorithms on various data relatively easily by utilizing a deep learning library. The next step is to examine how deep learning can be applied to a broad range of other fields and how to utilize it for practical applications.</p><p>Therefore, in this chapter, we'll first see how deep learning is actually applied. Here, you will see that the actual cases where deep learning is utilized are still very few. But why aren't there many cases even though it is such an innovative method? What is the problem? Later on, we'll think about the reasons. Furthermore, going forward we will also consider which fields we can apply deep learning to and will have the chance to apply deep learning and all the related areas of artificial intelligence.</p><p>The topics covered in this chapter include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Image recognition, natural language processing, and the neural networks models and algorithms related to them</li><li class="listitem" style="list-style-type: disc">The difficulties of turning deep learning models into practical applications</li><li class="listitem" style="list-style-type: disc">The possible fields where deep learning can be applied, and ideas on how to approach these fields</li></ul></div><p>We'll explore the potential of this big AI boom, which will lead to ideas and hints that you can utilize in deep learning for your research, business, and many sorts of activities.</p><div class="section" title="Fields where deep learning is active"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec31"/>Fields where deep learning is active</h1></div></div></div><p>We often <a id="id394" class="indexterm"/>hear that research for deep learning has always been ongoing and that's a fact. Many corporations, especially large tech companies such as Google, Facebook, Microsoft, and IBM, invest huge amounts of money into the research of deep learning, and we frequently hear news that some corporation has bought these research groups. However, as we look through, deep learning itself has various types of algorithms, and fields where these algorithms can be applied. Even so, it is a fact that is it not widely known which fields deep learning is utilized in or can be used <a id="id395" class="indexterm"/>in. Since the word "AI" is so broadly used, people can't properly understand which technology is used for which product. Hence, in this section, we will go through the fields where people have been trying to adopt deep learning actively for practical applications.</p><div class="section" title="Image recognition"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec32"/>Image recognition</h2></div></div></div><p>The field in which <a id="id396" class="indexterm"/>deep learning is most frequently <a id="id397" class="indexterm"/>incorporated is image recognition. It was Prof. Hinton and his team's invention that led to the term "deep learning." Their algorithm recorded the lowest error rates ever in an image recognition competition. The continuous research done to improve the algorithm led to even better results. Now, image recognition utilizing deep learning has gradually been adopted not only for studies, but also for practical applications and products. For example, Google utilizes deep learning to auto-generate thumbnails for YouTube or auto-tag and search photos in Google Photos. Like these popular products, deep learning is mainly applied to image tagging or categorizing and, for example, in the field of robotics, it is used for robots to specify things around them.</p><p>The reason why we can support these products and this industry is because deep learning is more suited to image processing, and this is because it can achieve higher precision rates than applications in any other field. Only because the precision and recall rate of image recognition is so high does it mean this industry has broad potential. An error rate of MNIST <a id="id398" class="indexterm"/>image classification is recorded at 0.21 percent with a deep learning algorithm (<a class="ulink" href="http://cs.nyu.edu/~wanli/dropc/">http://cs.nyu.edu/~wanli/dropc/</a>), and this rate can be no lower than the record for a human (<a class="ulink" href="http://arxiv.org/pdf/0710.2231v1.pdf">http://arxiv.org/pdf/0710.2231v1.pdf</a>). In other words, if you narrow it down to just image recognition, it's nothing more than the fact that a machine may overcome a human. Why does only image recognition get such high precision while other fields need far more improvement in their methods?</p><p>One of the reasons is that the structure of feature extractions in deep learning is well suited for image data. In deep neural networks, many layers are stacked and features are extracted from training data step by step at each layer. Also, it can be said that image data is featured as a layered structure. When you look at images, you will unconsciously catch brief features first and then look into a more detailed feature. Therefore, the inherent property of deep learning feature extraction is similar to how an image is perceived and hence we can get an accurate realization of the features. Although image recognition with deep learning still needs more improvements, especially of how machines can understand images and their contents, obtaining high precision by just adopting deep learning to sample image data without preprocessing obviously means that deep learning and image data are a good match.</p><p>The other reason is that people have been working to improve algorithms slowly but steadily. For example, in deep learning algorithms, CNN, which can get the best precision for image recognition, has been improved every time it faces difficulties/tasks. Local receptive fields substituted with kernels of convolutional layers were introduced to avoid networks becoming too dense. Also, downsampling methods such as max-pooling were invented to avoid the overreaction of networks towards a gap of image location. This was originally generated from a trial and error process on how to recognize handwritten letters written in <a id="id399" class="indexterm"/>a certain frame such as a postal code. As such, there are many cases where a new approach is sought to adapt neural <a id="id400" class="indexterm"/>networks algorithms for practical applications. A complicated model, CNN is also built based on these accumulated yet steady improvements. While we don't need feature engineering with deep learning, we still need to consider an appropriate approach to solve specific problems, that is, we can't build omnipotent <a id="id401" class="indexterm"/>models, and this is known as the <span class="strong"><strong>No Free Lunch Theorem</strong></span> (<span class="strong"><strong>NFLT</strong></span>) for optimization.</p><p>In the image recognition field, the classification accuracy that can be achieved by deep learning is extremely high, and it is actually beginning to be used for practical applications. However, there should be more fields where deep learning can be applied. Images have a close connection to many industries. In the future, there will be many cases and many more industries that utilize deep learning. In this book, let's think about what industries we can apply image recognition to, considering the emergence of deep learning in the next sections.</p></div><div class="section" title="Natural language processing"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec33"/>Natural language processing</h2></div></div></div><p>The second <a id="id402" class="indexterm"/>most active field, after image <a id="id403" class="indexterm"/>recognition, where the research of deep learning has progressed is <span class="strong"><strong>natural language processing</strong></span> (<span class="strong"><strong>NLP</strong></span>). The research in this field might become the most active going forward. With regard to image recognition, the prediction precision we could obtain almost reaches the ceiling, as it can perform even better classification than a human could. On the other hand, in NLP, it is true that the performance of a model gets a lot better thanks to deep learning, but it is also a fact that there are many tasks that still need to be solved.</p><p>For some products and practical applications, deep learning has already been applied. For example, NLP based on deep learning is applied to Google's voice search or voice recognition and Google translation. Also, IBM Watson, the cognitive computing system that understands and learns natural language and supports human decision-making, extracts keywords and entities from tons of documents, and has functions to label documents. And these functions are open to the public as the Watson API and anyone can utilize it without constraints.</p><p>As you can see from the preceding examples, NLP itself has a broad and varied range of types. In terms of fundamental techniques, we have the classification of sentence contents, the classification of words, and the specification of word meanings. Furthermore, languages such as Chinese or Japanese that don't leave a space between words require morphological analysis, which is also another technique available in NLP.</p><p>NLP contains a lot of things that need to be researched, therefore it needs to clarify what its purpose is, what its problems are, and how these problems can be solved. What model is the best to use <a id="id404" class="indexterm"/>and how to get good precision <a id="id405" class="indexterm"/>properly are topics that should be examined cautiously. As for image recognition, the CNN method was invented by solving tasks that were faced. Now, let's consider what approach we can think of and what the difficulties will be respectively for neural networks and NLP. Understanding  past trial and error processes will be useful for research and applications going forward.</p><div class="section" title="Feed-forward neural networks for NLP"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec03"/>Feed-forward neural networks for NLP</h3></div></div></div><p>The <a id="id406" class="indexterm"/>fundamental problem of NLP is "to predict the next word given a specific word or words". The <a id="id407" class="indexterm"/>problem is too simple, however; if you try to solve it with neural networks, then you will soon face several difficulties because documents or sentences as sample data using NLP have the following features:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The length of each sentence is not fixed but variable, and the number of words is astronomical</li><li class="listitem" style="list-style-type: disc">There can be unforeseen problems such as misspelled words, acronyms, and so on</li><li class="listitem" style="list-style-type: disc">Sentences are sequential data, and so contain temporal information</li></ul></div><p>Why can these features pose a problem? Remember the model structure of general neural networks. For training and testing with neural networks, the number of neurons in each layer including the input layer needs to be fixed in advance and the networks need to be the same size for all the sample data. In the meantime, the length of the input data is not fixed and can vary a lot. This means that sample data cannot be applied to the model, at least as it is. Classification or generation by neural networks cannot be done without adding/amending something to this data.</p><p>We have to fix the length of input data, and one approach to handle this issue is a method that divides a sentence into a chunk of certain words from the beginning in order. This method is called <a id="id408" class="indexterm"/>
<span class="strong"><strong>N-gram</strong></span>. Here, <span class="emphasis"><em>N</em></span> represents the size of each item, and an <span class="strong"><strong>N-gram</strong></span> of size 1 is <a id="id409" class="indexterm"/>called a <span class="strong"><strong>unigram</strong></span>, size 2 is a <span class="strong"><strong>bigram</strong></span>, and <a id="id410" class="indexterm"/>size 3 is a <a id="id411" class="indexterm"/>
<span class="strong"><strong>trigram</strong></span>. When the size is larger, then it is simply called with the value of <span class="emphasis"><em>N</em></span>, such as <span class="emphasis"><em>four-gram</em></span>, <span class="emphasis"><em>five-gram</em></span>, and so on.</p><p>Let's look at how N-gram works with NLP. The goal here is to calculate the probability of a word <span class="inlinemediaobject"><img src="graphics/B04779_06_14.jpg" alt="Feed-forward neural networks for NLP"/></span> <a id="id412" class="indexterm"/>given some history <span class="inlinemediaobject"><img src="graphics/B04779_06_15.jpg" alt="Feed-forward neural networks for NLP"/></span>;<span class="inlinemediaobject"><img src="graphics/B04779_06_16.jpg" alt="Feed-forward neural networks for NLP"/></span>. We'll represent a sequence of <span class="inlinemediaobject"><img src="graphics/B04779_06_17.jpg" alt="Feed-forward neural networks for NLP"/></span> words as <span class="inlinemediaobject"><img src="graphics/B04779_06_18.jpg" alt="Feed-forward neural networks for NLP"/></span>. Then, the probability we <a id="id413" class="indexterm"/>want to compute is <span class="inlinemediaobject"><img src="graphics/B04779_06_19.jpg" alt="Feed-forward neural networks for NLP"/></span>, and by applying the chain rule of probability to this term, we get:</p><div class="mediaobject"><img src="graphics/B04779_06_20.jpg" alt="Feed-forward neural networks for NLP"/></div><p>It might look at first glance like these conditional probabilities help us, but actually they don't because we have no way of calculating the exact probability of a word following a long sequence of preceding words, <span class="inlinemediaobject"><img src="graphics/B04779_06_21.jpg" alt="Feed-forward neural networks for NLP"/></span>. Since the structure of a sentence is very flexible, we can't simply utilize sample documents and a corpus to estimate the probability. This is where N-gram works. Actually, we have two approaches to solve this problem: the original N-gram model and the neural networks model based on N-gram. We'll look at the first one to fully understand how the fields of NLP have developed before we dig into neural networks.</p><p>With N-gram, we don't compute the probability of a word given its whole history, but approximate the history with the last <span class="emphasis"><em>N</em></span> words. For example, the bigram model approximates the probability of a word just by the conditional probability of the preceding word, <span class="inlinemediaobject"><img src="graphics/B04779_06_21.jpg" alt="Feed-forward neural networks for NLP"/></span>, and so follows the equation:</p><div class="mediaobject"><img src="graphics/B04779_06_22.jpg" alt="Feed-forward neural networks for NLP"/></div><p>Similarly, we <a id="id414" class="indexterm"/>can generalize <a id="id415" class="indexterm"/>and expand the equation for N-gram. In this case, the probability of a word can be represented as follows:</p><div class="mediaobject"><img src="graphics/B04779_06_23.jpg" alt="Feed-forward neural networks for NLP"/></div><p>We get the following equation:</p><div class="mediaobject"><img src="graphics/B04779_06_24.jpg" alt="Feed-forward neural networks for NLP"/></div><p>Just bear in mind that these approximations with N-gram are based on the probabilistic model called the <a id="id416" class="indexterm"/>
<span class="strong"><strong>Markov model</strong></span>, where the probability of a word depends only on the previous word.</p><p>Now what we need to do is estimate these N-gram probabilities, but how do we estimate them? One simple way of <a id="id417" class="indexterm"/>doing this is called the <span class="strong"><strong>maximum likelihood estimation</strong></span> (<span class="strong"><strong>MLE</strong></span>). This method estimates the probabilities by taking counts from a corpus and normalizing them. So when we think of a bigram as an example, we get:</p><div class="mediaobject"><img src="graphics/B04779_06_25.jpg" alt="Feed-forward neural networks for NLP"/></div><p>In the preceding formula, <span class="inlinemediaobject"><img src="graphics/B04779_06_26.jpg" alt="Feed-forward neural networks for NLP"/></span> denotes the counts of a word or a sequence of words. Since the <a id="id418" class="indexterm"/>denominator, that is, the sum of all bigram counts starting with a word, <span class="inlinemediaobject"><img src="graphics/B04779_06_27.jpg" alt="Feed-forward neural networks for NLP"/></span> is equal to the unigram count of <span class="inlinemediaobject"><img src="graphics/B04779_06_27.jpg" alt="Feed-forward neural networks for NLP"/></span>, the preceding equation can be described as follows:</p><div class="mediaobject"><img src="graphics/B04779_06_28.jpg" alt="Feed-forward neural networks for NLP"/></div><p>Accordingly, we can <a id="id419" class="indexterm"/>generalize MLE for N-gram as well:</p><div class="mediaobject"><img src="graphics/B04779_06_29.jpg" alt="Feed-forward neural networks for NLP"/></div><p>Although this is a fundamental approach of NLP with N-gram, we now know how to compute N-gram probabilities.</p><p>In contrast to this approach, the neural network models predict the conditional probability of a word <span class="inlinemediaobject"><img src="graphics/B04779_06_30.jpg" alt="Feed-forward neural networks for NLP"/></span> given a specific history, <span class="inlinemediaobject"><img src="graphics/B04779_06_31.jpg" alt="Feed-forward neural networks for NLP"/></span>; <span class="inlinemediaobject"><img src="graphics/B04779_06_32.jpg" alt="Feed-forward neural networks for NLP"/></span>. One of the models of NLP is called the <span class="strong"><strong>Neural </strong></span><a id="id420" class="indexterm"/>
<span class="strong"><strong>Network Language Model</strong></span> (<span class="strong"><strong>NLMM</strong></span>) (<a class="ulink" href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf</a>), and it can be illustrated as follows:</p><div class="mediaobject"><img src="graphics/B04779_06_01.jpg" alt="Feed-forward neural networks for NLP"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_06_17.jpg" alt="Feed-forward neural networks for NLP"/></span> is <a id="id421" class="indexterm"/>the size of the <a id="id422" class="indexterm"/>vocabulary, and each word in the vocabulary is an N-dimensional vector where only the index of the word is set to 1 and all the other indices to 0. This method of representation is called <span class="emphasis"><em>1-of-N coding</em></span>. The inputs of NLMM are the indices of the <span class="inlinemediaobject"><img src="graphics/B04779_06_33.jpg" alt="Feed-forward neural networks for NLP"/></span> previous words <span class="inlinemediaobject"><img src="graphics/B04779_06_34.jpg" alt="Feed-forward neural networks for NLP"/></span> (so they are <span class="emphasis"><em>n-grams</em></span>). Since the size <span class="emphasis"><em>N</em></span> is typically within the range of 5,000 to 200,000, input vectors of NLMM are very sparse. Then, each word is mapped to the projection layer, for continuous space representation. This linear projection (activation) from a discrete to a continuous space is basically a look-up table with <span class="inlinemediaobject"><img src="graphics/B04779_06_35.jpg" alt="Feed-forward neural networks for NLP"/></span> entries, where <span class="inlinemediaobject"><img src="graphics/B04779_06_36.jpg" alt="Feed-forward neural networks for NLP"/></span> denotes the feature dimension. The projection matrix is shared for the different word positions in the context, and activates the word vectors to projection <a id="id423" class="indexterm"/>layer units <span class="inlinemediaobject"><img src="graphics/B04779_06_37.jpg" alt="Feed-forward neural networks for NLP"/></span> with <span class="inlinemediaobject"><img src="graphics/B04779_06_38.jpg" alt="Feed-forward neural networks for NLP"/></span>. After the projection comes the hidden layer. Since the projection layer is in the continuous space, the model <a id="id424" class="indexterm"/>structure is just the same as the other neural networks from here. So, the activation can be represented as follows:</p><div class="mediaobject"><img src="graphics/B04779_06_39.jpg" alt="Feed-forward neural networks for NLP"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_06_40.jpg" alt="Feed-forward neural networks for NLP"/></span> denotes the activation function, <span class="inlinemediaobject"><img src="graphics/B04779_06_41.jpg" alt="Feed-forward neural networks for NLP"/></span> the weights between the projection layer and the hidden layer, and <span class="inlinemediaobject"><img src="graphics/B04779_06_42.jpg" alt="Feed-forward neural networks for NLP"/></span> the biases of the hidden layer. Accordingly, we can get the output units as follows:</p><div class="mediaobject"><img src="graphics/B04779_06_43.jpg" alt="Feed-forward neural networks for NLP"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_06_44.jpg" alt="Feed-forward neural networks for NLP"/></span> denotes the weights between the hidden layer and the output layer, and <span class="inlinemediaobject"><img src="graphics/B04779_06_45.jpg" alt="Feed-forward neural networks for NLP"/></span> denotes the biases of the output layer. The probability of a word <span class="emphasis"><em>i</em></span> given a specific history <span class="inlinemediaobject"><img src="graphics/B04779_06_31.jpg" alt="Feed-forward neural networks for NLP"/></span> can then <a id="id425" class="indexterm"/>be calculated using <a id="id426" class="indexterm"/>the softmax function:</p><div class="mediaobject"><img src="graphics/B04779_06_46.jpg" alt="Feed-forward neural networks for NLP"/></div><p>As you can see, in NNLM, the model predicts the probability of all the words at the same time. Since the model is now described with the standard neural network, we can train the model using the standard backpropagation algorithm.</p><p>NNLM is one approach of NLP using neural networks with N-gram. Though NNLM solves the problem of how to fix the number of inputs, the best <span class="emphasis"><em>N</em></span> can only be found by trial and error, and it is the most difficult part of the whole model building process. In addition, we have to make sure that we don't put too much weight on the temporal information of the inputs here.</p></div><div class="section" title="Deep learning for NLP"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec04"/>Deep learning for NLP</h3></div></div></div><p>Neural networks <a id="id427" class="indexterm"/>with N-gram may work with certain cases, but contain some issues, such as what n-grams would return the best results, and do n-grams, the inputs of the model, still have a context? These are the problems not only of NLP, but of all the other fields that have time sequential data such as precipitation, stock prices, yearly crop of potatoes, movies, and so on. Since we have <a id="id428" class="indexterm"/>such a massive amount of this data in the real world, we can't ignore the potential issue. But then, how would it be possible to let neural networks be trained with time sequential data?</p><div class="section" title="Recurrent neural networks"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl4sec01"/>Recurrent neural networks</h4></div></div></div><p>One of the <a id="id429" class="indexterm"/>neural network models that is able to preserve the context of data within networks is <span class="strong"><strong>recurrent neural </strong></span><a id="id430" class="indexterm"/>
<span class="strong"><strong>network</strong></span> (<span class="strong"><strong>RNN</strong></span>), the model that actively studies the evolution of deep learning algorithms. The following is a very simple graphical model of RNN:</p><div class="mediaobject"><img src="graphics/B04779_06_02.jpg" alt="Recurrent neural networks"/></div><p>The difference between standard neural networks is that RNN has connections between hidden layers with respect to time. The input at time <span class="inlinemediaobject"><img src="graphics/B04779_06_47.jpg" alt="Recurrent neural networks"/></span> is activated in the hidden layer at time <span class="inlinemediaobject"><img src="graphics/B04779_06_47.jpg" alt="Recurrent neural networks"/></span>, preserved in the hidden layer, and then propagated to the hidden layer at time <span class="inlinemediaobject"><img src="graphics/B04779_06_48.jpg" alt="Recurrent neural networks"/></span> with the input at time <span class="inlinemediaobject"><img src="graphics/B04779_06_48.jpg" alt="Recurrent neural networks"/></span>. This enables the networks to contain the states of past data and reflect them. You might think that RNN is rather a dynamic model, but if you unfold the model at each time step, you can see that RNN is a static model:</p><div class="mediaobject"><img src="graphics/B04779_06_03.jpg" alt="Recurrent neural networks"/></div><p>Since the model structure at each time step is the same as in general neural networks, you can train this model using the backpropagation algorithm. However, you need to consider time <a id="id431" class="indexterm"/>relevance when <a id="id432" class="indexterm"/>training, and there is a technique called <span class="strong"><strong>Backpropagation through Time</strong></span> (<span class="strong"><strong>BPTT</strong></span>) to handle this. In BPTT, the errors and gradients of the parameter are backpropagated to the layers of the past:</p><div class="mediaobject"><img src="graphics/B04779_06_04.jpg" alt="Recurrent neural networks"/></div><p>Thus, RNN can preserve contexts within the model. Theoretically, the network at each time step should consider the whole sequence up to then, but practically, time windows with a certain length are often applied to the model to make the calculation less complicated or to prevent the vanishing gradient problem and the exploding gradient problem. BPTT has enabled training among layers and this is why RNN is often considered to be one of the deep neural networks. We also have algorithms of deep RNN such as stacked RNN where hidden layers are stacked.</p><p>RNN has <a id="id433" class="indexterm"/>been adapted for NLP, and is actually one of the most successful models in this field. The original model <a id="id434" class="indexterm"/>optimized for NLP is called the <span class="strong"><strong>recurrent neural network language model</strong></span> (<span class="strong"><strong>RNNLM</strong></span>), introduced by Mikolov et al. (<a class="ulink" href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf">http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf</a>). The model architecture can be illustrated as follows:</p><div class="mediaobject"><img src="graphics/B04779_06_05.jpg" alt="Recurrent neural networks"/></div><p>The network has three layers: an input layer <span class="inlinemediaobject"><img src="graphics/B04779_06_49.jpg" alt="Recurrent neural networks"/></span>, a hidden layer <span class="inlinemediaobject"><img src="graphics/B04779_06_50.jpg" alt="Recurrent neural networks"/></span>, and an output layer <span class="inlinemediaobject"><img src="graphics/B04779_06_51.jpg" alt="Recurrent neural networks"/></span>. The hidden layer is also often called the context layer or the state layer. The value of each layer with <a id="id435" class="indexterm"/>respect to the time <span class="inlinemediaobject"><img src="graphics/B04779_06_47.jpg" alt="Recurrent neural networks"/></span> can be represented as follows:</p><div class="mediaobject"><img src="graphics/B04779_06_52.jpg" alt="Recurrent neural networks"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_06_53.jpg" alt="Recurrent neural networks"/></span> denotes the sigmoid function, and <span class="inlinemediaobject"><img src="graphics/B04779_06_54.jpg" alt="Recurrent neural networks"/></span> the softmax function. Since the input layer contains the state layer at time <span class="inlinemediaobject"><img src="graphics/B04779_06_55.jpg" alt="Recurrent neural networks"/></span>, it can reflect the whole context to the network. The model <a id="id436" class="indexterm"/>architecture implies that RNNLM can look up much broader contexts than feed-forward NNLM, in which the length of the context is constrained to <span class="emphasis"><em>N</em></span> (-gram).</p><p>The whole time and the entire context should be considered while training RNN, but as mentioned previously, we often truncate the time length because BPTT requires a lot of calculations and often causes the gradient vanishing/exploding problem when learning long-term <a id="id437" class="indexterm"/>dependencies, hence the algorithm is often called <span class="strong"><strong>truncated BPTT</strong></span>. If we unfold RNNLM with respect to time, the model can be illustrated as follows (in the figure, the unfolded time <span class="inlinemediaobject"><img src="graphics/B04779_06_56.jpg" alt="Recurrent neural networks"/></span>):</p><div class="mediaobject"><img src="graphics/B04779_06_13.jpg" alt="Recurrent neural networks"/></div><p>Here <span class="inlinemediaobject"><img src="graphics/B04779_06_57.jpg" alt="Recurrent neural networks"/></span> is <a id="id438" class="indexterm"/>the label vector of the output. Then, the error vector of the output can be represented as follows:</p><div class="mediaobject"><img src="graphics/B04779_06_58.jpg" alt="Recurrent neural networks"/></div><p>We get the following equation:</p><div class="mediaobject"><img src="graphics/B04779_06_59.jpg" alt="Recurrent neural networks"/></div><p>Here <span class="inlinemediaobject"><img src="graphics/B04779_06_60.jpg" alt="Recurrent neural networks"/></span> is the unfolding time:</p><div class="mediaobject"><img src="graphics/B04779_06_61.jpg" alt="Recurrent neural networks"/></div><p>The <a id="id439" class="indexterm"/>preceding image is the derivative of the activation function of the hidden layer. Since we use the sigmoid function here, we get the preceding equation. Then, we can get the error of the past as follows:</p><div class="mediaobject"><img src="graphics/B04779_06_62.jpg" alt="Recurrent neural networks"/></div><p>With these equations, we can now update the weight matrices of the model:</p><div class="mediaobject"><img src="graphics/B04779_06_63.jpg" alt="Recurrent neural networks"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B04779_06_64.jpg" alt="Recurrent neural networks"/></span> is the learning rate. What is interesting in RNNLM is that each vector in the matrix shows the difference between words after training. This is because <span class="inlinemediaobject"><img src="graphics/B04779_06_65.jpg" alt="Recurrent neural networks"/></span> is the matrix that maps each word to a latent space, so after the training, mapped word vectors contain the meaning <a id="id440" class="indexterm"/>of the words. For example, the vector calculation of "king" – "man" + "woman" would return "queen". DL4J supports RNN, so you can easily implement this model with the library.</p></div><div class="section" title="Long short term memory networks"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl4sec02"/>Long short term memory networks</h4></div></div></div><p>Training <a id="id441" class="indexterm"/>with the standard RNN requires the truncated BPTT. You might well doubt then that BPTT <a id="id442" class="indexterm"/>can really train the model enough to reflect the whole context, and this is very true. This is why a special kind of RNN, the <span class="strong"><strong>long short term memory</strong></span> (<span class="strong"><strong>LSTM</strong></span>) network, was introduced to solve the long-term dependency problem. LSTM is rather intimidating, but let's briefly explore the concept of LSTM.</p><p>To begin with, we have to think about how we can store and tell past information in the network. While the gradient exploding problem can be mitigated simply by setting a ceiling to the connection, the gradient vanishing problem still needs to be deeply considered. One possible approach is to introduce a unit that permanently preserves the value of its inputs and its gradient. So, when you look at a unit in the hidden layer of standard neural networks, it is simply described as follows:</p><div class="mediaobject"><img src="graphics/B04779_06_06.jpg" alt="Long short term memory networks"/></div><p>There's nothing special here. Then, by adding a unit below to the network, the network can now memorize the past information within the neuron. The neuron added here has linear activation <a id="id443" class="indexterm"/>and its value is often set to 1. This neuron, or cell, is called <span class="strong"><strong>constant error carousel</strong></span> (<span class="strong"><strong>CEC</strong></span>) because the error stays in the neuron like a carousel and won't vanish. CEC works as a storage cell and stores past inputs. This solves the gradient vanishing problem, but raises another problem. Since all data propagated through is stocked in the neuron, it probably stores noise data as well:</p><div class="mediaobject"><img src="graphics/B04779_06_07.jpg" alt="Long short term memory networks"/></div><p>This problem can be broken down into two problems: <span class="emphasis"><em>input weight conflicts</em></span> and <span class="emphasis"><em>output weight conflicts</em></span>. The key idea of input weight conflicts is to keep certain information within the network until it's necessary; the neuron is to be activated only when the relevant information comes, but is not to be activated otherwise. Similarly, output weight conflicts can occur in all types of neural networks; the value of neurons is to be propagated only when necessary, and not to be propagated otherwise. We can't solve these problems as <a id="id444" class="indexterm"/>long as the connection between neurons is represented with the weight of the network. Therefore, another method or technique of representation is required that controls the propagation of inputs and outputs. But <a id="id445" class="indexterm"/>how do we do this? The answer is putting units that act like "gates" before and behind the CEC, and these are called <span class="strong"><strong>input gate</strong></span> and <span class="strong"><strong>output gate</strong></span>, respectively. The <a id="id446" class="indexterm"/>graphical model of the gate can be described as follows:</p><div class="mediaobject"><img src="graphics/B04779_06_08.jpg" alt="Long short term memory networks"/></div><p>Ideally, the gate should return the discrete value of 0 or 1 corresponding to the input, 0 when the gate is closed and 1 when open, because it is a gate, but programmatically, the gate is set to return the value in the range of 0 to 1 so that it can be well trained with BPTT.</p><p>It may seem like we can now put and fetch exact information at an exact time, yet another problem still remains. With just two gates, the input gate and output gate, memories stored in the CEC can't be refreshed easily in a few steps. Therefore, we need an additional gate that dynamically changes the value of the CEC. To do this, we add a <span class="strong"><strong>forget gate</strong></span> to the <a id="id447" class="indexterm"/>architecture to control when the memory should be erased. The value preserved in the CEC is overridden with a new memory when the value of the gate takes a 0 or close to it. With these three gates, a unit can now memorize information or contexts of <a id="id448" class="indexterm"/>the past, and so it is called an <span class="strong"><strong>LSTM block</strong></span> or an <span class="strong"><strong>LSTM </strong></span><a id="id449" class="indexterm"/>
<span class="strong"><strong>memory block</strong></span> because it is more of a <a id="id450" class="indexterm"/>block than a single neuron. The following is a figure that represents an LSTM block:</p><div class="mediaobject"><img src="graphics/B04779_06_09.jpg" alt="Long short term memory networks"/></div><p>The standard LSTM structure was fully explained previously, but there's a technique to get better performance from it, which we'll explain now. Each gate receives connections from the input units and the outputs of all the units in LSTM, but there is no direct connection from the CEC. This means we can't see the true hidden state of the network because the output of a block depends so much on the output gate; as long as the output gate is closed, none of the gates can access the CEC and it is devoid of essential information, which may debase the performance of LSTM. One simple yet effective solution is to add connections from the CEC to the gates in a block. These are called <span class="strong"><strong>peephole connections</strong></span>, and <a id="id451" class="indexterm"/>act as standard weighted connections except that no errors are backpropagated from the gates through the peephole connections. The peephole connections let all gates assume the hidden state even when the output gate is closed. You've learned a lot of terms now, but as a result, the basic architecture of the whole connection can be described as follows:</p><div class="mediaobject"><img src="graphics/B04779_06_10.jpg" alt="Long short term memory networks"/></div><p>For simplicity, a single LSTM block is described in the figure. You might be daunted because the preceding model is very intricate. However, when you look at the model step by step, you can understand how an LSTM network has figured out how to overcome difficulties in <a id="id452" class="indexterm"/>NLP. Given an input sequence <span class="inlinemediaobject"><img src="graphics/B04779_06_66.jpg" alt="Long short term memory networks"/></span>, each network unit can be calculated as follows:</p><div class="mediaobject"><img src="graphics/B04779_06_67.jpg" alt="Long short term memory networks"/></div><p>In the preceding formulas, <span class="inlinemediaobject"><img src="graphics/B04779_06_68.jpg" alt="Long short term memory networks"/></span> is the matrix of weights from the input gate to the input, <span class="inlinemediaobject"><img src="graphics/B04779_06_68.jpg" alt="Long short term memory networks"/></span> is the one from the forget gate to the input, and <span class="inlinemediaobject"><img src="graphics/B04779_06_69.jpg" alt="Long short term memory networks"/></span> is the one from the output gate to the input. <span class="inlinemediaobject"><img src="graphics/B04779_06_70.jpg" alt="Long short term memory networks"/></span> is the weight matrix from the cell to the input, <span class="inlinemediaobject"><img src="graphics/B04779_06_71.jpg" alt="Long short term memory networks"/></span> is the one from the cell to the LSTM output, and <span class="inlinemediaobject"><img src="graphics/B04779_06_72.jpg" alt="Long short term memory networks"/></span> is the one from the output to the LSTM output. <span class="inlinemediaobject"><img src="graphics/B04779_06_73.jpg" alt="Long short term memory networks"/></span>, <span class="inlinemediaobject"><img src="graphics/B04779_06_74.jpg" alt="Long short term memory networks"/></span>, and <span class="inlinemediaobject"><img src="graphics/B04779_06_75.jpg" alt="Long short term memory networks"/></span> are diagonal <a id="id453" class="indexterm"/>weight matrices for peephole connections. The <span class="inlinemediaobject"><img src="graphics/B04779_06_76.jpg" alt="Long short term memory networks"/></span> terms denote the bias vectors, <span class="inlinemediaobject"><img src="graphics/B04779_06_77.jpg" alt="Long short term memory networks"/></span> is the input gate bias vector, <span class="inlinemediaobject"><img src="graphics/B04779_06_78.jpg" alt="Long short term memory networks"/></span> is the forget gate bias vector, <span class="inlinemediaobject"><img src="graphics/B04779_06_79.jpg" alt="Long short term memory networks"/></span> is the output gate bias vector, <span class="inlinemediaobject"><img src="graphics/B04779_06_80.jpg" alt="Long short term memory networks"/></span> is the CEC cell bias vector, and <span class="inlinemediaobject"><img src="graphics/B04779_06_81.jpg" alt="Long short term memory networks"/></span> is the output bias vector. Here, <span class="inlinemediaobject"><img src="graphics/B04779_06_82.jpg" alt="Long short term memory networks"/></span> and <span class="inlinemediaobject"><img src="graphics/B04779_06_15.jpg" alt="Long short term memory networks"/></span> are activation functions of the cell input and cell output. <span class="inlinemediaobject"><img src="graphics/B04779_06_83.jpg" alt="Long short term memory networks"/></span> denotes the sigmoid function, and <span class="inlinemediaobject"><img src="graphics/B04779_06_84.jpg" alt="Long short term memory networks"/></span> the softmax function. <span class="inlinemediaobject"><img src="graphics/B04779_06_85.jpg" alt="Long short term memory networks"/></span> is the element-wise product of the vectors.</p><p>We won't follow the further math equations in this book because they become too complicated just by applying BPTT, but you can try LSTM with DL4J as well as RNN. As CNN was developed within the field of image recognition, RNN and LSTM have been developed to <a id="id454" class="indexterm"/>resolve the issues of NLP that arise one by one. While both algorithms are just one approach to get a better performance using NLP and still need to be improved, since we are living beings that communicate using languages, the development of NLP will certainly lead to technological innovations. For applications of LSTM, you can reference <span class="emphasis"><em>Sequence to Sequence Learning with Neural Networks</em></span> (Sutskever et al., <a class="ulink" href="http://arxiv.org/pdf/1409.3215v3.pdf">http://arxiv.org/pdf/1409.3215v3.pdf</a>), and for more recent algorithms, you can reference <span class="emphasis"><em>Grid Long Short-Term Memory</em></span> (Kalchbrenner et al., <a class="ulink" href="http://arxiv.org/pdf/1507.01526v1.pdf">http://arxiv.org/pdf/1507.01526v1.pdf</a>) and <span class="emphasis"><em>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</em></span> (Xu et al., <a class="ulink" href="http://arxiv.org/pdf/1502.03044v2.pdf">http://arxiv.org/pdf/1502.03044v2.pdf</a>).</p></div></div></div></div></div>
<div class="section" title="The difficulties of deep learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec32"/>The difficulties of deep learning</h1></div></div></div><p>Deep learning has <a id="id455" class="indexterm"/>already got higher precision than humans in the image recognition field and has been applied to quite a lot of practical applications. Similarly, in the NLP field, many models have been researched. Then, how much deep learning is utilized in other fields? Surprisingly, there are still few fields where deep learning is successfully utilized. This is because deep learning is indeed innovative compared to past algorithms and definitely lets us take a big step towards materializing AI; however, it has some problems when used for practical applications.</p><p>The first problem is that there are too many model parameters in deep learning algorithms. We didn't look in detail when you learned about the theory and implementation of algorithms, but actually deep neural networks have many hyper parameters that need to be decided compared to the past neural networks or other machine learning algorithms. This means we have to go through more trial and error to get high precision. Combinations of parameters that define a structure of neural networks, such as how many hidden layers are to be set or how many units each hidden layer should have, need lots of experiments. Also, the parameters for training and test configurations such as the learning rate need to be determined. Furthermore, peculiar parameters for each algorithm such as the corruption level in SDA and the size of kernels in CNN need additional trial and error. Thus, the great performance that deep learning provides is supported by steady parameter-tuning. However, people only look at one side of deep learning—that it can get great precision— and they tend to forget the hard process required to reach that point. Deep learning is not magic.</p><p>In addition, deep learning often fails to train and classify data from simple problems. The shape of deep neural networks is so deep and complicated that the weights can't be well optimized. In terms of optimization, data quantities are also important. This means that deep neural networks require a significant amount of time for each training. To sum up, deep learning shows its worth when:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">It solves complicated and hard problems when people have no idea what feature they can be classified as</li><li class="listitem" style="list-style-type: disc">There is sufficient training data to properly optimize deep neural networks</li></ul></div><p>Compared to applications that constantly update a model using continuously updated data, once a model is built using a large-scale dataset that doesn't change drastically, applications that use the model universally are rather well suited for deep learning.</p><p>Therefore, when you look at business fields, you can say that there are more cases where the existing machine learning can get better results than using deep learning. For example, let's assume we would like to recommend appropriate products to users in an EC. In this EC, many users buy a lot of products daily, so purchase data is largely updated daily. In this case, do you use deep learning to get high-precision classification and recommendations to increase the conversion rates of users' purchases using this data? Probably not, because using the existing machine learning algorithms such as Naive Bayes, collaborative filtering, SVM, and so on, we can get sufficient precision from a practical perspective and can update the model and calculate quicker, which is usually more appreciated. This <a id="id456" class="indexterm"/>is why deep learning is not applied much in business fields. Of course, getting higher precision is better in any field, but in reality, higher precision and the necessary calculation time are in a trade-off relationship. Although deep learning is significant in the research field, it has many hurdles yet to clear considering practical applications.</p><p>Besides, deep learning algorithms are not perfect, and they still need many improvements to their model itself. For example, RNN, as mentioned earlier, can only satisfy either how past information can be reflected to a network or how precision can be obtained, although it's contrived with techniques such as LSTM. Also, deep learning is still far from the true AI, although it's definitely a great technique compared to the past algorithms. Research on algorithms is progressing actively, but in the meantime, we need one more breakthrough to spread out and infiltrate deep learning into broader society. Maybe this is not just the problem of a model. Deep learning is suddenly booming because it is reinforced by huge developments in hardware and software. Deep learning is closely related to development of the surrounding technology.</p><p>As mentioned earlier, there are still many hurdles to clear before deep learning can be applied more practically in the real world, but this is not impossible to achieve. It isn't possible to suddenly invent AI to achieve technological singularity, but there are some fields and methods where deep learning can be applied right away. In the next section, we'll think about what kinds of industries deep learning can be utilized in. Hopefully, it will sow the seeds for new ideas in your business or research fields.</p></div>
<div class="section" title="The approaches to maximizing deep learning possibilities and abilities"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec33"/>The approaches to maximizing deep learning possibilities and abilities</h1></div></div></div><p>There are several <a id="id457" class="indexterm"/>approaches to how we can apply deep <a id="id458" class="indexterm"/>learning to various industries. While it is true that an approach could be different depending on the task or purpose, we can briefly categorize the approaches in the following three ways:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Field-oriented approach</strong></span>: This utilizes deep learning algorithms or models that are <a id="id459" class="indexterm"/>already thoroughly researched and can lead to great performance</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Breakdown-oriented approach</strong></span>: This replaces the problems to be solved that <a id="id460" class="indexterm"/>deep learning can apparently be applied to with a different problem where deep learning can be well adopted</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Output-oriented approach</strong></span>: This explores new ways of how we express the output with <a id="id461" class="indexterm"/>deep learning</li></ul></div><p>These approaches are all explained in detail in the following subsections. Each approach is divided into its suitable industries or areas where it is not suitable, but any of them could be a big hint for your activities going forward. There are still very few use cases of deep learning and bias against fields of use, but this means there should be many chances to create innovative and new things. Start-ups that utilize deep learning have been emerging recently <a id="id462" class="indexterm"/>and some of them have already achieved <a id="id463" class="indexterm"/>success to some extent. You can have a significant impact on the world depending on your ideas.</p><div class="section" title="Field-oriented approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec34"/>Field-oriented approach</h2></div></div></div><p>This approach doesn't <a id="id464" class="indexterm"/>require new techniques or algorithms. There are obviously fields that are well suited to the current deep learning techniques, and the concept here is to dive into these fields. As explained previously, since deep learning algorithms that have been practically studied and developed are mostly in image recognition and NLP, we'll explore some fields that can work in great harmony with them.</p><div class="section" title="Medicine"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec05"/>Medicine</h3></div></div></div><p>Medical fields <a id="id465" class="indexterm"/>should be developed by deep learning. Tumors or cancers are detected on scanned images. This means nothing more than being able to utilize one of the strongest features of deep learning—the technique of image recognition. It is possible to dramatically increase precision using deep learning to help with the early detection of an illness and identifying the kind of illness. Since CNN can be applied to 3D images, 3D scanned images should be able to be analyzed relatively easily. By adopting deep learning more in the current medical field, deep learning should greatly contribute.</p><p>We can also say that deep learning can be significantly useful for the medical field in the future. The medical field has been under strict regulations; however, there is a movement progressing to ease the regulations in some countries, probably because of the recent development of IT and its potential. Therefore, there will be opportunities in business for the medical field and IT to have a synergistic effect. For example, if telemedicine is more infiltrated, there is the possibility that diagnosing or identifying a disease can be done not only by a scanned image, but also by an image shown in real time on a display. Also, if electronic charts become widespread, it would be easier to analyze medical data using deep learning. This is because medical records are compatible with deep learning as they are a dataset of texts <a id="id466" class="indexterm"/>and images. Then the symptoms of unknown diseases can be found.</p></div><div class="section" title="Automobiles"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec06"/>Automobiles</h3></div></div></div><p>We can say that the <a id="id467" class="indexterm"/>surroundings of running cars are image sequences and text. Other cars and views are images and a road sign has text. This means we can also utilize deep learning techniques here, and it is possible to reduce the risk of accidents by improving driving assistance functions. It can be said that the ultimate type of driving assistance is self-driving cars, which is being tackled mainly by Google and Tesla. An example that is both famous and fascinating was when George Hotz, the first person to hack the iPhone, built a self-driving car in his garage. The appearance of the car was introduced in an article by Bloomberg Business (<a class="ulink" href="http://www.bloomberg.com/features/2015-george-hotz-self-driving-car/">http://www.bloomberg.com/features/2015-george-hotz-self-driving-car/</a>), and the following image was included in the article:</p><div class="mediaobject"><img src="graphics/B04779_06_11.jpg" alt="Automobiles"/></div><p>Self-driving cars have been already tested in the U.S., but since other countries have different traffic rules and road conditions, this idea requires further studying and development before self-driving cars are commonly used worldwide. The key to success in this field is in learning and recognizing surrounding cars, people, views, and traffic signs, and properly judging how to process them.</p><p>In the meantime, we don't have to just focus on utilizing deep learning techniques for the actual body of a car. Let's assume we could develop a smartphone app that has the same function as we just described, that is, recognizing and classifying surrounding images and text. Then, if you just <a id="id468" class="indexterm"/>set up the smartphone in your car, you could utilize it as a car-navigation app. In addition, for example, it could be used as a navigation app for blind people, providing them with good, reliable directions.</p></div><div class="section" title="Advert technologies"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec07"/>Advert technologies</h3></div></div></div><p>Advert (ad) technologies could expand their coverage with deep learning. When we say ad technologies, this currently means recommendation or ad networks that optimize ad banners or <a id="id469" class="indexterm"/>products to be shown. On the other hand, when we say advertising, this doesn't only mean banners or ad networks. There are various kinds of ads in the world depending on the type of media, such as TV ads, radio ads, newspaper ads, posters, flyers, and so on. We have also digital ad campaigns with YouTube, Vine, Facebook, Twitter, Snapchat, and so on. Advertising itself has changed its definition and content, but all ads have one thing in common: they consist of images and/or language. This means they are fields that deep learning is good at. Until now, we could only use user-behavior-based indicators, such as <span class="strong"><strong>page view</strong></span> (<span class="strong"><strong>PV</strong></span>), <span class="strong"><strong>click through rate</strong></span> (<span class="strong"><strong>CTR</strong></span>), and <span class="strong"><strong>conversion rate</strong></span> (<span class="strong"><strong>CVR</strong></span>), to estimate the effect of an ad, but if we apply deep learning technologies, we might be able to analyze the actual content of an ad and autogenerate ads going forward. Especially since movies and videos can only be analyzed as a result of image recognition and NLP, video recognition, not image recognition, will gather momentum besides ad technologies.</p></div><div class="section" title="Profession or practice"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec08"/>Profession or practice</h3></div></div></div><p>Professions <a id="id470" class="indexterm"/>such as doctor, lawyer, patent attorney, and accountant are considered to be roles that deep learning can replace. For example, if NLP's precision and accuracy gets higher, any perusal that requires expertise can be left to a machine. As a machine can cover these time-consuming reading tasks, people can focus more on high-value tasks. In addition, if a machine classifies past judicial cases or medical cases on what disease caused what symptoms and so on, we would be able to build an app like Apple's Siri that answers simple questions that usually require professional knowledge. Then the machine could handle these professional cases to some extent if a doctor or a lawyer is too busy to help in a timely manner.</p><p>It's often said that AI takes away a human's job, but personally, this seems incorrect. Rather, a machine takes away menial work, which should support humans. A software engineer who works on AI programming can be described as having a professional job, but this work will also be changed in the future. For example, think about a car-related job, where the current work is building standard automobiles, but in the future, engineers will be in a position just like pit crews for Formula 1 cars.</p></div><div class="section" title="Sports"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec09"/>Sports</h3></div></div></div><p>Deep learning can <a id="id471" class="indexterm"/>certainly contribute to sports as well. In the study field known as sports science, it has become increasingly important to analyze and examine data from sports. As an example, you may know the book or movie <span class="emphasis"><em>Moneyball</em></span>. In this film, they hugely increased the win percentage of the team by adopting a regression model in baseball. Watching sports itself is very exciting, but on the other hand, sport can be seen as a chunk of image sequences and number data. Since deep learning is good at identifying features that humans can't find, it will become easier to find out why certain players get good scores while others don't.</p><p>These fields we have mentioned are only a small part of the many fields where deep learning is capable of significantly contributing to development. We have looked into these fields from the perspective of whether a field has images or text, but of course deep learning should also show great performance for simple analysis with general number data. It should be possible to apply deep learning to various other fields, such as bioinformatics, finance, agriculture, chemistry, astronomy, economy, and more.</p></div></div><div class="section" title="Breakdown-oriented approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec35"/>Breakdown-oriented approach</h2></div></div></div><p>This approach <a id="id472" class="indexterm"/>might be similar to the approach considered in traditional machine learning algorithms. We already talked about how feature engineering is the key to improving precision in machine learning. Now we can say that this feature engineering can be divided into the following two parts:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Engineering <a id="id473" class="indexterm"/>under the constraints of a machine learning model. The typical case is to make inputs discrete or continuous.</li><li class="listitem" style="list-style-type: disc">Feature engineering to increase precision by machine learning. This tends to rely on the sense of a researcher.</li></ul></div><p>In a narrower meaning, feature engineering is considered as the second one, and this is the part that deep learning doesn't have to focus on, whereas the first one is definitely the important part, even for deep learning. For example, it's difficult to predict stock prices using deep learning. Stock prices are volatile and it's difficult to define inputs. Besides, how to apply an output value is also a difficult problem. Enabling deep learning to handle these inputs and outputs is also said to be feature engineering in the wider sense. If there is no limitation to the value of original data and/or data you would like to predict, it's difficult to insert these datasets into machine learning and deep learning algorithms, including neural networks.</p><p>However, we can take a certain approach and apply a model to these previous problems by breaking down the inputs and/or outputs. In terms of NLP, as explained earlier, you might have thought, for example, that it would be impossible to put numberless words into features in the first place, but as you already know, we can train feed-forward neural networks with words by representing them with sparse vectors and combining N-grams into them. Of course, we can not only use neural networks, but also other machine learning algorithms such as SVM here. Thus, we can cultivate a new field where deep learning hasn't been applied by engineering to fit features well into deep learning models. In the meantime, when we focus on NLP, we can see that RNN and LSTM were developed to properly resolve the difficulties or tasks encountered in NLP. This can be considered as the opposite approach to feature engineering because in this case, the problem is solved by breaking down a model to fit into features.</p><p>Then, how do we do <a id="id474" class="indexterm"/>utilize engineering for stock prediction as we just mentioned? It's actually not difficult to think of inputs, that is, features. For example, if you predict stock prices daily, it's hard to calculate if you use daily stock prices as features, but if you use a rate of price change between a day and the day before, then it should be much easier to process as the price stays within a certain range and the gradients won't explode easily. Meanwhile, what is difficult is how to deal with outputs. Stock prices are of course continuous values, hence outputs can be various values. This means that in the neural network model where the number of units in the output layer is fixed, they can't handle this problem. What should we do here—should we give up?! No, wait a minute. Unfortunately, we can't predict a stock price itself, but there is an alternative prediction method.</p><p>Here, the problem is that we can classify stock prices to be predicted into infinite patterns. Then, can we <a id="id475" class="indexterm"/>make them into limited patterns? Yes, we can. Let's forcibly make them. Think about the most extreme but easy to understand case: predicting whether tomorrow's stock price, strictly speaking a close price, is up or down using the data from the stock price up to today. For this case, we can show it with a deep learning model as follows:</p><div class="mediaobject"><img src="graphics/B04779_06_12.jpg" alt="Breakdown-oriented approach"/></div><p>In the preceding image, <span class="inlinemediaobject"><img src="graphics/B04779_06_86.jpg" alt="Breakdown-oriented approach"/></span> denotes the open price of a day, <span class="inlinemediaobject"><img src="graphics/B04779_06_47.jpg" alt="Breakdown-oriented approach"/></span>; <span class="inlinemediaobject"><img src="graphics/B04779_06_87.jpg" alt="Breakdown-oriented approach"/></span> denotes the close price, <span class="inlinemediaobject"><img src="graphics/B04779_06_88.jpg" alt="Breakdown-oriented approach"/></span> is the high price, and <span class="inlinemediaobject"><img src="graphics/B04779_06_89.jpg" alt="Breakdown-oriented approach"/></span> is the actual price. The features used here are mere examples, and need to be fine-tuned when applied to real applications. The point here is that replacing the original task with this type of problem enables deep neural networks to theoretically classify data. Furthermore, if you classify the data by how much it will go up or down, you could <a id="id476" class="indexterm"/>make more detailed predictions. For <a id="id477" class="indexterm"/>example, you could classify data as shown in the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Class</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Class 1</p>
</td><td style="text-align: left" valign="top">
<p>Up more than 3 percent from the closing price</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Class 2</p>
</td><td style="text-align: left" valign="top">
<p>Up more than 1~3 percent from the closing price</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Class 3</p>
</td><td style="text-align: left" valign="top">
<p>Up more than 0~1 percent from the closing price</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Class 4</p>
</td><td style="text-align: left" valign="top">
<p>Down more than 0~-1 percent from the closing price</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Class 5</p>
</td><td style="text-align: left" valign="top">
<p>Down more than -1~-3 percent from the closing price</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Class 6</p>
</td><td style="text-align: left" valign="top">
<p>Down more than -3 percent from the closing price</p>
</td></tr></tbody></table></div><p>Whether the prediction actually works, in other words whether the classification works, is unknown until we examine it, but the fluctuation of stock prices can be predicted in quite a narrow range by dividing the outputs into multiple classes. Once we can adopt the task into neural networks, then what we should do is just examine which model gets better results. In this example, we may apply RNN because the stock price is time sequential data. If we look at charts showing the price as image data, we can also use CNN to predict the future price.</p><p>So now we've thought about the approach by referring to examples, but to sum up in general, we can say that:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Feature engineering for models</strong></span>: This is designing inputs or adjusting values to fit deep learning models, or enabling classification by setting a limitation for the outputs</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Model engineering for features</strong></span>: This is devising new neural network models or <a id="id478" class="indexterm"/>algorithms to solve problems in <a id="id479" class="indexterm"/>a focused field</li></ul></div><p>The first one needs ideas for the part of designing inputs and outputs to fit to a model, whereas the second one needs to take a mathematical approach. Feature engineering might be easier to start if you are conscious of making an item prediction-limited.</p></div><div class="section" title="Output-oriented approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec36"/>Output-oriented approach</h2></div></div></div><p>The two <a id="id480" class="indexterm"/>previously mentioned approaches <a id="id481" class="indexterm"/>are to increase the percentage of correct answers for a certain field's task or problem using deep learning. Of course, it is essential and the part where deep learning proves its worth; however, increasing precision to the ultimate level may not be the only way of utilizing deep learning. Another approach is to devise the outputs using deep learning by slightly changing the point of view. Let's see what this means.</p><p>Deep learning is applauded as an innovative approach among researchers and technical experts of AI, but the world in general doesn't know much about its greatness yet. Rather, they pay attention to what a machine can't do. For example, people don't really focus on the image recognition capabilities of MNIST using CNN, which generates a lower error rate than humans, but they criticize that a machine can't recognize images perfectly. This is probably because people expect a lot when they hear and imagine AI. We might need to change this mindset. Let's consider DORAEMON, a Japanese national cartoon character who is also famous worldwide—a robot who has high intelligence and AI, but often makes silly mistakes. Do we criticize him? No, we just laugh it off or take it as a joke and don't get serious. Also, think about DUMMY / DUM-E, the robot arm in the movie <span class="emphasis"><em>Iron Man</em></span>. It has AI as well, but makes silly mistakes. See, they make mistakes but we still like them.</p><p>In this way, it might be better to emphasize the point that machines make mistakes. Changing the expression part of a user interface could be the trigger for people to adopt AI rather than just studying an algorithm the most. Who knows? It's highly likely that you can gain the world's interest by thinking of ideas in creative fields, not from the perspective of precision. Deep Dream by Google is one good example. We can do more exciting things <a id="id482" class="indexterm"/>when art or design and deep learning <a id="id483" class="indexterm"/>collaborate.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec34"/>Summary</h1></div></div></div><p>In this chapter, you learned how to utilize deep learning algorithms for practical applications. The fields that are well studied are image recognition and NLP. While learning about the field of NLP, we looked through two new deep learning models: the RNN and LSTM networks, which can be trained with time sequential data. The training algorithm used in these models is BPTT. You also learned that there are three approaches to make the best of the deep learning ability: the field-oriented approach, the breakdown-oriented approach, and the output-oriented approach. Each approach has a different angle, and can maximize the possibility for deep learning.</p><p>And …congratulations! You've just accomplished the learning part of deep learning with Java. Although there are still some models that have not been mentioned yet in this book, you can be sure there will be no problem in acquiring and utilizing them. The next chapter will introduce some libraries that are implemented with other programming languages, so just relax and take a look.</p></div></body></html>