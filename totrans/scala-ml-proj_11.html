<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Image Classification using Convolutional Neural Networks</h1>
                </header>
            
            <article>
                
<p>So far, we haven't developed any <strong>machine learning</strong> (<strong>ML</strong>) projects for image processing tasks. Linear ML models and other regular <strong>deep neural network</strong> (<strong>DNN</strong>) models, such as <strong>Multilayer Perceptrons</strong> (<strong>MLPs</strong>) or <strong>Deep Belief Networks</strong> (<strong>DBNs</strong>), cannot learn or model non-linear features from images.</p>
<p>On the other hand, a <strong>convolutional neural network</strong> (<strong>CNN</strong>) is a type of feedforward neural network in which the connectivity pattern between its neurons is inspired by the animal visual cortex. In the last few years, CNNs have demonstrated superhuman performance in complex visual tasks such as image search services, self-driving cars, automatic video classification, voice recognition, and <strong>natural language processing </strong>(<strong>NLP</strong>).</p>
<p>In this chapter, we will see how to develop an end-to-end project for handling multi-label (that is, each entity can belong to multiple classes) image classification problems using CNN based on the Scala and <strong>Deeplearning4j</strong> (<strong>DL4j</strong>) framework with real <strong>Yelp</strong> image datasets. We will also discuss some theoretical aspects of CNNs and how to tune hyperparameters for better classification results <span>before getting started</span>.</p>
<p>In a nutshell, we will learn the following topics throughout this end-to-end project:</p>
<ul>
<li>The drawbacks of regular DNNs</li>
<li>CNN architectures: convolution operations and pooling layers</li>
<li>Image classification using CNNs</li>
<li>Tuning CNN hyperparameters</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image classification and drawbacks of DNNs</h1>
                </header>
            
            <article>
                
<p>Before we start developing the end-to-end project for image classification using CNN, we need some background studies, such as the drawbacks of regular DNNs, suitability of CNNs over DNNs for image classification, CNN constructions, CNN's different operations, and so on. Although regular DNNs work fine for small images (for example, MNIST, CIFAR-10), it breaks down for larger images because of the huge number of parameters it requires. For example, a 100 x 100 image has 10,000 pixels, and if the first layer has just 1,000 neurons (which already severely restricts the amount of information transmitted to the next layer), this means a total of 10 million connections. And that's just for the first layer.</p>
<p>CNNs solve this problem using partially connected layers. Because consecutive layers are only partially connected and because it heavily reuses its weights, a CNN has far fewer parameters than a fully connected DNN, which makes it much faster to train, reduces the risk of overfitting, and requires much less training data. Moreover, when a CNN has learned a kernel that can detect a particular feature, it can detect that feature anywhere on the image. In contrast, when a DNN learns a feature in one location, it can detect it only in that particular location. Since images typically have very repetitive features, CNNs are able to generalize much better than DNNs for image processing tasks, such as classification, using fewer training examples.</p>
<p>Importantly, DNN has no prior knowledge of how pixels are organized; it does not know that nearby pixels are close. A CNN's architecture embeds this prior knowledge. Lower layers typically identify features in small areas of the images, while higher layers combine the lower-level features into larger features. This works well with most natural images, giving CNNs a decisive head start compared to DNNs:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/045f23a6-f77a-48ab-8620-85fcdc44fe31.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 1: Regular DNN versus CNN</div>
<p>For example, in <em>Figure 1</em>, on the left, you can see a regular three-layer neural network. On the right, a ConvNet arranges its neurons in three dimensions (width, height, and depth), as visualized in one of the layers. Every layer of a ConvNet transforms the 3D input volume to a 3D output volume of neuron activations. The red input layer holds the image, so its width and height would be the dimensions of the image, and the depth would be three (red, green, and blue channels).</p>
<p>So, all the multilayer neural networks we looked at had layers composed of a long line of neurons, and we had to flatten input images or data to 1D before feeding them to the neural network. However, what happens once you try to feed them a 2D image directly? The answer is that, in CNN, each layer is represented in 2D, which makes it easier to match neurons with their corresponding inputs. We will see examples of it in upcoming sections.</p>
<p>Another important fact is all the neurons in a feature map share the same parameters, so it dramatically reduces the number of parameters in the model, but, more importantly, it means that once the CNN has learned to recognize a pattern in one location, it can recognize it in any other location. In contrast, once a regular DNN has learned to recognize a pattern in one location, it can recognize it only in that particular location.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CNN architecture</h1>
                </header>
            
            <article>
                
<p>In multilayer networks, such as MLP or DBN, the outputs of all neurons of the input layer are connected to each neuron in the hidden layer, so the output will again act as the input to the fully-connected layer. In CNN networks, the connection scheme that defines the convolutional layer is significantly different. The convolutional layer is the main type of layer in CNN, where each neuron is connected to a certain region of the input area called the <strong>receptive field</strong>.</p>
<p>In a typical CNN architecture, a few convolutional layers are connected in a cascade style, where each layer is followed by a <strong>rectified linear unit</strong> (<strong>ReLU</strong>) layer, then a pooling layer, then a few more convolutional layers (+ReLU), then another pooling layer, and so on.</p>
<p>The output from each convolution layer is a set of objects called <strong>feature maps</strong> that are generated by a single kernel filter. The feature maps can then be used to define a new input to the next layer. Each neuron in a CNN network produces an output followed by an activation threshold, which is proportional to the input and not bound:</p>
<div class="CDPAlignCenter CDPAlign"><img height="133" width="512" class="alignnone size-full wp-image-571 image-border" src="assets/86c20f01-65a5-4544-a6ee-7fc6355e20e8.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 2: A conceptual architecture of CNN</div>
<p>As you can see in <em>Figure 2</em>, the pooling layers are usually placed after the convolutional layers. The convolutional region is then divided by a pooling layer into sub-regions. Then, a single representative value is selected using either a max-pooling or average pooling technique to reduce the computational time of subsequent layers.</p>
<p>This way, the robustness of the feature with respect to its spatial position gets increased too. To be more specific, when the image properties, as feature maps, pass through the image, they get smaller and smaller as they progress through the network, but they also typically get deeper and deeper, since more feature maps will be added. At the top of the stack, a regular feedforward neural network is added, just like an MLP, which might compose of a few fully connected layers (+ReLUs), and the final layer outputs the prediction, for example, a softmax layer that outputs estimated class probabilities for a multiclass classification.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional operations</h1>
                </header>
            
            <article>
                
<p>A convolution is a mathematical operation that slides one function over another and measures the integral of their pointwise multiplication. It has deep connections with the Fourier transform and the Laplace transform and is heavily used in signal processing. Convolutional layers actually use cross-correlations, which are very similar to convolutions.</p>
<p>Thus, the most important building block of a CNN is the convolutional layer: neurons in the first convolutional layer are not connected to every single pixel in the input image (as they were in previous chapters), but only to pixels in their receptive fields—see <em>Figure 3</em>. In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer:</p>
<div class="CDPAlignCenter CDPAlign"><img height="196" width="334" class="alignnone size-full wp-image-572 image-border" src="assets/b2d6b3bc-542d-42ce-9287-6c21eff983ed.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 3: CNN layers with rectangular local receptive fields</div>
<p>This architecture allows the network to concentrate on low-level features in the first hidden layer, and then assemble them into higher-level features in the next hidden layer, and so on. This hierarchical structure is common in real-world images, which is one of the reasons why CNNs work so well for image recognition.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pooling layer and padding operations</h1>
                </header>
            
            <article>
                
<p>Once you understand how convolutional layers work, the pooling layers are quite easy to grasp. A pooling layer typically works on every input channel independently, so the output depth is the same as the input depth. You may alternatively pool over the depth dimension, as we will see next, in which case the image's spatial dimensions (height and width) remain unchanged, but the number of channels is reduced. Let's see a formal definition of pooling layers from a well-known TensorFlow website:</p>
<div class="packt_quote">"The pooling ops sweep a rectangular window over the input tensor, computing a reduction operation for each window (average, max, or max with argmax). Each pooling op uses rectangular windows of size called ksize separated by offset strides. For example, if strides are all ones, every window is used, if strides are all twos, every other window is used in each dimension, and so on."</div>
<p>Therefore, just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the inputs using an aggregation function such as the max or mean.</p>
<p>Well, the goal of using pooling is to subsample the input image in order to reduce the computational load, the memory usage, and the number of parameters. This helps to avoid overfitting in the training stage. Reducing the input image size also makes the neural network tolerate a little bit of image shift. In the following example, we use a 2 x 2 pooling kernel and a stride of 2 with no padding. Only the max input value in each kernel makes it to the next layer since the other inputs are dropped:</p>
<div class="CDPAlignCenter CDPAlign"><img height="272" width="696" class="alignnone size-full wp-image-573 image-border" src="assets/5c2fcbec-0188-4c45-8a7c-54f19f36d0d9.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 4: An example using max pooling, that is, subsampling</div>
<p>Usually, <em>(stride_length)* x + filter_size &lt;= input_layer_size</em> is recommended for most CNN-based network development.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Subsampling operations</h1>
                </header>
            
            <article>
                
<p>As stated earlier, a neuron located in a given layer is connected to the outputs of the neurons in the previous layer. Now, in order for a layer to have the same height and width as the previous layer, it is common to add zeros around the inputs, as shown in the diagram. This is called <strong>SAME</strong> or <strong>zero padding</strong>.</p>
<p>The term SAME means that the output feature map has the same spatial dimensions as the input feature map. Zero padding is introduced to make the shapes match as needed, equally on every side of the input map. On the other hand, VALID means no padding and only drops the right-most columns (or bottom-most rows):</p>
<div class="CDPAlignCenter CDPAlign"><img height="337" width="626" class="alignnone size-full wp-image-574 image-border" src="assets/dfdd4a04-457d-444c-9299-ae5649690df6.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5: SAME versus VALID padding with CNN</div>
<p>Now we have the minimum theoretical knowledge about CNNs and their architectures, it's time to do some hands-on work and create convolutional, pooling, and subsampling operations using Deeplearning4j (aka. DL4j), which is one of the first commercial-grade <span>distributed</span> open source deep-learning libraries written for Java and Scala. It also provides integrated support for Hadoop and Spark. DL4j is designed to be used in business environments on distributed GPUs and CPUs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional and subsampling operations in DL4j</h1>
                </header>
            
            <article>
                
<p>Before getting started, setting up our programming environment is a prerequisite. So let's do that first.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring DL4j, ND4s, and ND4j</h1>
                </header>
            
            <article>
                
<p>The following libraries can be integrated with DL4j. They will make your JVM experience easier, whether you're developing your ML application in Java or Scala:</p>
<ul>
<li><strong>DL4j</strong>: Neural net platform</li>
<li><strong>ND4J</strong>: NumPy for the JVM</li>
<li><strong>DataVec</strong>: Tool for ML ETL operations</li>
<li><strong>JavaCPP</strong>: The bridge between Java and native C++</li>
<li><strong>Arbiter</strong>: Evaluation tool for ML algorithms</li>
<li><strong>RL4J</strong>: Deep reinforcement learning for the JVM</li>
</ul>
<p>ND4j is just like NumPy for JVM. It comes with some basic operations of linear algebra, such as matrix creation, addition, and multiplication. ND4S, on the other hand, is a scientific computing library for linear algebra and matrix manipulation. Basically, it supports n-dimensional arrays for JVM-based languages.</p>
<p>If you are using Maven on Eclipse (or any other editor—that is, IntelliJ IDEA), use the following dependencies in the <kbd>pom.xml</kbd> file (inside the <kbd>&lt;dependencies&gt;</kbd> tag) for dependency resolution for DL4j, ND4s, and ND4j:</p>
<pre>&lt;dependency&gt;<br/>    &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;<br/>    &lt;artifactId&gt;deeplearning4j-core&lt;/artifactId&gt;<br/>    &lt;version&gt;0.4-rc3.9&lt;/version&gt;<br/>&lt;/dependency&gt;<br/>&lt;dependency&gt;<br/>    &lt;artifactId&gt;canova-api&lt;/artifactId&gt;<br/>    &lt;groupId&gt;org.nd4j&lt;/groupId&gt;<br/>    &lt;version&gt;0.4-rc3.9&lt;/version&gt;<br/>&lt;/dependency&gt;<br/>&lt;dependency&gt;<br/>    &lt;groupId&gt;org.nd4j&lt;/groupId&gt;<br/>    &lt;artifactId&gt;nd4j-native&lt;/artifactId&gt;<br/>    &lt;version&gt;0.4-rc3.9&lt;/version&gt;<br/>&lt;/dependency&gt;<br/>&lt;dependency&gt;<br/>    &lt;groupId&gt;org.nd4j&lt;/groupId&gt;<br/>    &lt;artifactId&gt;canova-api&lt;/artifactId&gt;<br/>    &lt;version&gt;0.0.0.17&lt;/version&gt;<br/>&lt;/dependency&gt;</pre>
<div class="packt_infobox">I used old versions since I was facing some compatibility issues, and it is still under active development. But feel free to adopt the latest upgrades. I believe readers can do it with minimal efforts.</div>
<p class="mce-root">Additionally, if a native system BLAS is not configured on your machine, ND4j's performance will be reduced. You will experience a warning once you execute simple code written in Scala:</p>
<pre>****************************************************************<br/>WARNING: COULD NOT LOAD NATIVE SYSTEM BLAS<br/>ND4J performance WILL be reduced<br/>****************************************************************</pre>
<p>However, installing and configuring BLAS such as OpenBLAS or IntelMKL is not that difficult; you can invest some time and do it. Refer to the following URL for details: <a href="http://nd4j.org/getstarted.html#open" target="_blank">http://nd4j.org/getstarted.html#open</a>. It is also to be noted that the following are prerequisites when working with DL4j:</p>
<ul>
<li>Java (developer version) 1.8+ (<span>only </span>64-bit versions supported)</li>
<li>Apache Maven for automated build and dependency manager</li>
<li>IntelliJ IDEA or Eclipse</li>
<li>Git</li>
</ul>
<p>Well done! Our programming environment is ready for a simple deep learning application development. Now it's time to get your hands dirty with some sample code. Let's see how to construct and train a simple CNN, using the CIFAR-10 dataset. CIFAR-10 is one of the most popular benchmark datasets and has thousands of labelled images.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional and subsampling operations in DL4j</h1>
                </header>
            
            <article>
                
<p>In this subsection, we will see an example of how to construct a CNN for MNIST data classification. The network will have two convolutional layers, two subsampling layers, one dense layer, and the output layer as the fully connected layer. The first layer is a convolutional layer followed by a subsampling layer, which is again followed by another convolutional layer. Then, a subsampling layer is followed by a dense layer, which is followed by an output layer.</p>
<p>Let's see how these layers would look like using DL4j. The first convolution layer with ReLU as activation function:</p>
<pre><strong>val</strong> layer_0 = <strong>new</strong> ConvolutionLayer.Builder(5, 5)<br/>    .nIn(nChannels)<br/>    .stride(1, 1)<br/>    .nOut(20)<br/>    .activation("relu")<br/>    .build()</pre>
<p>The following activation functions are currently supported in DL4j:</p>
<ul>
<li>ReLU</li>
<li>Leaky ReLU</li>
<li>Tanh</li>
<li>Sigmoid</li>
<li>Hard Tanh</li>
<li>Softmax</li>
<li>Identity</li>
<li><strong>ELU</strong> (<strong>Exponential Linear Units</strong>)</li>
<li>Softsign</li>
<li>Softplus</li>
</ul>
<p>The second layer (that is, the first subsampling layer) is a subsampling layer with pooling type <kbd>MAX</kbd>, with kernel size 2 x 2 and stride size of 2 x 2 but no activation function:</p>
<pre><strong>val</strong> layer_1 = <strong>new</strong> SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)<br/>    .kernelSize(2, 2)<br/>    .stride(2, 2)<br/>    .build()</pre>
<p>The third layer (2nd convolution layer) is a convolutional layer with ReLU as activation function, 1*1 stride:</p>
<pre><br/><strong>val</strong> layer_2 = <strong>new</strong> ConvolutionLayer.Builder(5, 5)<br/>    .nIn(nChannels)<br/>    .stride(1, 1)<br/>    .nOut(50)<br/>    .activation("relu")<br/>    .build()</pre>
<p>The fourth layer (that is, the second subsampling layer) is a subsampling layer with pooling type <kbd>MAX</kbd>, with kernel size 2 x 2, and stride size of 2 x 2 but no activation function:</p>
<pre><strong>val</strong> layer_3 = <strong>new</strong> SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)<br/>    .kernelSize(2, 2)<br/>    .stride(2, 2)<br/>    .build()</pre>
<p>The fifth layer is a dense layer with ReLU as an activation function:</p>
<pre><strong>val</strong> layer_4 = <strong>new</strong> DenseLayer.Builder()<br/>    .activation("relu")<br/>    .nOut(500)<br/>    .build()</pre>
<p>The sixth (that is, the final and fully connected layer) has Softmax as the activation function with the number of classes to be predicted (that is, 10):</p>
<pre><strong>val</strong> layer_5 = <strong>new</strong> OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)<br/>    .nOut(outputNum)<br/>    .activation("softmax")<br/>    .build()</pre>
<p>Once the layers are constructed, the next task is to construct and build the CNN by chaining all the layers. Using DL4j, it goes as follows:</p>
<pre><strong>val</strong> builder: MultiLayerConfiguration.Builder = <strong>new</strong> NeuralNetConfiguration.Builder()<br/>    .seed(seed)<br/>    .iterations(iterations)<br/>    .regularization(<strong>true</strong>).l2(0.0005)<br/>    .learningRate(0.01)<br/>    .weightInit(WeightInit.XAVIER)<br/>   .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)<br/>    .updater(Updater.NESTEROVS).momentum(0.9)<br/>    .list()<br/>        .layer(0, layer_0)<br/>        .layer(1, layer_1)<br/>        .layer(2, layer_2)<br/>        .layer(3, layer_3)<br/>        .layer(4, layer_4)<br/>        .layer(5, layer_5)<br/>    .backprop(<strong>true</strong>).pretrain(<strong>false</strong>) // feedforward and supervised so no pretraining</pre>
<p>Finally, we set up all the convolutional layers and initialize the network as follows:</p>
<pre><strong>new</strong> ConvolutionLayerSetup(builder, 28, 28, 1) //image size is 28*28<br/><strong>val</strong> conf: MultiLayerConfiguration = builder.build()<br/><strong>val</strong> model: MultiLayerNetwork = <strong>new</strong> MultiLayerNetwork(conf)<br/>model.init()</pre>
<p>Conventionally, to train a CNN, all images need to be the same shape and size. So I placed the dimension as 28 x 28 in the preceding lines for simplicity. Now, you may be thinking, how do we train such a network? Well, now we will see this but, before that, we need to prepare the MNIST dataset, using the <kbd>MnistDataSetIterator ()</kbd> method, as follows:</p>
<pre><strong>val</strong> nChannels = 1 // for grayscale image<br/><strong>val</strong> outputNum = 10 // number of class<br/><strong>val</strong> nEpochs = 10 // number of epoch<br/><strong>val</strong> iterations = 1 // number of iteration<br/><strong>val</strong> seed = 12345 // Random seed for reproducibility<br/><strong>val</strong> batchSize = 64 // number of batches to be sent<br/>log.info("Load data....")<br/><strong>val</strong> mnistTrain: DataSetIterator = <strong>new</strong> MnistDataSetIterator(batchSize, <strong>true</strong>, 12345)<br/><strong>val</strong> mnistTest: DataSetIterator = <strong>new</strong> MnistDataSetIterator(batchSize, <strong>false</strong>, 12345)</pre>
<p>Now let's start training the CNN, using the train set and iterate for each epoch:</p>
<pre>log.info("Model training started...")<br/>model.setListeners(<strong>new</strong> ScoreIterationListener(1))<br/><strong>var</strong> i = 0<br/><strong>while</strong> (i &lt;= nEpochs) {<br/>    model.fit(mnistTrain);<br/>    log.info("*** Completed epoch {} ***", i)<br/>    i = i + 1<br/>    }<br/><strong>var</strong> ds: DataSet = <strong>null<br/></strong><strong>var</strong> output: INDArray = <strong>null</strong></pre>
<p>Once we have trained the CNN, the next task is to evaluate the model on the test set, as follows:</p>
<pre>log.info("Model evaluation....")<br/><strong>val</strong> eval: Evaluation = <strong>new</strong> Evaluation(outputNum)<br/><strong>while</strong> (mnistTest.hasNext()) {<br/>    ds = mnistTest.next()<br/>    output = model.output(ds.getFeatureMatrix(), <strong>false</strong>)<br/>    }<br/>eval.eval(ds.getLabels(), output)</pre>
<p>Finally, we compute some performance matrices, such as <kbd>Accuracy</kbd>, <kbd>Precision</kbd>, <kbd>Recall</kbd>, and <kbd>F1 measure</kbd>, as follows:</p>
<pre>println("Accuracy: " + eval.accuracy())<br/>println("F1 measure: " + eval.f1())<br/>println("Precision: " + eval.precision())<br/>println("Recall: " + eval.recall())<br/>println("Confusion matrix: " + "n" + eval.confusionToString())<br/>log.info(eval.stats())<br/>mnistTest.reset()<br/>&gt;&gt;&gt;<br/>==========================Scores=======================================<br/> Accuracy: 1<br/> Precision: 1<br/> Recall: 1<br/> F1 Score: 1<br/>=======================================================================</pre>
<p>For your ease, here I provided the full source code for this simple image classifier:</p>
<pre><strong>package</strong> com.example.CIFAR<br/><br/><strong>import</strong> org.canova.api.records.reader.RecordReader<br/><strong>import</strong> org.canova.api.split.FileSplit<br/><strong>import</strong> org.canova.image.loader.BaseImageLoader<br/><strong>import</strong> org.canova.image.loader.NativeImageLoader<br/><strong>import</strong> org.canova.image.recordreader.ImageRecordReader<br/><strong>import</strong> org.deeplearning4j.datasets.iterator.DataSetIterator<br/><strong>import</strong> org.canova.image.recordreader.ImageRecordReader<br/><strong>import</strong> org.deeplearning4j.datasets.canova.RecordReaderDataSetIterator<br/><strong>import</strong> org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator<br/><strong>import</strong> org.deeplearning4j.eval.Evaluation<br/><strong>import</strong><span> org.deeplearning4j.nn.api.OptimizationAlgorithm<br/></span><strong>import</strong> org.deeplearning4j.nn.conf.MultiLayerConfiguration<br/><strong>import</strong> org.deeplearning4j.nn.conf.NeuralNetConfiguration<br/><strong>import</strong> org.deeplearning4j.nn.conf.Updater<br/><strong>import</strong> org.deeplearning4j.nn.conf.layers.ConvolutionLayer<br/><strong>import</strong> org.deeplearning4j.nn.conf.layers.DenseLayer<br/><strong>import</strong> org.deeplearning4j.nn.conf.layers.OutputLayer<br/><strong>import</strong><span> org.deeplearning4j.nn.conf.layers.SubsamplingLayer<br/></span><strong>import</strong> org.deeplearning4j.nn.conf.layers.setup.ConvolutionLayerSetup<br/><strong>import</strong> org.deeplearning4j.nn.multilayer.MultiLayerNetwork<br/><strong>import</strong> org.deeplearning4j.nn.weights.WeightInit<br/><strong>import</strong> org.deeplearning4j.optimize.listeners.ScoreIterationListener<br/><strong>import</strong> org.nd4j.linalg.api.ndarray.INDArray<br/><strong>import</strong> org.nd4j.linalg.api.rng.Random<br/><strong>import</strong> org.nd4j.linalg.dataset.DataSet<br/><strong>import</strong> org.nd4j.linalg.dataset.SplitTestAndTrain<br/><strong>import</strong> org.nd4j.linalg.lossfunctions.LossFunctions<br/><strong>import</strong> org.slf4j.Logger<br/><strong>import</strong> org.slf4j.LoggerFactory<br/><strong>import</strong> java.io.File<br/><strong>import</strong> java.util.ArrayList<br/><strong>import</strong> java.util.List<br/><br/><strong>object</strong> MNIST {<br/><strong>    val</strong> log: Logger = LoggerFactory.getLogger(MNIST.getClass)<br/><strong>    def</strong> main(args: Array[String]): Unit = {<br/><strong>    val</strong> nChannels = 1 // for grayscale image<br/><strong>    val</strong> outputNum = 10 // number of class<br/><strong>    val</strong><span> nEpochs = 1 // number of epoch<br/></span><strong>    val</strong> iterations = 1 // number of iteration<br/><strong>    val</strong> seed = 12345 // Random seed for reproducibility<br/><strong>    val</strong> batchSize = 64 // number of batches to be sent<br/><br/>    log.info("Load data....")<br/><strong>    val</strong> mnistTrain: DataSetIterator = <strong>new</strong> MnistDataSetIterator(batchSize, <strong>true</strong>, 12345)<br/><strong>    val</strong> mnistTest: DataSetIterator = <strong>new</strong> MnistDataSetIterator(batchSize, <strong>false</strong>, 12345)<br/><br/>    log.info("Network layer construction started...")<br/>    //First convolution layer with ReLU as activation function<br/><strong>    val</strong> layer_0 = <strong>new</strong> ConvolutionLayer.Builder(5, 5)<br/>        .nIn(nChannels)<br/>        .stride(1, 1)<br/>        .nOut(20)<br/>        .activation("relu")<br/>        .build()<br/><br/>    //First subsampling layer<br/><strong>    val</strong> layer_1 = <strong>new</strong> SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)<br/>        .kernelSize(2, 2)<br/>        .stride(2, 2)<br/>        .build()<br/><br/>    //Second convolution layer with ReLU as activation function<br/><strong>    val</strong> layer_2 = <strong>new</strong> ConvolutionLayer.Builder(5, 5)<br/>        .nIn(nChannels)<br/>        .stride(1, 1)<br/>        .nOut(50)<br/>        .activation("relu")<br/>        .build()<br/><br/>    //Second subsampling layer<br/><strong>    val</strong> layer_3 = <strong>new</strong> SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)<br/>        .kernelSize(2, 2)<br/>        .stride(2, 2)<br/>        .build()<br/><br/>    //Dense layer<br/><strong>    val</strong> layer_4 = <strong>new</strong> DenseLayer.Builder()<br/>        .activation("relu")<br/>        .nOut(500)<br/>        .build()<br/><br/>    // Final and fully connected layer with Softmax as activation function<br/><strong>    val</strong> layer_5 = <strong>new</strong> OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)<br/>        .nOut(outputNum)<br/>        .activation("softmax")<br/>        .build()<br/><br/>    log.info("Model building started...")<br/><strong>    val</strong> builder: MultiLayerConfiguration.Builder = <strong>new</strong> NeuralNetConfiguration.Builder()<br/>        .seed(seed)<br/>        .iterations(iterations)<br/>        .regularization(<strong>true</strong>).l2(0.0005)<br/>        .learningRate(0.01)<br/>        .weightInit(WeightInit.XAVIER)<br/>        .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)<br/>        .updater(Updater.NESTEROVS).momentum(0.9)<br/>        .list()<br/>            .layer(0, layer_0)<br/>            .layer(1, layer_1)<br/>            .layer(2, layer_2)<br/>            .layer(3, layer_3)<br/>            .layer(4, layer_4)<br/>            .layer(5, layer_5)<br/>    .backprop(<strong>true</strong>).pretrain(<strong>false</strong>) // feedforward so no backprop<br/><br/>// Setting up all the convlutional layers and initialize the network<br/><strong>new</strong> ConvolutionLayerSetup(builder, 28, 28, 1) //image size is 28*28<br/><strong>val</strong> conf: MultiLayerConfiguration = builder.build()<br/><strong>val</strong> model: MultiLayerNetwork = <strong>new</strong> MultiLayerNetwork(conf)<br/>model.init()<br/><br/>log.info("Model training started...")<br/>model.setListeners(<strong>new</strong> ScoreIterationListener(1))<br/><strong>    var</strong> i = 0<br/><strong>    while</strong> (i &lt;= nEpochs) {<br/>        model.fit(mnistTrain);<br/>        log.info("*** Completed epoch {} ***", i)<br/>        i = i + 1<br/><strong>        var</strong> ds: DataSet = <strong>null<br/></strong><strong>        var</strong> output: INDArray = <strong>null<br/></strong>        log.info("Model evaluation....")<br/><strong>        val</strong> eval: Evaluation = <strong>new</strong> Evaluation(outputNum)<br/><br/><strong>        while</strong> (mnistTest.hasNext()) {<br/>            ds = mnistTest.next()<br/>            output = model.output(ds.getFeatureMatrix(), <strong>false</strong>)<br/>                }<br/>        eval.eval(ds.getLabels(), output)<br/>        println("Accuracy: " + eval.accuracy())<br/>        println("F1 measure: " + eval.f1())<br/>        println("Precision: " + eval.precision())<br/>        println("Recall: " + eval.recall())<br/>        println("Confusion matrix: " + "n" + eval.confusionToString())<br/>        log.info(eval.stats())<br/>        mnistTest.reset()<br/>                }<br/>    log.info("****************Example finished********************")<br/>            }<br/>    }</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Large-scale image classification using CNN</h1>
                </header>
            
            <article>
                
<p>In this section, we will show a step-by-step example of developing a real-life ML project for image classification. However, we need to know the problem description first, to learn what sort of image classification needs to be done. Moreover, learning about the dataset is a mandate before getting started.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem description</h1>
                </header>
            
            <article>
                
<p>Nowadays, food selfies and photo-centric social storytelling are becoming social trends. Food lovers willingly upload an enormous amount of selfies taken with food and a picture of the restaurant to social media and respective websites. And, of course, they also provide a written review that can significantly boost the popularity of the restaurant:</p>
<div class="CDPAlignCenter CDPAlign"><img height="304" width="464" class="alignnone size-full wp-image-575 image-border" src="assets/454a5472-e1c8-4bfe-823d-5161d6510316.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 6: Mining some insights about business from Yelp dataset</div>
<p>For example, millions of unique visitors visit Yelp and have written more than 135 million reviews. There are lots of photos and lots of users who are uploading photos. Business owners can post photos and message their customers. This way, Yelp makes money by <strong>selling ads</strong> to those local businesses. An interesting fact is that these photos provide rich local business information across categories. Thus, training a computer to understand the context of these photos is not a trivial one and also not an easy task (see <em>Figure 6</em> to get an insight).</p>
<p>Now, the idea of this project is a challenging one: how can we turn those pictures into words? Let's give it a try. More specifically, you are given photos that belong to a business. Now we need to build a model so that it can tags restaurants with multiple labels of the user-submitted photos automatically—that is, to predict the business attributes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Description of the image dataset</h1>
                </header>
            
            <article>
                
<p>For such a challenge we need to have a real dataset. Don't worry, there are several platforms where such datasets are publicly available or can be downloaded with some terms and conditions. One such platform is <strong>Kaggle</strong>, which provides a platform for data analytics and ML practitioners to try ML challenges and win prizes. The Yelp dataset and the description can be found at: <a href="https://www.kaggle.com/c/yelp-restaurant-photo-classification">https://www.kaggle.com/c/yelp-restaurant-photo-classification</a>.</p>
<p>The labels of the restaurants are manually selected by Yelp users when they submit a review. There are nine different labels annotated by the Yelp community associated in the dataset:</p>
<ul>
<li><kbd>0: good_for_lunch</kbd></li>
<li><kbd>1: good_for_dinner</kbd></li>
<li><kbd>2: takes_reservations</kbd></li>
<li><kbd>3: outdoor_seating</kbd></li>
<li><kbd>4: restaurant_is_expensive</kbd></li>
<li><kbd>5: has_alcohol</kbd></li>
<li><kbd>6: has_table_service</kbd></li>
<li><kbd>7: ambience_is_classy</kbd></li>
<li><kbd>8: good_for_kids</kbd></li>
</ul>
<p>So we need to predict these labels as accurately as possible. One thing to be noted is that since Yelp is a community-driven website, there are duplicated images in the dataset for several reasons. For example, users can accidentally upload the same photo to the same business more than once, or chain businesses can upload the same photo to different branches. There are six files in the dataset, as follows:</p>
<ul>
<li><kbd>train_photos.tgz</kbd>: Photos to be used as the training set (234,545 images)</li>
<li><kbd>test_photos.tgz</kbd>: Photos to be used as the test set (500 images)</li>
<li><kbd>train_photo_to_biz_ids.csv</kbd>: Provides the mapping between the photo ID to business ID (234,545 rows)</li>
<li><kbd>test_photo_to_biz_ids.csv</kbd>: Provides the mapping between the photo ID to business ID ( 500 rows)</li>
<li><kbd>train.csv</kbd>: This is the main training dataset including business IDs, and their corresponding labels (1996 rows)</li>
<li><kbd>sample_submission.csv</kbd>: A sample submission—reference correct format for your predictions including <kbd>business_id</kbd> and the corresponding predicted labels</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Workflow of the overall project</h1>
                </header>
            
            <article>
                
<p>In this project, we will see how to read images from <kbd>.jpg</kbd> format into a matrix representation in Scala. Then, we will further process and prepare those images feedable by the CNNs. We will see several image manipulations, such as squaring all the images and resizing every image to the same dimensions, before we apply a grayscale filter to the image:</p>
<div class="CDPAlignCenter CDPAlign"><img height="217" width="685" class="alignnone size-full wp-image-576 image-border" src="assets/1501815a-0ebd-4b08-a671-19a7990e0bc4.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 7: A conceptualize view of a CNN for image classification</div>
<p>Then we train nine CNNs on training data for each class. Once the training is completed, we save the trained model, CNN configurations and parameters, so that they can be restored later on, and then we apply a simple aggregate function to assign classes to each restaurant, where each one has multiple images associated with it, each with its own vector of probabilities for each of the nine classes. Then we score test data and finally, evaluate the model using test images.</p>
<p>Now let's see the structure of each CNN. Each network will have two convolutional layers, two subsampling layers, one dense layer, and the output layer as the fully connected layer. The first layer is a convolutional layer followed by a subsampling layer, which is again followed by another convolutional layer, then a subsampling layer, then a dense layer, which is followed by an output layer. We will see each layer's structure later on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing CNNs for image classification</h1>
                </header>
            
            <article>
                
<p>The Scala object containing the <kbd>main()</kbd> method has the following workflow:</p>
<ol>
<li>We read all the business labels from the <kbd>train.csv</kbd> file</li>
<li>We read and create a map from image ID to business ID of form <kbd>imageID</kbd> → <kbd>busID</kbd></li>
<li>We get a list of images from the <kbd>photoDir</kbd> directory to load and process and, finally, get the image IDs of 10,000 images (feel free to set the range)</li>
<li>We then read and process images into a <kbd>photoID</kbd> → vector map</li>
<li>We chain the output of <em>step 3</em> and <em>step 4</em> to align the business feature, image IDs, and label IDs to get the feature extracted for the CNN</li>
<li>We construct nine CNNs.</li>
<li>We train all the CNNs and specify the model savings locations</li>
<li>We then repeat <em>step 2</em> to <em>step 6</em> to extract the features from the test set</li>
<li>Finally, we evaluate the model and save the prediction in a CSV file</li>
</ol>
<p>Now let's see how the preceding steps would look in a high-level diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img height="282" width="444" class="alignnone size-full wp-image-577 image-border" src="assets/ffa6d1ab-e72d-4d69-b4b2-a358679a7ed5.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 8: DL4j image processing pipeline for image classification</div>
<p>Programmatically, the preceding steps can be represented as follows:</p>
<pre><strong>val</strong> labelMap = readBusinessLabels("data/labels/train.csv")<br/><strong>val</strong> businessMap = readBusinessToImageLabels("data/labels/train_photo_to_biz_ids.csv")<br/><strong>val</strong> imgs = getImageIds("data/images/train/", businessMap, businessMap.map(_._2).toSet.toList).slice(0,100) // 20000 images<br/><br/>println("Image ID retreival done!")<br/><strong>val</strong> dataMap = processImages(imgs, resizeImgDim = 128)<br/>println("Image processing done!")<br/><strong>val</strong> alignedData = <strong>new</strong> featureAndDataAligner(dataMap, businessMap, Option(labelMap))()<br/><br/>println("Feature extraction done!")<br/><strong>val</strong> cnn0 = trainModelEpochs(alignedData, businessClass = 0, saveNN = "models/model0")<br/><strong>val</strong> cnn1 = trainModelEpochs(alignedData, businessClass = 1, saveNN = "models/model1")<br/><strong>val</strong> cnn2 = trainModelEpochs(alignedData, businessClass = 2, saveNN = "models/model2")<br/><strong>val</strong> cnn3 = trainModelEpochs(alignedData, businessClass = 3, saveNN = "models/model3")<br/><strong>val</strong> cnn4 = trainModelEpochs(alignedData, businessClass = 4, saveNN = "models/model4")<br/><strong>val</strong> cnn5 = trainModelEpochs(alignedData, businessClass = 5, saveNN = "models/model5")<br/><strong>val</strong> cnn6 = trainModelEpochs(alignedData, businessClass = 6, saveNN = "models/model6")<br/><strong>val</strong> cnn7 = trainModelEpochs(alignedData, businessClass = 7, saveNN = "models/model7")<br/><strong>val</strong> cnn8 = trainModelEpochs(alignedData, businessClass = 8, saveNN = "models/model8")<br/><br/><strong>val</strong> businessMapTE = readBusinessToImageLabels("data/labels/test_photo_to_biz.csv")<br/><br/><strong>val</strong> imgsTE = getImageIds("data/images/test//", businessMapTE, businessMapTE.map(_._2).toSet.toList)<br/><br/><strong>val</strong> dataMapTE = processImages(imgsTE, resizeImgDim = 128) // make them 128*128<br/><br/><strong>val</strong> alignedDataTE = <strong>new</strong> featureAndDataAligner(dataMapTE, businessMapTE, None)()<br/><strong>val</strong> Results = SubmitObj(alignedDataTE, "results/ModelsV0/")<br/><strong>val</strong> SubmitResults = writeSubmissionFile("kaggleSubmitFile.csv", Results, thresh = 0.9)</pre>
<p>Too much of a mouthful? Don't worry, we will now see each step in detail. If you look at the preceding steps carefully, you'll see <em>steps 1</em> to <em>step 5</em> are basically image processing and feature constructions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image processing</h1>
                </header>
            
            <article>
                
<p class="mce-root">When I tried to develop this application, I found that the photos are of different size and shape: some images are tall, some of them are wide, some of them are outside, some images are inside, and most of them are pictures of food. However, some are other, random things too. Another important aspect is, while training images varied in portrait/landscape and the number of pixels, most were roughly square, and many of them were exactly 500 x 375:</p>
<div class="CDPAlignCenter CDPAlign"><img height="170" width="292" class="alignnone size-full wp-image-578 image-border" src="assets/40a41e7c-baad-4e63-b049-12851134ce52.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9: Resized figure (left the original and tall one, right the squared one)</div>
<p class="mce-root">As we have already seen, CNN cannot work with images with a heterogeneous size and shape. There are many robust and efficient image processing techniques to extract only the <strong>region of interest</strong> (<strong>ROI</strong>). But, honestly, I am not an image processing expert, so I decided to keep this resizing step simpler.</p>
<div class="packt_tip">CNN has a serious limitation as it cannot address the orientational and relative spatial relationships. Therefore, these components are not very important to a CNN. In short, CNN is not that suitable for images having heterogeneous shape and orientation. For why, people are now talking about the Capsule Networks. See more at the original paper at <a href="https://arxiv.org/pdf/1710.09829v1.pdf">https://arxiv.org/pdf/1710.09829v1.pdf</a> and <a href="https://openreview.net/pdf?id=HJWLfGWRb">https://openreview.net/pdf?id=HJWLfGWRb</a>.</div>
<p class="mce-root">Naively, I made all the images square, but still, I tried to preserve the quality. <span>The ROIs are centered in most cases,</span> so capturing only the center-most square of each image is not that trivial. Nevertheless, we also need to convert each image to a grayscale image. Let's make irregularly shaped images square. Take a look at the following image, where the original one is on the left and the right is the square one (see <em>Figure 9)</em>.</p>
<p>Now we have generated a square one, how did we achieve this? Well, I checked first if the height and the width are the same, if so, no resizing takes place. In the other two cases, I cropped the center region. The following method does the trick (but feel free to execute the <kbd>SquaringImage.scala</kbd> script to see the output):</p>
<pre><strong>def</strong> makeSquare(img: java.awt.image.BufferedImage): java.awt.image.BufferedImage = {<br/><strong>    val</strong> w = img.getWidth<br/><strong>    val</strong> h = img.getHeight<br/><strong>    val</strong> dim = List(w, h).min<br/>    img <strong>match</strong> {<br/><strong>        case</strong> x <br/><strong>            if</strong> w == h =&gt; img // do nothing and returns the original one<br/><strong>        case</strong> x <br/><strong>            if</strong> w &gt; h =&gt; Scalr.crop(img, (w - h) / 2, 0, dim, dim)<br/><strong>        case</strong> x <br/><strong>            if</strong> w &lt; h =&gt; Scalr.crop(img, 0, (h - w) / 2, dim, dim)<br/>        }<br/>    }</pre>
<p>Well done! Now that all of our training images are square, the next import preprocessing task is to resize them all. I decided to make all the images 128 x 128 in size. Let's see how the previous (the original) one looks after resizing:</p>
<div class="CDPAlignCenter CDPAlign"><img height="145" width="219" class="alignnone size-full wp-image-579 image-border" src="assets/a529084c-1479-4008-ac6b-37c1de1bebb7.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 10: Image resizing (256 x 256, 128 x 128, 64 x 64 and 32 x 32 respectively)</div>
<p>The following method did the trick (but feel free to execute the <kbd>ImageResize.scala</kbd> script to see a demo):</p>
<pre><strong>def</strong> resizeImg(img: java.awt.image.BufferedImage, width: Int, height: Int) = {<br/>    Scalr.resize(img, Scalr.Method.BALANCED, width, height) <br/>}</pre>
<p>By the way, for the image resizing and squaring, I used some built-in packages for image reading and some third-party packages for processing:</p>
<pre><strong>import</strong> org.imgscalr._<br/><strong>import</strong> java.io.File<br/><strong>import</strong> javax.imageio.ImageIO</pre>
<p>To use the preceding packages, add the following dependencies in a Maven-friendly <kbd>pom.xml</kbd> file:</p>
<pre>&lt;dependency&gt;<br/>    &lt;groupId&gt;org.imgscalr&lt;/groupId&gt;<br/>    &lt;artifactId&gt;imgscalr-lib&lt;/artifactId&gt;<br/>    &lt;version&gt;4.2&lt;/version&gt;<br/>&lt;/dependency&gt;<br/>&lt;dependency&gt;<br/>    &lt;groupId&gt;org.datavec&lt;/groupId&gt;<br/>    &lt;artifactId&gt;datavec-data-image&lt;/artifactId&gt;<br/>    &lt;version&gt;0.9.1&lt;/version&gt;<br/>&lt;/dependency&gt;<br/>&lt;dependency&gt;<br/>    &lt;groupId&gt;com.sksamuel.scrimage&lt;/groupId&gt;<br/>    &lt;artifactId&gt;scrimage-core_2.10&lt;/artifactId&gt;<br/>    &lt;version&gt;2.1.0&lt;/version&gt;<br/>&lt;/dependency&gt;</pre>
<p>Although DL4j-based CNNs can handle color images, it's better to simplify the computation with grayscale images. Although color images are more exciting and effective, this way we can make the overall representation simpler and space efficient.</p>
<p>Let's give an example of our previous step. We resized each image to a 256 x 256 pixel image represented by 16,384 features, rather than 16,384 x 3 for a color image having three RGB channels (execute <kbd>GrayscaleConverter.scala</kbd> to see a demo). Let's see how the converted image would look:</p>
<div class="CDPAlignCenter CDPAlign"><img height="268" width="481" class="alignnone size-full wp-image-580 image-border" src="assets/5c109324-5f47-4bf4-9aec-c9007d184095.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11: Left - original image, right the grayscale one RGB averaging</div>
<p>The preceding conversion is done using two methods called <kbd>pixels2Gray()</kbd> and <kbd>makeGray()</kbd>:</p>
<pre><strong>def</strong> pixels2Gray(R: Int, G: Int, B: Int): Int = (R + G + B) / 3<br/><strong>def</strong> makeGray(testImage: java.awt.image.BufferedImage): java.awt.image.BufferedImage = {<br/><strong>    val</strong> w = testImage.getWidth<br/><strong>    val</strong> h = testImage.getHeight<br/><strong>        for</strong> { <br/>        w1 &lt;- (0 until w).toVector<br/>        h1 &lt;- (0 until h).toVector<br/>        } <br/><strong>    yield</strong> <br/>    {<br/><strong>    val</strong> col = testImage.getRGB(w1, h1)<br/><strong>    val</strong> R = (col &amp; 0xff0000) / 65536<br/><strong>    val</strong> G = (col &amp; 0xff00) / 256<br/><strong>    val</strong> B = (col &amp; 0xff)<br/><strong>    val</strong> graycol = pixels2Gray(R, G, B)<br/>testImage.setRGB(w1, h1, <strong>new</strong> Color(graycol, graycol, graycol).getRGB)<br/>    }<br/>testImage<br/>}</pre>
<p>So what happens under the hood? We chain the preceding three steps: make all the images square, then convert all of them to 25 x 256, and finally convert the resized image into a grayscale one:</p>
<pre><strong>val</strong> demoImage = ImageIO.read(<strong>new</strong> File(x))<br/>    .makeSquare<br/>    .resizeImg(resizeImgDim, resizeImgDim) // (128, 128)<br/>    .image2gray</pre>
<p>So, in summary, we now have all the images in gray after squaring and resizing. The following image gives some sense of the conversion step:</p>
<div class="CDPAlignCenter CDPAlign"><img height="192" width="481" class="alignnone size-full wp-image-581 image-border" src="assets/4f8c2de5-bfa6-4582-871b-690811daae45.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 12: Resized figure (left the original and tall one, right the squared one)</div>
<p>The following chaining also comes with some additional effort. Now we put these three steps together in the code, and we can finally prepare all of our images:</p>
<pre><strong>import</strong> scala.Vector<br/><strong>import</strong> org.imgscalr._<br/><br/><strong>object</strong> imageUtils {<br/><strong>    implicit</strong><strong>class</strong> imageProcessingPipeline(img: java.awt.image.BufferedImage) {<br/>    // image 2 vector processing<br/><strong>    def</strong> pixels2gray(R: Int, G:Int, B: Int): Int = (R + G + B) / 3<br/><strong>    def</strong> pixels2color(R: Int, G:Int, B: Int): Vector[Int] = Vector(R, G, B)<br/><strong>    private </strong><strong>def</strong> image2vec[A](f: (Int, Int, Int) =&gt; A ): Vector[A] = {<br/><strong>        val</strong> w = img.getWidth<br/><strong>        val</strong> h = img.getHeight<br/><strong>        for</strong> {<br/>            w1 &lt;- (0 until w).toVector<br/>            h1 &lt;- (0 until h).toVector<br/>            } <br/><strong>        yield</strong> {<br/><strong>            val</strong> col = img.getRGB(w1, h1)<br/><strong>            val</strong> R = (col &amp; 0xff0000) / 65536<br/><strong>            val</strong> G = (col &amp; 0xff00) / 256<br/><strong>            val</strong> B = (col &amp; 0xff)<br/>        f(R, G, B)<br/>                }<br/>            }<br/><br/><strong>    def</strong> image2gray: Vector[Int] = image2vec(pixels2gray)<br/><strong>    def</strong> image2color: Vector[Int] = image2vec(pixels2color).flatten<br/><br/>    // make image square<br/><strong>    def</strong> makeSquare = {<br/><strong>        val</strong> w = img.getWidth<br/><strong>        val</strong> h = img.getHeight<br/><strong>        val</strong> dim = List(w, h).min<br/>        img <strong>match</strong> {<br/><strong>            case</strong> x     <br/><strong>                if</strong> w == h =&gt; img<br/><strong>            case</strong> x <br/><strong>                if</strong> w &gt; h =&gt; Scalr.crop(img, (w-h)/2, 0, dim, dim)<br/><strong>            case</strong> x <br/><strong>                if</strong> w &lt; h =&gt; Scalr.crop(img, 0, (h-w)/2, dim, dim)<br/>              }<br/>            }<br/><br/>    // resize pixels<br/><strong>    def</strong> resizeImg(width: Int, height: Int) = {<br/>        Scalr.resize(img, Scalr.Method.BALANCED, width, height)<br/>            }<br/>        }<br/>    }</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extracting image metadata</h1>
                </header>
            
            <article>
                
<p>Up too this point, we have loaded and pre-processed raw images, but we have no idea about the image metadata that we need to make our CNNs learn. Thus, it's time to load the CSV files that contain metadata about each image.</p>
<p>I wrote a method to read such metadata in CSV format, called <kbd>readMetadata()</kbd>, which is used later on by two other methods called <kbd>readBusinessLabels</kbd> and <kbd>readBusinessToImageLabels</kbd>. These three methods are defined in the <kbd>CSVImageMetadataReader.scala</kbd> script. Here's the signature of the <kbd>readMetadata()</kbd> method:</p>
<pre><strong>def</strong> readMetadata(csv: String, rows: List[Int]=List(-1)): List[List[String]] = {<br/><strong>    val</strong> src = Source.fromFile(csv)<br/><br/><strong>    def</strong> reading(csv: String): List[List[String]]= {<br/>        src.getLines.map(x =&gt; x.split(",").toList)<br/>            .toList<br/>            }<br/><strong>        try</strong> {<br/><strong>            if</strong>(rows==List(-1)) reading(csv)<br/><strong>            else</strong> rows.map(reading(csv))<br/>            } <br/><strong>        finally</strong> {<br/>            src.close<br/>            }<br/>        }</pre>
<p>The <kbd>readBusinessLabels()</kbd> method maps from business ID to labels of the form <kbd>businessID</kbd> → Set (labels):</p>
<pre><strong>def</strong> readBusinessLabels(csv: String, rows: List[Int]=List(-1)): Map[String, Set[Int]] = {<br/><strong>    val</strong> reader = readMetadata(csv)<br/>    reader.drop(1)<br/>        .map(x =&gt; x <strong>match</strong> {<br/><strong>        case</strong> x :: Nil =&gt; (x(0).toString, Set[Int]())<br/><strong>        case</strong> _ =&gt; (x(0).toString, x(1).split(" ").map(y =&gt; y.toInt).toSet)<br/>        }).toMap<br/>}</pre>
<p>The <kbd>readBusinessToImageLabels ()</kbd> method maps from image ID to business ID of the form <kbd>imageID</kbd> → <kbd>businessID</kbd>:</p>
<pre><strong>def</strong> readBusinessToImageLabels(csv: String, rows: List[Int] = List(-1)): Map[Int, String] = {<br/><strong>    val</strong> reader = readMetadata(csv)<br/>    reader.drop(1)<br/>        .map(x =&gt; x <strong>match</strong> {<br/><strong>        case</strong> x :: Nil =&gt; (x(0).toInt, "-1")<br/><strong>        case</strong> _ =&gt; (x(0).toInt, x(1).split(" ").head)<br/>        }).toMap<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image feature extraction</h1>
                </header>
            
            <article>
                
<p>So far we have seen how to preprocess images so that features from those images can be extracted and fed into CNNs. Additionally, we have seen how to extract and map metadata and link it with the original images. Now it's time to extract features from those preprocessed images.</p>
<p>We also need to keep in mind the provenance of the metadata of each image. As you can guess, we need three map operations for feature extractions. Essentially, we have three maps. For details see the <kbd>imageFeatureExtractor.scala</kbd> script:</p>
<ol>
<li>Business mapping with the form <kbd>imageID</kbd> → <kbd>businessID</kbd></li>
<li>Data map of the form <kbd>imageID</kbd> → image data</li>
<li>Label map of the form <kbd>businessID</kbd> → labels</li>
</ol>
<p>We first define a regular expression pattern to extract the <kbd>.jpg</kbd> name from the CSV <kbd>ImageMetadataReader</kbd> class, which is used to match against training labels:</p>
<pre><strong>val</strong> patt_get_jpg_name = <strong>new</strong> Regex("[0-9]")</pre>
<p>Then we extract all the image IDs associated with their respective business ID:</p>
<pre><strong>def</strong> getImgIdsFromBusinessId(bizMap: Map[Int, String], businessIds: List[String]): List[Int] = {<br/>    bizMap.filter(x =&gt; businessIds.exists(y =&gt; y == x._2)).map(_._1).toList <br/>    }</pre>
<p>Now we need to load and process all the images that are already preprocessed to extract the image IDs by mapping with the IDs extracted from the business IDs, as shown earlier:</p>
<pre><strong>def</strong> getImageIds(photoDir: String, businessMap: Map[Int, String] = Map(-1 -&gt; "-1"), businessIds:         <br/>    List[String] = List("-1")): List[String] = {<br/><strong>    val</strong> d = <strong>new</strong> File(photoDir)<br/><strong>    val</strong> imgsPath = d.listFiles().map(x =&gt; x.toString).toList<br/><strong>    if</strong> (businessMap == Map(-1 -&gt; "-1") || businessIds == List(-1)) {<br/>        imgsPath<br/>    } <br/><strong>    else</strong> {<br/><strong>        val</strong> imgsMap = imgsPath.map(x =&gt; patt_get_jpg_name.findAllIn(x).mkString.toInt -&gt; x).toMap<br/><strong>        val</strong> imgsPathSub = getImgIdsFromBusinessId(businessMap, businessIds)<br/>        imgsPathSub.filter(x =&gt; imgsMap.contains(x)).map(x =&gt; imgsMap(x))<br/>        } <br/>    }</pre>
<p>So far, we have been able to extract all the image IDs that are somehow associated with at least one business. The next move would be to read and process these images into an <kbd>imageID</kbd> → vector map:</p>
<pre><strong>def</strong> processImages(imgs: List[String], resizeImgDim: Int = 128, nPixels: Int = -1): Map[Int,Vector[Int]]= {<br/>    imgs.map(x =&gt; patt_get_jpg_name.findAllIn(x).mkString.toInt -&gt; {<br/><strong>        val</strong> img0 = ImageIO.read(<strong>new</strong> File(x))<br/>        .makeSquare<br/>        .resizeImg(resizeImgDim, resizeImgDim) // (128, 128)<br/>        .image2gray<br/><strong>   if</strong>(nPixels != -1) img0.slice(0, nPixels)<br/><strong>   else</strong> img0<br/>        }<br/>    ).filter( x =&gt; x._2 != ())<br/>    .toMap<br/>    }</pre>
<p>Well done! We are just one step behind to extract required to train our CNNs. The final step in feature extraction is to extract the pixel data:</p>
<div class="CDPAlignCenter CDPAlign"><img height="338" width="1013" class="alignnone size-full wp-image-582 image-border" src="assets/7bee7abb-8627-4ec1-84b4-66d95a1b5bb4.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 13: Image data representation</div>
<p>In summary, we need to keep track of four pieces of the object for each image—that is, <kbd>imageID</kbd>, <kbd>businessID</kbd>, labels, and pixel data. Thus, as shown in the preceding figure, the primary data structure is constructed with four data types (four tuples)—<kbd>imgID</kbd>, <kbd>businessID</kbd>, pixel data vector, and labels:</p>
<pre>List[(Int, String, Vector[Int], Set[Int])]</pre>
<p>Thus, we should have a class containing all pieces of these objects. Don't worry, everything we need is defined in the <kbd>featureAndDataAligner.scala</kbd> script. Once we have instantiated an instance of <kbd>featureAndDataAligner</kbd> using the following line of code in the <kbd>Main.scala</kbd> script (under the <kbd>main</kbd> method), the <kbd>businessMap</kbd>, <kbd>dataMap</kbd>, and <kbd>labMap</kbd> are provided:</p>
<pre><strong>val</strong> alignedData = <strong>new</strong> featureAndDataAligner(dataMap, businessMap, Option(labelMap))()</pre>
<p>Here, the option type for <kbd>labMap</kbd> is used since we don't have this information when we score on test data—that is, <kbd>None</kbd> is used for that invocation:</p>
<pre><strong>class</strong> featureAndDataAligner(dataMap: Map[Int, Vector[Int]], bizMap: Map[Int, String], labMap: Option[Map[String, Set[Int]]])(rowindices: List[Int] = dataMap.keySet.toList) {<br/><strong>    def </strong><strong>this</strong>(dataMap: Map[Int, Vector[Int]], bizMap: Map[Int, String])(rowindices: List[Int]) =         <strong>this</strong>(dataMap, bizMap, None)(rowindices)<br/><br/><strong>    def</strong> alignBusinessImgageIds(dataMap: Map[Int, Vector[Int]], bizMap: Map[Int, String])<br/>        (rowindices: List[Int] = dataMap.keySet.toList): List[(Int, String, Vector[Int])] = {<br/><strong>        for</strong> { <br/>            pid &lt;- rowindices<br/><strong>            val</strong> imgHasBiz = bizMap.get(pid) <br/>            // returns None if img doe not have a bizID<br/><strong>            val</strong> bid = <strong>if</strong>(imgHasBiz != None) imgHasBiz.get <br/><strong>            else</strong> "-1"<br/><strong>            if</strong> (dataMap.keys.toSet.contains(pid) &amp;&amp; imgHasBiz != None)<br/>            } <br/><strong>        yield</strong> {<br/>        (pid, bid, dataMap(pid))<br/>           }<br/>        }<br/><strong>def</strong> alignLabels(dataMap: Map[Int, Vector[Int]], bizMap: Map[Int, String], labMap: Option[Map[String,     Set[Int]]])(rowindices: List[Int] = dataMap.keySet.toList): List[(Int, String, Vector[Int], Set[Int])] = {<br/><strong>    def</strong> flatten1[A, B, C, D](t: ((A, B, C), D)): (A, B, C, D) = (t._1._1, t._1._2, t._1._3, t._2)<br/><strong>        val</strong> al = alignBusinessImgageIds(dataMap, bizMap)(rowindices)<br/><strong>        for</strong> { p &lt;- al<br/>        } <br/><strong>        yield</strong> {<br/><strong>            val</strong> bid = p._2<br/><strong>            val</strong> labs = labMap <strong>match</strong> {<br/><strong>            case</strong> None =&gt; Set[Int]()<br/><strong>            case</strong> x =&gt; (<strong>if</strong>(x.get.keySet.contains(bid)) x.get(bid) <br/>        <strong>else</strong> Set[Int]())<br/>            }<br/>            flatten1(p, labs)<br/>        }<br/>    }<br/><strong>    lazy </strong><strong>val</strong> data = alignLabels(dataMap, bizMap, labMap)(rowindices)<br/>   // getter functions<br/><strong>    def</strong> getImgIds = data.map(_._1)<br/><strong>    def</strong> getBusinessIds = data.map(_._2)<br/><strong>    def</strong> getImgVectors = data.map(_._3)<br/><strong>    def</strong> getBusinessLabels = data.map(_._4)<br/><strong>    def</strong> getImgCntsPerBusiness = getBusinessIds.groupBy(identity).mapValues(x =&gt; x.size) <br/>}</pre>
<p>Excellent! Up to this point, we have managed to extract the features to train our CNNs. However, the feature in current form is still not suitable to feed into the CNNs, because we only have the feature vectors without labels. Thus, we need an intermediate conversion.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the ND4j dataset</h1>
                </header>
            
            <article>
                
<p>As I said, we need an intermediate conversion and pre-process to get the training set to contain feature vectors as well as labels. The conversion is pretty straightforward: we need the feature vector and the business labels.</p>
<p>For this, we have the <kbd>makeND4jDataSets</kbd> class (see <kbd>makeND4jDataSets.scala</kbd> for details). The class creates a ND4j dataset object from the data structure from the <kbd>alignLables</kbd> function in the form of <kbd>List[(imgID, bizID, labels, pixelVector)]</kbd>. First, we prepare the dataset using the <kbd>makeDataSet()</kbd> method:</p>
<pre><strong>def</strong> makeDataSet(alignedData: featureAndDataAligner, bizClass: Int): DataSet = {<br/><strong>    val</strong> alignedXData = alignedData.getImgVectors.toNDArray<br/><strong>    val</strong> alignedLabs = alignedData.getBusinessLabels.map(x =&gt; <br/><strong>    if</strong> (x.contains(bizClass)) Vector(1, 0) <br/>    <strong>else</strong> Vector(0, 1)).toNDArray<br/><strong>    new</strong> DataSet(alignedXData, alignedLabs)<br/>    }</pre>
<p>Then we need to convert the preceding data structure further, into <kbd>INDArray</kbd>, which can then be consumed by the CNNs:</p>
<pre><strong>def</strong> makeDataSetTE(alignedData: featureAndDataAligner): INDArray = {<br/>    alignedData.getImgVectors.toNDArray<br/>    }</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the CNNs and saving the trained models</h1>
                </header>
            
            <article>
                
<p>So far, we have seen how to prepare the training set; now we have a challenge ahead. We have to train 234,545 images. Although the testing phase would be less exhaustive with only 500 images, it's better to train each CNN using batch-mode using DL4j's <kbd>MultipleEpochsIterator</kbd>. Here's a list of important hyperparameters and their details:</p>
<ul>
<li><strong>Layers</strong>: As we have already observed with our simple 5 layers MNIST, we received outstanding classification accuracy, which is very promising. Here I will try to construct a similar network having.</li>
<li><strong>The number of samples</strong>: If you're training all the images, it will take too long. If you train using the CPU, not the GPU, it will take days. When I tried with 50,000 images, it took one whole day for a machine with an i7 processor and 32 GB of RAM. Now you can imagine how long it would take for the whole dataset. In addition, it will require at least 256 GB of RAM even if you do the training in the batch model.</li>
<li><strong>Number of epochs</strong>: This is the number of iterations through all the training records.</li>
<li><strong>Number of output feature maps (that is, nOut)</strong>: This is the number of feature maps. Take a closer look at other examples in the DL4j GitHub repository.</li>
<li><strong>Learning Rate</strong>: From the TensorFlow-like framework, I got some insights. In my opinion, it would be great to set a learning rate of 0.01 and 0.001.</li>
<li><strong>Number of batch</strong>: This is the number of records in each batch—32, 64, 128, and so on. I used 128.</li>
</ul>
<p>Now, with the preceding hyperparameters, we can start training our CNNs. The following code does the trick. At first, we prepare the training set, then we define the required hyperparameters, then we normalize the dataset so the ND4j data frame is encoded and any labels that are considered true are 1s and the rest are zeros. Then we shuffle both the rows and labels of the encoded dataset.</p>
<p>Now we need to create epochs for the dataset iterator using the <kbd>ListDataSetIterator</kbd> and <kbd>MultipleEpochsIterator</kbd> respectively. Once the datasets are converted into batch-model, we are then ready to train the constructed CNNs:</p>
<pre><strong>def</strong> trainModelEpochs(alignedData: featureAndDataAligner, businessClass: Int = 1, saveNN: String = "") = {<br/><strong>    val</strong> ds = makeDataSet(alignedData, businessClass)<br/><strong>    val</strong> nfeatures = ds.getFeatures.getRow(0).length // Hyperparameter<br/><strong>    val</strong> numRows = Math.sqrt(nfeatures).toInt //numRows*numColumns == data*channels<br/><strong>    val</strong> numColumns = Math.sqrt(nfeatures).toInt //numRows*numColumns == data*channels<br/><strong>    val</strong> nChannels = 1 // would be 3 if color image w R,G,B<br/><strong>    val</strong> outputNum = 9 // # of classes (# of columns in output)<br/><strong>    val</strong> iterations = 1<br/><strong>    val</strong> splitTrainNum = math.ceil(ds.numExamples * 0.8).toInt // 80/20 training/test split<br/><strong>    val</strong> seed = 12345<br/><strong>    val</strong> listenerFreq = 1<br/><strong>    val</strong> nepochs = 20<br/><strong>    val</strong> nbatch = 128 // recommended between 16 and 128<br/><br/>    ds.normalizeZeroMeanZeroUnitVariance()<br/>    Nd4j.shuffle(ds.getFeatureMatrix, <strong>new</strong> Random(seed), 1) // shuffles rows in the ds.<br/>    Nd4j.shuffle(ds.getLabels, <strong>new</strong> Random(seed), 1) // shuffles labels accordingly<br/><br/><strong>    val</strong> trainTest: SplitTestAndTrain = ds.splitTestAndTrain(splitTrainNum, <strong>new</strong> Random(seed))<br/><br/>    // creating epoch dataset iterator<br/><strong>    val</strong> dsiterTr = <strong>new</strong> ListDataSetIterator(trainTest.getTrain.asList(), nbatch)<br/><strong>    val</strong> dsiterTe = <strong>new</strong> ListDataSetIterator(trainTest.getTest.asList(), nbatch)<br/><strong>    val</strong> epochitTr: MultipleEpochsIterator = <strong>new</strong> MultipleEpochsIterator(nepochs, dsiterTr)<br/><br/><strong>    val</strong> epochitTe: MultipleEpochsIterator = <strong>new</strong> MultipleEpochsIterator(nepochs, dsiterTe)<br/>    //First convolution layer with ReLU as activation function<br/><strong>    val</strong> layer_0 = <strong>new</strong> ConvolutionLayer.Builder(6, 6)<br/>        .nIn(nChannels)<br/>        .stride(2, 2) // default stride(2,2)<br/>        .nOut(20) // # of feature maps<br/>        .dropOut(0.5)<br/>        .activation("relu") // rectified linear units<br/>        .weightInit(WeightInit.RELU)<br/>        .build()<br/><br/>    //First subsampling layer<br/><strong>    val</strong> layer_1 = <strong>new</strong> SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)<br/>        .kernelSize(2, 2)<br/>        .stride(2, 2)<br/>        .build()<br/><br/>    //Second convolution layer with ReLU as activation function<br/><strong>    val</strong> layer_2 = <strong>new</strong> ConvolutionLayer.Builder(6, 6)<br/>        .nIn(nChannels)<br/>        .stride(2, 2)<br/>        .nOut(50)<br/>        .activation("relu")<br/>        .build()<br/><br/>    //Second subsampling layer<br/><strong>    val</strong> layer_3 = <strong>new</strong> SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)<br/>        .kernelSize(2, 2)<br/>        .stride(2, 2)<br/>        .build()<br/><br/>    //Dense layer<br/><strong>    val</strong> layer_4 = <strong>new</strong> DenseLayer.Builder()<br/>        .activation("relu")<br/>        .nOut(500)<br/>        .build()<br/><br/>    // Final and fully connected layer with Softmax as activation function<br/><strong>    val</strong> layer_5 = <strong>new</strong> OutputLayer.Builder(LossFunctions.LossFunction.MCXENT)<br/>        .nOut(outputNum)<br/>        .weightInit(WeightInit.XAVIER)<br/>        .activation("softmax")<br/>        .build()<br/><strong><br/>    val</strong> builder: MultiLayerConfiguration.Builder = <strong>new</strong> NeuralNetConfiguration.Builder()<br/>        .seed(seed)<br/>        .iterations(iterations)<br/>        .miniBatch(<strong>true</strong>)<br/>        .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)<br/>        .regularization(<strong>true</strong>).l2(0.0005)<br/>        .learningRate(0.01)<br/>        .list(6)<br/>            .layer(0, layer_0)<br/>            .layer(1, layer_1)<br/>            .layer(2, layer_2)<br/>            .layer(3, layer_3)<br/>            .layer(4, layer_4)<br/>            .layer(5, layer_5)<br/>    .backprop(<strong>true</strong>).pretrain(<strong>false</strong>)<br/><br/><strong>    new</strong> ConvolutionLayerSetup(builder, numRows, numColumns, nChannels)<br/><strong>    val</strong> conf: MultiLayerConfiguration = builder.build()<br/><strong>    val</strong> model: MultiLayerNetwork = <strong>new</strong> MultiLayerNetwork(conf)<br/><br/>    model.init()<br/>    model.setListeners(Seq[IterationListener](<strong>new</strong> ScoreIterationListener(listenerFreq)).asJava)<br/>    model.fit(epochitTr)<br/><br/><strong>    val</strong> eval = <strong>new</strong> Evaluation(outputNum)<br/><strong>    while</strong> (epochitTe.hasNext) {<br/><strong>        val</strong> testDS = epochitTe.next(nbatch)<br/><strong>        val</strong><span> output: INDArray = model.output(testDS.getFeatureMatrix)<br/></span>        eval.eval(testDS.getLabels(), output)<br/>        }<br/><strong>    if</strong> (!saveNN.isEmpty) {<br/>        // model config<br/>        FileUtils.write(<strong>new</strong> File(saveNN + ".json"), model.getLayerWiseConfigurations().toJson())<br/>        // model parameters<br/><strong>        val</strong> dos: DataOutputStream = <strong>new</strong> DataOutputStream(Files.newOutputStream(Paths.get(saveNN + ".bin")))<br/>        Nd4j.write(model.params(), dos)<br/>        }<br/>    }</pre>
<p class="mce-root">In the preceding code, we also save a <kbd>.json</kbd> file containing all the network configurations and a <kbd>.bin</kbd> file for holding all the weights and parameters of all the CNNs. This is done by two methods; namely, <kbd>saveNN()</kbd> and <kbd>loadNN()</kbd> that are defined in the <kbd>NeuralNetwok.scala</kbd> script. First, let's see the signature of the <kbd>saveNN()</kbd> method that goes as follows:</p>
<pre><strong>def</strong> saveNN(model: MultiLayerNetwork, NNconfig: String, NNparams: String) = {<br/>    // save neural network config<br/>    FileUtils.write(<strong>new</strong> File(NNconfig), model.getLayerWiseConfigurations().toJson())<br/>    // save neural network parms<br/><strong>    val</strong> dos: DataOutputStream = <strong>new</strong> DataOutputStream(Files.newOutputStream(Paths.get(NNparams)))<br/>    Nd4j.write(model.params(), dos)<br/>}</pre>
<p>The idea is visionary as well as important since, as I said, you wouldn't train your whole network for the second time to evaluate a new test set—that is, suppose you want to test just a single image. We also have another method named <kbd>loadNN()</kbd> that reads back the <kbd>.json</kbd> and <kbd>.bin</kbd> file we created earlier to a <kbd>MultiLayerNetwork</kbd> and is used to score new test data. The method goes as follows:</p>
<pre><strong>def</strong> loadNN(NNconfig: String, NNparams: String) = {<br/>    // get neural network config<br/><strong>    val</strong> confFromJson: MultiLayerConfiguration =                     <br/>    MultiLayerConfiguration.fromJson(FileUtils.readFileToString(<strong>new</strong> File(NNconfig)))<br/><br/>    // get neural network parameters<br/><strong>    val</strong> dis: DataInputStream = <strong>new</strong> DataInputStream(<strong>new</strong> FileInputStream(NNparams))<br/><strong>    val</strong> newParams = Nd4j.read(dis)<br/><br/>    // creating network object<br/><strong>    val</strong> savedNetwork: MultiLayerNetwork = <strong>new</strong> MultiLayerNetwork(confFromJson)<br/>    savedNetwork.init()<br/>    savedNetwork.setParameters(newParams)<br/>    savedNetwork <br/>    }</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating the model</h1>
                </header>
            
            <article>
                
<p>The scoring approach that we're going to use is pretty simple. It assigns business-level labels by averaging the image-level predictions. I know I did it naively, but you can try a better approach. What I have done is assign a business with label <kbd>0</kbd> if the average of the probabilities across all of its images that it belongs to class <kbd>0</kbd> is greater than 0.5:</p>
<pre><strong>def</strong> scoreModel(model: MultiLayerNetwork, ds: INDArray) = {<br/>    model.output(ds)<br/>}</pre>
<p>Then we collect the model predictions from the <kbd>scoreModel()</kbd> method and merge with <kbd>alignedData</kbd>:</p>
<pre><br/><strong>def</strong> aggImgScores2Business(scores: INDArray, alignedData: featureAndDataAligner ) = {<br/>    assert(scores.size(0) == alignedData.data.length, "alignedData and scores length are different. They     must be equal")<br/><br/><strong>def</strong> getRowIndices4Business(mylist: List[String], mybiz: String): List[Int] = mylist.zipWithIndex.filter(x     =&gt; x._1 == mybiz).map(_._2)<br/><br/><strong>def</strong> mean(xs: List[Double]) = xs.sum / xs.size<br/>    alignedData.getBusinessIds.distinct.map(x =&gt; (x, {<br/><strong>    val</strong> irows = getRowIndices4Business(alignedData.getBusinessIds, x)<br/><strong>    val</strong> ret = <br/><strong>    for</strong>(row &lt;- irows) <br/><strong>        yield</strong> scores.getRow(row).getColumn(1).toString.toDouble<br/>        mean(ret)<br/>        }))<br/>    }</pre>
<p>Finally, we can restore the trained and saved models, restore them back, and generate the submission file for Kaggle. The thing is that we need to aggregate image predictions to business scores for each model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Wrapping up by executing the main() method</h1>
                </header>
            
            <article>
                
<p>Let's wrap up the overall discussion by watching the performance of our model. The following code is an overall glimpse:</p>
<pre><strong>package</strong> Yelp.Classifier<br/><strong>import</strong> Yelp.Preprocessor.CSVImageMetadataReader._<br/><strong>import</strong> Yelp.Preprocessor.featureAndDataAligner<br/><strong>import</strong> Yelp.Preprocessor.imageFeatureExtractor._<br/><strong>import</strong> Yelp.Evaluator.ResultFileGenerator._<br/><strong>import</strong> Yelp.Preprocessor.makeND4jDataSets._<br/><strong>import</strong> Yelp.Evaluator.ModelEvaluation._<br/><strong>import</strong> Yelp.Trainer.CNNEpochs._<br/><strong>import</strong> Yelp.Trainer.NeuralNetwork._<br/><br/><strong>object</strong> YelpImageClassifier {<br/><strong>    def</strong> main(args: Array[String]): Unit = {<br/>        // image processing on training data<br/><strong>        val</strong> labelMap = readBusinessLabels("data/labels/train.csv")<br/><strong>        val</strong> businessMap = readBusinessToImageLabels("data/labels/train_photo_to_biz_ids.csv")<br/><strong>        val</strong> imgs = getImageIds("data/images/train/", businessMap, <br/>        businessMap.map(_._2).toSet.toList).slice(0,20000) // 20000 images<br/>    <br/>        println("Image ID retreival done!")<br/><strong>        val</strong> dataMap = processImages(imgs, resizeImgDim = 256)<br/>        println("Image processing done!")<br/><br/><strong>        val</strong> alignedData = <br/><strong>            new</strong> featureAndDataAligner(dataMap, businessMap, Option(labelMap))()<br/>        println("Feature extraction done!")<br/><br/>        // training one model for one class at a time. Many hyperparamters hardcoded within<br/><strong>        val</strong> cnn0 = trainModelEpochs(alignedData, businessClass = 0, saveNN = "models/model0")<br/><strong>        val</strong> cnn1 = trainModelEpochs(alignedData, businessClass = 1, saveNN = "models/model1")<br/><strong>        val</strong> cnn2 = trainModelEpochs(alignedData, businessClass = 2, saveNN = "models/model2")<br/><strong>        val</strong> cnn3 = trainModelEpochs(alignedData, businessClass = 3, saveNN = "models/model3")<br/><strong>        val</strong> cnn4 = trainModelEpochs(alignedData, businessClass = 4, saveNN = "models/model4")<br/><strong>        val</strong> cnn5 = trainModelEpochs(alignedData, businessClass = 5, saveNN = "models/model5")<br/><strong>        val</strong> cnn6 = trainModelEpochs(alignedData, businessClass = 6, saveNN = "models/model6")<br/><strong>        val</strong> cnn7 = trainModelEpochs(alignedData, businessClass = 7, saveNN = "models/model7")<br/><strong>        val</strong> cnn8 = trainModelEpochs(alignedData, businessClass = 8, saveNN = "models/model8")<br/><br/>    // processing test data for scoring<br/><strong>        val</strong> businessMapTE = readBusinessToImageLabels("data/labels/test_photo_to_biz.csv")<br/><strong>        val</strong> imgsTE = getImageIds("data/images/test//", businessMapTE,     <br/>        businessMapTE.map(_._2).toSet.toList)<br/><br/><strong>        val</strong> dataMapTE = processImages(imgsTE, resizeImgDim = 128) // make them 256x256<br/><strong>        val</strong> alignedDataTE = <strong>new</strong> featureAndDataAligner(dataMapTE, businessMapTE, None)()<br/><br/>        // creating csv file to submit to kaggle (scores all models)<br/><strong>        val</strong> Results = SubmitObj(alignedDataTE, "results/ModelsV0/")<br/><strong>        val</strong> SubmitResults = writeSubmissionFile("kaggleSubmitFile.csv", Results, thresh = 0.9)<br/>        }<br/>    }<br/>&gt;&gt;&gt;<br/>==========================Scores======================================<br/> Accuracy: 0.6833<br/> Precision: 0.53<br/> Recall: 0.5222<br/> F1 Score: 0.5261<br/>======================================================================</pre>
<p>So, what's your impression? It's true that we haven't received outstanding classification accuracy. But we can still try with tuned hyperparameters. The next section provides some insight.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tuning and optimizing CNN hyperparameters</h1>
                </header>
            
            <article>
                
<p>The following hyperparameters are very important and must be tuned to achieve optimized results.</p>
<ul>
<li><strong>Dropout</strong>: Used for random omission of feature detectors to prevent overfitting</li>
<li><strong>Sparsity</strong>: Used to force activations of sparse/rare inputs</li>
<li><strong>Adagrad</strong>: Used for feature-specific learning-rate optimization</li>
<li><strong>Regularization</strong>: L1 and L2 regularization</li>
<li><strong>Weight transforms</strong>: Useful for deep autoencoders</li>
<li><strong>Probability distribution manipulation</strong>: Used for initial weight generation</li>
<li>Gradient normalization and clipping</li>
</ul>
<p>Another important question is: when do you want to add a max pooling layer rather than a convolutional layer with the same stride? A max pooling layer has no parameters at all, whereas a convolutional layer has quite a few. Sometimes, adding a local response normalization layer that makes the neurons that most strongly activate inhibit neurons at the same location but in neighboring feature maps, encourages different feature maps to specialize and pushes them apart, forcing them to explore a wider range of features. It is typically used in the lower layers to have a larger pool of low-level features that the upper layers can build upon.</p>
<p>One of the main advantages observed during the training of large neural networks is overfitting, that is, generating very good approximations for the training data but emitting noise for the zones between single points. In the case of overfitting, the model is specifically adjusted to the training dataset, so it will not be used for generalization. Therefore, although it performs well on the training set, its performance on the test dataset and subsequent tests is poor because it lacks the generalization property:</p>
<div class="CDPAlignCenter CDPAlign"><img height="564" width="1499" class="alignnone size-full wp-image-583 image-border" src="assets/e7c1d031-2428-4f36-b4d1-a703de85c86e.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 14: Dropout versus without dropout</div>
<p>The main advantage of this method is that it avoids all neurons in a layer to synchronously optimize their weights. This adaptation made in random groups avoids all the neurons converging on the same goals, thus de-correlating the adapted weights. A second property discovered in the dropout application is that the activation of the hidden units becomes sparse, which is also a desirable characteristic.</p>
<p>Since in CNN, one of the objective functions is to minimize the evaluated cost, we must define an optimizer. The following optimizers are supported by DL4j:</p>
<ul>
<li>SGD (learning rate only)</li>
<li>Nesterovs momentum</li>
<li>Adagrad</li>
<li>RMSProp</li>
<li>Adam</li>
<li>AdaDelta</li>
</ul>
<p>In most of the cases, we can adopt the implemented RMSProp, which is an advanced form of gradient descent, if the performance is not satisfactory. RMSProp performs better because it divides the learning rate by an exponentially decaying average of squared gradients. The suggested setting value of the decay parameter is 0.9, while a good default value for the learning rate is 0.001.</p>
<p>More technically, by using the most common optimizer, such as <strong>Stochastic Gradient Descent</strong> (<strong>SGD</strong>), the learning rates must scale with 1/T to get convergence, where T is the number of iterations. RMSProp tries to overcome this limitation automatically by adjusting the step size so that the step is on the same scale as the gradients. So, if you're training a neural network, but computing the gradients is mandatory, using RMSProp would be the faster way of learning in a mini-batch setting. Researchers also recommend using a Momentum optimizer while training a deep CNN or DNN.</p>
<p>From the layering architecture's perspective, CNN is different compared to DNN; it has a different requirement as well as tuning criteria. Another problem with CNNs is that the convolutional layers require a huge amount of RAM, especially during training, because the reverse pass of backpropagation requires all the intermediate values computed during the forward pass. During inference (that is, when making a prediction for a new instance), the RAM occupied by one layer can be released as soon as the next layer has been computed, so you only need as much RAM as required by two consecutive layers.</p>
<p>However, during training, everything computed during the forward pass needs to be preserved for the reverse pass, so the amount of RAM needed is (at least) the total amount of RAM required by all layers. If your GPU runs out of memory while training a CNN, here are five things you could try to solve the problem (other than purchasing a GPU with more RAM):</p>
<ul>
<li>Reduce the mini-batch size</li>
<li>Reduce dimensionality using a larger stride in one or more layers</li>
<li>Remove one or more layers</li>
<li>Use 16-bit floats instead of 32-bit floats</li>
<li>Distribute the CNN across multiple devices</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have seen how to use and build real-life applications using CNNs, which are a type of feedforward artificial neural network in which the connectivity pattern between neurons is inspired by the organization of the animal visual cortex. Our image classifier application using CNN can classify real-life images with an acceptable level of accuracy, although we did not achieve higher accuracy. However, readers are encouraged to tune hyperparameters in the code and also try the same approach with another dataset.</p>
<p>Nevertheless, and importantly since the i<span class="markup--quote markup--pullquote-quote is-other">nternal data representation of a convolutional neural network does not take into account important spatial hierarchies between simple and complex objects, CNN has some serious drawbacks and limitation for certain instances. Therefore, I would suggest you take a look at the recent activities around capsule networks on GitHub at <a href="https://github.com/topics/capsule-network">https://github.com/topics/capsule-network</a>. Hopefully, you can get something useful out from there</span></p>
<p>This is, more-or-less, the end of our little journey in developing ML projects using Scala and different open source frameworks. Throughout the chapters, I tried to provide you with several examples of how to use these wonderful technologies efficiently for developing ML projects. During the writing of this book, I had to keep many constraints in my mind, for example, the page count, API availability, and my expertise. But I tried to make the book more-or-less simple, and I also tried to avoid details on the theory, as you can read about that in many books, blogs, and websites on Apache Spark, DL4j, and H2O itself.</p>
<p>I will also keep the code of this book updated on my GitHub repo at: <a href="https://github.com/PacktPublishing/Scala-Machine-Learning-Projects" target="_blank">https://github.com/PacktPublishing/Scala-Machine-Learning-Projects</a>. Feel free to open a new issue or any pull request for improving this book and <span>stay tuned.</span></p>
<p>Finally, I did not write this book to earn money but a major portion of the royalties will be spent for the child education in the rural areas of my home district in Bangladesh. I would like to say thanks and express my sincere gratitude for buying and enjoying this book!</p>


            </article>

            
        </section>
    </body></html>