<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;11.&#xA0;Learning from the Environment with Reinforcement" id="335QG1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch11" class="calibre1"/>Chapter 11. Learning from the Environment with Reinforcement</h1></div></div></div><p class="calibre8">Supervised and unsupervised learning describe the presence or the absence of labels or targets during training. A more natural learning environment for an agent is to receive rewards when the correct decision has been taken. This reward, such as <span class="strong"><em class="calibre12">playing correctly tennis</em></span> for example, may be attributed in a complex environment, and the result of multiple actions, delayed or cumulative.</p><p class="calibre8">In order <a id="id401" class="calibre1"/>to optimize the reward from the environment for an artificial agent, the <span class="strong"><strong class="calibre2">Reinforcement Learning</strong></span> (<span class="strong"><strong class="calibre2">RL</strong></span>) field has seen the emergence of many algorithms, such as Q-learning, or Monte Carlo Tree Search, and with the advent of deep learning, these algorithms have been revised into new methods, such as deep-Q-networks, policy networks, value networks, and policy gradients.</p><p class="calibre8">We'll begin with a presentation of the reinforcement learning frame, and its potential application to virtual environments. Then, we'll develop its algorithms and their integration with deep learning, which has solved the most challenging problems in artificial intelligence.</p><p class="calibre8">The points covered in this chapter are the following:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Reinforcement learning</li><li class="listitem">Simulation environments</li><li class="listitem">Q-learning</li><li class="listitem">Monte Carlo Tree Search</li><li class="listitem">Deep Q-networks</li><li class="listitem">Policy gradients</li><li class="listitem">Asynchronous gradient descents</li></ul></div><p class="calibre8">To simplify the development of our neural nets in this chapter, we'll use Keras, the high level deep learning library on top of Theano I presented in <a class="calibre1" title="Chapter 5. Analyzing Sentiment with a Bidirectional LSTM" href="part0060_split_000.html#1P71O2-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 5</a>, <span class="strong"><em class="calibre12">Analyzing Sentiment with a Bidirectional LSTM</em></span>.</p></div>

<div class="book" title="Chapter&#xA0;11.&#xA0;Learning from the Environment with Reinforcement" id="335QG1-ccdadb29edc54339afcb9bdf9350ba6b">
<div class="book" title="Reinforcement learning tasks"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch11lvl1sec96" class="calibre1"/>Reinforcement learning tasks</h1></div></div></div><p class="calibre8">Reinforcement <a id="id402" class="calibre1"/>learning consists of training an <span class="strong"><strong class="calibre2">agent, </strong></span>that just needs occasional feedback from the <span class="strong"><strong class="calibre2">environment</strong></span>, to learn to get the best feedback at the end. The agent performs<span class="strong"><strong class="calibre2"> actions</strong></span>, modifying the <span class="strong"><strong class="calibre2">state</strong></span> of the environment.</p><p class="calibre8">The actions to navigate in the environment can be represented as directed edges from one state to another state as a graph, as shown in the following figure:</p><div class="mediaobject"><img src="../images/00201.jpeg" alt="Reinforcement learning tasks" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">A robot, working in a real environment (walking robots, control of motors, and so on) or a virtual environment (video game, online games, chat room, and so on) has to decide which movements (or keys to strike) to receive the maximum reward:</p><div class="mediaobject"><img src="../images/00202.jpeg" alt="Reinforcement learning tasks" class="calibre9"/></div><p class="calibre10"> </p></div></div>
<div class="book" title="Simulation environments" id="344B21-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec97" class="calibre1"/>Simulation environments</h1></div></div></div><p class="calibre8">Virtual <a id="id403" class="calibre1"/>environments make it possible to simulate thousands to millions of gameplays, at no other cost than the computations. For the purpose of benchmarking different reinforcement learning algorithms, simulation environments have been developed by the research community.</p><p class="calibre8">In order to find the solutions that generalize well, the Open-AI non-profit artificial intelligence research company, associated with business magnate Elon Musk, that aims to carefully promote and develop friendly AI in such a way as to benefit humanity as a whole, has <a id="id404" class="calibre1"/>gathered in its open source simulation environment, <span class="strong"><strong class="calibre2">Open-AI Gym</strong></span> (<a class="calibre1" href="https://gym.openai.com/">https://gym.openai.com/</a>), a collection of reinforcement <a id="id405" class="calibre1"/>tasks and environments in a Python toolkit to test our own approaches on them. Among these environments, you'll find:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Video games from Atari 2600, a home video game console released by Atari Inc in 1977, wrapping the simulator from the Arcade Learning Environment, one of the most common RL benchmark environment:<div class="mediaobject"><img src="../images/00203.jpeg" alt="Simulation environments" class="calibre9"/></div><p class="calibre27"> </p></li><li class="listitem">MuJoCo, a physics simulator for evaluating agents on continuous control tasks:<div class="mediaobject"><img src="../images/00204.jpeg" alt="Simulation environments" class="calibre9"/></div><p class="calibre27"> </p></li><li class="listitem">Other <a id="id406" class="calibre1"/>well-known games such as Minecraft, Soccer, Doom, and many others:<div class="mediaobject"><img src="../images/00205.jpeg" alt="Simulation environments" class="calibre9"/></div><p class="calibre27"> </p></li></ul></div><p class="calibre8">Let's install Gym and its Atari 2600 environment:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">pip</strong></span> install gym
<span class="strong"><strong class="calibre2">pip</strong></span> install gym[atari]</pre></div><p class="calibre8">It is also possible to install all environments with:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">pip</strong></span> install gym[all]</pre></div><p class="calibre8">Interacting with the gym environment is pretty simple with the <code class="email">env.step()</code> method that, given an action we choose for the agent, returns the new state, the reward, and whether the game has terminated.</p><p class="calibre8">For example, let's sample a random action:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">import</strong></span> gym

env = gym.make(<span class="strong"><strong class="calibre2">'CartPole-v0'</strong></span>)
env.reset()

<span class="strong"><strong class="calibre2">for</strong></span> _ <span class="strong"><strong class="calibre2">in</strong></span> range(1000):
    env.render()
    action = env.action_space.sample()
    next_state, reward, done, info = env.step(action)
    <span class="strong"><strong class="calibre2">if</strong></span> done:
        env.reset()</pre></div><p class="calibre8">Gym also <a id="id407" class="calibre1"/>provides sophisticated monitoring methods, to record videos and algorithm performance. The records can be uploaded to Open-AI API for scoring with other algorithms.</p><p class="calibre8">One might also look at:</p><div class="book"><ul class="itemizedlist"><li class="listitem">3D car <a id="id408" class="calibre1"/>racing simulator Torcs (<a class="calibre1" href="http://torcs.sourceforge.net/">http://torcs.sourceforge.net/</a>), which is more realistic with smaller discretization of actions, but with less sparse rewards than simple Atari games, and also fewer possible actions than continuous motor control in MuJoCo:<div class="mediaobject"><img src="../images/00206.jpeg" alt="Simulation environments" class="calibre9"/></div><p class="calibre27"> </p></li><li class="listitem">3D environment called Labyrinth for randomly generated mazes:<div class="mediaobject"><img src="../images/00207.jpeg" alt="Simulation environments" class="calibre9"/></div><p class="calibre27"> </p></li></ul></div></div>
<div class="book" title="Q-learning"><div class="book" id="352RK2-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec98" class="calibre1"/>Q-learning</h1></div></div></div><p class="calibre8">A major <a id="id409" class="calibre1"/>approach to solve games has been the Q-learning approach. In order to fully understand the approach, a basic example will illustrate a simplistic case where the number of states of the environment is limited to 6, state <span class="strong"><strong class="calibre2">0</strong></span> is the entrance, state <span class="strong"><strong class="calibre2">5</strong></span> is the exit. At each stage, some actions make it possible to jump to another state, as described in the following figure:</p><div class="mediaobject"><img src="../images/00208.jpeg" alt="Q-learning" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">The reward is, let's say, 100, when the agent leaves state <span class="strong"><strong class="calibre2">4</strong></span> to state <span class="strong"><strong class="calibre2">5</strong></span>. There isn't any other reward for other states since the goal of the game in this example is to find the exit. The <a id="id410" class="calibre1"/>reward is time-delayed and the agent has to scroll through multiple states from state 0 to state 4 to find the exit.</p><p class="calibre8">In this case, Q-learning consists of learning a matrix Q, representing the <span class="strong"><strong class="calibre2">value of a state-action pair</strong></span>:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Each row in the Q-matrix corresponds to a state the agent would be in</li><li class="listitem">Each column the target state from that state</li></ul></div><p class="calibre8">the value representing how much choosing that action in that state will move us close to the exit. If there isn't any action from state <span class="strong"><em class="calibre12">i</em></span> leading to state <span class="strong"><em class="calibre12">j</em></span>, we define a zero or negative value at position <span class="strong"><em class="calibre12">(i,j)</em></span> in the Q-matrix. If there are one or more possible actions from state <span class="strong"><em class="calibre12">i</em></span> to state <span class="strong"><em class="calibre12">j</em></span>, then the value in the Q-matrix will be chosen to represent how state <span class="strong"><em class="calibre12">j</em></span> will help us to achieve our goal.</p><p class="calibre8">For example, leaving state <span class="strong"><strong class="calibre2">3</strong></span> for state <span class="strong"><strong class="calibre2">0</strong></span>, will move the agent away from the exit, while leaving state <span class="strong"><strong class="calibre2">3</strong></span> for state <span class="strong"><strong class="calibre2">4</strong></span> gets us closer to the goal. A commonly used algorithm, known as a <span class="strong"><em class="calibre12">greedy</em></span> algorithm, to estimate <span class="strong"><strong class="calibre2">Q</strong></span> in the discrete space, is given by the recursive <span class="strong"><em class="calibre12">Bellman equation</em></span> which is demonstrated to converge:</p><div class="mediaobject"><img src="../images/00209.jpeg" alt="Q-learning" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Here, <span class="strong"><em class="calibre12">S'</em></span> is the new state when taking action <span class="strong"><em class="calibre12">a</em></span> on state <span class="strong"><em class="calibre12">S</em></span>; <span class="strong"><em class="calibre12">r</em></span> defines the reward on the path from state <span class="strong"><em class="calibre12">S</em></span> to <span class="strong"><em class="calibre12">S'</em></span> (in this case it is null) and <span class="strong"><img src="../images/00210.jpeg" alt="Q-learning" class="calibre23"/></span> is the discounting factor to discourage actions to states too far in the graph. The application of this equation multiple times will result <a id="id411" class="calibre1"/>in the following Q values:</p><div class="mediaobject"><img src="../images/00211.jpeg" alt="Q-learning" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">In Q-learning, <span class="strong"><em class="calibre12">Q</em></span> stands for <span class="strong"><em class="calibre12">quality</em></span> representing the power of the action to get the best rewards. Since late rewards are discounted, the values correspond to <span class="strong"><strong class="calibre2">maximum discounted future rewards</strong></span> for each (state, action) couple.</p><p class="calibre8">Note <a id="id412" class="calibre1"/>that the full graph outcome is not required as soon as we know the <span class="strong"><strong class="calibre2">state values</strong></span> for the output nodes of the search subtree:</p><div class="mediaobject"><img src="../images/00212.jpeg" alt="Q-learning" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">In this figure, the value <span class="strong"><strong class="calibre2">10</strong></span> for nodes <span class="strong"><strong class="calibre2">1</strong></span> and <span class="strong"><strong class="calibre2">3</strong></span> are the <span class="strong"><strong class="calibre2">optimal state value function v(s);</strong></span> that is, the <a id="id413" class="calibre1"/>outcome of a game under perfect play / optimal path. In practice, the exact value function is not known but approximated.</p><p class="calibre8">Such an approximation is used in combination with a <span class="strong"><strong class="calibre2">Monte Carlo Tree Search</strong></span> (<span class="strong"><strong class="calibre2">MCTS</strong></span>) in the <span class="strong"><strong class="calibre2">DeepMind </strong></span>algorithm <span class="strong"><strong class="calibre2">AlphaGo</strong></span> to beat the world champion in Go. MCTS consists <a id="id414" class="calibre1"/>of sampling actions given a policy, so that <a id="id415" class="calibre1"/>only the most likely actions from the current node to <a id="id416" class="calibre1"/>estimate <a id="id417" class="calibre1"/>its Q-value are retained in the Bellman equation:</p><div class="mediaobject"><img src="../images/00213.jpeg" alt="Q-learning" class="calibre9"/></div><p class="calibre10"> </p></div>
<div class="book" title="Deep Q-network" id="361C61-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec99" class="calibre1"/>Deep Q-network</h1></div></div></div><p class="calibre8">While the <a id="id418" class="calibre1"/>number of possible actions is usually limited (number of keyboard keys or movements), the number of possible states can be dramatically huge, the search space can be enormous, for example, in the case of a robot equipped with cameras in a real-world environment or a realistic video game. It becomes natural to use a computer vision neural net, such as the ones we used for classification in <a class="calibre1" title="Chapter 7. Classifying Images with Residual Networks" href="part0075_split_000.html#27GQ61-ccdadb29edc54339afcb9bdf9350ba6b">Chapter 7</a>, <span class="strong"><em class="calibre12">Classifying Images with Residual Networks</em></span>, to represent the value of an action given an input image (the state), instead of a matrix:</p><div class="mediaobject"><img src="../images/00214.jpeg" alt="Deep Q-network" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">The Q-network is <a id="id419" class="calibre1"/>called a <span class="strong"><strong class="calibre2">state-action value network</strong></span> and predicts action values given a state. To train the Q-network, one natural way of <a id="id420" class="calibre1"/>doing it is to have it fit the Bellman equation via gradient descent:</p><div class="mediaobject"><img src="../images/00215.jpeg" alt="Deep Q-network" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Note that, <span class="strong"><img src="../images/00216.jpeg" alt="Deep Q-network" class="calibre23"/></span> is evaluated and fixed, while the descent is computed for the derivatives in, <span class="strong"><img src="../images/00217.jpeg" alt="Deep Q-network" class="calibre23"/></span> and that the value of each state can be estimated as the maximum of all state-action values.</p><p class="calibre8">After initializing the Q-network with random weights, the initial predictions are random, but as the network converges, the action given a particular state will become more and more predictable, so the exploration of new states drops. Exploiting a model trained online requires the forcing of the algorithm to <span class="strong"><strong class="calibre2">continue to explore</strong></span>: the <span class="strong"><img src="../images/00218.jpeg" alt="Deep Q-network" class="calibre23"/></span> <span class="strong"><strong class="calibre2">greedy approach</strong></span> consists <a id="id421" class="calibre1"/>of doing a random action with a probability epsilon, otherwise following the maximum-reward action given by the Q-network. It is a kind <a id="id422" class="calibre1"/>of learning by trial-and-error. After a certain number of epochs, <span class="strong"><img src="../images/00218.jpeg" alt="Deep Q-network" class="calibre23"/></span> is decayed to reduce exploration.</p></div>
<div class="book" title="Training stability"><div class="book" id="36VSO2-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec100" class="calibre1"/>Training stability</h1></div></div></div><p class="calibre8">Different <a id="id423" class="calibre1"/>methods are possible to improve stability during training. <span class="strong"><strong class="calibre2">Online training</strong></span>, that is, training the model while playing the game, forgetting previous <a id="id424" class="calibre1"/>experiences, just considering the last one, is fundamentally unstable with deep neural networks: states that are close in time, such as the most recent states, are usually strongly similar or correlated, and taking the most recent states during training does not converge well.</p><p class="calibre8">To avoid <a id="id425" class="calibre1"/>such a failure, one possible solution has been to store the experiences in a <span class="strong"><strong class="calibre2">replay memory</strong></span> or to use a database of human gameplays. Batching and shuffling random samples from the replay memory or the human gameplay database leads <a id="id426" class="calibre1"/>to more stable training, but <span class="strong"><strong class="calibre2">off-policy</strong></span> training.</p><p class="calibre8">A second solution to improve stability is to fix the value of the parameter <span class="strong"><img src="../images/00219.jpeg" alt="Training stability" class="calibre23"/></span> in the <span class="strong"><strong class="calibre2">target evaluation</strong></span> <span class="strong"><img src="../images/00216.jpeg" alt="Training stability" class="calibre23"/></span> for several thousands of updates of <span class="strong"><img src="../images/00217.jpeg" alt="Training stability" class="calibre23"/></span>, reducing the correlations between the target and the Q-values:</p><div class="mediaobject"><img src="../images/00220.jpeg" alt="Training stability" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">It is possible to train more efficiently with n-steps Q-learning, propagating the rewards on <span class="strong"><em class="calibre12">n</em></span> preceding actions instead of one:</p><p class="calibre8">Q learning formula:</p><div class="mediaobject"><img src="../images/00221.jpeg" alt="Training stability" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">n-steps Q-learning formula:</p><div class="mediaobject"><img src="../images/00222.jpeg" alt="Training stability" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Here, each <a id="id427" class="calibre1"/>step will benefit from <span class="strong"><em class="calibre12">n</em></span> next rewards:</p><div class="mediaobject"><img src="../images/00223.jpeg" alt="Training stability" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">A last <a id="id428" class="calibre1"/>solution for training stability and efficiency is an <span class="strong"><strong class="calibre2">asynchronous gradient descent</strong></span> with multiple agents executing in parallel, on multiple instances of the environment, with different exploration policies, so that each gradient update is less correlated: each learning agent runs in a different thread on the same machine, sharing its model and target model parameters with other agents, but computing the gradients for a different part of the environment. The parallel actor learners have a stabilization effect, enable on-policy reinforcement, a reduction in training time, and comparable performances on GPU or multi-core CPU, which is great!</p><p class="calibre8">The stabilization effect leads to better <span class="strong"><strong class="calibre2">data efficiency</strong></span>: the data efficiency is measured by the number of epochs (an epoch is when the full training dataset has been presented to the algorithm) required to converge to a desired training loss or accuracy. Total training time is impacted by data efficiency, parallelism (number of threads or machines), and the parallelism overhead (it is sublinear in the number of threads, given the number of cores, machines and algorithm distribution efficiency).</p><p class="calibre8">Let's see <a id="id429" class="calibre1"/>it in practice. To implement multiple agents exploring different parts of the environment, we'll run multiple processes with the Python multiprocessing module, one process for the model to update (GPU), and <span class="strong"><em class="calibre12">n</em></span> processes for the agents exploring (CPU). The manager object of the multiprocessing module controls a server process holding the weights of the Q-network to share between processes. The communication channel to store the experiences of the agents and serve them once for the model update, is implemented with a process-safe queue:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">from</strong></span> multiprocessing <span class="strong"><strong class="calibre2">import</strong></span> *
manager = Manager()
weight_dict = manager.dict()
mem_queue = manager.Queue(args.queue_size)

pool = Pool(args.processes + 1, init_worker)

<span class="strong"><strong class="calibre2">for</strong></span> i <span class="strong"><strong class="calibre2">in</strong></span> range(args.processes):
    pool.apply_async(generate_experience_proc, (mem_queue, weight_dict, i))

pool.apply_async(learn_proc, (mem_queue, weight_dict))

pool.close()
pool.join()</pre></div><p class="calibre8">Now, let's generate experiences and enqueue them in the common queue object.</p><p class="calibre8">For that purpose, where each agent creates its game environment, compile the Q-network and load the weights from the manager:</p><div class="informalexample"><pre class="programlisting">env = gym.make(args.game)

load_net = build_networks(observation_shape, env.action_space.n)

load_net.compile(<span class="strong"><strong class="calibre2">optimizer</strong></span>='rmsprop', <span class="strong"><strong class="calibre2">loss</strong></span>='mse', <span class="strong"><strong class="calibre2">loss_weights</strong></span>=[0.5, 1.])

<span class="strong"><strong class="calibre2">while</strong></span> 'weights' <span class="strong"><strong class="calibre2">not in</strong></span> weight_dict:
    time.sleep(0.1)
load_net.set_weights(weight_dict['weights'])</pre></div><p class="calibre8">To generate one experience, the agent chooses an action and executes it in its environment:</p><div class="informalexample"><pre class="programlisting">observation, reward, done, _ = env.step(action)</pre></div><p class="calibre8">Each <a id="id430" class="calibre1"/>experience by the agent is stored in a list until the game is terminated or the list is longer than <span class="strong"><em class="calibre12">n_step</em></span>, to evaluate the state-action value with <span class="strong"><em class="calibre12">n-steps</em></span> Q-learning :</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">if</strong></span> done or counter <span class="strong"><strong class="calibre2">&gt;=</strong></span> args.n_step:
    r = 0.
    <span class="strong"><strong class="calibre2">if not</strong></span> done:
        r = value_net.predict(observations[None, ...])[0]
    <span class="strong"><strong class="calibre2">for</strong></span> i <span class="strong"><strong class="calibre2">in</strong></span> range(counter):
        r = n_step_rewards[i] + discount * r
        mem_queue.put((n_step_observations[i], n_step_actions[i], r))</pre></div><p class="calibre8">Once in a while, the agent updates its weights from the learning process:</p><div class="informalexample"><pre class="programlisting">load_net.set_weights(weight_dict['weights'])</pre></div><p class="calibre8">Let's see now how to update the weights in the learning agent.</p></div>
<div class="book" title="Policy gradients with REINFORCE algorithms"><div class="book" id="37UDA2-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec101" class="calibre1"/>Policy gradients with REINFORCE algorithms</h1></div></div></div><p class="calibre8">The idea <a id="id431" class="calibre1"/>of <span class="strong"><strong class="calibre2">Policy Gradients</strong></span> (<span class="strong"><strong class="calibre2">PG</strong></span>) / REINFORCE algorithms is very simple: it consists in re-using the classification loss function in the case <a id="id432" class="calibre1"/>of reinforcement learning tasks.</p><p class="calibre8">Let's remember that the classification loss is given by the negative log likelihood, and minimizing it with a gradient descent follows the negative log-likelihood derivative with respect to the network weights:</p><div class="mediaobject"><img src="../images/00224.jpeg" alt="Policy gradients with REINFORCE algorithms" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">Here, <span class="strong"><em class="calibre12">y</em></span> is the select action, <span class="strong"><img src="../images/00225.jpeg" alt="Policy gradients with REINFORCE algorithms" class="calibre23"/></span> the predicted probability of this action given inputs X and weights <span class="strong"><img src="../images/00219.jpeg" alt="Policy gradients with REINFORCE algorithms" class="calibre23"/></span>.</p><p class="calibre8">The REINFORCE theorem introduces the equivalent for reinforcement learning, where <span class="strong"><em class="calibre12">r</em></span> is the reward. The following derivative:</p><div class="mediaobject"><img src="../images/00226.jpeg" alt="Policy gradients with REINFORCE algorithms" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">represents an unbiased estimate of the derivative of the expected reward with respect to the <a id="id433" class="calibre1"/>network weights:</p><div class="mediaobject"><img src="../images/00227.jpeg" alt="Policy gradients with REINFORCE algorithms" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">So, following the derivative will encourage the agent to maximize the reward.</p><p class="calibre8">Such a gradient descent enables us to optimize a <span class="strong"><strong class="calibre2">policy network</strong></span> for our agents: a policy <span class="strong"><img src="../images/00228.jpeg" alt="Policy gradients with REINFORCE algorithms" class="calibre23"/></span> is a probability distribution over legal actions, to sample actions to execute during <a id="id434" class="calibre1"/>online learning, and can be approximated with a parameterized neural net as well.</p><p class="calibre8">It is particularly useful in the continuous case, for example for motor control, where discretization of the action space might lead to some suboptimal artifacts and the maximization over an action-value network Q is not possible under infinite action space.</p><p class="calibre8">Moreover, it is possible to enhance the policy network with recurrency (LSTM, GRU,) so that the agent selects its actions with respect to multiple previous states.</p><p class="calibre8">The REINFORCE theorem gives us a gradient descent to optimize the parametrized policy network. To encourage exploration in this policy-based case, it is also possible to add a regularization term, the entropy of the policy, to the loss function.</p><p class="calibre8">Under this policy, it is possible to compute the value of every state <span class="strong"><img src="../images/00229.jpeg" alt="Policy gradients with REINFORCE algorithms" class="calibre23"/></span>:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Either by playing the game from that state with the policy</li><li class="listitem">Or, if parameterized into a <span class="strong"><strong class="calibre2">state value network</strong></span>, by gradient descent, the current <a id="id435" class="calibre1"/>parameter serving as target, as for the state-action value network seen in the previous section with discounted rewards:<div class="mediaobject"><img src="../images/00230.jpeg" alt="Policy gradients with REINFORCE algorithms" class="calibre9"/></div><p class="calibre27"> </p></li></ul></div><p class="calibre8">This <a id="id436" class="calibre1"/>value is usually chosen as reinforcement baseline <span class="strong"><em class="calibre12">b</em></span> to reduce the variance of the estimate of the policy gradient, and the Q-value can be used as the expected reward:</p><div class="mediaobject"><img src="../images/00231.jpeg" alt="Policy gradients with REINFORCE algorithms" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">The first factor in the REINFORCE derivative:</p><div class="mediaobject"><img src="../images/00232.jpeg" alt="Policy gradients with REINFORCE algorithms" class="calibre9"/></div><p class="calibre10"> </p><p class="calibre8">is called the <span class="strong"><strong class="calibre2">advantage of action a in state </strong></span>
<span class="strong"><em class="calibre12">s</em></span>.</p><p class="calibre8">Both gradient descents, for the policy network and for the value network, can be performed asynchronously with our parallel actor learners.</p><p class="calibre8">Let's create our policy network and state value network, sharing their first layers, in Keras:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">from</strong></span> keras.models <span class="strong"><strong class="calibre2">import</strong></span> Model
<span class="strong"><strong class="calibre2">from</strong></span> keras.layers <span class="strong"><strong class="calibre2">import</strong></span> Input, Conv2D, Flatten, Dense

<span class="strong"><strong class="calibre2">def </strong></span>build_networks(input_shape, output_shape):
    state = Input(<span class="strong"><strong class="calibre2">shape</strong></span>=input_shape)
    h = Conv2D(16, (8, 8) , <span class="strong"><strong class="calibre2">strides</strong></span>=(4, 4), <span class="strong"><strong class="calibre2">activation</strong></span>='relu', <span class="strong"><strong class="calibre2">data_format</strong></span>="channels_first")(state)
    h = Conv2D(32, (4, 4) , <span class="strong"><strong class="calibre2">strides</strong></span>=(2, 2), <span class="strong"><strong class="calibre2">activation</strong></span>='relu', <span class="strong"><strong class="calibre2">data_format</strong></span>="channels_first")(h)
    h = Flatten()(h)
    h = Dense(256, <span class="strong"><strong class="calibre2">activation</strong></span>='relu')(h)

    value = Dense(1, <span class="strong"><strong class="calibre2">activation</strong></span>='linear', <span class="strong"><strong class="calibre2">name</strong></span>='value')(h)
    policy = Dense(output_shape, <span class="strong"><strong class="calibre2">activation</strong></span>='softmax', <span class="strong"><strong class="calibre2">name</strong></span>='policy')(h)

    value_network = Model(<span class="strong"><strong class="calibre2">inputs</strong></span>=state, <span class="strong"><strong class="calibre2">outputs</strong></span>=value)
    policy_network = Model(<span class="strong"><strong class="calibre2">inputs</strong></span>=state, <span class="strong"><strong class="calibre2">outputs</strong></span>=policy)
    train_network = Model(<span class="strong"><strong class="calibre2">inputs</strong></span>=state, <span class="strong"><strong class="calibre2">outputs</strong></span>=[value, policy])

    <span class="strong"><strong class="calibre2">return</strong></span> value_network, policy_network, train_network</pre></div><p class="calibre8">Our <a id="id437" class="calibre1"/>learning process builds the model as well, shares the weights to other processes, and compiles them for training with their respective losses:</p><div class="informalexample"><pre class="programlisting">_, _, train_network = build_networks(observation_shape, env.action_space.n)
weight_dict['weights'] = train_net.get_weights()
	
<span class="strong"><strong class="calibre2">from</strong></span> keras <span class="strong"><strong class="calibre2">import</strong></span> backend <span class="strong"><strong class="calibre2">as</strong></span> K

<span class="strong"><strong class="calibre2">def</strong></span> policy_loss(<span class="strong"><strong class="calibre2">advantage</strong></span>=0., <span class="strong"><strong class="calibre2">beta</strong></span>=0.01):
    <span class="strong"><strong class="calibre2">def</strong></span> loss(y_true, y_pred):
        <span class="strong"><strong class="calibre2">return </strong></span>-K.sum(K.log(K.sum(y_true * y_pred, axis=-1) + \<span class="strong"><strong class="calibre2">
</strong></span>K.epsilon()) * K.flatten(advantage)) + \
           	beta * K.sum(y_pred * K.log(y_pred + K.epsilon()))
    <span class="strong"><strong class="calibre2">return</strong></span> loss

<span class="strong"><strong class="calibre2">def </strong></span>value_loss():
    <span class="strong"><strong class="calibre2">def</strong></span> loss(y_true, y_pred):
        <span class="strong"><strong class="calibre2">return</strong></span> 0.5 * K.sum(K.square(y_true - y_pred))
    <span class="strong"><strong class="calibre2">return</strong></span> loss

train_net.compile(<span class="strong"><strong class="calibre2">optimizer</strong></span>=RMSprop(<span class="strong"><strong class="calibre2">epsilon</strong></span>=0.1, <span class="strong"><strong class="calibre2">rho</strong></span>=0.99),
            <span class="strong"><strong class="calibre2">loss</strong></span>=[value_loss(), policy_loss(advantage, args.beta)])</pre></div><p class="calibre8">The policy loss is a REINFORCE loss plus an entropy loss to encourage exploration. The value loss is a simple mean square error loss.</p><p class="calibre8">De-queueing the experiences into a batch, our learning process trains the model on the batch and updates the weights dictionary:</p><div class="informalexample"><pre class="programlisting">loss = train_net.train_on_batch([last_obs, advantage], [rewards, targets])</pre></div><p class="calibre8">To run the full code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">pip</strong></span> install -r requirements.txt

<span class="strong"><strong class="calibre2">python</strong></span> 1-train.py --game=Breakout-v0 --processes=16
<span class="strong"><strong class="calibre2">python</strong></span> 2-play.py --game=Breakout-v0 --model=model-Breakout-v0-35750000.h5</pre></div><p class="calibre8">Learning <a id="id438" class="calibre1"/>took about 24 hours.</p><p class="calibre8">A policy-based advantage actor critic usually outperforms value-based methods.</p></div>
<div class="book" title="Related articles" id="38STS1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec102" class="calibre1"/>Related articles</h1></div></div></div><p class="calibre8">You can refer to the following articles:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><em class="calibre12">Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning</em></span>, Ronald J. Williams, 1992</li><li class="listitem"><span class="strong"><em class="calibre12">Policy Gradient Methods for Reinforcement Learning with Function Approximation</em></span>, Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour, 1999</li><li class="listitem"><span class="strong"><em class="calibre12">Playing Atari with Deep Reinforcement Learning</em></span>, Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller, 2013</li><li class="listitem"><span class="strong"><em class="calibre12">Mastering the Game of Go with Deep Neural Networks and Tree Search</em></span>, David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel &amp; Demis Hassabis, 2016</li><li class="listitem"><span class="strong"><em class="calibre12">Asynchronous Methods for Deep Reinforcement Learning</em></span>, Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Tim Harley, Timothy P. LilliCrap, David Silver, Koray Kavukcuoglu, Feb 2016</li><li class="listitem"><span class="strong"><em class="calibre12">Deep Reinforcement Learning Radio Control and Signal Detection with KeRLym</em></span>, a Gym RL Agent Timothy J. O'Shea and T. Charles Clancy, 2016</li></ul></div></div>
<div class="book" title="Summary" id="39REE1-ccdadb29edc54339afcb9bdf9350ba6b"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch11lvl1sec103" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">Reinforcement learning describes the tasks of optimizing an agent stumbling into rewards episodically. Online, offline, value-based, or policy-based algorithms have been developed with the help of deep neural networks for various games and simulation environments.</p><p class="calibre8">Policy-gradients are a brute-force solution that require the sampling of actions during training and are better suited for small action spaces, although they provide first solutions for continuous search spaces.</p><p class="calibre8">Policy-gradients also work to train non-differentiable stochastic layers in a neural net and back propagate gradients through them. For example, when propagation through a model requires to sample following a parameterized submodel, gradients from the top layer can be considered as a reward for the bottom network.</p><p class="calibre8">In more complex environments, when there is no obvious reward (for example understanding and inferring possible actions from the objects present in the environment), reasoning helps humans optimize their actions, for which research does not provide any solution currently. Current RL algorithms are particularly suited for precise plays, fast reflexes, but no long term planning and reasoning. Also, RL algorithms require heavy datasets, which simulation environments provide easily. But this opens up the question of scaling in the real world.</p><p class="calibre8">In the next chapter, we'll explore the latest solutions to generate new data undistinguishable from real-world data.</p></div></body></html>