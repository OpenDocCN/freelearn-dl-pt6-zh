- en: Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to ingest and transform data to train
    or evaluate a model using a batch ETL approach. You would use this approach in
    the training or evaluation phases in most cases, but when running a model, streaming
    ingestion is needed. This chapter covers setting up streaming ingestion strategies
    for DL models using a combination of the Apache Spark, DL4J, DataVec, and Apache
    Kafka frameworks. Streaming data ingestion frameworks don't simply move data from
    source to destination such as in the traditional ETL approach. With streaming
    ingestion, any incoming data in any format can be simultaneously ingested, transformed,
    and/or enriched with other structured and previously stored data for DL purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Streaming data with Apache Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming data with Kafka and Apache Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming data with DL4J and Apache Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming data with Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](ad6da519-0705-4db6-aa38-2b98b85892cc.xhtml), *The Apache Spark
    Ecosystem*, the details about Spark Streaming and DStreams were covered. A new
    and different implementation of streaming, Structured Streaming, was introduced
    as an alpha release in Apache Spark 2.0.0\. It finally became stable starting
    from Spark 2.2.0.
  prefs: []
  type: TYPE_NORMAL
- en: Structured Streaming (which has been built on top of the Spark SQL engine) is
    a fault-tolerant, scalable stream-processing engine. Streaming can be done in
    the same way batch computation is done, that is, on static data, which we presented
    in [Chapter 1](ad6da519-0705-4db6-aa38-2b98b85892cc.xhtml), *The Apache Spark
    Ecosystem*. It is the Spark SQL engine that's responsible for incrementally and
    continuously running the computation and for finally updating the results as data
    continues to stream. In this scenario, end-to-end, exactly-once, and fault-tolerance
    guarantees are ensured through **Write Ahead Logs** (**WAL**) and check-pointing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The difference between the traditional Spark Streaming and the Structured Streaming
    programming models is sometimes not easy to grasp, especially so for experienced
    Spark developers who are approaching this concept for the first time. The best
    way to describe it is like this: you can think of it as a way of handling a live
    data stream as a table (where the table is thought of as an RDBMS) that is being
    continuously appended. The streaming computation is expressed as a standard batch-like
    query (in the same way it happens on a static table), but Spark runs it incrementally
    on the unbounded table.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how it works. The input data stream can be considered the input table.
    Every data item arriving in the stream is like a new row being appended to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d9851de-66a9-49c9-8371-c0c8fd522cda.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: A data stream as an unbounded table'
  prefs: []
  type: TYPE_NORMAL
- en: 'A query on the input generates the result table. With every trigger, new interval
    rows are appended to the input table, which then update the result table (as shown
    in the following diagram). Any time the result table gets updated, the changed
    result rows can be written to an external sink. There are different modes for
    the output that is written to external storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Complete mode**: In this mode, it is the entire updated result table being
    written to the external storage. How writing to the storage system of the entire
    table happens depends on the specific connector configuration or implementation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Append mode**: Only the new rows that are appended in the result table will
    be written to the external storage system. This means that it is possible to apply
    this mode in situations where the existing rows in the result table aren''t expected
    to change.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Update mode**: Only the rows that have been updated in the result table are
    written to the external storage system. The difference between this mode and the
    complete mode is that this one sends out only those rows that have changed since
    the last trigger:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/973dd2e8-1e6e-4759-b3d3-f7b574106ee2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Programming model for Structured Streaming'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s implement a simple Scala example – a streaming word count self-contained
    application, which is the same use case that we used in [Chapter 1](ad6da519-0705-4db6-aa38-2b98b85892cc.xhtml),
    *The Apache Spark Ecosystem*, but for Structured Streaming instead. The code that''s
    used for this class can be found among the examples that are bundled with a Spark
    distribution. The first thing we need to do is initialize a `SparkSession`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We must then create a DataFrame representing the stream of input lines from
    the connection to `host:port`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `lines` DataFrame represents the unbounded table. It contains the streaming
    text data. The content of that table is a value, that is, a single column of strings.
    Each incoming line in the streaming text becomes a row.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s split the lines into words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to count the words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can start running the query that prints the running counts to the
    console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We continue running until a termination signal is received:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Before running this example, first, you need to run netcat as a data server
    (or the data server that we implemented in Scala in [Chapter 1](ad6da519-0705-4db6-aa38-2b98b85892cc.xhtml),
    *The Apache Spark Ecosystem*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, in a different Terminal, you can start the example by passing the following
    as arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Any line typed in the Terminal when running the netcat server will be counted
    and printed to the application screen. An output such as the following will occur:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The event time is defined as the time that's embedded in the data itself. In
    many applications, such as those in an IoT context, when the number of events
    generated by devices every minute needs to be retrieved, the time the data was
    generated has to be used rather than the time Spark receives it. Event-time is
    naturally expressed in this programming model—each event from the device is a
    row in the table, and event-time is a column value in that row. This paradigm
    makes window-based aggregations simply a special type of aggregation on that event-time
    column. This grants consistency, because event-time and window-based aggregation
    queries can be defined in the same way on both static datasets (for example, events
    logs from devices) and streaming data.
  prefs: []
  type: TYPE_NORMAL
- en: Following the previous consideration, it is evident that this programming model
    naturally handles data that has arrived later than expected based on its event-time.
    Since it is Spark itself that updates the result table, it has full control over
    updating old aggregates when there is late data, as well as limiting the size
    of intermediate data by cleaning up old aggregates. Starting from Spark 2.1, there
    is also support for watermarking, which allows you to specify the threshold of
    late data and allows the underlying engine to clean up old states accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming data with Kafka and Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark Streaming with Kafka is a common combination of technologies in data pipelines.
    This section will present some examples of streaming Kafka with Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Kakfa
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Kafka ([http://kafka.apache.org/](http://kafka.apache.org/)) is an open
    source message broker written in Scala. Originally, it was developed by LinkedIn,
    but it was then released as open source in 2011 and is currently maintained by
    the Apache Software Foundation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the reasons why you might prefer Kafka to a traditional JMS
    message broker:'
  prefs: []
  type: TYPE_NORMAL
- en: '**It''s fast**: A single Kafka broker running on commodity hardware can handle
    hundreds of megabytes of reads and writes per second from thousands of clients'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Great scalability**: It can be easily and transparently expanded without
    downtime'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Durability and replication**: Messages are persisted on disk and replicated
    within the cluster to prevent data loss (by setting a proper configuration using
    the high number of available configuration parameters, you could achieve zero
    data loss)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance**: Each broker can handle terabytes of messages without performance
    impact'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It allows real-time stream processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be easily integrated with other popular open source systems for big data
    architectures such as Hadoop, Spark, and Storm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are the core concepts of Kafka that you should become familiar
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Topics**: These are categories or feed names to which upcoming messages are
    published'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Producers**: Any entity that publishes messages to a topic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consumers**: Any entity that subscribes to topics and consumes messages from
    them'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Brokers**: Services that handle read and write operations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows a typical Kafka cluster architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de224ee0-9bff-44c9-a960-f34697e9e593.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Kafka architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Kafka uses ZooKeeper ([https://zookeeper.apache.org/](https://zookeeper.apache.org/))
    behind the scenes to keep its nodes in sync. The Kafka binaries provide it, so
    if hosting machines don't have ZooKeeper on board, you can use the one that comes
    bundled with Kafka. The communication between clients and servers happens using
    a highly performant and language-agnostic TCP protocol.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typical use cases for Kafka are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Messaging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stream processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log aggregation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web activity tracking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Event sourcing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark Streaming and Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To use Spark Streaming with Kafka, you can do two things: either use a receiver
    or be direct. The first option is similar to streaming from other sources such
    as text files and sockets – data received from Kafka is stored in Spark executors
    and processed by jobs that are launched by a Spark Streaming context. This is
    not the best approach – it can cause data loss in the event of failures. This
    means that the direct approach (introduced in Spark 1.3) is better. Instead of
    using receivers to receive data, it periodically queries Kafka for the latest
    offsets in each topic and partitions, and accordingly defines, the offset ranges
    to process for each batch. When the jobs to process the data are executed, Kafka''s
    simple consumer API is used to read the defined ranges of offsets (almost in the
    same way as for reading files from a filesystem). The direct approach brings the
    following advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simplified parallelism**: There''s no need to create multiple input Kafka
    streams and then struggle trying to unify them. Spark Streaming creates as many
    RDD partitions as there are Kafka partitions to consume, which read data from
    Kafka in parallel. This means that there is 1:1 mapping between Kafka and RDD
    partitions that is easier to understand and tune.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improved efficiency**: Following the receiver approach, to achieve zero-data
    loss, we need the data to be stored in a WAL. However, this strategy is inefficient,
    as the data effectively gets replicated twice, by Kafka first and then by the
    WAL. In the direct approach, there is no receiver, and subsequently no need for
    WALs—messages can be recovered from Kafka, assuming there is sufficient Kafka
    retention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exactly-once semantics**: The receiver approach uses Kafka''s high-level
    API to store consumed offsets in ZooKeeper. While this approach (combined with
    WALs) can ensure zero data loss, there is a remote possibility that some records
    will get consumed twice when a failure happens. Inconsistencies between data being
    reliably received by Spark Streaming and offsets tracked by ZooKeeper lead to
    this. With the direct approach, the simple Kafka API involved doesn''t use ZooKeeper—the
    offsets are tracked by Spark Streaming itself within its checkpoints. This ensures
    that each record is received by Spark Streaming effectively exactly once, even
    when a failure happens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One disadvantage of the direct approach is that it doesn't update the offsets
    in ZooKeeper—this means that the ZooKeeper-based Kafka monitoring tools will not
    show any progress.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s implement a simple Scala example – a Kafka direct word count. The
    example that''s presented in this section works with Kafka release 0.10.0.0 or
    later. The first thing to do is to add the required dependencies (Spark Core,
    Spark Streaming, and Spark Streaming Kafka) to your project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This application expects two arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: A comma-separated list of one or more Kafka brokers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A comma-separated list of one or more Kafka topics to consume from:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to create the Spark Streaming context. Let''s choose a `5`-second batch
    interval:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create a direct Kafka stream with the given brokers and topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can implement the word count now, that is, get the lines from the stream,
    split them into words, count the words, and then print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s start the computation and keep it alive, waiting for a termination
    signal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: To run this example, we first need to start a Kafka cluster and create a topic.
    The Kafka binaries can be downloaded from the official website ([http://kafka.apache.org/downloads](http://kafka.apache.org/downloads)).
    Once it has been downloaded, we can follow the following instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start a `zookeeper` node first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: It will start listening to the default port, `2181`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, start a Kafka broker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: It will start listening to the default port, `9092`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a topic called `packttopic`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Check that the topic has been successfully created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The topic name, `packttopic`, should be in the list that was printed to the
    console output.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now start to produce messages for the new topic. Let''s start a command-line
    producer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can write some messages to the producer console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Let's build the Spark application and execute it through the `$SPARK_HOME/bin/spark-submit`
    command, specifying the JAR filename, the Spark master URL, the job name, the
    main class name, the maximum memory to be used by each executor, and the job arguments
    (`localhost:9092` and `packttopic`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The output printed by the Spark job for each consumed message line will be
    something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Streaming data with DL4J and Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to apply data streaming with Kafka and Spark to
    a use case scenario of a DL4J application. The DL4J module we are going to use
    is DataVec.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider the example that we presented in the *Spark Streaming and Kafka* section.
    What we want to achieve is direct Kafka streaming with Spark, then apply DataVec
    transformations on the incoming data as soon as it arrives, before using it downstream.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define the input schema first. This is the schema we expect for the
    messages that are consumed from a Kafka topic. The schema structure is the same
    as for the classic `Iris` dataset ([https://en.wikipedia.org/wiki/Iris_flower_data_set](https://en.wikipedia.org/wiki/Iris_flower_data_set)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define a transformation on it (we are going to remove the petal fields
    because we are going to do some analysis based on the sepal features only):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can generate the new schema (after applying the transformation to the
    data):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The next part of this Scala application is exactly the same as for the example
    in the *Spark Streaming and Kafka* section. Here, create a streaming context with
    a `5`-second batch interval and a direct Kafka stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s get the input lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '`lines` is a `DStream[String]`. We need to iterate for each RDD there, convert
    it to `javaRdd` (required by the DataVec reader), use a DataVec `CSVRecordReader`,
    parse the incoming comma-separated messages, apply the schema transformation,
    and print the result data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we start the streaming context and keep it alive, waiting for a termination
    signal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'To run this example, we need to start a Kafka cluster and create a new topic
    called `csvtopic`. The steps are the same as for the example described in the *Spark
    Streaming and Kafka* section. Once the topic has been created, we can start to
    produce comma-separated messages on it. Let''s start a command-line producer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can write some messages to the producer console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Let's build the Spark application and execute it through the `$SPARK_HOME/bin/spark-submit`
    command, specifying the JAR filename, the Spark master URL, the job name, the
    main class name, the maximum memory to be used by each executor, and the job arguments
    (`localhost:9092` and `csvtopic`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The output printed by the Spark job for each consumed message line will be
    something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The full code for this example can be found among the source code that's bundled
    with this book at [https://github.com/PacktPublishing/Hands-On-Deep-Learning-with-Apache-Spark](https://github.com/PacktPublishing/Hands-On-Deep-Learning-with-Apache-Spark).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To complete our overview of data ingestion possibilities when training, evaluating,
    and running DL models after exploring them in [Chapter 3](44fab060-12c9-4eec-9e15-103da589a510.xhtml),
    *Extract, Transform, Load*, in this chapter, we explored the different options
    that are available to us when we perform data streaming.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter concludes the exploration of Apache Spark features. Starting from
    the next chapter, the focus will be on DL4J and some other deep learning framework
    features. These will be used in different use case scenarios, where they will
    be implemented on top of Spark.
  prefs: []
  type: TYPE_NORMAL
