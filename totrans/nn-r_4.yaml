- en: Perceptron Neural Network Modeling – Basic Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen the basics of neural networks and how the learning portion
    works. In this chapter, we take a look at one of the basic and simple forms of
    neural network architecture, the perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: 'A **perceptron** is defined as a basic building block of a neural network.
    In machine learning, a perceptron is an algorithm for supervised learning of binary
    classifiers. They classify an output as binary: `TRUE`/`FALSE` or `1`/`0`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter helps understand the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Explanation of the perceptron
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear separable classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple perceptron implementation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-Layer Perceptrons** (**MLPs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of the chapter, we will understand the basic concepts of perceptrons
    and how they are used in neural network algorithm. We will discover the linear
    separable classifier. We will learn a simple perceptron implementation function
    in R environment. We will know how to train and model an MLP.
  prefs: []
  type: TYPE_NORMAL
- en: Perceptrons and their applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A perceptron can be understood as anything that takes multiple inputs and produces
    one output. It is the simplest form of a neural network. The perceptron was proposed
    by Frank Rosenblatt in 1958 as an entity with an input and output layer and a
    learning rule based on minimizing the error. This learning function called **error
    backpropagation** alters connective weights (synapses) based on the actual output
    of the network with respect to a given input, as the difference between the actual
    output and the desired output.
  prefs: []
  type: TYPE_NORMAL
- en: The enthusiasm was enormous and the cybernetics industry was born. But later,
    scientists Marvin Minsky and Seymour Papert (1969) demonstrated the limits of
    the perceptron. Indeed, a perceptron is able to recognize, after a suitable training,
    only linearly separable functions. For example, the XOR logic function cannot
    be implemented by a perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image showns Frank Rosenblatt at the Cornell Aeronautical Laboratory
    (1957-1959), while working on the Mark I Perceptron classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00080.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Potentially, a multilevel network of percetters could solve more complex problems,
    but the increasing computational complexity of training made this path impracticable.
    Only in recent times have we started to consider the utility of this operational
    entity.
  prefs: []
  type: TYPE_NORMAL
- en: In the single form, a perceptron has one neuron unit accepting inputs and producing
    a set of outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let us take a look at the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00081.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here **x[1], x[2],.., x[n]** are the set of inputs and **x[0]** is the bias.
    **x[0]** is set to **1**. The output **y** is the sum product of **w[i]x[i]**.
    The **signum** function is applied after the sum product has been executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'It separates the output as:'
  prefs: []
  type: TYPE_NORMAL
- en: If **y>0**, the output is **1**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If **y<=0**, the output is **-1**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bias is constant and is associated with weight **w[0]**. This perceptron
    functions as a linear separator, splitting the output into one category, **-1**
    or **+1**.
  prefs: []
  type: TYPE_NORMAL
- en: Note that here this is no backpropagation and the weight update updates through
    steps we will soon see. There is a threshold setup which determines the value
    of the output. The output here is binary (either **-1** or **+1**), which can
    be set as zero or one.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, a perceptron is a simple classification function that directly makes
    its prediction. The core of the functionality lives in the weights and how we
    update the weights to the best possible prediction of **y**.
  prefs: []
  type: TYPE_NORMAL
- en: 'This case is a **simple perceptron** or basic perceptron, and the outputs are
    binary in nature: *0/1* *true/false* *+1/-1*.'
  prefs: []
  type: TYPE_NORMAL
- en: There is another type of perceptron called the **multi-class perceptron**, which
    can classify many possible labels for an animal, such as dog, cat, or bird.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following figure is shown a simple perceptron architecture versus multi-class
    perceptron architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00082.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: By modifying the weighting vector, we can modify the output of a perceptron
    to improve the learning or storage properties. For example, we can try to instruct
    a perceptron such that given an input *x*, output *y* is as close as possible
    to a given a priori chosen *y* actual value. The computational capabilities of
    a single perceptron are, however, limited, and the performance that can be obtained
    depends heavily on both the input choice and the choice of function that you want
    to implement.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, inputs can be limited to a subset of all the possible inputs, or be
    randomly extracted according to a certain predetermined probability distribution.
    To a lesser extent, the performance of such a system also depends on how the distance
    between the actual outputs and the expected outputs is quantified.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have identified the problem of learning, you can try to find the optimal
    weight assignment for the given problem.
  prefs: []
  type: TYPE_NORMAL
- en: Simple perceptron – a linear separable classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we saw, a simple perceptron is a single layer neural unit which is a linear
    classifier. It is a neuron capable of producing only two output patterns, which
    can be synthesized in *active* or *inactive*. Its decision rule is implemented
    by a *threshold* behavior: if the sum of the activation patterns of the individual
    neurons that make up the input layer, weighted for their weights, exceeds a certain
    threshold, then the output neuron will adopt the output pattern *active*. Conversely,
    the output neuron will remain in the *inactive* state.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned, the output is the sum of *weights*inputs* and a function applied
    on top of it; output is *+1 (y>0)* or *-1(y<=0)*, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00083.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We can see the linear interaction here; the output *y* is linearly dependent
    on the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: As with most neural network models, it is possible to realize a learning function
    based on the modification of synaptic connective weights, even in perceptors.
    At the beginning of the training phase, weights *w* of perceptron synaptic connections
    assume completely random values. For training, we have a number of examples with
    its relative, correct, classification. The network is presented in turn, the different
    cases to be classified and the network processes each time its response (greater
    than the threshold or less than the threshold). If the classification is correct
    (network output is the same as expected), the training algorithm does not make
    any changes. On the contrary, if the classification is incorrect, the algorithm
    changes the synaptic weights in an attempt to improve the classification performance
    of the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The single perceptron is an online learner. The weight updates happen through
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Get *x* and output label *y*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update *w* for *f(x)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If *f(x)=y*, mark as completed; else, fix it
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now adjust score based on error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*f(x)= sign(sum of weights*inputs)*, the errors are possible'
  prefs: []
  type: TYPE_NORMAL
- en: if *y=+1* and *f(x)=-1, w*x* is too small, make it bigger
  prefs: []
  type: TYPE_NORMAL
- en: if *y=-1* and *f(x)=+1, w*x* is too large make it smaller
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply the following rules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: make *w=w-x* if *f(f)=+1* and *y=-1*
  prefs: []
  type: TYPE_NORMAL
- en: make *w=w+x* if *f(f)=-1* and *y=+1*
  prefs: []
  type: TYPE_NORMAL
- en: '*w=w* if *f(x)=y*'
  prefs: []
  type: TYPE_NORMAL
- en: Or simply, *w=w+yx* if *f(x)!=y*
  prefs: []
  type: TYPE_NORMAL
- en: Repeat steps 3 to 5, until *f(x) = y*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The perceptron is guaranteed to satisfy all our data, but only for a binary
    classifier with a single neuron. In step 5, we brought a term called **learning
    rate**. This helps our model converge. In step 5, *w* is written as: *w=w+αyx
    if f(x) != y*, where *α* is the learning rate chosen.'
  prefs: []
  type: TYPE_NORMAL
- en: The bias is also updated as *b=b+ αy* if *f(x) != y*. The *b* is actually our
    *w[0]*.
  prefs: []
  type: TYPE_NORMAL
- en: If the Boolean function is a linear threshold function (that is, if it is linearly
    separable), then the local perceptron rule can find a set of weights capable of
    achieving it in a finite number of steps.
  prefs: []
  type: TYPE_NORMAL
- en: This theorem, known as the **perceptron theorem**, is also applicable in the
    case of the global rule, which modifies the vector of synaptic weights *w*, not
    at a single input vector, but depending on the behavior of the perceptron on the
    whole set of input vectors.
  prefs: []
  type: TYPE_NORMAL
- en: We just mentioned the linearly separable function, but what is meant by this
    term? We will understand it in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Linear separation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When a set of output values can be split by a straight line, the output values
    are said to be linearly separable. Geometrically, this condition describes the
    situation in which there is a hyperplane that separates, in the vector space of
    inputs, those that require positive output from those that require a negative
    output, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00084.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, one side of the separator are those predicted to belong to one class whilst
    those on the other side are predicted to belong to a different class. The decision
    rule of the Boolean neuron corresponds to the breakdown of the input features
    space, operated by a hyperplane.
  prefs: []
  type: TYPE_NORMAL
- en: If, in addition to the output neuron, even the input of the neural network is
    Boolean, then using the neural network to perform a classification is equivalent
    to determining a Boolean function of the input vector. This function takes the
    value 1 where it exceeds the threshold value, 0 otherwise. For example, with two
    input and output Boolean neurons, it is possible to represent, in an extremely
    intuitive way, the *AND* and *OR* functions.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, the *AND* gate and *OR* gate are linearly separable. Let's test it in
    practice by first listing the possible cases in a table and then representing
    them on a two-dimensional plane.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first do this for the *AND* function. In the following table are listed
    all the possible cases with the logical results:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **x1** | **x2** | **y** (**AND gate**) |'
  prefs: []
  type: TYPE_TB
- en: '| *1* | *1* | *1* |'
  prefs: []
  type: TYPE_TB
- en: '| *1* | *0* | *0* |'
  prefs: []
  type: TYPE_TB
- en: '| *0* | *1* | *0* |'
  prefs: []
  type: TYPE_TB
- en: '| *0* | *0* | *0* |'
  prefs: []
  type: TYPE_TB
- en: 'The following figure shows all the four cases in a two-dimensional plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00085.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: All points above the hyperplane are assumed to be *1/TRUE*, while the ones below
    are assumed to be *0/FALSE*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do it now for the *OR* function. In the following table are listed all
    the possible cases with the logical results:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **x1** | **x2** | **y** (**OR gate**) |'
  prefs: []
  type: TYPE_TB
- en: '| *1* | *1* | *1* |'
  prefs: []
  type: TYPE_TB
- en: '| *1* | *0* | *1* |'
  prefs: []
  type: TYPE_TB
- en: '| *0* | *1* | *1* |'
  prefs: []
  type: TYPE_TB
- en: '| *0* | *0* | *0* |'
  prefs: []
  type: TYPE_TB
- en: 'The following figure shows all the four cases in a two-dimensional plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00086.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In this case also, all the points above the hyperplane are assumed to be *1/TRUE*,
    while the ones below are assumed to be *0/FALSE*.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, some Boolean functions cannot be replicated through a network structure,
    such as that seen up to here. The *XOR* and identity functions, for example, are
    not separable: to isolate them, two lines would be needed, which can be implemented
    only through the use of a more complex network structure.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table are listed all the possible cases with the logical results,
    for the *XOR* function:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **x1** | **x2** | **y (XOR gate)** |'
  prefs: []
  type: TYPE_TB
- en: '| *1* | *1* | *0* |'
  prefs: []
  type: TYPE_TB
- en: '| *1* | *0* | *1* |'
  prefs: []
  type: TYPE_TB
- en: '| *0* | *1* | *1* |'
  prefs: []
  type: TYPE_TB
- en: '| *0* | *0* | *0* |'
  prefs: []
  type: TYPE_TB
- en: 'In the following figure are shown all the four cases in a two-dimensional plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00087.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As anticipated, such a function requires two lines to group all possible cases.
  prefs: []
  type: TYPE_NORMAL
- en: After understanding the basics of perceptron theory, we can study a practical
    case.
  prefs: []
  type: TYPE_NORMAL
- en: The perceptron function in R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we understood the fundamental concepts underlying
    the use of a perceptron as a classifier. The time has come to put into practice
    what has been studied so far. We will do it by analyzing an example in which we
    will try to classify the floral species on the basis of the size of the petals
    and sepals of an Iris. As you will recall, the `iris` dataset has already been
    used in [Chapter 3](part0081.html#2D7TI0-263fb608a19f4bb5955f37a7741ba5c4), *Deep
    Learning Using Multilayer Neural Networks*. The reason for its re-use is not only
    due to the quality of the data contained in it that allows the reader to easily
    understand the concepts outlined, but also, and more importantly, to be able to
    compare the different algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you will recall, the dataset contains 50 samples from each of three species
    of Iris (Iris `setosa`, Iris `virginica`, and Iris `versicolor`). Four features
    were measured from each sample: the length and the width of the sepals and petals,
    in centimeters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following variables are contained:'
  prefs: []
  type: TYPE_NORMAL
- en: Sepal length in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sepal width in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal length in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal width in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Class: `setosa`, `versicolour`, `virginica`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the example presented, we will try to classify the `setosa` and `versicolor`
    species through linear separation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us implement a perceptron function in R for the `iris` dataset. The code
    is presented next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let us go through the code line-by-line. Following the style in the rest
    of this book, we will present a portion of the code first as follows and then
    explain it in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The first command loads the `iris` dataset, which is contained in the datasets
    library, and saves it in a given dataframe. Then we use the `head` function to
    display the first `20` lines of the dataset. Remember, the `head` function returns
    the first or last parts of a vector, matrix, table, dataframe, or function. In
    this case, we specify the number of lines that must be displayed (`n=20`). The
    following is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s go back to the code. We will get the binary output by extracting only
    *100* rows of the `iris` dataset, and extracting only `sepal` length and `petal`
    length with `species`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here, only the first `100` rows of the `iris` dataset are taken and columns
    `1`,`3`, and `5` are chosen. This is because the first `100` lines contain the
    data for the two species (`setosa` and `versicolor`) we are interested in, in
    the following example. The three columns are `sepal.length(x1)`, `petal.length(x2)`,
    and `species(y - output)`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: First we load the `ggplot2` library, and then we use `ggplot()` to get the scatterplot
    of the distribution of species with respect to `sepal.length` and `petal.length`.
    Of course, the library should have been installed beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, to install a library that is not present in the initial distribution
    of R, you must use the `install.package` function. This is the main function to
    install packages. It takes a vector of names and a destination library, downloads
    the packages from the repositories and installs them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective of the `perceptron` function is to find a linear separation of
    the `setosa` and `versicolor` species. The following figure shows **Sepal length**
    versus **Petal length** for the two species of Iris flower:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00088.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As can be seen, the two species are placed in distinct areas of the plane,
    so linear separation is possible. At this point, we need to define functions to
    do the perceptron processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the `perceptron` function as discussed in the algorithm for perceptron
    training. We apply `learning.rate` as `1` and try to update the weights in each
    loop. Once we have the output and the function *(weights*inputs)* equal, we stop
    the training and move out. The updated weights are returned by the function. The
    objective of the function is to get a set of optimal weights needed for the model
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: With the first line, we set the `x` inputs as the `sepal` and `petal` lengths.
    `sepal.length` and `petal.length` form the input matrix. In the second line, we
    set label output as positive for `setosa` and the rest as negative. The output
    is either `setosa` or not (`+1` or `-1`). In the third line, we run the `perceptron`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We call the `perceptron` function with `x` and `y`, which gives the optimal
    weights for the perceptron as shown in the following code sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous lines of code plot `x` and `y`, highlighting `setosa` and `versicolor`
    as `+` and `*` points in the graph. We then find the intercept and slope of the
    `p` variable (perceptron), returned by the perceptron. Plotting the linear separation
    line gives the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00089.gif)'
  prefs: []
  type: TYPE_IMG
- en: To summarize, we have implemented the perceptron using R code and found optimal
    weights. The linear separation has been achieved using the perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Layer Perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We saw that the *AND* and *OR* gate outputs are linearly separable and perceptron
    can be used to model this data. However, not all functions are separable. In fact,
    there are very few and their proportion to the total of achievable functions tends
    to zero as the number of bits increases. Indeed, as we anticipated, if we take
    the *XOR* gate, the linear separation is not possible. The crosses and the zeros
    are in different locations and we cannot put a line to separate them, as shown
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00090.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We could think of parsing more perceptrons. The resulting structure could thus
    learn a greater number of functions, all of which belong to the subset of linearly
    separable functions.
  prefs: []
  type: TYPE_NORMAL
- en: In order to achieve a wider range of functions, intermediate transmissions must
    be introduced into the perceptron between the input layer and the output layer,
    allowing for some kind of internal representation of the input. The resulting
    perceptron is called MLP.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have already seen this as feed forward networks in [Chapter 1](part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4),
    *Neural Network and Artificial Intelligence Concepts*. MLP consists of at least
    three layers of nodes: input, hidden, and output nodes. Except for the input nodes,
    each node is a neuron using a non-linear activation function. MLP uses a supervised
    learning technique and back propagation for training. The multiple layers and
    non-linear nature distinguishes MLP from simple perceptrons. MLP is specifically
    used when the data is not linearly separable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, an MLP, such as that shown in the following figure, is able to
    realize the **XOR** function, which we have previously seen cannot be achieved
    through a simple perceptron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00091.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '**XOR** is achieved using a three layer network and is a combination of **OR**
    and **AND** perceptrons. The output layer contains one neuron which gives the
    **XOR** output. A configuration of this kind allows the two neurons to specialize
    each on a particular logic function. For example, in the case of **XOR,** the
    two neurons can respectively carry out the **AND** and **OR** logic functions.'
  prefs: []
  type: TYPE_NORMAL
- en: The term MLP does not refer to a single perceptron that has multiple layers.
    Rather, it contains many perceptrons that are organized into layers. An alternative
    is an MLP network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Applications of MLP are:'
  prefs: []
  type: TYPE_NORMAL
- en: MLPs are extremely useful for complex problems in research.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLPs are universal function approximators and they can be used to create mathematical
    models by regression analysis. MLPs also make good classifier algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLPs are used in diverse fields, such as speech recognition, image recognition,
    and language translation. They form the basis for deep learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will now implement an MLP using the R package SNNS.
  prefs: []
  type: TYPE_NORMAL
- en: MLP R implementation using RSNNS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The package `RSNNS` is taken from CRAN for this example of `mlp()` model build.
    The SNNS is a library written in C++ and contains many standard implementations
    of neural networks. This `RSNNS` package wraps the SNNS functionality to make
    it available from within R. Using the `RSNNS` low-level interface, all the algorithmic
    functionality and flexibility of SNNS can be accessed. The package contains a
    high-level interface for most commonly used neural network topologies and learning
    algorithms, which integrate seamlessly into R. A brief description of the `RSNNS`
    package, extracted from the official documentation, is shown in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **RSNNS package** |'
  prefs: []
  type: TYPE_TB
- en: '| **Description**: |'
  prefs: []
  type: TYPE_TB
- en: '| The SNNS is a library containing many standard implementations of neural
    networks. This package wraps the SNNS functionality to make it available from
    within R. Using the `RSNNS` low-level interface, all the algorithmic functionality
    and flexibility of SNNS can be accessed. Furthermore, the package contains a convenient
    high-level interface, so that the most common neural network topologies and learning
    algorithms integrate seamlessly into R. |'
  prefs: []
  type: TYPE_TB
- en: '| **Details**: |'
  prefs: []
  type: TYPE_TB
- en: '| Package: `RSNNS` Type: Package'
  prefs: []
  type: TYPE_NORMAL
- en: 'Version: 0.4-9'
  prefs: []
  type: TYPE_NORMAL
- en: 'Date: 2016-12-16'
  prefs: []
  type: TYPE_NORMAL
- en: 'License: LGPL (>=2) |'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Authors**: |'
  prefs: []
  type: TYPE_TB
- en: '| *Christoph Bergmeir* *José M. Benítez* |'
  prefs: []
  type: TYPE_TB
- en: '| **Usage**: |'
  prefs: []
  type: TYPE_TB
- en: '| `mlp(x, y,` `size = c(5),`'
  prefs: []
  type: TYPE_NORMAL
- en: '`maxit = 100,`'
  prefs: []
  type: TYPE_NORMAL
- en: '`initFunc = "Randomize_Weights",`'
  prefs: []
  type: TYPE_NORMAL
- en: '`initFuncParams = c(-0.3, 0.3),`'
  prefs: []
  type: TYPE_NORMAL
- en: '`learnFunc = "Std_Backpropagation",`'
  prefs: []
  type: TYPE_NORMAL
- en: '`learnFuncParams = c(0.2, 0),`'
  prefs: []
  type: TYPE_NORMAL
- en: '`updateFunc = "Topological_Order",`'
  prefs: []
  type: TYPE_NORMAL
- en: '`updateFuncParams = c(0),`'
  prefs: []
  type: TYPE_NORMAL
- en: '`hiddenActFunc = "Act_Logistic",`'
  prefs: []
  type: TYPE_NORMAL
- en: '`shufflePatterns = TRUE,`'
  prefs: []
  type: TYPE_NORMAL
- en: '`linOut = FALSE,`'
  prefs: []
  type: TYPE_NORMAL
- en: '`outputActFunc = if (linOut) "Act_Identity" else "Act_Logistic",`'
  prefs: []
  type: TYPE_NORMAL
- en: '`inputsTest = NULL,`'
  prefs: []
  type: TYPE_NORMAL
- en: '`targetsTest = NULL,`'
  prefs: []
  type: TYPE_NORMAL
- en: '`pruneFunc = NULL,`'
  prefs: []
  type: TYPE_NORMAL
- en: '`pruneFuncParams = NULL, ...)` |'
  prefs: []
  type: TYPE_NORMAL
- en: We use the `mlp()` function which creates an MLP and trains it. Training is
    usually performed by backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most commonly used parameters are listed in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `x` | A matrix with training inputs for the network |'
  prefs: []
  type: TYPE_TB
- en: '| `y` | The corresponding targets values |'
  prefs: []
  type: TYPE_TB
- en: '| `size` | Number of units in the hidden layers |'
  prefs: []
  type: TYPE_TB
- en: '| `maxit` | Maximum iterations to learn |'
  prefs: []
  type: TYPE_TB
- en: '| `hiddenActFunc` | The activation function of all hidden units |'
  prefs: []
  type: TYPE_TB
- en: '| `outputActFunc` | The activation function of all output units |'
  prefs: []
  type: TYPE_TB
- en: '| `inputsTest` | A matrix with inputs to test the network |'
  prefs: []
  type: TYPE_TB
- en: '| `targetsTest` | The corresponding targets for the test input |'
  prefs: []
  type: TYPE_TB
- en: 'Let''s see the code for building a SNNS MLP using the full Iris dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Let's go through the code step-by-step.
  prefs: []
  type: TYPE_NORMAL
- en: 'This command loads the iris dataset, which is contained in the datasets library,
    and saves it in a given dataframe. Considering the many times we have used it,
    I do not think it''s necessary to add anything. This loads the `RSNNS` library
    for the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Remember, to install a library that is not present in the initial distribution
    of R, you must use the `install.package` function. This is the main function to
    install packages. It takes a vector of names and a destination library, downloads
    the packages from the repositories and installs them.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we must use the command `install.packages("RSNNS")`. The install
    package is required only the first time, to install the `RSNNS` package from CRAN.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In this preceding line, the `iris` dataset is shuffled within rows. This operation
    makes the order of the rows in the dataset random. In fact, in the original dataset,
    the observations are ordered by floral species: the first *50* occurrences of
    the `setosa` species, followed by the *50* occurrences of the `versicolor` species,
    and finally the *50* occurrences of the virginica species. After this operation,
    the rows happen randomly. To verify this, we print the first `20` lines of the
    modified dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The numbers in the first column are the row numbers of the original dataset.
    How can we notice the shuffling flawed perfectly. To compare with the original
    sequence, see the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The independent and target variables are set up and assigned to `irisValues`
    and `irisTargets` respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the first line, the training data and the test data is split up through
    the `splitForTrainingAndTest()` function. This function splits the input and target
    values to a training and a testing set. A test set is taken from the end of the
    data. If the data is to be shuffled, this should be done before calling this function.
    In particular, the data is split as follows: *85* percent for training and *15*
    percent for testing. In the second line, the data is normalized. To do this, the
    `normTrainingAndTestSet()` function is used. This function normalizes the training
    and test set in the following way: The `inputsTrain` member is normalized using
    `normalizeData` with the parameters given in type. The normalization parameters
    obtained during this normalization are then used to normalize the `inputsTest`
    member. If the `dontNormTargets` argument is not set, then the targets are normalized
    in the same way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `mlp()` function is called with the training dataset to build the model.
    This function creates an MLP and trains it. MLPs are fully connected feed-forward
    networks, and probably the most common network architecture in use. Training is
    usually performed by error backpropagation or a related procedure. The test dataset
    is also passed to provide the test results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'These lines of code allow us to extract useful information from the newly created
    model. The `summary()` function prints out a summary of the network. The printed
    information can be either all information of the network in the original SNNS
    file format, or the information given by `extractNetInfo`. This behavior is controlled
    with the parameter `origSnnsFormat`, while the `weightMatrix()` function extracts
    the weight matrix of an `rsnns` object. The following figure shows a screenshot
    of the summary results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00092.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we measure the performance of the algorithm in model training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `plotIterativeError()` function plots the iterative training and test error
    of the net of the model. The results are shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00093.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The previous figure showns the iterative fit error as a black line and the iterative
    test error as a red line. As can be seen, both lines have a strongly decreasing
    trend, demonstrating that the algorithm quickly converges.
  prefs: []
  type: TYPE_NORMAL
- en: 'After properly training the model, it is time to use it to make predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we have used the `predict()` function. This is a generic function
    for predictions from the results of various model fitting functions. The function
    invokes particular methods which depend on the class of the first argument. We
    have both the predictions and the actual data; we just have to compare them through
    the regression error calculus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To plot the regression error, we have used the `plotRegressionError()` function.
    This function shows target values on the *X* axis and fitted/predicted values
    on the *Y* axis. The optimal fit would yield a line through zero with gradient
    one. This optimal line is shown in black in the following figure. A linear fit
    to the actual data is shown in red. The following figure shows the regression
    error for the model which we previously trained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00094.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now evaluate the model performance in predicting data by computing the
    confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: To compute the confusion matrix, we have used the `confusionMatrix()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, the confusion matrix shows how many times a pattern with the real
    class `x` was classified as class `y`. A perfect method should result in a diagonal
    matrix. All values not on the diagonal are errors of the method.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first line of the code, we calculated the confusion matrix for the data
    used in the training (which is *85* percent of the data), while in the second
    line, we calculated the confusion matrix for the data used in the test (which
    is the remaining *15* percent of data). The results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'As can be seen, there were four mistakes in the training phase, that only concerned
    the `versicolor` and `virginica` species. Remember, we obtained the same result
    in the example presented in [Chapter 3](part0081.html#2D7TI0-263fb608a19f4bb5955f37a7741ba5c4),
    *Deep Learning Using Multilayer Neural Networks*. In the test, however, we did
    not make any mistakes. I would say very good results, although the processed data
    is actually small. We graphically evaluate these results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: To evaluate network performance, we have plotted the receiver operating characteristic.
    The previous commands plot the ROC for both the phases (training and testing).
  prefs: []
  type: TYPE_NORMAL
- en: 'The ROC is a metric used to check the quality of classifiers. For each class
    of a classifier, ROC applies threshold values across the interval *[0,1]* to outputs.
    The ROC curve is a plot of the TPR versus the FPR, as the threshold is varied.
    A perfect test would show points in the upper-left corner, with *100* percent
    sensitivity and *100* percent specificity. The better the lines approach the upper-left
    corner, the better is the network performance. The following figure shows the
    ROC curves for both the phases (training to the left and test to the right):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00095.gif)'
  prefs: []
  type: TYPE_IMG
- en: As already mentioned, in the training phase there were errors that are absent
    in the test.
  prefs: []
  type: TYPE_NORMAL
- en: Note, we used the `par()` function to display both the charts in a single window.
    In it we have set to display the graphs as a matrix with a row and two columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is no `plot` function within `RSNNS`, hence we use a `plot` function
    from GitHub to plot the following MLP for the neural network we just built. There
    are three classes of output and there are four input nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00096.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We have seen a simple implementation of an `iris` dataset neural network using
    `RSNNS`. The same `mlp()` function can be used for any MLP neural network architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced you the concept of perceptrons, which are the
    basic building blocks of a neural network. We also saw multi-layer perceptrons
    and an implementation using `RSNNS`. The simple perceptron is useful only for
    a linear separation problem and cannot be used where the output data is not linearly
    separable. These limits are exceeded by the use of the MLP algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: We understood the basic concepts of perceptron and how they are used in neural
    network algorithms. We discovered the linear separable classifier and the functions
    this concept applies to. We learned a simple perceptron implementation function
    in R environment and then we learnt how to train and model an MLP.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will understand how to train, test, and evaluate a dataset
    using the neural network model. We will learn how to visualize the neural network
    model in R environment. We will cover concepts like early stopping, avoiding overfitting,
    generalization of neural network, and scaling of neural network parameters.
  prefs: []
  type: TYPE_NORMAL
