["```py\n>>> from sklearn.datasets import fetch_20newsgroups\n>>> newsgroups_train = fetch_20newsgroups(subset='train')\n>>> newsgroups_test = fetch_20newsgroups(subset='test')\n>>> x_train = newsgroups_train.data\n>>> x_test = newsgroups_test.data\n>>> y_train = newsgroups_train.target\n>>> y_test = newsgroups_test.target\n>>> print (\"List of all 20 categories:\")\n>>> print (newsgroups_train.target_names)\n>>> print (\"\\n\")\n>>> print (\"Sample Email:\")\n>>> print (x_train[0])\n>>> print (\"Sample Target Category:\")\n>>> print (y_train[0])\n>>> print (newsgroups_train.target_names[y_train[0]])\n```", "```py\n# Used for pre-processing data\n>>> import nltk\n>>> from nltk.corpus import stopwords\n>>> from nltk.stem import WordNetLemmatizer\n>>> import string\n>>> import pandas as pd\n>>> from nltk import pos_tag\n>>> from nltk.stem import PorterStemmer\n```", "```py\n>>> def preprocessing(text): \n```", "```py\n...     text2 = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in text]).split()) \n```", "```py\n...     tokens = [word for sent in nltk.sent_tokenize(text2) for word in nltk.word_tokenize(sent)] \n\n```", "```py\n...     tokens = [word.lower() for word in tokens]\n```", "```py\n\n...     stopwds = stopwords.words('english') \n...     tokens = [token for token in tokens if token not in stopwds]\n```", "```py\n...     tokens = [word for word in tokens if len(word)>=3] \n```", "```py\n...     stemmer = PorterStemmer() \n...     tokens = [stemmer.stem(word) for word in tokens]  \n```", "```py\n...     tagged_corpus = pos_tag(tokens)     \n```", "```py\n...     Noun_tags = ['NN','NNP','NNPS','NNS'] \n...    Verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ'] \n...     lemmatizer = WordNetLemmatizer()  \n```", "```py\n...     def prat_lemmatize(token,tag): \n...       if tag in Noun_tags: \n...         return lemmatizer.lemmatize(token,'n') \n...       elif tag in Verb_tags: \n...         return lemmatizer.lemmatize(token,'v') \n...       else: \n...         return lemmatizer.lemmatize(token,'n')\n```", "```py\n...     pre_proc_text =  \" \".join([prat_lemmatize(token,tag) for token,tag in tagged_corpus])              \n...     return pre_proc_text \n```", "```py\n>>> x_train_preprocessed = []\n>>> for i in x_train:\n... x_train_preprocessed.append(preprocessing(i))\n>>> x_test_preprocessed = []\n>>> for i in x_test:\n... x_test_preprocessed.append(preprocessing(i))\n# building TFIDF vectorizer\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english', max_features= 10000,strip_accents='unicode', norm='l2')\n>>> x_train_2 = vectorizer.fit_transform(x_train_preprocessed).todense()\n>>> x_test_2 = vectorizer.transform(x_test_preprocessed).todense()\n```", "```py\n# Deep Learning modules\n>>> import numpy as np\n>>> from keras.models import Sequential\n>>> from keras.layers.core import Dense, Dropout, Activation\n>>> from keras.optimizers import Adadelta,Adam,RMSprop\n>>> from keras.utils import np_utils\n```", "```py\n# Definition hyper parameters\n>>> np.random.seed(1337)\n>>> nb_classes = 20\n>>> batch_size = 64\n>>> nb_epochs = 20\n```", "```py\n>>> Y_train = np_utils.to_categorical(y_train, nb_classes)\n```", "```py\n#Deep Layer Model building in Keras\n#del model\n>>> model = Sequential()\n>>> model.add(Dense(1000,input_shape= (10000,)))\n>>> model.add(Activation('relu'))\n>>> model.add(Dropout(0.5))\n>>> model.add(Dense(500))\n>>> model.add(Activation('relu'))\n>>> model.add(Dropout(0.5))\n>>> model.add(Dense(50))\n>>> model.add(Activation('relu'))\n>>> model.add(Dropout(0.5))\n>>> model.add(Dense(nb_classes))\n>>> model.add(Activation('softmax'))\n>>> model.compile(loss='categorical_crossentropy', optimizer='adam')\n>>> print (model.summary())\n```", "```py\n# Model Training\n>>> model.fit(x_train_2, Y_train, batch_size=batch_size, epochs=nb_epochs,verbose=1)\n```", "```py\n#Model Prediction\n>>> y_train_predclass = model.predict_classes(x_train_2,batch_size=batch_size)\n>>> y_test_predclass = model.predict_classes(x_test_2,batch_size=batch_size)\n>>> from sklearn.metrics import accuracy_score,classification_report\n>>> print (\"\\n\\nDeep Neural Network - Train accuracy:\"),(round(accuracy_score( y_train, y_train_predclass),3))\n>>> print (\"\\nDeep Neural Network - Test accuracy:\"),(round(accuracy_score( y_test,y_test_predclass),3))\n>>> print (\"\\nDeep Neural Network - Train Classification Report\")\n>>> print (classification_report(y_train,y_train_predclass))\n>>> print (\"\\nDeep Neural Network - Test Classification Report\")\n>>> print (classification_report(y_test,y_test_predclass))\n```", "```py\n>>> import pandas as pd\n>>> from keras.preprocessing import sequence\n>>> from keras.models import Sequential\n>>> from keras.layers import Dense, Dropout, Activation\n>>> from keras.layers import Embedding\n>>> from keras.layers import Conv1D, GlobalMaxPooling1D\n>>> from keras.datasets import imdb\n>>> from sklearn.metrics import accuracy_score,classification_report\n```", "```py\n# set parameters:\n>>> max_features = 6000\n>>> max_length = 400\n>>> (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n>>> print(len(x_train), 'train observations')\n>>> print(len(x_test), 'test observations')\n```", "```py\n# Creating numbers to word mapping\n>>> wind = imdb.get_word_index()\n>>> revind = dict((v,k) for k,v in wind.iteritems())\n>>> print (x_train[0])\n>>> print (y_train[0])\n```", "```py\n>>> def decode(sent_list):\n... new_words = []\n... for i in sent_list:\n... new_words.append(revind[i])\n... comb_words = \" \".join(new_words)\n... return comb_words\n>>> print (decode(x_train[0]))\n```", "```py\n#Pad sequences for computational efficiency\n>>> x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n>>> x_test = sequence.pad_sequences(x_test, maxlen=max_length)\n>>> print('x_train shape:', x_train.shape)\n>>> print('x_test shape:', x_test.shape)\n```", "```py\n# Deep Learning architecture parameters\n>>> batch_size = 32\n>>> embedding_dims = 60\n>>> num_kernels = 260\n>>> kernel_size = 3\n>>> hidden_dims = 300\n>>> epochs = 3\n# Building the model\n>>> model = Sequential()\n>>> model.add(Embedding(max_features,embedding_dims, input_length= max_length))\n>>> model.add(Dropout(0.2))\n>>> model.add(Conv1D(num_kernels,kernel_size, padding='valid', activation='relu', strides=1))\n>>> model.add(GlobalMaxPooling1D())\n>>> model.add(Dense(hidden_dims))\n>>> model.add(Dropout(0.5))\n>>> model.add(Activation('relu'))\n>>> model.add(Dense(1))\n>>> model.add(Activation('sigmoid'))\n>>> model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n>>> print (model.summary())\n```", "```py\n>>> model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs, validation_split=0.2)\n```", "```py\n#Model Prediction\n>>> y_train_predclass = model.predict_classes(x_train,batch_size=batch_size)\n>>> y_test_predclass = model.predict_classes(x_test,batch_size=batch_size)\n>>> y_train_predclass.shape = y_train.shape\n>>> y_test_predclass.shape = y_test.shape\n\n# Model accuracies and metrics calculation\n>>> print ((\"\\n\\nCNN 1D - Train accuracy:\"),(round(accuracy_score(y_train, y_train_predclass),3)))\n>>> print (\"\\nCNN 1D of Training data\\n\",classification_report(y_train, y_train_predclass))\n>>> print (\"\\nCNN 1D - Train Confusion Matrix\\n\\n\",pd.crosstab(y_train, y_train_predclass,rownames = [\"Actuall\"],colnames = [\"Predicted\"]))\n>>> print ((\"\\nCNN 1D - Test accuracy:\"),(round(accuracy_score(y_test, y_test_predclass),3)))\n>>> print (\"\\nCNN 1D of Test data\\n\",classification_report(y_test, y_test_predclass))\n>>> print (\"\\nCNN 1D - Test Confusion Matrix\\n\\n\",pd.crosstab(y_test, y_test_predclass,rownames = [\"Actuall\"],colnames = [\"Predicted\"]))\n```", "```py\n>>> from __future__ import print_function\n>>> import numpy as np\n>>> import pandas as pd\n>>> from keras.preprocessing import sequence\n>>> from keras.models import Sequential\n>>> from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n>>> from keras.datasets import imdb\n>>> from sklearn.metrics import accuracy_score,classification_report\n\n# Max features are limited\n>>> max_features = 15000\n>>> max_len = 300\n>>> batch_size = 64\n\n# Loading data\n>>> (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n>>> print(len(x_train), 'train observations')\n>>> print(len(x_test), 'test observations')\n```", "```py\n# Pad sequences for computational efficiently\n>>> x_train_2 = sequence.pad_sequences(x_train, maxlen=max_len)\n>>> x_test_2 = sequence.pad_sequences(x_test, maxlen=max_len)\n>>> print('x_train shape:', x_train_2.shape)\n>>> print('x_test shape:', x_test_2.shape)\n>>> y_train = np.array(y_train)\n>>> y_test = np.array(y_test)\n```", "```py\n# Model Building\n>>> model = Sequential()\n>>> model.add(Embedding(max_features, 128, input_length=max_len))\n>>> model.add(Bidirectional(LSTM(64)))\n>>> model.add(Dropout(0.5))\n>>> model.add(Dense(1, activation='sigmoid'))\n>>> model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n# Print model architecture\n>>> print (model.summary())\n```", "```py\n#Train the model\n>>> model.fit(x_train_2, y_train,batch_size=batch_size,epochs=4, validation_split=0.2)\n```", "```py\n#Model Prediction\n>>> y_train_predclass = model.predict_classes(x_train_2,batch_size=1000)\n>>> y_test_predclass = model.predict_classes(x_test_2,batch_size=1000)\n>>> y_train_predclass.shape = y_train.shape\n>>> y_test_predclass.shape = y_test.shape\n\n# Model accuracies and metrics calculation\n>>> print ((\"\\n\\nLSTM Bidirectional Sentiment Classification - Train accuracy:\"),(round(accuracy_score(y_train,y_train_predclass),3)))\n>>> print (\"\\nLSTM Bidirectional Sentiment Classification of Training data\\n\",classification_report(y_train, y_train_predclass))\n>>> print (\"\\nLSTM Bidirectional Sentiment Classification - Train Confusion Matrix\\n\\n\",pd.crosstab(y_train, y_train_predclass,rownames = [\"Actuall\"],colnames = [\"Predicted\"]))\n>>> print ((\"\\nLSTM Bidirectional Sentiment Classification - Test accuracy:\"),(round(accuracy_score(y_test,y_test_predclass),3)))\n>>> print (\"\\nLSTM Bidirectional Sentiment Classification of Test data\\n\",classification_report(y_test, y_test_predclass))\n>>> print (\"\\nLSTM Bidirectional Sentiment Classification - Test Confusion Matrix\\n\\n\",pd.crosstab(y_test, y_test_predclass,rownames = [\"Actuall\"],colnames = [\"Predicted\"]))\n```", "```py\n>>> from __future__ import print_function\n>>> import os\n\"\"\" First change the following directory link to where all input files do exist \"\"\"\n>>> os.chdir(\"C:\\\\Users\\\\prata\\\\Documents\\\\book_codes\\\\NLP_DL\")\n>>> import nltk\n>>> from nltk.corpus import stopwords\n>>> from nltk.stem import WordNetLemmatizer\n>>> from nltk import pos_tag\n>>> from nltk.stem import PorterStemmer\n>>> import string\n>>> import numpy as np\n>>> import pandas as pd\n>>> import random\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.preprocessing import OneHotEncoder\n>>> import matplotlib.pyplot as plt\n>>> def preprocessing(text):\n... text2 = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in text]).split())\n... tokens = [word for sent in nltk.sent_tokenize(text2) for word in\nnltk.word_tokenize(sent)]\n... tokens = [word.lower() for word in tokens]\n... stopwds = stopwords.words('english')\n... tokens = [token for token in tokens if token not in stopwds]\n... tokens = [word for word in tokens if len(word)>=3]\n... stemmer = PorterStemmer()\n... tokens = [stemmer.stem(word) for word in tokens]\n... tagged_corpus = pos_tag(tokens)\n... Noun_tags = ['NN','NNP','NNPS','NNS']\n... Verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ']\n... lemmatizer = WordNetLemmatizer()\n... def prat_lemmatize(token,tag):\n... if tag in Noun_tags:\n... return lemmatizer.lemmatize(token,'n')\n... elif tag in Verb_tags:\n... return lemmatizer.lemmatize(token,'v')\n... else:\n... return lemmatizer.lemmatize(token,'n')\n... pre_proc_text = \" \".join([prat_lemmatize(token,tag) for token,tag in tagged_corpus])\n... return pre_proc_text\n>>> lines = []\n>>> fin = open(\"alice_in_wonderland.txt\", \"rb\")\n>>> for line in fin:\n... line = line.strip().decode(\"ascii\", \"ignore\").encode(\"utf-8\")\n... if len(line) == 0:\n... continue\n... lines.append(preprocessing(line))\n>>> fin.close()\n```", "```py\n>>> import collections\n>>> counter = collections.Counter()\n>>> for line in lines:\n... for word in nltk.word_tokenize(line):\n... counter[word.lower()]+=1\n>>> word2idx = {w:(i+1) for i,(w,_) in enumerate(counter.most_common())}\n>>> idx2word = {v:k for k,v in word2idx.items()}\n```", "```py\n>>> xs = []\n>>> ys = []\n>>> for line in lines:\n... embedding = [word2idx[w.lower()] for w in nltk.word_tokenize(line)]\n... triples = list(nltk.trigrams(embedding))\n... w_lefts = [x[0] for x in triples]\n... w_centers = [x[1] for x in triples]\n... w_rights = [x[2] for x in triples]\n... xs.extend(w_centers)\n... ys.extend(w_lefts)\n... xs.extend(w_centers)\n... ys.extend(w_rights)\n```", "```py\n>>> print (len(word2idx))\n>>> vocab_size = len(word2idx)+1\n```", "```py\n>>> ohe = OneHotEncoder(n_values=vocab_size)\n>>> X = ohe.fit_transform(np.array(xs).reshape(-1, 1)).todense()\n>>> Y = ohe.fit_transform(np.array(ys).reshape(-1, 1)).todense()\n>>> Xtrain, Xtest, Ytrain, Ytest,xstr,xsts = train_test_split(X, Y,xs, test_size=0.3, random_state=42)\n>>> print(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)\n```", "```py\n>>> from keras.layers import Input,Dense,Dropout\n>>> from keras.models import Model\n>>> np.random.seed(42)\n>>> BATCH_SIZE = 128\n>>> NUM_EPOCHS = 20\n>>> input_layer = Input(shape = (Xtrain.shape[1],),name=\"input\")\n>>> first_layer = Dense(300,activation='relu',name = \"first\")(input_layer)\n>>> first_dropout = Dropout(0.5,name=\"firstdout\")(first_layer)\n>>> second_layer = Dense(2,activation='relu',name=\"second\") (first_dropout)\n>>> third_layer = Dense(300,activation='relu',name=\"third\") (second_layer)\n>>> third_dropout = Dropout(0.5,name=\"thirdout\")(third_layer)\n>>> fourth_layer = Dense(Ytrain.shape[1],activation='softmax',name = \"fourth\")(third_dropout)\n>>> history = Model(input_layer,fourth_layer)\n>>> history.compile(optimizer = \"rmsprop\",loss= \"categorical_crossentropy\", metrics=[\"accuracy\"])\n```", "```py\n>>> history.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE,epochs=NUM_EPOCHS, verbose=1,validation_split = 0.2)\n```", "```py\n# Extracting Encoder section of the Model for prediction of latent variables\n>>> encoder = Model(history.input,history.get_layer(\"second\").output)\n\n# Predicting latent variables with extracted Encoder model\n>>> reduced_X = encoder.predict(Xtest)\nConverting the outputs into Pandas data frame structure for better representation\n>>> final_pdframe = pd.DataFrame(reduced_X)\n>>> final_pdframe.columns = [\"xaxis\",\"yaxis\"]\n>>> final_pdframe[\"word_indx\"] = xsts\n>>> final_pdframe[\"word\"] = final_pdframe[\"word_indx\"].map(idx2word)\n>>> rows = random.sample(final_pdframe.index, 100)\n>>> vis_df = final_pdframe.ix[rows]\n>>> labels = list(vis_df[\"word\"]);xvals = list(vis_df[\"xaxis\"])\n>>> yvals = list(vis_df[\"yaxis\"])\n\n#in inches\n>>> plt.figure(figsize=(8, 8))\n>>> for i, label in enumerate(labels):\n... x = xvals[i]\n... y = yvals[i]\n... plt.scatter(x, y)\n... plt.annotate(label,xy=(x, y),xytext=(5, 2),textcoords='offset points', ha='right',va='bottom')\n>>> plt.xlabel(\"Dimension 1\")\n>>> plt.ylabel(\"Dimension 2\")\n>>> plt.show()\n```"]