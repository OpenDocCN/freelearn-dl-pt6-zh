<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building an ML Model to Predict Car Damage Using TensorFlow</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will build a system that detects the level of damage  that's been done to a vehicle by analyzing photographs using <strong>transfer learning</strong>. A solution like this will be helpful in reducing the cost of insurance claims, as well as making the process simpler for vehicle owners. If the system is implemented properly, in an ideal scenario, the user will upload a bunch of photographs of the damaged vehicle, the photos will go through damage assessment, and the insurance claim will be processed automatically.</p>
<p>There are a lot of risks and challenges involved in implementing a perfect solution for this use case. To start with, there are multiple unknown conditions that could have caused damage to the car. We are not aware of the outdoor environment, surrounding objects, light in the area, and the quality of the vehicle before the incident. Passing through all these hurdles and figuring out a common solution for the problem is challenging. This is a common problem across any computer vision-based scenario.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Transfer learning basics</li>
<li>Image dataset collections</li>
<li>Setting up a web application</li>
<li>Training our own TensorFlow model</li>
<li>Building a web app that consumes the model</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transfer learning basics</h1>
                </header>
            
            <article>
                
<p>To implement the car damage prediction system, we are going to build our own TensorFlow-based <strong>machine learning</strong> (<strong>ML</strong>) model for the vehicle datasets. Millions of parameters are needed with modern recognition models. We need a lot of time and data to train a new model from scratch, as well as hundreds of <strong>Graphical Processing Units</strong> (<strong>GPUs</strong>) or <strong>Tensor Processing Units</strong> (<strong>TPUs</strong>) that run for hours.</p>
<p>Transfer learning makes this task easier by taking an existing model that is already trained and reusing it on a new model. In our example, we will use the feature extraction capabilities from the <strong>MobileNet</strong> model and train our own classifiers on top of it. Even if we don't get 100% accuracy, this works best in a lot of cases, especially on a mobile phone where we don't have heavy resources. We can easily train this model on a typical laptop for a few hours, even without a GPU. The model was built on a MacBook Pro with a 2.6 GHz Intel i5 processor and 8 GB RAM.</p>
<p>Transfer learning is one of the most popular approaches in deep learning, where a model that's been developed for one task is reused for another model on a different task. Here, pre-trained models are used as a first step in computer vision-based tasks or <strong>natural language processing</strong> (<strong>NLP</strong>) based tasks, provided we have very limited computational resources and time.</p>
<p>In a typical computer vision-based problem, neural networks try to detect edges in their initial level layers, shapes in the middle level layers, and more specific features in the final level layers. With transfer learning, we will use the initial and middle level layers and only retrain the final level layers.</p>
<p>For example, if we have a model that's trained to recognize an apple from the input image, it could be reused to detect water bottles. In the initial layers, the model has been trained to recognize objects, so we will retrain only the final level layers. In that way, our model will learn what will differentiate water bottles from other objects. This process can be seen in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-761 image-border" src="assets/72e3df71-eb01-438a-b39c-4ccd737203f9.png" style="width:28.92em;height:21.92em;"/></div>
<p>Typically, we need a lot of data to train our model, but most of the time we will not have enough relevant data. That is where transfer learning comes into the picture, and is where you can train your model with very little data.</p>
<p>If your old classifier was developed and trained using TensorFlow, you can reuse the same model to retrain a few of the layers for your new classifier. This will work perfectly, but only if the features that were learned from the old task are more generic in nature. For example, you can't reuse a model that was developed for a text classifier on an image classification-based task. Also, the input data size should match for both the models. If the size doesn't match, we need to add an additional preprocessing step to resize the input data accordingly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Approaches to transfer learning</h1>
                </header>
            
            <article>
                
<p>Let's look into different approaches to transfer learning. There could be different names given to the approaches, but the concepts remain the same:</p>
<ol>
<li><strong>Using a pre-trained model</strong>: There are a lot of pre-trained models out there to satisfy your basic deep learning research. In this book, we have used a lot of pre-trained models from where we derive our results.</li>
</ol>
<ol start="2">
<li><strong>Training a model for reuse</strong>: Let's assume that you want to solve problem A, but you don't have enough data to achieve the results. To solve this issue, we have another problem, B, where we have enough data. In that case, we can develop a model for problem B, and use the model as a starting point for problem A. Whether we need to reuse all the layers or only a few layers is dependent on the type of problem that we are solving.</li>
<li><strong>Feature ex</strong><strong>traction</strong><span>: With dee</span><span>p learning, we can extract the features of the dataset. Most of the time, the features are handcrafted by the developers. Neural networks have the ability to learn which features you have to pass on, and which ones you don't. For example, we will only use the initial layers to detect the right representation of features, but we will not use the output because it might be more specific to one particular task. We will simply feed the data into our network and use one of the immediate middle level layers as the output layer.</span></li>
</ol>
<p>With this, we will start building our model using TensorFlow.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the TensorFlow model</h1>
                </header>
            
            <article>
                
<p>Building your own custom model requires following a step-by-step procedure. To begin, we are going to use the TensorFlow Hub to feed images using pre-trained models.</p>
<div class="packt_infobox">To learn more about TensorFlow Hub, please refer to <a href="https://www.tensorflow.org/hub" target="_blank">https://www.tensorflow.org/hub</a>.<a href="https://www.tensorflow.org/hub" target="_blank"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing TensorFlow</h1>
                </header>
            
            <article>
                
<p>While writing this book, TensorFlow r1.13 was available. Revision 2.0.0 is also available on the alpha stage, but we will stay with a stable version. The TensorFlow Hub has a dependency on the TensorFlow library that can be installed with <kbd>pip</kbd>, as follows:</p>
<pre><strong>$ pip install tensorflow</strong><br/><strong>$ pip install tensorflow-hub</strong></pre>
<p>When the <kbd>tensorflow</kbd> library is installed, we need to start collecting our image dataset before the training process starts. We need to look into a lot of things before we begin our training.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the images</h1>
                </header>
            
            <article>
                
<p>In this section, we will collect the images and keep them organized under their respective folder categories.</p>
<p>A few common steps for choosing your own dataset of images are as follows:</p>
<ol>
<li>First of all, you need at least 100 photos of each image category that you want to recognize. The accuracy of your model is directly proportional to the number of images in the set.</li>
<li>You need to make sure that you have more relevant images in the image set. For example, if you have taken an image set with a single color background let's say all the objects in the images have a white background and are shot indoors and users are trying to recognize objects with distracting backgrounds (for example, colorful backgrounds shot outdoors), this won't result in better accuracy.</li>
<li>Choose images with a variety of backgrounds. For example, if you are picking images with only two background colors, then your prediction will have a preference toward those two colors, rather than the object in the image.</li>
<li>Try to split bigger categories into smaller divisions. For example, instead of animal, you might use cat, dog, or tiger.</li>
<li>Make sure that you select all the input images that contain the objects that you are trying to recognize. For example, if you have a dog-recognizing app, we wouldn't use cars, buildings, or mountains as input images. In that case, it is better to have a separate classifier for the unrecognizable images.</li>
<li>Ensure that you label images properly. For example, labeling a flower as jasmine might have the whole plant in the picture or a person behind it. The accuracy of our algorithm will differ when there are distracting objects in the input images. Let's say you have taken a few food item images from Google Images. These images have reusable permissions, so always ensure that you have this when collecting images for your model. You can do this by searching any keyword from Google Images and filter the images based on labelled for reuse usage rights. You can find this option by clicking on tools beneath the search bar.</li>
</ol>
<p><span>We have collected a few images from the internet for educational purposes in this chapter. This is discussed in further detail in the next</span> section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building our own model</h1>
                </header>
            
            <article>
                
<p>Here, we are going to build our own ML model using TensorFlow to analyze the damage level of the vehicle. We need to be careful in picking the dataset that will play a crucial part in the damage evaluation phase. Here are the steps that we are going to follow in order to build the model:</p>
<ol>
<li>Find the image dataset of the damaged vehicles.</li>
<li>Categorize the images based on their damage levels. First, we need to identify that the object in the picture is actually a car. To do that, we need to have two categories of image sets that do and do not have cars in them. Then, we need to have three more categories to find the damage level of the cars categorized under high, medium, or low levels. Make sure that you have at least 1,000+ images under each of the five categories. Once the dataset is ready, we are ready to train our model.</li>
<li>We will train our model using TensorFlow.</li>
<li>We will build a web application to analyze the damage level of the vehicle.</li>
<li>Update the result.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Retraining with our own images</h1>
                </header>
            
            <article>
                
<p>We will now use the <kbd>retrain.py</kbd> script in our project directory.</p>
<p>Download this script using <kbd>curl</kbd>, as follows:</p>
<pre><strong>mkdir -/Chapter5/images</strong><br/><strong>cd -/Chapter5/images</strong><br/><strong>curl -LO https://github.com/tensorflow/hub/raw/master/examples/image_retraining/ retrain.py</strong><br/><strong>python retrain.py --image_dir ./images/</strong></pre>
<p>There are a few parameters that have to be passed to the training script and looked into before the training starts.</p>
<p>Once our dataset is ready, we need to look into improving the results. We can do this by altering the number of steps in the learning process.</p>
<p>The simplest way to do this is by using the following code:</p>
<pre><strong>--how_many_training_steps = 4000</strong></pre>
<p>The rate of accuracy improvement slows down when the number of steps increases, and the accuracy will stop improving beyond a certain point. You can experiment with this and decide what works best for you.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Architecture</h1>
                </header>
            
            <article>
                
<p>MobileNet is a smaller, low-power, low-latency model that's designed to meet the constraints of mobile devices. In our application, we have picked the following architecture from the MobileNet datasets as one of the parameters, <span>as shown in the following code, for </span>while we build the model, which has a better accuracy benchmark:</p>
<pre><strong>--architecture=" mobilenet_v2_1.4_224"</strong></pre>
<p>The power and latency of the network grows with the number of <strong>Multiply-Accumulates</strong> (<strong>MACs</strong>), which measure the number of fused multiplication and addition operations, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-762 image-border" src="assets/6155f1ac-e985-414e-b7db-c94dd8fa8b2d.png" style="width:53.83em;height:51.17em;"/></p>
<p>You can download the model from <span class="MsoHyperlink"><a href="https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet">https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet</a>.<a href="https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet"/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Distortions</h1>
                </header>
            
            <article>
                
<p>We can improve the results by giving tough input images during training. Training images can be generated by cropping, brightening, and deforming the input images randomly. This will help in generating an effective training dataset.</p>
<p>However, there is a disadvantage of enabling distortion here, since bottleneck caching is not useful. Consequently, the input images are not reused, increasing the training time period. There are multiple ways to enable distortion, as shown here:</p>
<pre><strong>--random_crop</strong><br/><strong>--random_scale</strong><br/><strong>--random_brightness</strong></pre>
<p>This won't be useful in all cases. For example, it won't be helpful in a digit classifier system, since flipping and distorting the image won't make sense when it comes to producing a possible output.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hyperparameters</h1>
                </header>
            
            <article>
                
<p>We can try a few more parameters to check whether additional parameters will help to improve results.</p>
<p>Specify them in the form that's given in the following bullet points. The hyperparameters are explained as follows:</p>
<ul>
<li><kbd>--learning_rate</kbd>: This parameter controls the updates to the final layer while training. If this value is small, the training will take more time. This may not always help when it comes to improving accuracy.</li>
<li><kbd>--train_batch_size</kbd>: This parameter helps with controlling the number of images that are examined during training to estimate the final-layer updates. Once the images are ready, the script splits them into three different sets. The largest set is used in training. This division is mainly useful for preventing the model from recognizing unnecessary patterns in the input images. If a model is trained using a certain background pattern, it won't give a proper result when it faces images with new backgrounds because it remembers unnecessary information from the input images. This is known as <strong>overfitting</strong>.</li>
<li><kbd>--testing_percentage</kbd> and <kbd>--validation_percentage</kbd> flags: To avoid overfitting, we keep 80% of the data inside the main training set. Of this data, 10% is then used to run validation during the training process and the final 10% is used to test the model.</li>
</ul>
<p class="mce-root"/>
<ul>
<li><kbd>--validation_batch_size</kbd>: We can see that the accuracy of validation fluctuates between iterations. </li>
</ul>
<p>If you are new to this, you can run default values without making any changes to these parameters. Let's jump into building our model. For this, we need the training image data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image dataset collection</h1>
                </header>
            
            <article>
                
<p>For our experiment, we need the datasets for cars in good condition as well as in damaged condition. If you have a data source that adheres to the privacy policy, then this is a good place to start. Otherwise, we need to find a way to build our model on top of a dataset. There are multiple datasets that are publicly available. We need to start building our dataset if there is no existing reference of a similar data model because this could be a time-consuming task as well as an important step toward getting better results.</p>
<p>We are going to use a simple Python script to download images from Google. Just make sure that you filter images that can be reused. We don't encourage using pictures with non-reusable licenses.</p>
<p>With the Python script, we will pull out and save the images from Google, and then we will use a library to do the same task. This step is one of the most basic steps for building any ML model.</p>
<p>We will use a Python library called <strong>Beautiful Soup</strong> to scrap images from the internet.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to Beautiful Soup</h1>
                </header>
            
            <article>
                
<p>Beautiful Soup is a Python library that's used to pull data out of HTML and XML files. It is useful with projects that involve scraping. With this library, we can navigate, search, and modify the HTML and XML files.</p>
<p>This library parses anything you feed in and does tree traversal work on the data. You can ask the library to find all the links whose URLs match <kbd>google.com</kbd>, find all the links with class bold URLs, or find all the table headers with bold text.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>There are a few features that makes it useful, and they are as follows:</p>
<ol>
<li>Beautiful Soup provides us with some simple methods and Pythonic idioms to navigate, search, and modify a parse tree, which is a toolkit that is used to dissect a document and then extract what you need. We need less code to write an application.</li>
<li>Beautiful Soup automatically converts incoming documents into Unicode and outgoing documents into UTF-8. Unless the document doesn't specify anything about encoding and Beautiful Soup isn't able to detect any, we don't have to think about encoding. Then, we will have to specify only the original encoding.</li>
<li>Beautiful Soup can be used on top of popular Python parsers, such as <kbd>lxml</kbd> (<a href="https://lxml.de/">https://lxml.de/</a>) and <kbd>html5lib</kbd> (<a href="https://github.com/html5lib/">https://github.com/html5lib/</a>), and lets you try various parsing strategies or trade speed for flexibility.</li>
<li>Beautiful Soup saves you time by extracting the information you need and so makes your job easier.</li>
</ol>
<p>Here is the simple version of the code:</p>
<pre>import argparse<br/>import json<br/>import itertools<br/>import logging<br/>import re<br/>import os<br/>import uuid<br/>import sys<br/>from urllib.request import urlopen, Request<br/>from bs4 import BeautifulSoup<br/>#logger will be useful for your debugging need<br/>def configure_logging():<br/>logger = logging.getLogger()<br/>logger.setLevel(logging.DEBUG)<br/>handler = logging.StreamHandler()<br/>handler.setFormatter(<br/>logging.Formatter('[%(asctime)s %(levelname)s %(module)s]: %(message)s'))<br/>logger.addHandler(handler)<br/>return logger<br/>logger = configure_logging()</pre>
<p>Setting the user-agent to avoid 403 error code:</p>
<pre><br/>REQUEST_HEADER = {<br/>'User-Agent': "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.134 Safari/537.36"}<br/>def get_soup(url, header):<br/>response = urlopen(Request(url, headers=header))<br/>return BeautifulSoup(response, 'html.parser')<br/># initialize place for links<br/>def get_query_url(query):<br/>return "<a href="https://www.google.co.in/search?q=%25s&amp;source=lnms&amp;tbm=isch">https://www.google.co.in/search?q=%s&amp;source=lnms&amp;tbm=isch</a>" % query<br/># pull out specific data through navigating into source data tree<br/>def extract_images_from_soup(soup):<br/>image_elements = soup.find_all("div", {"class": "rg_meta"})<br/>metadata_dicts = (json.loads(e.text) for e in image_elements)<br/>link_type_records = ((d["ou"], d["ity"]) for d in metadata_dicts)<br/>return link_type_records</pre>
<p>Pass the number of images you want to pull out. By default google provides 100 images:</p>
<pre><br/>def extract_images(query, num_images):<br/>url = get_query_url(query)<br/>logger.info("Souping")<br/>soup = get_soup(url, REQUEST_HEADER)<br/>logger.info("Extracting image urls")<br/>link_type_records = extract_images_from_soup(soup)<br/>return itertools.islice(link_type_records, num_images)<br/>def get_raw_image(url):<br/>req = Request(url, headers=REQUEST_HEADER)<br/>resp = urlopen(req)<br/>return resp.read()</pre>
<p>Saving all the downloaded images along with its extension, as shown in the following code block:</p>
<pre>def save_image(raw_image, image_type, save_directory):<br/>extension = image_type if image_type else 'jpg'<br/>file_name = str(uuid.uuid4().hex) + "." + extension<br/>save_path = os.path.join(save_directory, file_name)<br/>with open(save_path, 'wb+') as image_file:<br/>image_file.write(raw_image)<br/>def download_images_to_dir(images, save_directory, num_images):<br/>for i, (url, image_type) in enumerate(images):<br/>try:<br/>logger.info("Making request (%d/%d): %s", i, num_images, url)<br/>raw_image = get_raw_image(url)<br/>save_image(raw_image, image_type, save_directory)<br/>except Exception as e:<br/>logger.exception(e)<br/>def run(query, save_directory, num_images=100):<br/>query = '+'.join(query.split())<br/>logger.info("Extracting image links")<br/>images = extract_images(query, num_images)<br/>logger.info("Downloading images")<br/>download_images_to_dir(images, save_directory, num_images)<br/>logger.info("Finished")<br/>#main method to initiate the scrapper<br/>def main():<br/>parser = argparse.ArgumentParser(description='Scrape Google images')<br/>#change the search term here<br/>parser.add_argument('-s', '--search', default='apple', type=str, help='search term')</pre>
<p>Change number of images parameter here. By default it is set to 1, as shown in following code:</p>
<pre>parser.add_argument('-n', '--num_images', default=1, type=int, help='num images to save')<br/>#change path according to your need<br/>parser.add_argument('-d', '--directory', default='/Users/karthikeyan/Downloads/', type=str, help='save directory')<br/>args = parser.parse_args()<br/>run(args.search, args.directory, args.num_images)<br/>if __name__ == '__main__':<br/>main()</pre>
<p>Save the script as a Python file and then run the code by executing the following command:</p>
<pre><strong>python imageScrapper.py --search "alien" --num_images 10 --directory "/Users/Karthikeyan/Downloads"</strong></pre>
<p>Google image scraping with a better library, including more configurable options. We will use <a href="https://github.com/hardikvasa/google-images-download">https://github.com/hardikvasa/google-images-download</a>.</p>
<p>This is a command line Python program that's used to search for keywords or key phrases on Google Images, and optionally download images to your computer. You can also invoke this script from another Python file.</p>
<p>This is a small and ready-to-run program. No dependencies are required for it to be installed if you only want to download up to 100 images per keyword. If you want more than 100 images per keyword, then you will need to install the <kbd>Selenium</kbd> library, along with <strong>ChromeDriver</strong>. Detailed instructions are provided in the <em>Troubleshooting</em> section.</p>
<p>You can use a library with more useful options<span><span>.</span></span></p>
<p>If you prefer command line-based installation, use the following code:</p>
<pre><strong>$ git clone https://github.com/hardikvasa/google-images-download.git</strong><br/><strong>$ cd google-images-download &amp;&amp; sudo python setup.py install</strong></pre>
<p>Alternatively, you can install the library through <kbd>pip</kbd>:</p>
<pre><strong>$ pip install google_images_download</strong></pre>
<p>If installed via <kbd>pip</kbd> or using a <strong>command language interpreter</strong> (<strong>CLI</strong>), use the following command:</p>
<pre><strong>$ googleimagesdownload [Arguments...]</strong></pre>
<p>If downloaded via the UI from <kbd>github.com</kbd>, unzip the downloaded file, go to the <kbd>google_images_download</kbd> directory, and use one of the following commands:</p>
<pre><strong>$ python3 google_images_download.py [Arguments...]<br/></strong><br/><strong>$ python google_images_download.py [Arguments...]</strong></pre>
<p>If you want to use this library from another Python file, use this command:</p>
<pre><strong>from google_images_download import google_images_download</strong><br/><strong>response = google_images_download.googleimagesdownload()</strong><br/><strong> absolute_image_paths = response.download({&lt;Arguments...&gt;})</strong></pre>
<p>You can either pass the arguments directly from the command, as shown in the following examples, or you can pass it through a config file. </p>
<p>You can pass more than one record through a config file. The following sample consists of two sets of records. The code will iterate through each of the records and download images based on the arguments that are passed.</p>
<p><span>The following is a sample of what a config file looks like:</span></p>
<pre>{<br/> "Records": [<br/> {<br/> "keywords": "apple",<br/> "limit": 55,<br/> "color": "red",<br/> "print_urls": true<br/> },<br/> {<br/> "keywords": "oranges",<br/> "limit": 105,<br/> "size": "large",<br/> "print_urls": true<br/> }<br/> ]<br/> }</pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Examples</h1>
                </header>
            
            <article>
                
<p>If you are calling this library from another Python file, the following is the sample code from Google:</p>
<pre>_images_download import google_images_download <br/><br/>#importing the library<br/><br/>response = google_images_download.googleimagesdownload() <br/><br/>#class instantiation<br/><br/>arguments = {"keywords":"apple, beach, cat","limit":30,"print_urls":True} #creating list of arguments<br/>paths = response.download(arguments) #passing the arguments to the function<br/>print(paths)<br/><br/>#printing absolute paths of the downloaded images</pre>
<p>If you are passing arguments from a config file, simply pass the <kbd>config_file</kbd> argument with the name of your <strong>JSON</strong> file:</p>
<pre><strong>$ googleimagesdownload -cf example.json</strong></pre>
<p>The following is a simple example of using keywords and limit arguments:</p>
<pre><strong>$ googleimagesdownload --keywords "apple, beach, cat" --limit 20</strong></pre>
<p>Using suffix keywords allows you to specify words after the main keyword. For example, if the keyword is <kbd>car</kbd> and the suffix keywords are <kbd>red</kbd> and <kbd>blue</kbd>, then it will first search for a red car and then a blue car:</p>
<pre><strong>$ googleimagesdownload --k "car" -sk 'yellow,blue,green' -l 10</strong></pre>
<p>To use the short-hand command, use the following code:</p>
<pre><strong>$ googleimagesdownload -k "apple, beach, cat" -l 20</strong></pre>
<p>To download images with specific image extension, or formats<span>, use the following code</span>:</p>
<pre><strong>$ googleimagesdownload --keywords "logo" --format svg</strong></pre>
<p>To use color filters for the images<span>, use the following code</span>:</p>
<pre><strong>$ googleimagesdownload -k "playground" -l 20 -co red</strong></pre>
<p>To use non-English keywords for image searches<span>, use the following code:</span></p>
<pre><strong>$ googleimagesdownload -k "<img class="alignnone size-full wp-image-914 image-border" src="assets/5a19f409-e391-4710-bbeb-9b051bbcb914.png" style="width:3.00em;height:1.33em;"/>" -l 5</strong></pre>
<p>To download images from the Google Images link<span>, use the following code:</span></p>
<pre><strong>$ googleimagesdownload -k "sample" -u &lt;google images page URL&gt;</strong></pre>
<p>To save images in a specific main directory (instead of in <kbd>Downloads</kbd>)<span>, use the following code:</span></p>
<pre><strong>$ googleimagesdownload -k "boat" -o "boat_new"</strong></pre>
<p>To download one single image within the image URL<span>, use the following code:</span></p>
<pre><strong>$ googleimagesdownload --keywords "baloons" --single_image &lt;URL of the images&gt;</strong></pre>
<p>To download images with size and type constraints<span>, use the following code:</span></p>
<pre><strong>$ googleimagesdownload --keywords "baloons" --size medium --type animated</strong></pre>
<p>To download images with specific usage rights<span>, use the following code:</span></p>
<pre><strong>$ googleimagesdownload --keywords "universe" --usage_rights labeled-for-reuse</strong></pre>
<p>To download images with specific color types<span>, use the following code:</span></p>
<pre><strong>$ googleimagesdownload --keywords "flowers" --color_type black-and-white</strong></pre>
<p>To download images with specific aspect ratios<span>, use the following code:</span></p>
<pre><strong>$ googleimagesdownload --keywords "universe" --aspect_ratio panoramic</strong></pre>
<p>To download images that are similar to the image in the image URL that you provided (known as a reverse image search)<span>, use the following code:</span></p>
<pre><strong>$ googleimagesdownload -si &lt;image url&gt; -l 10</strong></pre>
<p>To download images from a specific website or domain name for a given keyword<span>, use the following code:</span></p>
<pre><strong>$ googleimagesdownload --keywords "universe" --specific_site google.com</strong></pre>
<p>The images will be downloaded to their own sub-directories inside the main directory (either the one you provided or in <kbd>Downloads</kbd>) in the same folder you are in.</p>
<p><span>Now, we need to start preparing our dataset.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dataset preparation</h1>
                </header>
            
            <article>
                
<p>We need to build four different datasets. For  car damage detection, we will think about all the possible inputs. It could be a car in good condition, or a car with different damage levels, or it could be an unrelated image of a car.</p>
<p>We will do the same as shown in the following screenshots:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-763 image-border" src="assets/3e462e68-395a-4695-822d-8bfb139e9001.png" style="width:60.92em;height:31.92em;"/></p>
<p>Here is the dataset to identify heavily damaged cars:</p>
<pre>googleimagesdownload -k "heavily damaged car" -sk 'red,blue,white,black,green,brown,pink,yellow' -l 500</pre>
<p>Here are some sample pictures that were captured for heavily damaged cars that are red:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-764 image-border" src="assets/bd32cffc-c38c-44d6-a8c1-063d1ca5623a.png" style="width:41.58em;height:24.42em;"/></p>
<p><span>Here are some sample pictures that were captured for heavily damaged cars that are blue:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-765 image-border" src="assets/35715172-9404-41d5-9fa0-fcd066f68962.png" style="width:41.00em;height:24.00em;"/></p>
<p>We also have another set of images for cars with slightly less damage:</p>
<pre>googleimagesdownload -k "car dent" -sk 'red,blue,white,black,green,brown,pink,yellow' -l 500</pre>
<p>Here are some sample pictures that were captured for dented cars that are red:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-766 image-border" src="assets/d7311d72-0cb5-46c0-8ff6-be07fa51b7e1.png" style="width:57.00em;height:34.83em;"/></p>
<p><span>Here are some sample pictures that were captured for dented cars that are blue:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-767 image-border" src="assets/e734ec97-4ad5-46ba-a45e-bb5070a9b8c3.png" style="width:50.17em;height:30.92em;"/></p>
<p>The following command can be used to retrieve a dataset for normal cars without any damage applied to them:</p>
<pre><strong>googleimagesdownload -k "car" -l 500</strong></pre>
<p>Here are some sample pictures that have been captured for cars that are red:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-768 image-border" src="assets/1f2f86a4-57d8-488f-b6c5-527c5e76b905.png" style="width:42.42em;height:25.25em;"/></p>
<p><span>Here are some sample pictures that have been captured for cars that are blue:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-769 image-border" src="assets/a0f7a6b5-c7c8-4c31-b52d-4aa8d34ca420.png" style="width:41.42em;height:23.83em;"/></p>
<p>The following command can be used to retrieve random objects that aren't cars:</p>
<pre><strong>googleimagesdownload -k "bike,flight,home,road,tv" -l 500</strong></pre>
<p>Here are some sample pictures that have been captured for bikes:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-770 image-border" src="assets/1190176f-354a-409a-955b-3d1cde7e9751.png" style="width:55.67em;height:34.50em;"/></p>
<p class="mce-root"/>
<p>Here are some sample pictures that have been captured for flights:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-771 image-border" src="assets/5707db31-bdf6-4b65-8f52-28af4fbb343f.png" style="width:49.50em;height:29.00em;"/></p>
<p>Once we have 500 images for each dataset, it's time to do some training. In ideal conditions, we should have at least 1,000 images for each dataset.</p>
<p>The main issue we will face here is in removing the noise data. For our example, we are going to do that manually. There are a few sample images that we have listed here that could be noise, and don't provide valid input so that we can build the data model:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-772 image-border" src="assets/c882ff8e-5bae-45bf-8cab-56de998bc6e6.png" style="width:52.33em;height:34.92em;"/></p>
<p>Once we have all the image datasets ready, we can move on to our top four categories. Right now, all the images are separated by colors and categories, as shown in the following screenshot: </p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-773 image-border" src="assets/fa51d73a-9148-4428-bfea-bc12de51692d.png" style="width:39.33em;height:27.75em;"/></p>
<p>We will group them into <kbd>damaged car</kbd>, <kbd>car with dent</kbd>, <kbd>car</kbd>, and <kbd>not a car</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-774 image-border" src="assets/9fdc8aec-4e65-47a3-a5ae-82556b39d479.png" style="width:14.25em;height:14.75em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running the training script</h1>
                </header>
            
            <article>
                
<p>With all the parameter-related details discussed, we can start the training with the downloaded script:</p>
<pre>python retrain.py \<br/>--bottleneck_dir=./ \<br/>--how_many_training_steps=4000 \<br/>--model_dir=./ \<br/>--output_graph=./retrained_graph.pb \<br/>--output_labels=retrained_labels.txt \<br/>--architecture=" mobilenet_v2_1.4_224" \<br/>--image_dir=/Users/karthikeyan/Documents/ /book/Chapter5/images</pre>
<p>Based on our processor's capability, as well as the number of images we have, the script might take longer for training. For me, it took more than 10 hours for 50 different car categories containing 10,000 images each. Once the script has completed, we will get the TensorFlow model in its output.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up a web application</h1>
                </header>
            
            <article>
                
<p>We will use the <strong>Flask</strong> framework to build a simple application to detect the car's damage.</p>
<div class="packt_tip packt_infobox">To learn more about Flask, please refer to <a href="https://www.fullstackpython.com/flask.html" target="_blank">https://www.fullstackpython.com/flask.html</a>.</div>
<p><span>We are not going to go deeper into Flask basics here. Instead, we are simply adding our model with an existing file upload example from Flask.</span></p>
<p>The file's structure is shown in the following screenshot:</p>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-775 image-border" src="assets/b4e1b059-5fba-4642-a47a-815e114ef66e.png" style="width:19.75em;height:32.92em;"/></p>
<p>Here is a list of the contents in <kbd>app.py</kbd>:</p>
<pre>import os<br/>import glob<br/>from classify import prediction<br/>import tensorflow as tf<br/>import thread<br/>import time<br/>from flask import Flask, render_template, request, redirect, url_for, send_from_directory,flash<br/>from werkzeug import secure_filename<br/>app = Flask(__name__)<br/>app.config['UPLOAD_FOLDER'] = 'uploads/'<br/>app.config['ALLOWED_EXTENSIONS'] = set(['jpg', 'jpeg'])<br/>app.config['SECRET_KEY'] = '7d441f27d441f27567d441f2b6176a'<br/>def allowed_file(filename):<br/>return '.' in filename and \<br/>filename.rsplit('.', 1)[1] in app.config['ALLOWED_EXTENSIONS']<br/>@app.route('/')<br/>def index():<br/>return render_template('index.html')<br/>@app.route('/upload', methods=['POST'])<br/>def upload():<br/>file = request.files['file']<br/>if file and allowed_file(file.filename):<br/>filename = secure_filename(file.filename)<br/>filename = str(len(os.listdir(app.config['UPLOAD_FOLDER']))+1)+'.jpg'<br/>file_name_full_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)<br/>file.save(file_name_full_path)<br/>return render_template('upload_success.html')<br/>@app.route('/uploads/&lt;filename&gt;')<br/>def uploaded_file(filename):<br/>return send_from_directory(app.config['UPLOAD_FOLDER'],<br/>filename)<br/>@app.route('/claim', methods=['POST'])<br/>def predict():<br/>list_of_files = glob.glob('/Users/karthikeyan/Documents/code/play/acko/cardamage/Car-Damage-Detector/uploads/*.jpg') # * means all if need specific format then *.csv<br/>latest_file = max(list_of_files, key=os.path.getctime)<br/>print(latest_file)<br/>image_path = latest_file</pre>
<p>Next code block helps us with printing the output:</p>
<pre>#print(max(glob.glob(r'uploads\*.jpg'), key=os.path.getmtime))<br/>with tf.Graph().as_default():<br/>human_string, score= prediction(image_path)<br/>print('model one value' + str(human_string))<br/>print('model one value' + str(score))<br/>if (human_string == 'car'):<br/>label_text = 'This is not a damaged car with confidence ' + str(score) + '%. Please upload a damaged car image'<br/>print(image_path)<br/>return render_template('front.html', text = label_text, filename="http://localhost:5000/uploads/"+os.path.basename(image_path))<br/>elif (human_string == 'low'):<br/>label_text = 'This is a low damaged car with '+ str(score) + '% confidence.'<br/>print(image_path)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>After printing the image path, go through the following code:</p>
<pre><br/>return render_template('front.html', text = label_text, filename="http://localhost:5000/uploads/"+os.path.basename(image_path))<br/>elif (human_string == 'high'):<br/>label_text = 'This is a high damaged car with '+ str(score) + '% confidence.'<br/>print(image_path)<br/>return render_template('front.html', text = label_text, filename="http://localhost:5000/uploads/"+os.path.basename(image_path))<br/>elif (human_string == 'not'):<br/>label_text = 'This is not the image of a car with confidence ' + str(score) + '%. Please upload the car image.'<br/>print(image_path)<br/>return render_template('front.html', text = label_text, filename="http://localhost:5000/uploads/"+os.path.basename(image_path))<br/>def cleanDirectory(threadName,delay):</pre>
<p>The while loop starts from here:</p>
<pre>while True:<br/>time.sleep(delay)<br/>print ("Cleaning Up Directory")<br/>filelist = [ f for f in (os.listdir(app.config['UPLOAD_FOLDER'])) ]<br/>for f in filelist:<br/>#os.remove("Uploads/"+f)<br/>os.remove(os.path.join(app.config['UPLOAD_FOLDER'], f))<br/>if __name__ == '__main__':<br/>try:<br/>_thread.start_new_thread( cleanDirectory, ("Cleaning Thread", 99999999, ) )<br/>except:<br/>print("Error: unable to start thread" )<br/>app.run()<br/>Classify.py does the model classification using TensorFlow.<br/>import tensorflow as tf<br/>import sys<br/>import os<br/>import urllib</pre>
<p>Disable TensorFlow compilation warnings:</p>
<pre>os.environ['TF_CPP_MIN_LOG_LEVEL']='2'<br/>import tensorflow as tf<br/>def prediction(image_path):<br/>image_data = tf.gfile.FastGFile(image_path, 'rb').read()<br/>print(image_path)<br/>label_lines = [line.rstrip() for line<br/>in tf.gfile.GFile(r"./models/tf_files/retrained_labels.txt")]<br/>with tf.gfile.FastGFile(r"./models/tf_files/retrained_graph.pb", 'rb') as f:<br/>graph_def = tf.GraphDef()<br/>graph_def.ParseFromString(f.read())<br/>_ = tf.import_graph_def(graph_def, name='')<br/>with tf.Session() as sess:</pre>
<p>Once <kbd>image_data</kbd> is given as the input to the graph, we then receive the first prediction:</p>
<pre>softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')<br/>predictions = sess.run(softmax_tensor, \<br/>{'DecodeJpeg/contents:0': image_data})<br/>top_k = predictions[0].argsort()[-len(predictions[0]):][::-1]<br/>for node_id in top_k:<br/>count = 1<br/>human_string = label_lines[node_id]<br/>score = predictions[0][node_id]<br/>print(count)<br/>count += 1<br/>print('%s (score = %.5f)' % (human_string, score))<br/>score = (round((score * 100), 2))<br/>return human_string,score</pre>
<p>The controller Python files are lined in frontend HTML files:</p>
<pre> &lt;!DOCTYPE html&gt;<br/> &lt;html lang="en"&gt;<br/> &lt;head&gt;<br/> &lt;meta charset="utf-8"&gt;<br/> &lt;meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"&gt;<br/> &lt;meta name="description" content=""&gt;<br/> &lt;meta name="author" content="Karthikeyan NG"&gt;<br/> &lt;title&gt;Damage Estimator&lt;/title&gt;<br/> &lt;!-- Bootstrap core CSS --&gt;<br/> &lt;link href="{{ url_for('static', filename='vendor/bootstrap/css/bootstrap.min.css') }}" rel="stylesheet"/&gt;<br/> &lt;!-- Custom fonts for this template --&gt;<br/> &lt;link href="{{ url_for('static', filename='vendor/font-awesome/css/font-awesome.min.css') }}" rel="stylesheet" type="text/css"/&gt;<br/> &lt;link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'&gt;<br/> &lt;link href='https://fonts.googleapis.com/css?family=Merriweather:400,300,300italic,400italic,700,700italic,900,900italic' rel='stylesheet' type='text/css'&gt;<br/> &lt;!-- Plugin CSS --&gt;<br/> &lt;link href="{{ url_for('static', filename='vendor/magnific-popup/magnific-popup.css') }}" rel="stylesheet" /&gt;<br/> &lt;!-- Custom styles for this template --&gt;<br/> &lt;link href="{{ url_for('static', filename='css/creative.min.css') }}" rel="stylesheet" /&gt;<br/> &lt;/head&gt;<br/> &lt;body id="page-top"&gt;<br/> &lt;!-- Navigation --&gt;<br/> &lt;nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav"&gt;<br/> &lt;a class="navbar-brand" href="#page-top"&gt;Damage Estimator&lt;/a&gt;<br/> &lt;button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"&gt;<br/> &lt;span class="navbar-toggler-icon"&gt;&lt;/span&gt;<br/> &lt;/button&gt;<br/> &lt;div class="collapse navbar-collapse" id="navbarResponsive"&gt;<br/> &lt;/div&gt;<br/> &lt;/nav&gt;<br/> &lt;section class="bg-primary" id="about"&gt;<br/> &lt;div class="container"&gt;<br/> &lt;div class="row"&gt;<br/> &lt;div class="col-lg-8 mx-auto text-center"&gt;<br/> &lt;h2 class="section-heading text-white"&gt;Do you have a damaged vehicle?&lt;/h2&gt;<br/> &lt;hr class="light"&gt;<br/> &lt;p class="text-faded"&gt;Machine Learning allows for a classification process that is automated and makes lesser error. Besides risk group classification, Deep Learning algorithms can be applied to images of vehicle damage, allowing for automated claim classification.&lt;/p&gt;<br/> &lt;br/&gt;<br/> &lt;div class="contr"&gt;&lt;h4 class="section-heading text-white"&gt;Select the file (image) and Upload&lt;/h4&gt;&lt;/div&gt;<br/> &lt;br/&gt;<br/> &lt;form action="upload" method="post" enctype="multipart/form-data"&gt;<br/> &lt;div class="form-group"&gt;<br/> &lt;input type="file" name="file" class="file"&gt;<br/> &lt;div class="input-group col-xs-12"&gt;<br/> &lt;span class="input-group-addon"&gt;&lt;i class="glyphicon glyphicon-picture"&gt;&lt;/i&gt;&lt;/span&gt;<br/> &lt;input type="text" class="form-control input-lg" disabled placeholder="Upload Image"&gt;<br/> &lt;span class="input-group-btn"&gt;<br/> &lt;button class="browse btn btn-primary input-lg" type="button"&gt;&lt;i class="glyphicon glyphicon-search"&gt;&lt;/i&gt; Browse&lt;/button&gt;<br/> &lt;/span&gt;<br/> &lt;/div&gt;<br/> &lt;/div&gt;<br/> &lt;input type="submit" class="btn btn-primary" value="Upload"&gt;&lt;br /&gt;&lt;br /&gt;<br/> &lt;/form&gt;<br/> &lt;/div&gt;<br/> &lt;/div&gt;<br/> &lt;/section&gt;</pre>
<p>In continuation with the previous script, let's Bootstrap the core JavaScript:</p>
<pre> &lt;!-- Bootstrap core JavaScript --&gt;<br/> &lt;script src="{{ url_for('static', filename='vendor/jquery/jquery.min.js') }}"&gt;&lt;/script&gt;<br/> &lt;script src="{{ url_for('static', filename='vendor/popper/popper.min.js') }}"&gt;&lt;/script&gt;<br/> &lt;script src="{{ url_for('static', filename='vendor/bootstrap/js/bootstrap.min.js') }}"&gt;&lt;/script&gt;<br/> &lt;!-- Plugin JavaScript --&gt;<br/> &lt;script src="{{ url_for('static', filename='vendor/jquery-easing/jquery.easing.min.js') }}"&gt;&lt;/script&gt;<br/> &lt;script src="{{ url_for('static', filename='vendor/scrollreveal/scrollreveal.min.js') }}"&gt;&lt;/script&gt;<br/> &lt;script src="{{ url_for('static', filename='vendor/magnific-popup/jquery.magnific-popup.min.js') }}"&gt;&lt;/script&gt;<br/> &lt;!-- Custom scripts for this template --&gt;<br/> &lt;script src="{{ url_for('static', filename='js/creative.min.js') }}"&gt;&lt;/script&gt;<br/> &lt;script&gt;<br/> $(document).on('click', '.browse', function(){<br/> var file = $(this).parent().parent().parent().find('.file');<br/> file.trigger('click');<br/> });<br/> $(document).on('change', '.file', function(){<br/> $(this).parent().find('.form-control').val($(this).val().replace(/C:\\fakepath\\/i, ''));<br/> });<br/> &lt;/script&gt;<br/> &lt;/body&gt;<br/> &lt;/html&gt;</pre>
<p>You can pull the rest of the file's content directly from the GitHub repository. Once the complete file structure is ready, you can run the application from the command line, as follows:</p>
<pre><strong>$ python app.py</strong></pre>
<p class="mce-root"/>
<p>Now, launch your browser with <kbd>http://localhost:5000/</kbd><span>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-854 image-border" src="assets/70376108-e081-4f77-985a-fdc6d7e28fe1.png" style="width:54.92em;height:33.08em;"/></p>
<p>The following are a few screenshots from the application.</p>
<p>Here is the home page after running the application:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-777 image-border" src="assets/900aed2c-449d-4a99-8f6e-f12f7d17c09c.png" style="width:41.17em;height:19.08em;"/></p>
<p>Here is the screen after uploading the image:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-778 image-border" src="assets/fa01449b-871f-418b-b211-b2e022243fc1.png" style="width:51.67em;height:22.33em;"/></p>
<p>Here is a screenshot showing an image of a car with low level damage</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-779 image-border" src="assets/6df28f3f-8d2a-40d8-b8a9-8d0dc8300130.png" style="width:41.25em;height:24.92em;"/></p>
<p>The data in the preceding screenshot may not be accurate, since our dataset size is very small.</p>
<p>The following is a screenshot showing an image of the car prediction model that does not show a car:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-780 image-border" src="assets/16421a34-bccf-4996-821e-a67347a7d33d.png" style="width:34.17em;height:20.50em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have seen how we can build a model from scratch and train it using TensorFlow.</p>
<p>With this knowledge, we can start building more Android and iOS-based applications in the upcoming chapters.</p>


            </article>

            
        </section>
    </body></html>