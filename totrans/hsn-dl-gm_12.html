<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Imitation and Transfer Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span><span>At </span></span>the time of writing, a new AI called AlphaStar, a <strong>deep reinforcement learning</strong> (<strong>DRL</strong>) agent, used <strong>imitation learning</strong> (<strong>IL</strong>) to beat a human opponent five-nil playing the real-time strategy game StarCraft II. AlphaStar was the continuation of David Silver and Google DeepMind's work to build a smarter and more intelligent AI. The specific techniques AlphaStar used to win could fill a book, and IL and the use of learning to copy human play is now of keen interest. Fortunately, Unity has already implemented IL in the form of offline and online training scenarios. While we won't make it to the level of AlphaStar in this chapter, we still will learn about the underlying technologies of IL and other forms of transfer learning. </p>
<p>In this chapter, we will look at the implementation of IL in ML-Agents and then look to other applications of transfer learning. We will cover the following topics in this chapter:</p>
<ul>
<li>IL or behavioral cloning</li>
<li>Online training</li>
<li>Offline training</li>
<li>Transfer Learning</li>
<li>Imitation Transfer Learning</li>
</ul>
<div class="packt_infobox">While AlphaStar performed a stunning tactical victory against a human pro player in an RTS game, it has still come under scrutiny for the type of play and actions it used. Many human players stated that the AI's tactical abilities were clearly superior, but the overall strategy and planning were abysmal. It should be interesting to see how Google DeepMind approaches this criticism.</div>
<p>This will be an exciting chapter, and will provide you with plenty of training possibilities for your future developments, which all starts in the next section.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">IL, or behavioral cloning</h1>
                </header>
            
            <article>
                
<p>IL, or behavioral cloning, is the process by which observations and actions are captured from a human, or perhaps another AI, and used as input into training an agent. The agent essentially becomes guided by the human and learns by their actions and observations. A set of learning observations can be received by real-time play (online) or be extracted from saved games (offline). This provides the ability to capture play from multiple agents and train them in tandem or individually. IL provides the ability to train or, in effect, program agents for tasks you may find impossible to train for using regular RL, and because of this, it will likely become a key RL technique that we use for most tasks in the near future.</p>
<p>It is hard to gauge the value something gives you until you see what things are like without it. With that in mind, we will first start by looking at an example that uses no IL, but certainly could benefit from it. Open up the Unity editor and follow this exercise:</p>
<ol>
<li>Open up the <span class="packt_screen">Tennis</span> scene from the <span class="packt_screen">Assets</span> | <span class="packt_screen">ML-Agents</span> | <span class="packt_screen">Examples</span> | <span class="packt_screen">Tennis</span> | <span class="packt_screen">Scenes</span> folder.</li>
<li>Select and disable the extra agent training areas, <span class="packt_screen">TennisArea(1)</span> to <span class="packt_screen">TennisArea(17)</span>.</li>
<li>Select <span class="packt_screen">AgentA</span> and make sure <span class="packt_screen">Tennis Agent</span> | <span class="packt_screen">Brain</span> is set to <span class="packt_screen">TennisLearning</span>. We want each agent to be against the other agent in this example.</li>
<li>Select <span class="packt_screen">AgentB</span> and make sure <span class="packt_screen">Tennis Agent</span> | <span class="packt_screen">Brain</span> is set to <span class="packt_screen">TennisLearning</span>. <br/>
In this example, for a brief instance, we are training multiple agents in the same environment. We will cover more scenarios where agents play other agents as a way of learning in <a href="15e7adeb-8b67-4b93-81d4-5f129772cd97.xhtml">Chapter 11</a>, <em>Building Multi-Agent Environments</em>.</li>
<li>Select <span class="packt_screen">Academy</span> and make sure that <span class="packt_screen">Tennis Academy</span> | <span class="packt_screen">Brains</span> is set to <span class="packt_screen">TennisLearning</span> and the <span class="packt_screen">Control</span> option is enabled, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/cd61436d-3edf-44b8-9966-13f9ab48b633.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Setting Control to enabled on Academy</span></div>
<ol start="6">
<li>Open a Python/Anaconda window and prepare it for training. We will launch training with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=tennis --train</strong></pre>
<ol start="7">
<li>Watch the training for several thousand iterations, enough to convince yourself the agents are not going to learn this task easily. When you are convinced, stop the training and move on.</li>
</ol>
<p>You can see by just looking at this first example that ordinary training and the other advanced methods we looked at, such as Curriculum and Curiosity Learning, would be difficult to implement, and in this case could be counterproductive. In the next section, we look at how to run this example with IL in online training mode.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Online training</h1>
                </header>
            
            <article>
                
<p>Online Imitation Learning is where you teach the agent to learn the observations of a player or another agent in real time. It also is one of the most fun and engaging ways to train agents or bots. Let's jump in and set up the tennis environment for online Imitation Learning in the next exercise:</p>
<ol>
<li>Select the <span class="packt_screen">TennisArea</span> | <span class="packt_screen">AgentA</span> object and set <span class="packt_screen">Tennis Agent</span> | <span class="packt_screen">Brain</span> to <span class="packt_screen">TennisPlayer</span>. In this IL scenario, we have one brain acting as a teacher, the player, and a second brain acting as the student, the learner.</li>
<li>Select the <span class="packt_screen">AgentB</span> object and make sure <span class="packt_screen">Tennis Agent</span> | <span class="packt_screen">Brain</span> is set to <span class="packt_screen">TennisLearning</span>. This will be the student brain.</li>
<li>Open the <kbd>online_bc_config.yaml</kbd> <span>file from the <kbd>ML-Agents/ml-agents/config</kbd> folder. IL does not use the same configuration as PPO so the parameters will have similar names but may not respond to what you have become used to.</span></li>
<li>Scroll down in the file to the <strong><kbd>TennisLearning</kbd></strong> brain configuration as shown in the following code snippet:</li>
</ol>
<pre style="padding-left: 60px"> TennisLearning:<br/>    <strong>trainer: online_bc</strong><br/>    max_steps: 10000<br/>    summary_freq: 1000<br/>    <strong>brain_to_imitate: TennisPlayer</strong><br/>    batch_size: 16<br/>    batches_per_epoch: 5<br/>    num_layers: 4<br/>    hidden_units: 64<br/>    use_recurrent: false<br/>    sequence_length: 16</pre>
<ol start="5">
<li>Looking over the hyperparameters, we can see there are two new parameters of interest. A summary of those parameters is as follows:
<ul>
<li><kbd>trainer</kbd>: <kbd>online_</kbd> <em>or</em> <kbd>offline_bc</kbd>—using online or offline Behavioral Cloning. In this case, we are performing online.</li>
<li><kbd>brain_to_imitate</kbd>: <kbd>TennisPlayer</kbd>—this sets the brain that the learning brain should attempt to imitate.<br/>
<em>We won't make any changes to the file at this point.</em></li>
</ul>
</li>
<li>Open your prepared Python/Anaconda window and launch training with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>mlagents-learn config/online_bc_config.yaml </span>--run-id=tennis_il <span>--train --slow</span></strong></pre>
<ol start="7">
<li>After you press <span class="packt_screen">Play</span> in the editor, you will be able to control the left paddle with the <em>W</em>, <em>A</em>, <em>S</em>, <em>D</em> keys. Play the game, and you may be surprised at how quickly the agent learns and can get quite good. The following is an example of the game being played:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/77df8273-6f46-4876-aa07-4564b8092696.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Playing and teaching the agent with IL</span></div>
<ol start="8">
<li>Keep playing the example until completion if you like. It can also be interesting to switch players during a game, or even train the brain and use the trained model to play against later. You do remember how to run a trained model, right?</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>
<p>At some point while playing through the last exercise, you may have wondered why we don't we train all RL agents this way. A good question, but as you can imagine, it depends. While IL is very powerful, and quite a capable learner, it doesn't always do what we expect it to do. Also, an IL agent is only going to learn the search space (observations) it is shown and remain within those limitations. In the case of AlphaStar, IL was the main input for training, but the team also mentioned that the AI did have plenty of time to self-play, which likely accounted for many of its winning strategies. So, while IL is cool and powerful, it is not the golden goose that will solve all our RL problems. However, you are likely to have a new and greater appreciation for RL, and in particular IL, after this exercise. In the next section, we explore using offline IL.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Offline training</h1>
                </header>
            
            <article>
                
<p>Offline training is where a recorded gameplay file is generated from a player or agent playing a game or performing a task, and is then fed back as training observations to help an agent learn later on. While online learning certainly is more fun, and in some ways more applicable to the Tennis scene or other multiplayer games, it is less practical. After all, you generally need to play an agent in real time for several hours before an agent will become good. Likewise, in online training scenarios, you are typically limited to single agent training, whereas in offline training a demo playback can be fed to multiple agents for better overall learning. This also allows us to perform interesting training scenarios, similar to AlphaStar training, where we can teach an agent so that it can teach other agents. </p>
<div class="packt_infobox">We will learn more about multi-agent gameplay in <a href="15e7adeb-8b67-4b93-81d4-5f129772cd97.xhtml">Chapter 11</a>, <em>Building Multi-Agent Environments</em>.</div>
<p>For this next exercise, we are going to revisit our old friend the <span class="packt_screen">Hallway</span>/<span class="packt_screen">VisualHallway</span> example. Again, we are doing this so we can compare our results to the previous sample exercises we ran with this environment. Follow this exercise to set up a new offline training session:</p>
<ol>
<li>Clone and download the ML-Agents code to a new folder, perhaps choosing <kbd>ml-agents_b</kbd>, <kbd>ml-agents_c</kbd>, or some other name. The reason we do this is to make sure that we run these new exercises with a clean environment. Also, it can sometimes help to go back to old environments and recall settings or configuration that you may forget to update. </li>
<li>Launch Unity and open the <strong><span class="packt_screen">UnitySDK</span></strong> project and the <span class="packt_screen">Hallway</span> or <span class="packt_screen">VisualHallway</span> scene, your choice.</li>
</ol>
<ol start="3">
<li>The scene should be set to run in <span class="packt_screen">Player</span> mode. Just confirm this. If you need to change it, then do so.</li>
<li>Disable any additional agent training environments in the scene if others are active.</li>
<li>Select <span class="packt_screen">HallwayArea</span> | <span class="packt_screen">Agent</span> in the <span class="packt_screen">Hierarchy</span> window.</li>
<li>Click the <span class="packt_screen">Add Component</span> button at the bottom of the <span class="packt_screen">Inspector</span> window, type <kbd>demo</kbd>, and select the <span class="packt_screen">Demonstration Recorder</span> component as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/76cb16eb-4858-4a5c-b6a5-a95d32b15a5f.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Adding a Demonstration Recorder</span></div>
<ol start="7">
<li>Click <span class="packt_screen">Record</span> on the new <span class="packt_screen">Demonstration Recorder</span> component, as shown in the preceding screenshot, check throughout. Also, fill in the <span class="packt_screen">Demonstration Name</span> property of the recording, which is also shown.</li>
<li>Save the scene and project.</li>
<li>Press <span class="packt_screen">Play</span> and play the scene for a fair amount of time, more than a few minutes but perhaps less than hours. Of course, how well you play will also determine how well the agent learns. If you play poorly, so will the agent.</li>
<li>After you think enough time has passed, and you have played as well as you could, stop the game.</li>
</ol>
<p>After playing the game, you should see a new folder called <span class="packt_screen">Demonstrations</span> created in the <span class="packt_screen">Assets</span> root folder in your <span class="packt_screen">Project</span> window. Inside the folder will be your demonstration recording. This is the recording we will feed the agent in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up for training</h1>
                </header>
            
            <article>
                
<p>Now that we have our demonstration recording, we can do more on the training part. This time, however, we will play back our observation file to multiple agents in multiple environments. Open the <span class="packt_screen">Hallway</span>/<span class="packt_screen">VisualHallway</span> sample scene and follow the next exercise to set up for training:</p>
<ol>
<li>Select and enable all the <span class="packt_screen">HallwayArea</span> training environments <span class="packt_screen">HallwayArea(1)</span> to <span class="packt_screen">HallwayArea(15)</span></li>
<li>Select <span class="packt_screen">HallwayArea</span> | <span class="packt_screen">Agent</span> in the <span class="packt_screen">Hierarchy</span> and then switch <span class="packt_screen">Hallway Agent</span> | <span class="packt_screen">Brain</span> to <span class="packt_screen">HallwayLearning</span>, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/984fc851-9110-491a-83b8-37da2b240c42.png" style="width:30.67em;height:41.17em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Setting the agent components</span></div>
<ol start="3">
<li>Also, select and disable the <span class="packt_screen">Demonstration Recording</span> component as shown in the preceding screen excerpt</li>
<li>Make sure all the agents in the scene are using <span class="packt_screen">HallwayLearning</span> brains</li>
</ol>
<ol start="5">
<li>Select <span class="packt_screen">Academy</span> in the <span class="packt_screen">Hierarchy</span> and then enable the <span class="packt_screen">Hallway Academy</span> | <span class="packt_screen">Brains</span> | <span class="packt_screen">Control</span> option as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/85138784-78f7-4990-ab15-0c0b38439ae5.png" style="width:31.50em;height:25.92em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Enabling Academy to Control the Brains</span></div>
<ol start="6">
<li>Save the scene and project</li>
</ol>
<p>Now that we have the scene configured for agent learning, we can move on to feeding the agent in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feeding the agent</h1>
                </header>
            
            <article>
                
<p>When we performed online IL, we only fed one agent at a time in the tennis scene. This time, however, we are going to train multiple agents from the same demonstration recording in order to improve training performance.</p>
<p>We have already set up for training, so let's start feeding the agent in the following exercise:</p>
<ol>
<li><span>Open a</span> Python/Anaconda <span>window and set it up for training from the new</span> <kbd>ML-Agents</kbd> <span>folder. You did reclone the source, right?</span></li>
<li>Open the <kbd>offline_bc_config.yaml</kbd> file from the <kbd>ML-Agents/ml-agents_b/config</kbd> folder. The contents of the file are as follows for reference:</li>
</ol>
<pre style="padding-left: 60px">default:<br/>    trainer: offline_bc<br/>    batch_size: 64<br/>    summary_freq: 1000<br/>    max_steps: 5.0e4<br/>    batches_per_epoch: 10<br/>    use_recurrent: false<br/>    hidden_units: 128<br/>    learning_rate: 3.0e-4<br/>    num_layers: 2<br/>    sequence_length: 32<br/>    memory_size: 256<br/>    demo_path: ./UnitySDK/Assets/Demonstrations/&lt;Your_Demo_File&gt;.demo<br/><br/>HallwayLearning:<br/>    trainer: offline_bc<br/>    max_steps: 5.0e5<br/>    num_epoch: 5<br/>    batch_size: 64<br/>    batches_per_epoch: 5<br/>    num_layers: 2<br/>    hidden_units: 128<br/>    sequence_length: 16<br/>    use_recurrent: true<br/>    memory_size: 256<br/>    sequence_length: 32<br/>    <strong>demo_path: ./UnitySDK/Assets/Demonstrations/demo.demo</strong></pre>
<ol start="3">
<li>Change the last line of the <kbd>HallwayLearning</kbd> or <kbd>VisualHallwayLearning</kbd> brain to the following:</li>
</ol>
<pre style="padding-left: 60px"><strong>HallwayLearning:</strong><br/>    trainer: offline_bc<br/>    max_steps: 5.0e5<br/>    num_epoch: 5<br/>    batch_size: 64<br/>    batches_per_epoch: 5<br/>    num_layers: 2<br/>    hidden_units: 128<br/>    sequence_length: 16<br/>    use_recurrent: true<br/>    memory_size: 256<br/>    sequence_length: 32<br/>    <strong>demo_path: ./UnitySDK/Assets/Demonstrations/AgentRecording.demo</strong></pre>
<ol start="4">
<li>Note that if you are using the <kbd>VisualHallwayLearning</kbd> brain, you will need to also change the name in the preceding config script.</li>
<li>Save your changes when you are done editing.</li>
<li>Go back to your Python/Anaconda window and launch training with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>mlagents-learn config/offline_bc_config.yaml --run-id=hallway_il --train</strong></pre>
<ol start="7">
<li>When prompted, press <span class="packt_screen">Play</span> in the editor and watch the training unfold. You will see the agent play using very similar moves to yourself, and if you played well, the agent will quickly start learning and you should see some impressive training, all thanks to IL.</li>
</ol>
<p>RL can be thought of as the brute-force approach to learning, while the refinement of Imitation Learning and training by observation will clearly dominate the future of agent training. Of course, is it really any wonder? After all, we simple humans learn that way.</p>
<p>In the next section, we look at another exciting area of deep learning, transfer learning, and how it applies to games and DRL.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transfer learning</h1>
                </header>
            
            <article>
                
<p>Imitation Learning, by definition, falls into a category of <strong>Transfer Learning</strong> (<strong>TL</strong>). We can define Transfer Learning as the process by which an agent or DL network is trained by transference of experiences from one to the other. This could be as simple as the observation training we just performed, or as complex as swapping layers/layer weights in an agent's brain, or just training an agent on a similar task. </p>
<p>Intransfer learningwe need to make sure the experiences or previous weights we use are generalized. Through the foundational chapters in this book (chapters 1-3), we learned the value of generalization using techniques such as dropout and batch normalization. We learned that these techniques are important for more general training; the form of training that allows the agent/network better inference on test data. This is no different than if we were to use an agent trained on one task to learn on another task. A more general agent will, in effect, be able to transfer knowledge more readily than a specialist agent could, if at all.</p>
<p>We can demonstrate this in a quick example starting with training the following simple exercise:</p>
<ol>
<li>Open up the <span class="packt_screen">VisualHallway</span> scene in the Unity editor.</li>
<li>Disable any additional training areas.</li>
<li>Confirm that <span class="packt_screen">Academy</span> is in <span class="packt_screen">Control</span> of the <span class="packt_screen">Brain</span>.</li>
<li>Select the <span class="packt_screen">VisualHallwayLearning</span> brain from the <span class="packt_screen">Hallway</span>/<span class="packt_screen">Brains</span> folder and set <span class="packt_screen">Vector Action</span> | <span class="packt_screen">Branches Size</span> | <span class="packt_screen">Branch 0 Size</span><span class="packt_screen"> </span>to <kbd>7</kbd>,<span class="packt_screen"> </span>as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4d4cb9a9-503a-43d5-8c7f-bdb2c1970824.png" style="width:32.08em;height:23.17em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Increasing the vector action space of the agent</span></div>
<ol start="5">
<li>We increase the action space for the brain so that it is compatible with the required action space for our transfer learning environment, which we will get to later.</li>
<li>Save the scene and project.</li>
<li>Open a Python/Anaconda window that is prepared for training.</li>
<li>Launch a training session with the following code:</li>
</ol>
<pre style="padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=vishall --train  --save-freq=10000</strong></pre>
<ol start="9">
<li>Here, we have introduced a new parameter that controls the frequency at which model checkpoints are created. The default is currently set to 50,000, but we just don't want to wait that long.</li>
<li>Run the agent in training in the editor for at least one model checkpoint save, as shown in the following screen excerpt:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/32d4ca69-78e1-4a52-b2a4-f386d69cf508.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>The ML-Agents trainer creating a checkpoint</span></div>
<ol start="11">
<li>Checkpoints are a way of taking snapshots of a brain and saving them for later. This allows you to go back and continue training where you left off.</li>
<li>Let the agent train to a checkpoint and then terminate training by pressing <em>Ctrl </em>+ <em>C </em>or c<em>ommand </em>+ <em>C</em> on Mac in the Python/Anaconda window.</li>
</ol>
<p>When you have terminated training, it is time to try this saved brain on another learning environment in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transferring a brain</h1>
                </header>
            
            <article>
                
<p>We now want to take the brain we have just been training and reuse it in a new, but similar, environment. Since our agent uses visual observations, this makes our task easier, but you could try and perform this example with other agents as well.</p>
<p>Let's open<span> Unity and navigate to the</span> <span class="packt_screen">VisualPushBlock</span> <span>example scene and follow this exercise:</span></p>
<ol>
<li>Select <span class="packt_screen">Academy</span> and enable it for <span class="packt_screen">Control</span> of the <span class="packt_screen">Brains.</span></li>
<li>Select the <span class="packt_screen">Agent</span> and set it to use the <span class="packt_screen">VisualPushBlockLearning</span> brain. You should also confirm that this brain is configured in the same way as the <span class="packt_screen">VisualHallwayLearning</span> brain we just ran, meaning that the <span class="packt_screen">Visual Observation</span> and <span class="packt_screen">Vector Action</span> spaces match.</li>
<li>Open the <kbd>ML-Agents/ml-agents_b/models/vishall-0</kbd> folder in File Explorer or another file explorer.</li>
<li>Change the name of the file and folder from <kbd>VisualHallwayLearning</kbd> to <kbd>VisualPushBlockLearning</kbd> as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d71f90f1-f500-4065-bc70-994600137b07.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Changing the model path manually</span></div>
<ol start="5">
<li>By changing the name of the folder, we are essentially telling the model loading system to restore our <span class="packt_screen">VisualHallway</span> brain as <span class="packt_screen">VisualPushBlockBrain</span>. The trick here is making sure that both brains have all the same hyperparameters and configuration settings.</li>
</ol>
<ol start="6">
<li>Speaking of hyperparameters, open the <kbd>trainer_config.yaml</kbd> file and make sure that the <span class="packt_screen">VisualHallwayLearning</span> and <span class="packt_screen">VisualPushBlockLearning</span> parameters are the same. The configuration for both is shown in the following code snippet for reference:</li>
</ol>
<pre style="padding-left: 60px">VisualHallwayLearning:<br/>    use_recurrent: true<br/>    sequence_length: 64<br/>    num_layers: 1<br/>    hidden_units: 128<br/>    memory_size: 256<br/>    beta: 1.0e-2<br/>    gamma: 0.99<br/>    num_epoch: 3<br/>    buffer_size: 1024<br/>    batch_size: 64<br/>    max_steps: 5.0e5<br/>    summary_freq: 1000<br/>    time_horizon: 64<br/><br/>VisualPushBlockLearning:<br/>    use_recurrent: true<br/>    <strong>sequence_length: 64</strong><br/>    num_layers: 1<br/>    hidden_units: 128<br/>    memory_size: 256<br/>    beta: 1.0e-2<br/>    gamma: 0.99<br/>    num_epoch: 3<br/>    buffer_size: 1024<br/>    batch_size: 64<br/>    max_steps: 5.0e5<br/>    summary_freq: 1000<br/>    time_horizon: 64</pre>
<ol start="7">
<li>Save the configuration file when you are done editing.</li>
<li>Open your Python/Anaconda window and launch training with the following code:</li>
</ol>
<pre style="padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=vishall --train --save-freq=10000 --load</strong></pre>
<ol start="9">
<li>The previous code is not a misprint; it is the exact same command we used to run the <span class="packt_screen">VisualHallway</span> example, except with <kbd>--load</kbd> appended on the end. This should launch the training and prompt you to run the editor.</li>
<li>Feel free to run the training for as long as you like, but keep in mind that we barely trained the original agent.</li>
</ol>
<p>Now, in this example, even if we had trained the agent to complete <span class="packt_screen">VisualHallway</span>, this likely would not have been very effective in transferring that knowledge to <span class="packt_screen">VisualPushBlock</span>. For the purposes of this example, we chose both since they are quite similar, and transferring one trained brain to the other was less complicated. For your own purposes, being able to transfer trained brains may be more about retraining agents on new or modified levels, perhaps even allowing the agents to train on progressively more difficult levels.</p>
<p><span>Depending on your version of ML-Agents, this example may or may not work so well. The particular problem is the complexity of the model, number of hyperparameters, input space, and reward system that we are running. Keeping all of these factors the same also requires keen attention to detail. In the next section, we will take a short diversion to explore how complex these models are.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring TensorFlow checkpoints</h1>
                </header>
            
            <article>
                
<p>TensorFlow is quickly becoming the underlying graph calculation engine that is powering most deep learning infrastructure. While we haven't covered how these graph engines are constructed in much detail, it can be helpful to review these TensorFlow models visually. Not only can we start to appreciate the complexity of these systems better, but a good visual is often worth a thousand words. Let's open up a web browser and follow the next exercise:</p>
<ol>
<li>Search for the phrase <kbd>netron tensorflow</kbd> in your browser with your favorite search engine. Netron is an OpenSource TensorFlow model viewer that is perfect for our needs.</li>
<li>Find a link to the <span class="packt_screen">GitHub</span> page and on the page the links to download the binary installers. Select the installer for your platform and click <span class="packt_screen">Download</span>. This will take you to another download page where you can select the file for download.</li>
<li>Use the installer for your platform to install the <span class="packt_screen">Netron</span> application. On Windows, this is as simple as downloading the exe installer and running it. </li>
</ol>
<ol start="4">
<li>Run the <span class="packt_screen">Netron</span> application, and after it launches, you will see the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/95bc6be0-a29e-4677-9a94-c3686ce9f21f.png" style="width:39.33em;height:29.67em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>The Netron application</span></div>
<ol start="5">
<li>Click the <span class="packt_screen">Open Model...</span> button in the middle of the window</li>
</ol>
<ol start="6">
<li>Use File Explorer to locate the <kbd>ML-Agents/ml-agents/models/vishall-0\VisualHallwayLearning</kbd> folder, and locate the <kbd>raw_graph.def</kbd> file as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ef7efa51-c1a1-4b3d-b189-619ab4a4f0c4.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Selecting the model graph definition to load</span></div>
<ol start="7">
<li>After loading the graph, use the - button in the top-right to zoom the view as far out as you can, similar to the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/091ce181-d2c3-428c-bc00-a6ce885bb62c.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>The TensorFlow graph model of our agent's brain</span></div>
<ol start="8">
<li>As the inset shows, this graph is beyond complex, and not something we would be easily able to make sense of. However, it can be interesting to look through and see how the model/graph is constructed.</li>
</ol>
<ol start="9">
<li>Scroll to the top of the graph and find a node called <span class="packt_screen">advantages</span>, then select the node and note the <span class="packt_screen">Graph</span> and <span class="packt_screen">Inputs</span>, model properties as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/366e58fa-35bf-4cb4-be05-4b6c39f238fa.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Properties of the advantages graph model</span></div>
<ol start="10">
<li>Within the properties view of this model, you should be able to see some very familiar terms and settings, such as <span class="packt_screen">visual_observation_0</span>, for instance, which shows the model input is a tensor of shape [84,84,3].</li>
</ol>
<p>When you are done, feel free to look over other models, and perhaps even explore with other models even outside Unity. While this tool isn't quite capable of summarizing a complex model like we have, it does show how powerful these types of tools are becoming. What's more, if you can find your way around, you can even export variables for later inspection or use. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Imitation Transfer Learning</h1>
                </header>
            
            <article>
                
<p>One of the problems with Imitation Learning is that it often focuses the agent down a path that limits its possible future moves. This isn't unlike you being shown the improper way to perform a task and then doing it that way, perhaps without thinking, only to find out later that there was a better way. Humanity, in fact, has been prone to this type of problem over and over again throughout history. Perhaps you learned as a child that swimming right after eating was dangerous, only to learn later in life through your own experimentation, or just common knowledge, that that was just a myth, a myth that was taken as fact for a very long time. Training an agent through observation is no different you limit the agent's vision <span>in many ways</span> to a narrow focus that is limited by what it was taught. However, there is a way to allow an agent to revert back to the partial brute-force or trial-and error exploration in order to expand its training. </p>
<p>With ML-Agents we can combine IL with a form oftransfer learningin order to allow an agent to learn first from observation, then by furthering its training by learning from the once student. This form of IL chaining, if you will, allows you to train an agent to auto-train multiple agents. Let's open up Unity to the <span class="packt_screen">TennisIL</span> scene and follow the next exercise:</p>
<ol>
<li>Select the <span class="packt_screen">TennisArea</span> | <span class="packt_screen">Agent</span> object and in the <span class="packt_screen">Inspector</span>,<span class="packt_screen"> </span>disable the <span class="packt_screen">BC Teacher Helper</span> component, and then add a new <span class="packt_screen">Demonstration Recorder</span> as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/65d19e63-93e6-436c-85c1-02a528d4c683.png" style="width:33.25em;height:40.83em;"/></p>
<p class="mce-root"/>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Checking that the BC Teacher is attached to the Agent</span></div>
<ol start="2">
<li><span class="packt_screen">BC Teacher Helper</span> is a recorder that works just like the <span class="packt_screen">Demonstration Recorder</span>. The BC recorder allows you to turn the recording on and off as the agent runs, which is perfect for online training, but at the time of writing, the component was not working.</li>
</ol>
<ol start="3">
<li>Make sure <span class="packt_screen">Academy</span> is set to <span class="packt_screen">Control</span> the <span class="packt_screen">TennisLearning</span> brain.</li>
<li>Save the scene and project.</li>
<li>Open a Python/Anaconda window and launch training with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>mlagents-learn config/online_bc_config.yaml --run-id=tennis_il --train --slow</strong></pre>
<ol start="6">
<li>Press <span class="packt_screen">Play</span> when prompted to run the game in the editor. Control the blue paddle with the <em>W</em>, <em>A</em>, <em>S</em>, <em>D</em> keys and play for a few seconds to warm up.</li>
<li>After you are warmed up, press the <em>R</em> key to begin recording a demo observation. Play the game for several minutes and let the agent become capable. After the agent is able to return the ball, stop the training session.</li>
</ol>
<p>This will not only train the agent, which is fine, but it will also create a demo recording playback we can use to further train the agents to learn how to play each other in a similar way to how AlphaStar was trained. We will set up our tennis scene to now run in offline training mode with multiple agents in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training multiple agents with one demonstration</h1>
                </header>
            
            <article>
                
<p>Now, with the recording of us playing tennis, we can use this to feed into the training of multiple agents all feeding back into one policy. Open Unity to the tennis scene, the one with the multiple environments, and follow the next exercise:</p>
<ol>
<li>Type <kbd>agent</kbd> into the Filter bar at the top of the <span class="packt_screen">Hierarchy</span> window as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/88535449-155c-425c-911b-f57a448b2b5d.png" style="width:32.08em;height:42.67em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Searching for all the agents in the scene</span></div>
<ol start="2">
<li>Select all the agent objects in the scene and bulk change their <span class="packt_screen">Brain</span> to use <span class="packt_screen">TennisLearning</span> and not <span class="packt_screen">TennisPlayer</span>.</li>
<li>Select <span class="packt_screen">Academy</span> and make sure to enable it to control the brains.</li>
<li>Open the <kbd>config/offline_bc_config.yaml</kbd> file.</li>
</ol>
<ol start="5">
<li>Add the following new section for the <kbd>TennisLearning</kbd> brain at the bottom:</li>
</ol>
<pre style="padding-left: 60px">TennisLearning:<br/>    trainer: offline_bc<br/>    max_steps: 5.0e5<br/>    num_epoch: 5<br/>    batch_size: 64<br/>    batches_per_epoch: 5<br/>    num_layers: 2<br/>    hidden_units: 128<br/>    sequence_length: 16<br/>    use_recurrent: true<br/>    memory_size: 256<br/>    sequence_length: 32<br/>    demo_path: ./UnitySDK/Assets/Demonstrations/TennisAgent.demo</pre>
<ol start="6">
<li>Save the scene and the project.</li>
<li>Open the Python/Anaconda window and run training with the following code:</li>
</ol>
<pre style="padding-left: 60px"><strong>mlagents-learn config/offline_bc_config.yaml --run-id=tennis_ma --train</strong></pre>
<ol start="8">
<li>You may want to add the <kbd>--slow</kbd> switch in order to watch the training, but it should not be required.</li>
<li>Let the agents train for some time and notice its improved progress. Even with a short observation recording input, the agent becomes a capable player rather quickly.</li>
</ol>
<p>There are multiple ways to perform this type of IL andtransfer learningchaining that will allow your agent some flexibility in training. You could even use the trained model's checkpoint without IL and run the agents with transfer learning as we did earlier. The possibilities are limitless, and it remains to be seen what will emerge as best practices.</p>
<p>In the next section, we'll provide some exercises that you can use for your own personal learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<p>The exercises at the end of this chapter could likely provide several hours of fun. Try and only complete one or two exercises, as we still need to finish the book:</p>
<ol>
<li>Set up and run the <span class="packt_screen">PyramidsIL</span> scene to run online IL.</li>
<li>Set up and run the <span class="packt_screen">PushBlockIL</span> scene to run online IL.</li>
<li>Set up and run the <span class="packt_screen">WallJump</span> scene to run with online IL. This requires you to modify the scene.</li>
<li>Set up and run the <span class="packt_screen">VisualPyramids</span> scene to use offline recording. Record a training session then train an agent.</li>
<li>Set up and run the <span class="packt_screen">VisualPushBlock</span> scene to use offline recording. Use offline IL to train the agent.</li>
<li>Set up the <span class="packt_screen">PushBlockIL</span> scene to record an observation demo. Then use this offline training to train multiple agents in the regular <span class="packt_screen">PushBlock</span> scene.</li>
<li>Set up the <span class="packt_screen">PyramidsIL</span> scene to record a demo recording. Then use this for offline training to train multiple agents in the regular <span class="packt_screen">Pyramids</span> scene.</li>
<li>Train an agent in the <span class="packt_screen">VisualHallway</span> scene using any form of learning you like. After training, modify the <span class="packt_screen">VisualHallway</span> scene to use different materials on the walls and floor. Changing materials on Unity objects is quite easy. Then, use the technique of swapping model checkpoints as a way of transfer learning the previously trained brain into a new environment.</li>
<li>Do exercise eight, but using the <span class="packt_screen">VisualPyramids</span> scene. You could also add other objects or blocks in this scene.</li>
<li>Do exercise eight, but using the <span class="packt_screen">VisualPushBlock</span> scene. Try adding other blocks or other objects that the agent may have to work around.</li>
</ol>
<p>Just remember that, if you are attempting any of the Transfer Learning exercises, attention to detail is important when matching the complex graphs. In the next section, we summarize what we have covered in this chapter.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we covered an emerging technique in RL called Imitation Learning or Behavioral Cloning. This technique, as we learned, takes the captured observations of a player playing a game and then uses those observations in an online or offline setting to further train the agent. We further learned that IL is just a form of Transfer Learning. We then covered a technique with ML-Agents that will allow you to transfer brains across environments. Finally, we looked at how to chain IL andtransfer learningas a way of stimulating the agent's training into developing new strategies on its own. </p>
<p>In the next chapter, we will further our understanding of DRL in games by looking at multiple agent training scenarios.</p>


            </article>

            
        </section>
    </body></html>