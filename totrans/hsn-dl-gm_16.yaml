- en: Obstacle Tower Challenge and Beyond
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, our final one, we will take a look at the current and future
    state of **deep learning** (**DL**) and **deep reinforcement learning** (**DRL**)
    for games. We take an honest and candid look to see whether these technologies
    are ready for prime-time commercial games or whether they are just novelties.
    Are we poised to see DRL agents beating human players at every game imaginable
    a few years from now? While that remains to be seen, and things are changing quickly,
    the question really is this: is DL ready for your game? It likely is a question
    you are asking yourself at this very moment, and it is hopefully one we will answer
    in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will be a mix of hands-on exercises and general discussions with
    unfortunately no exercises. Well, there is one big exercise, but we will get to
    that shortly. Here is what we will cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The Unity Obstacle Tower challenge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Learning for your game?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building your game
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More foundations of learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter assumes you have covered numerous exercises in this book in order
    to understand the context. We will refer to those sections in order to remind
    the reader, but please don't jump to this chapter first.
  prefs: []
  type: TYPE_NORMAL
- en: The Unity Obstacle Tower Challenge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Unity Obstacle Tower Challenge** was introduced in February 2019 as a
    discrete visual learning problem. As we have seen before, this is the holy grail
    of learning for games, robotics, and other simulations. What makes it more interesting
    is this challenge was introduced outside of ML-Agents and requires the challenger
    to write their own Python code from scratch to control the game—something we have
    come close to learning how to do in this book, but we omitted the technical details.
    Instead, we focused on the fundamentals of tuning hyperparameters, understanding
    rewards, and the agent state. All of these fundamentals will come in handy if
    you decide to tackle the tower challenge.
  prefs: []
  type: TYPE_NORMAL
- en: At the time this book was written, the ML-Agents version used for developing
    was `0.6`. If you have run all the exercises to completion, you will have noticed
    that all of the visual learning environments using a discrete action space suffer
    from a vanishing or exploding gradient problem. What you will see happen is the
    agent essentially learning nothing and performing random actions; this often takes
    several hundred thousand iterations to see. But we don't see this problem in environments
    with a smaller state space using vector observations. In visual environments with
    a large input state, though, the problem can be seen quite regularly. This means
    that, essentially, at the time of writing anyway, you would not want to use the
    Unity code; it currently is a poor visual learner of discrete actions.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the Unity Obstacle Tower Challenge has just started,
    and early metrics are already being reported. The current leading algorithm from
    Google, DeepMind, not surprisingly, is an algorithm called **Rainbow**. In short,
    Rainbow is the culmination of many different DRL algorithms and techniques all
    combined to better learn the discrete action visual-learning space that the tower
    so well defines.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have established that you likely want to write your own code, we
    will understand the high-level critical pieces your agent needs to address. It
    likely would take another book to explain how to do the coding and other technical
    aspects of that, so we will instead talk about the overall challenges and the
    critical elements you need to address. Also, the winners will more than likely
    need to use more probabilistic methods in order to address the problem, and that
    is currently not covered very well anywhere.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s set up the challenge and get it running in the next exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the Obstacle Tower Environment as a binary from [https://github.com/Unity-Technologies/obstacle-tower-env](https://github.com/Unity-Technologies/obstacle-tower-env).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Follow the instructions and download the zip file for your environment as directed.
    On most systems, this just requires downloading and unzipping the file into a
    folder you will execute from later.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unzip the file into a well-known folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Launch the program by double-clicking on it (Windows) to enter the name in
    a console. After you launch the challenge, you can actually play it as a human.
    Play the game and see how many floors you can climb. An example of the running
    challenge is shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4dbf2be9-5c36-431e-bdd0-504185403822.png)'
  prefs: []
  type: TYPE_IMG
- en: The Obstacle Tower Challenge in player mode
  prefs: []
  type: TYPE_NORMAL
- en: One of the first things you will learn as you progress through the game is that
    the game starts out quite simply, but on the later floors, it gets quite difficult,
    even for a human.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, as we mentioned, solving this challenge is well beyond the scope of this
    book, but hopefully you can now appreciate some of the complexities that currently
    stifle the field of deep reinforcement learning. We have reviewed the major challenges
    that you will face when undertaking this method in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Problem** | **Chapter** | **Current** **Status** | **Future** |'
  prefs: []
  type: TYPE_TB
- en: '| Visual observation state—you will need to build a complex enough CNN and
    possibly recurrent networks to encode enough details in the visual state. | [Chapter
    7](9b7b6ff8-8daa-42bd-a80f-a7379c37c011.xhtml), *Agent and the Environment* |
    The current Unity visual encoder is far from acceptable. | Fortunately, there
    is plenty of work always being done with CNN and recurrent networks for analysis
    of videos. Remember, you don''t just want to capture static images; you also want
    to encode the sequence of the images. |'
  prefs: []
  type: TYPE_TB
- en: '| DQN, DDQN, or Rainbow | [Chapter 5](6ca7a117-1a8c-49f9-89c0-ee2f2a1e8baf.xhtml),
    *Introducing DRL* | Rainbow is currently the best, and it is available on the
    GCP. | As we have seen in this book, PPO only performs well on continuous action
    spaces. In order to tackle the discrete action space, we look back to more fundamental
    methods such as DQN or the newcomer Rainbow, which is the summation of all base
    methods. We will also discuss future ways in which further use of deep probabilistic
    methods may be the answer. |'
  prefs: []
  type: TYPE_TB
- en: '| Intrinsic rewards | [Chapter 9](ae184eca-6c9d-456e-a72b-85274ddcc10c.xhtml),
    *Rewards and Reinforcement Learning* | The use of an intrinsic reward system shows
    promise for exploration. | Being able to introduce intrinsic reward systems such
    as **Curiosity Learning** allows the agent to explore new environments based on
    some expectation of state. This method will be essential for any algorithm that
    plans to reach the higher levels of the tower. |'
  prefs: []
  type: TYPE_TB
- en: '| Understanding | [Chapter 6](b422aff5-b743-4696-ba80-e0a222ea5b4d.xhtml),
    *Unity ML-Agents* | Unity provides an excellent sample environment to build and
    test models on. | You can easily build and test a similar environment in Unity
    quite quickly and on your own. It is no wonder Unity never released the raw Unity
    environment as a project. This was more than likely because this would have attracted
    many novices, thinking they could overcome the problem with just training. Sometimes,
    training is just not the answer. |'
  prefs: []
  type: TYPE_TB
- en: '| Sparse rewards | [Chapter 9](ae184eca-6c9d-456e-a72b-85274ddcc10c.xhtml),
    *Rewards and Reinforcement Learning*[Chapter 10](1525f2f4-b9e1-4b7f-ac40-33e801c668ed.xhtml),
    *Imitation and Transfer Learning* | Could implement Curriculum or Imitation Learning.
    | We have already covered many examples of ways to manage the sparse rewards problem.
    It will be interesting to see how much the winners depend on one of these methods,
    such as IL, to win. |'
  prefs: []
  type: TYPE_TB
- en: '| Discrete actions | [Chapter 8](1393797c-79cd-46c3-8e43-a09a7750fc92.xhtml), *Understanding
    PPO* | We learned how PPO allowed continuous action problems to learn, using stochastic
    methods. | As we alluded to before, it will likely take new work into more deep
    probabilistic methods and techniques to work around some of the current problems.
    This will likely require the development of new techniques using new algorithms,
    and how long that takes remains to be seen. |'
  prefs: []
  type: TYPE_TB
- en: Each of the problems highlighted in the preceding table will likely need to
    be solved in part or wholly in order to get an agent from floor 1 to 100 to complete
    the entire challenge. It remains to be seen how this will play out for Unity,
    the winner, and DRL as a whole. In the next section, we discuss the practical
    applications of DL and DRL, and how they can be used for your game.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning for your game?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It''s likely the reason you picked this book up was to learn about DL and DLR
    for games in the hope of landing your dream job or completing your dream game.
    In either case, we come to a point where you decide whether this technology is
    worth including in your own game and to what extent. The following is a list of
    ten questions you can use to determine whether DL is right for your game:'
  prefs: []
  type: TYPE_NORMAL
- en: Have you already made the decision and need to build the game with DL or DRL?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yes – 10 points
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: No – 0 points
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Will your game benefit from some form of automation, either through testing
    or managing repetitious player tasks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yes – 10 points
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: No – 0 points
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you want to make training and AI or another similar activity part of the
    game?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yes – (-5) points. *You may be better off using a more robust from of AI to
    simulate the training. Training DRL takes too many iterations and samples to be
    effective as an inline game-training tool, at least for now.*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: No – 0 points.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you want cutting-edge AI to feature in your game?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yes – 10 points. *There are certainly ways of layering AI technologies and making
    a DRL solution work. When it comes to current AI, there really is no better cutting-edge
    technology.*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: No – 0 points.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you have hours of time to train an AI?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yes – 10 points
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: No – (-10) points
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Have you read a good portion of this book and completed at least a few of the
    exercises?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yes – 10 points, +5 if you completed more than 50%
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: No – (-10) points; thanks for the honesty
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you have a background or affinity for math?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yes – 10 points
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: No – (-10) points
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How many papers have you read on reinforcement learning at an academic level?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 10+ – 25 points
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 5–10 – 10 points
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1–5 – 5 points
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 – 0 points
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What is your completion timeline?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1–3 months – (-10) points
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 3–6 months – 0 points
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 6–12 months – 10 points
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1–2+ years – 25 points
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the size of your team?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solo – (-10) points
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 2–5 – 0 points
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 6–10 – 10 points
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 11+ – 25 points
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Answer all the questions and score your points to determine your full readiness
    score. Consult the following to determine how ready you and/or your team are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**<0 points** - How did you even make it this far into the book? You''re not
    ready, and it''s best you just put this book down.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**0-50** - You certainly show promise, but you are going to need some more
    help; check out the following section on next steps and further areas of learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**50-100** - You certainly are on your way to building the knowledge base and
    implementing some fun DRL in games, but you may still need a little help. Check
    the section on next steps and further areas of learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**100+** - You are well beyond ready, and we appreciate you taking the time
    to read this book. Perhaps take some of your own personal time and pass your own
    or your team members'' knowledge on to people you know.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, there are no absolute rules to the results of the preceding test,
    and you may find that you score quite low but then go on to make the next great
    AI game. How you approach the results is up to you, and how you take your next
    steps is also entirely up to you.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we look at the next steps you can take to learn more about
    DRL and how to build better automation and AI in games.
  prefs: []
  type: TYPE_NORMAL
- en: Building your game
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that you have decided to use deep learning and/or deep reinforcement learning
    for your game, it is time to determine how you plan to implement various functionality
    in your game. In order to do that, we are going to go through a table outlining
    the steps you need to go through in order to build your game''s AI agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Step** | **Action** | **Summary** |'
  prefs: []
  type: TYPE_TB
- en: '| Start | Determine at what level you want the AI in the game to operate, from
    basic, perhaps for just testing and simple automation, to advanced, where the
    AI will complete against the player. | Determine the level of AI. |'
  prefs: []
  type: TYPE_TB
- en: '| Resourcing | Determine the amount of resources. Basic AI or automation could
    be handled within the team itself, whereas more complex AI may require one or
    many experienced members of staff. | Team requirements. |'
  prefs: []
  type: TYPE_TB
- en: '| Knowledge | Determine the level of knowledge the team possesses and what
    will be required. It is a given that any team implementing new AI will need to
    learn new skills.  | Knowledge-gap analysis. |'
  prefs: []
  type: TYPE_TB
- en: '| Demonstration | Always start by building a simple but workable proof of concept
    that demonstrates all critical aspects of the system. | Demonstrate the team can
    complete the basic premise. |'
  prefs: []
  type: TYPE_TB
- en: '| Implementation | Build the actual system in a way that is simplistic and
    maintainable. Keep all the things you know simple and clean. | Build the system.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Testing | Test the system over and over again. It is critical that the system
    is tested thoroughly, and of course what better way to do that than with a DRL
    automated test system. | Test the system. |'
  prefs: []
  type: TYPE_TB
- en: '| Fix | As anyone who has developed software for more than a few weeks will
    tell you, the process is build, test, fix, and repeat. That essentially is the
    software development process, so try not to add too many other bells and whistles
    to distract from that. | Fixing the system. |'
  prefs: []
  type: TYPE_TB
- en: '| Release | Releasing software to users/players is absolutely critical to a
    successful game or software product of any kind. You will always want to release
    early and often, which means your players must be encouraged to test, and to provide
    feedback. | Let the bugs out. |'
  prefs: []
  type: TYPE_TB
- en: '| Repeat | The cycle is endless and will continue as long as your product/game
    makes money. | Support the system. |'
  prefs: []
  type: TYPE_TB
- en: The preceding process is the basic premise and will work for most of your development
    needs. In most cases, you may want to track individual work items such as features
    or bugs on a work or task board. You may want to use a more defined process such
    as Scrum, but often keeping things simple is your best course of action.
  prefs: []
  type: TYPE_NORMAL
- en: Scrum and other software development processes are great examples to learn from,
    but unless you have formally trained staff, it's better to avoid trying to implement
    these yourself. There are often subtle rules that need to be enforced in these
    processes for them to work as they claim to. Even trained Scrum Masters may need
    to battle daily to enforce these rules in many organizations, and in the end their
    value becomes more management-driven than developer-focused. Use the previous
    table as a guide for the steps you take in building your next game, and always
    remember that build, release, fix, and repeat is the key to good software.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at other things you can use to expand your
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: More foundations of learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is an ever-growing resource for learning about machine learning, DL,
    and of course DLR. The list is becoming very large, and there are many materials
    to choose from. For that reason, we will now summarize the areas we feel show
    the most promise for developing AI and DL for games:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Basic Data Science Course**: If you have never taken a basic fundamentals
    course on data science, then you certainly should. The foundations of understanding
    the qualities of data, statistics, probability, and variability are too numerous
    to mention. Be sure to cover this foundation first.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Probabilistic Programming**: This is a combination of various variational
    inference methods by which to answer problems given a probability of events with
    an answer of the probability that some event may occur. These types of models
    and languages have been used to analyze financial information and risk for years,
    but they are now coming to the forefront in ML technologies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep Probabilistic Programming**: This is the combination of variational
    inference and DL models. Variational inference is the process by which you answer
    a question with a probability given the input of possibly multiple probabilities.
    So, instead of using a series of weights to train a network, we use a series of
    probability distributions. This method has proven to be very effective and has
    recently performed visual image classification tasks with a modified probabilistic
    CNN model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visual state classification and encoding**: A critical aspect to a DL system
    is the development of CNN models to classify images. You will need to understand
    this space very well in order to build the networks for your game environment.
    Recall that different environments may require CNN models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory**: Memory can of course come in all forms, but the primary one of
    interest is the **recurrent neural network** (**RNN**). Early on in this book,
    we looked at the current standard recurrent network model we use called the **long
    short-term memory** (**LSTM**) **block**. Even at the time of writing, there is
    a renewed interest in the **gated recurrent unit** (**GRU**), a more complex recurrent
    network that has been shown to handle the vanishing gradient problem better. There
    is always an interest in cloud or other supported technologies and how they may
    interact with new DL technologies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DL as a Service**: Companies such as Google, Amazon, Microsoft, OpenAI, and
    others who claim to be all about openness are often far from it. In most cases,
    if you want to incorporate these technologies into your game, you will need to
    subscribe to their service—which of course has its own pluses and minuses. The
    major problem is that if your game becomes popular and if you rely heavily on
    the DL service, your profits will be tied to it. Fortunately, Unity has yet to
    take this approach, but that does remain to be seen depending on how easily the
    community solves the Obstacle Tower Challenge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Math**: In general, you will want to always advance your math skills whether
    you plan to dig deep into building your own models or not. In the end, your gut
    understanding of the math will provide you with the insights you need to overcome
    these complex technologies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Perseverance**: Learn to fail, and then move on. This is critical and something
    many new developers often get disgruntled with and then move on to something easier,
    simpler, and less rewarding. Be happy when you fail, as failing is learning to
    understand. If you never fail, you really never learn, so learn to fail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A hard-coded list of learning resources would likely get out of date before
    this book is even printed or released. Use the preceding list to generalize your
    learning and broaden your basic machine learning and data science knowledge as
    well. First and foremost, DL is a data science pursuit that serves respect to
    the data; never forget that as well.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section for our final chapter, we will summarize this chapter and
    the book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took a short tour of many basic concepts involving your
    next steps in DL and DRL; perhaps you will decide to pursue the Unity Obstacle
    Tower Challenge and complete that or just use DRL in your own project. We looked
    at simple quizzes in order to evaluate your potential for diving in and using
    DRL in a game. From there, we looked at the next steps in development, and then
    finally we looked at other areas of learning may want to focus on.
  prefs: []
  type: TYPE_NORMAL
- en: This book was an exercise in understanding how effective DL can be when applied
    to your game project in the future. We explored many areas of basic DL principles
    early on and looked at more specific network types such as CNN and LSTM. Then,
    we looked at how these basics network forms could be applied to applications for
    driving and building a chatbot. From there, we looked at the current king of machine
    learning algorithms, reinforcement and deep reinforcement learning. We then looked
    at one of the current leaders, Unity ML-Agents, and how to implement this technology,
    over several chapters by looking at how simple environments are built to more
    complex multi-agent environments. This also allowed us to explore different forms
    of intrinsic/extrinsic rewards and learning systems, including curriculum, curiosity,
    imitation, and transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, before finishing this chapter, we completed a long exercise regarding
    using DRL for automatic testing and debugging with the added option of using IL
    as a way of enhancing testing.
  prefs: []
  type: TYPE_NORMAL
