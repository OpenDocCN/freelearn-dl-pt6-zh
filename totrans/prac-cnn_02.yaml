- en: Introduction to Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Convolutional Neural Networks** (**CNNs**) are everywhere. In the last five
    years, we have seen a dramatic rise in the performance of visual recognition systems
    due to the introduction of deep architectures for feature learning and classification.
    CNNs have achieved good performance in a variety of areas, such as automatic speech
    understanding, computer vision, language translation, self-driving cars, and games
    such as Alpha Go. Thus, the applications of CNNs are almost limitless. DeepMind
    (from Google) recently published WaveNet, which uses a CNN to generate speech
    that mimics any human voice ([https://deepmind.com/blog/wavenet-generative-model-raw-audio/](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: History of CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of a CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image augmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: History of CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There have been numerous attempts to recognize pictures by machines for decades.
    It is a challenge to mimic the visual recognition system of the human brain in
    a computer. Human vision is the hardest to mimic and most complex sensory cognitive
    system of the brain. We will not discuss biological neurons here, that is, the
    primary visual cortex, but rather focus on artificial neurons. Objects in the
    physical world are three dimensional, whereas pictures of those objects are two
    dimensional. In this book, we will introduce neural networks without appealing
    to brain analogies. In 1963, computer scientist Larry Roberts, who is also known
    as the **father of computer vision**, described the possibility of extracting
    3D geometrical information from 2D perspective views of blocks in his research
    dissertation titled**BLOCK WORLD**. This was the first breakthrough in the world
    of computer vision. Many researchers worldwide in machine learning and artificial
    intelligence followed this work and studied computer vision in the context of
    BLOCK WORLD. Human beings can recognize blocks regardless of any orientation or
    lighting changes that may happen. In this dissertation, he said that it is important
    to understand simple edge-like shapes in images. He extracted these edge-like
    shapes from blocks in order to make the computer understand that these two blocks
    are the same irrespective of orientation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e8981b5-bdf4-4023-a271-417a9c96d29f.png)'
  prefs: []
  type: TYPE_IMG
- en: The vision starts with a simple structure. This is the beginning of computer
    vision as an engineering model. David Mark, an MIT computer vision scientist,
    gave us the next important concept, that vision is hierarchical. He wrote a very
    influential book named *VISION*. This is a simple book. He said that an image
    consists of several layers. These two principles form the basis of deep learning
    architecture, although they do not tell us what kind of mathematical model to
    use.
  prefs: []
  type: TYPE_NORMAL
- en: In the 1970s, the first visual recognition algorithm, known as the **generalized
    cylinder model**, came from the AI lab at Stanford University. The idea here is
    that the world is composed of simple shapes and any real-world object is a combination
    of these simple shapes. At the same time, another model, known as the **pictorial
    structure model**, was published from SRI Inc. The concept is still the same as
    the generalized cylinder model, but the parts are connected by springs; thus,
    it introduced a concept of variability. The first visual recognition algorithm
    was used in a digital camera by Fujifilm in 2006.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CNNs, or ConvNets, are quite similar to regular neural networks. They are still
    made up of neurons with weights that can be learned from data. Each neuron receives
    some inputs and performs a dot product. They still have a loss function on the
    last fully connected layer. They can still use a nonlinearity function. All of
    the tips and techniques that we learned from the last chapter are still valid
    for CNN. As we saw in the previous chapter, a regular neural network receives
    input data as a single vector and passes through a series of hidden layers. Every
    hidden layer consists of a set of neurons, wherein every neuron is fully connected
    to all the other neurons in the previous layer. Within a single layer, each neuron
    is completely independent and they do not share any connections. The last fully
    connected layer, also called the **output layer**, contains class scores in the
    case of an image classification problem. Generally, there are three main layers
    in a simple ConvNet. They are the **convolution layer**, the **pooling layer**,
    and the **fully connected layer**. We can see a simple neural network in the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22900b9c-4754-4b53-8023-c1f50bfc7eda.png)'
  prefs: []
  type: TYPE_IMG
- en: A regular three-layer neural network
  prefs: []
  type: TYPE_NORMAL
- en: So, what changes? Since a CNN mostly takes images as input, this allows us to
    encode a few properties into the network, thus reducing the number of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of real-world image data, CNNs perform better than **Multi-Layer
    Perceptrons** (**MLPs**). There are two reasons for this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the last chapter, we saw that in order to feed an image to an MLP, we convert
    the input matrix into a simple numeric vector with no spatial structure. It has
    no knowledge that these numbers are spatially arranged. So, CNNs are built for
    this very reason; that is, to elucidate the patterns in multidimensional data.
    Unlike MLPs, CNNs understand the fact that image pixels that are closer in proximity
    to each other are more heavily related than pixels that are further apart:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*CNN = Input layer + hidden layer + fully connected layer*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'CNNs differ from MLPs in the types of hidden layers that can be included in
    the model. A ConvNet arranges its neurons in three dimensions: **width**, **height**,
    and **depth**. Each layer transforms its 3D input volume into a 3D output volume
    of neurons using activation functions. For example, in the following figure, the
    red input layer holds the image. Thus its width and height are the dimensions
    of the image, and the depth is three since there are Red, Green, and Blue channels:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/2909a83d-f11b-4e4f-be2b-ed1365c13eae.png)'
  prefs: []
  type: TYPE_IMG
- en: ConvNets are deep neural networks that share their parameters across space.
  prefs: []
  type: TYPE_NORMAL
- en: How do computers interpret images?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Essentially, every image can be represented as a matrix of pixel values. In
    other words, images can be thought of as a function (*f*) that maps from *R²*
    to *R*.
  prefs: []
  type: TYPE_NORMAL
- en: '*f(x, y)* gives the intensity value at the position *(x, y)*. In practice,
    the value of the function ranges only from *0* to *255*. Similarly, a color image
    can be represented as a stack of three functions. We can write this as a vector
    of:'
  prefs: []
  type: TYPE_NORMAL
- en: '* f( x, y) = [ r(x,y) g(x,y) b(x,y)]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Or we can write this as a mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f: R x R --> R3*'
  prefs: []
  type: TYPE_NORMAL
- en: So, a color image is also a function, but in this case, a value at each *(x,y)*
    position is not a single number. Instead it is a vector that has three different
    light intensities corresponding to three color channels. The following is the
    code for seeing the details of an image as input to a computer.
  prefs: []
  type: TYPE_NORMAL
- en: Code for visualizing an image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at how an image can be visualized with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We obtain the following image as a result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7becc16a-b4b4-4e07-8f41-30ae0768b2aa.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/896620ef-6e56-4fb5-9e79-9d5aa17f9c09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous chapter, we used an MLP-based approach to recognize images.
    There are two issues with that approach:'
  prefs: []
  type: TYPE_NORMAL
- en: It increases the number of parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It only accepts vectors as input, that is, flattening a matrix to a vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This means we must find a new way to process images, in which 2D information
    is not completely lost. CNNs address this issue. Furthermore, CNNs accept matrices
    as input. Convolutional layers preserve spatial structures. First, we define a
    convolution window, also called a **filter**, or **kernel**; then slide this over
    the image.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A neural network can be thought of as a search problem. Each node in the neural
    network is searching for correlation between the input data and the correct output
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout randomly turns nodes off while forward-propagating and thus helps ward
    off weights from converging to identical positions. After this is done, it turns
    on all the nodes and back-propagates. Similarly, we can set some of the layer's
    values to zero at random during forward propagation in order to perform dropout
    on a layer.
  prefs: []
  type: TYPE_NORMAL
- en: Use dropout only during training. Do not use it at runtime or on your testing
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Input layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **input layer** holds the image data. In the following figure, the input
    layer consists of three inputs. In a **fully connected layer**, the neurons between
    two adjacent layers are fully connected pairwise but do not share any connection
    within a layer. In other words, the neurons in this layer have full connections
    to all activations in the previous layer. Therefore, their activations can be
    computed with a simple matrix multiplication, optionally adding a bias term. The
    difference between a fully connected and convolutional layer is that neurons in
    a convolutional layer are connected to a local region in the input, and that they
    also share parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d65ee007-a008-4252-88b6-f4ac9d2d13d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Convolutional layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main objective of convolution in relation to ConvNet is to extract features
    from the input image. This layer does most of the computation in a ConvNet. We
    will not go into the mathematical details of convolution here but will get an
    understanding of how it works over images.
  prefs: []
  type: TYPE_NORMAL
- en: The ReLU activation function is extremely useful in CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers in Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To create a convolutional layer in Keras, you must first import the required
    modules as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you can create a convolutional layer by using the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You must pass the following arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`filters`: The number of filters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel_size`: A number specifying both the height and width of the (square)
    convolution window. There are also some additional optional arguments that you
    might like to tune.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strides`: The stride of the convolution. If you don''t specify anything, this
    is set to one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding`: This is either `valid` or `same`. If you don''t specify anything,
    the padding is set to `valid`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation`: This is typically `relu`. If you don''t specify anything, no
    activation is applied. You are strongly encouraged to add a ReLU activation function
    to every convolutional layer in your networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is possible to represent both `kernel_size` and `strides` as either a number
    or a tuple.
  prefs: []
  type: TYPE_NORMAL
- en: When using your convolutional layer as the first layer (appearing after the
    input layer) in a model, you must provide an additional `input_shape` argument—`input_shape`.
    It is a tuple specifying the height, width, and depth (in that order) of the input.
  prefs: []
  type: TYPE_NORMAL
- en: Please make sure that the  `input_shape` argument is not included if the convolutional
    layer is not the first layer in your network.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many other tunable arguments that you can set to change the behavior
    of your convolutional layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 1**: In order to build a CNN with an input layer that accepts images
    of 200 x 200 pixels in grayscale. In such cases, the next layer would be a convolutional
    layer of 16 filters with width and height as 2\. As we go ahead with the convolution
    we can set the filter to jump 2 pixels together. Therefore, we can build a convolutional,
    layer with a filter that doesn''t pad the images with zeroes with the following
    code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Example 2**: After we build our CNN model, we can have the next layer in
    it to be a convolutional layer. This layer will have 32 filters with width and
    height as 3, which would take the layer that was constructed in the previous example
    as its input. Here, as we proceed with the convolution, we will set the filter
    to jump one pixel at a time, such that the convolutional layer will be able to
    see all the regions of the previous layer too. Such a convolutional layer can
    be constructed with the help of the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Example 3**: You can also construct convolutional layers in Keras of size
    2 x 2, with 64 filters and a ReLU activation function. Here, the convolution utilizes
    a stride of 1 with padding set to `valid` and all other arguments set to their
    default values. Such a convolutional layer can be built using the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Pooling layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have seen, a convolutional layer is a stack of feature maps, with one
    feature map for each filter. More filters increase the dimensionality of convolution.
    Higher dimensionality indicates more parameters. So, the pooling layer controls
    overfitting by progressively reducing the spatial size of the representation to
    reduce the number of parameters and computation. The pooling layer often takes
    the convolutional layer as input. The most commonly used pooling approach is **max
    pooling**. In addition to max pooling, pooling units can also perform other functions
    such as **average pooling**. In a CNN, we can control the behavior of the convolutional
    layer by specifying the size of each filter and the number of filters. To increase
    the number of nodes in a convolutional layer, we can increase the number of filters,
    and to increase the size of the pattern, we can increase the size of the filter.
    There are also a few other hyperparameters that can be tuned. One of them is the
    stride of the convolution. Stride is the amount by which the filter slides over
    the image. A stride of 1 moves the filter by 1 pixel horizontally and vertically.
    Here, the convolution becomes the same as the width and depth of the input image.
    A stride of 2 makes a convolutional layer of half of the width and height of the
    image. If the filter extends outside of the image, then we can either ignore these
    unknown values or replace them with zeros. This is known as **padding**. In Keras,
    we can set `padding = ''valid''` if it is acceptable to lose a few values. Otherwise,
    set `padding = ''same''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ffc13c90-ca46-446d-a5c9-e1c0b2712ba8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A very simple ConvNet looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f67c1764-58d0-4d31-b204-34f22089b652.png)'
  prefs: []
  type: TYPE_IMG
- en: Practical example – image classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The convolutional layer helps to detect regional patterns in an image. The
    max pooling layer, present after the convolutional layer, helps reduce dimensionality.
    Here is an example of image classification using all the principles we studied
    in the previous sections. One important notion is to first make all the images
    into a standard size before doing anything else. The first convolution layer requires
    an additional `input.shape()` parameter. In this section, we will train a CNN
    to classify images from the CIFAR-10 database. CIFAR-10 is a dataset of 60,000
    color images of 32 x 32 size. These images are labeled into 10 categories with
    6,000 images each. These categories are airplane, automobile, bird, cat, dog,
    deer, frog, horse, ship, and truck. Let''s see how to do this with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Image augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While training a CNN model, we do not want the model to change any prediction
    based on the size, angle, and position of the image. The image is represented
    as a matrix of pixel values, so the size, angle, and position have a huge effect
    on the pixel values. To make the model more size-invariant, we can add different
    sizes of the image to the training set. Similarly, in order to make the model
    more rotation-invariant, we can add images with different angles. This process
    is known as **image data augmentation**. This also helps to avoid overfitting.
    Overfitting happens when a model is exposed to very few samples. Image data augmentation
    is one way to reduce overfitting, but it may not be enough because augmented images
    are still correlated. Keras provides an image augmentation class called `ImageDataGenerator`
    that defines the configuration for image data augmentation. This also provides
    other features such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Sample-wise and feature-wise standardization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random rotation, shifts, shear, and zoom of the image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizontal and vertical flip
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ZCA whitening
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimension reordering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saving the changes to disk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An augmented image generator object can be created as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This API generates batches of tensor image data in real-time data augmentation,
    instead of processing an entire image dataset in memory. This API is designed
    to create augmented image data during the model fitting process. Thus, it reduces
    the memory overhead but adds some time cost for model training.
  prefs: []
  type: TYPE_NORMAL
- en: 'After it is created and configured, you must fit your data. This computes any
    statistics required to perform the transformations to image data. This is done
    by calling the `fit()` function on the data generator and passing it to the training
    dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The batch size can be configured, the data generator can be prepared, and batches
    of images can be received by calling the `flow()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, call the `fit_generator()` function instead of calling the `fit()` function
    on the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Let's look at some examples to understand how the image augmentation API in
    Keras works. We will use the MNIST handwritten digit recognition task in these
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by taking a look at the first nine images in the training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code snippet creates augmented images from the CIFAR-10 dataset.
    We will add these images to the training set of the last example and see how the
    classification accuracy increases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We began this chapter by briefly looking into the history of CNNs. We introduced
    you to the implementation of visualizing images.
  prefs: []
  type: TYPE_NORMAL
- en: We studied image classification with the help of a practical example, using
    all the principles we learned about in the chapter. Finally, we learned how image
    augmentation helps us avoid overfitting and studied the various other features
    provided by image augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to build a simple image classifier CNN
    model from scratch.
  prefs: []
  type: TYPE_NORMAL
