<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building Convolutional Neural Networks</h1>
                </header>
            
            <article>
                
<p>In this chapter, we are going to develop a <strong>convolutional neural network</strong><span> (</span><strong>CNN</strong><span>)</span> for an image classification example using DL4J. We will develop the components of our application step by step while we progress through the recipes. The chapter assumes that you have read <a href="f88b350b-16e2-425b-8425-4631187c7803.xhtml" target="_blank">Chapter 1</a>, <em>Introduction to Deep Learning in Java</em>, and <a href="6ac5dff5-cc98-4d52-bc59-1da01b2aeded.xhtml" target="_blank">Chapter 2</a>, <em>Data Extraction, Transformation, and Loading</em>, and that you have set up DL4J on your computer, as mentioned in <a href="f88b350b-16e2-425b-8425-4631187c7803.xhtml"/><a href="f88b350b-16e2-425b-8425-4631187c7803.xhtml" target="_blank">Chapter 1</a>, <em>Introduction to Deep Learning in Java</em>. Let's go ahead and discuss the specific changes required for this chapter.</p>
<p>For demonstration purposes, we will have classifications for four different species. CNNs convert complex images into an abstract format that can be used for prediction. Hence, a CNN would be an optimal choice for this image classification problem.</p>
<p class="mce-root">CNNs are just like any other deep neural network that abstracts the decision process and gives us an interface to transform input to output. The only difference is that they support other types of layers and different orderings of layers. Unlike other forms of input, such as text or CSV, images are complex. Considering the fact that each pixel is a source of information, training will become resource intensive and time consuming for large numbers of high-resolution images.</p>
<p class="mce-root">In this chapter, we will cover the following recipes:</p>
<ul>
<li>Extracting images from disk</li>
<li>Creating image variations for training data</li>
<li>Image preprocessing and the design of input layers</li>
<li>Constructing hidden layers for a CNN</li>
<li>Constructing output layers for output classification</li>
<li>Training images and evaluating CNN output</li>
<li>Creating an API endpoint for the image classifier</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p><span>Implementation of the use case discussed in this chapter can be found here: </span><a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/04_Building_Convolutional_Neural_Networks/sourceCode">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/04_Building_Convolutional_Neural_Networks/sourceCode</a>.</p>
<p>After cloning our GitHub repository, navigate to the following directory: <kbd>Java-Deep-Learning-Cookbook/04_Building_Convolutional_Neural_Networks/sourceCode</kbd>. Then, import the <kbd>cookbookapp</kbd> <span>project </span>as a Maven project by importing <kbd>pom.xml</kbd>.</p>
<p>You will also find a basic Spring project, <kbd>spring-dl4j</kbd>, which can be imported as a Maven project as well.</p>
<p>We will be using the dog breeds classification dataset from Oxford for this chapter.</p>
<p>The principal dataset can be downloaded from the following link:<br/>
<a href="https://www.kaggle.com/zippyz/cats-and-dogs-breeds-classification-oxford-dataset">https://www.kaggle.com/zippyz/cats-and-dogs-breeds-classification-oxford-dataset</a>.</p>
<p>To run this chapter's source code, download the dataset (four labels only) from here:<br/>
<a href="https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/raw/master/04_Building%20Convolutional%20Neural%20Networks/dataset.zip">https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/raw/master/04_Building%20Convolutional%20Neural%20Networks/dataset.zip</a> (it can be found in the <kbd>Java-Deep-Learning-Cookbook/04_Building Convolutional Neural Networks/</kbd> <span>directory).</span></p>
<p>Extract the compressed dataset file. Images are kept in different directories. Each directory represents a label/category. For demonstration purposes, we have used four labels. However, you are allowed to experiment with more images from different categories in order to run our example from GitHub. </p>
<p>Note that our example is optimized for four species. Experimentation with a larger number of labels requires further <span>network configuration </span>optimization.</p>
<p>To leverage the capabilities of the OpenCV library in your CNN, add the following Maven dependency:</p>
<pre>&lt;dependency&gt;<br/> &lt;groupId&gt;org.bytedeco.javacpp-presets&lt;/groupId&gt;<br/> &lt;artifactId&gt;opencv-platform&lt;/artifactId&gt;<br/> &lt;version&gt;4.0.1-1.4.4&lt;/version&gt;<br/> &lt;/dependency&gt;</pre>
<p class="mce-root"/>
<p><span>We will be using the Google Cloud SDK to deploy the application in the cloud. For instructions in this regard, refer to <a href="https://github.com/GoogleCloudPlatform/app-maven-plugin">https://github.com/GoogleCloudPlatform/app-maven-plugin</a>. For Gradle instructions, refer to <a href="https://github.com/GoogleCloudPlatform/app-gradle-plugin">https://github.com/GoogleCloudPlatform/app-gradle-plugin</a>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extracting images from disk</h1>
                </header>
            
            <article>
                
<p>For classification based on <em>N</em> labels, there are <em>N</em> subdirectories created in the parent directory. The parent directory path is mentioned for image extraction. Subdirectory names will be regarded as labels. In this recipe, we will extract images from disk using DataVec.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Use <kbd>FileSplit</kbd> to define the range of files to load into the neural network:</li>
</ol>
<pre style="padding-left: 60px">FileSplit fileSplit = new FileSplit(parentDir, NativeImageLoader.ALLOWED_FORMATS,new Random(42));<br/> int numLabels = fileSplit.getRootDir().listFiles(File::isDirectory).length;</pre>
<ol start="2">
<li>Use <kbd>ParentPathLabelGenerator</kbd> and <kbd>BalancedPathFilter</kbd> to sample the labeled dataset and split it into train/test sets:</li>
</ol>
<pre style="padding-left: 60px">ParentPathLabelGenerator parentPathLabelGenerator = new ParentPathLabelGenerator();<br/> BalancedPathFilter balancedPathFilter = new BalancedPathFilter(new Random(42),NativeImageLoader.ALLOWED_FORMATS,parentPathLabelGenerator);<br/> InputSplit[] inputSplits = fileSplit.sample(balancedPathFilter,trainSetRatio,testSetRatio);<br/><br/></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, we used <kbd><span class="s1">FileSplit</span></kbd><span class="s2"> to filter the images based on the file type (PNG, JPEG, TIFF, and so on).</span></p>
<p class="mce-root"/>
<p><span class="s2">We also passed in a random number generator based on a single seed. This seed value is an integer (<kbd>42</kbd> in our example). <kbd>FileSplit</kbd> will be able to generate a list of file paths in random order (random order of files) by making use of a random seed. This will introduce more randomness to the probabilistic decision and thereby increase the model's performance (accuracy metrics). </span></p>
<p><span class="s3">If you have a ready-made dataset with an unknown number of labels, it is crucial to calculate </span><kbd><span class="s1">numLabels</span></kbd><span class="s3">. Hence, we used <kbd>FileSplit</kbd> to calculate them programmatically:</span></p>
<pre>int numLabels = fileSplit.getRootDir().listFiles(File::isDirectory).length;<span class="s3"> </span></pre>
<p><span class="s3">In step 2, we used <strong><kbd>ParentPathLabelGenerator</kbd><span> </span></strong><span>to generate the label for files based on the directory path. Also, <kbd>BalancedPathFilter</kbd> is used to randomize the order of paths in an array. Randomization will help overcome overfitting issues. <kbd>BalancedPathFilter</kbd> also ensures the same number of paths for each label and helps to obtain optimal batches for training. </span></span></p>
<p><span class="s3">With </span><kbd><span class="s1">testSetRatio</span></kbd><span class="s3"> as <kbd>20</kbd>, 20 percent of the dataset will be used as the test set for the model evaluation. After step 2, the array elements in <span><kbd>inputSplits</kbd> </span>will represent the train/test datasets:</span></p>
<ul class="ul1">
<li class="li6"><kbd><span class="s1">inputSplits[0]</span></kbd><span class="s3"> will represent the train dataset.</span></li>
<li class="li6"><kbd><span class="s1">inputSplits[1]</span></kbd><span class="s3"> will represent the test dataset.</span></li>
</ul>
<ul class="ul1">
<li class="li1"><kbd><span class="s1">N</span><span class="s2">ativeImageLoader.ALLOWED_FORMATS</span></kbd><span class="s3"> uses <kbd>JavaCV</kbd> to load images. Allowed image formats a</span>re <kbd>.bmp</kbd>, <kbd>.gif</kbd>, <kbd>.jpg</kbd>, <kbd>.jpeg</kbd>, <kbd>.jp2</kbd>, <kbd>.pbm</kbd>, <kbd>.pgm</kbd>, <kbd>.ppm</kbd>, <kbd>.pnm</kbd>, <kbd>.png</kbd>, <kbd>.tif</kbd>, <kbd>.tiff</kbd>, <kbd>.exr</kbd>, and <kbd>.webp</kbd>.</li>
<li class="li2"><kbd><span class="s5">BalancedPathFilter</span></kbd><span class="s6"> randomizes the order of file paths in an array and removes them randomly to have the same number of paths for each label. It will also form the paths on the output based on their labels, so as to obtain easily optimal batches for training. So, it is more than just random sampling. </span></li>
<li class="li2"><kbd><span class="s5">fileSplit.sample()</span></kbd><span class="s6"> samples the file paths based on the path filter mentioned.</span></li>
</ul>
<p class="p3"><span class="s6">It will further split the results into an array of </span><kbd><span class="s5">InputSplit</span></kbd><span class="s6"> objects. Each object will refer to the train/test set, and its size is proportional to the weights mentioned. </span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating image variations for training data</h1>
                </header>
            
            <article>
                
<p>We create image variations and further train our network model on top of them to increase the generalization power of the CNN. It is crucial to train our CNN with as many image variations as possible so as to increase the accuracy. We basically obtain more samples of the same image by flipping or rotating them. In this recipe, we will transform and create samples of images using a concrete implementation of <kbd>ImageTransform</kbd> in DL4J.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Use <kbd>FlipImageTransform</kbd> to flip the images horizontally or vertically (randomly or not randomly):</li>
</ol>
<pre style="padding-left: 60px">ImageTransform flipTransform = new FlipImageTransform(new Random(seed));</pre>
<ol start="2">
<li>Use <kbd>WarpImageTransform</kbd> to warp the perspective of images deterministically or randomly:</li>
</ol>
<pre style="padding-left: 60px">ImageTransform warpTransform = new WarpImageTransform(new Random(seed),delta);</pre>
<ol start="3">
<li>Use <kbd>RotateImageTransform</kbd> to rotate the images deterministically or randomly:</li>
</ol>
<pre style="padding-left: 60px">ImageTransform rotateTransform = new RotateImageTransform(new Random(seed), angle);</pre>
<ol start="4">
<li>Use <kbd>PipelineImageTransform</kbd> to add image transformations to the pipeline:</li>
</ol>
<pre style="padding-left: 60px">List&lt;Pair&lt;ImageTransform,Double&gt;&gt; pipeline = Arrays.asList(<br/> new Pair&lt;&gt;(flipTransform, flipImageTransformRatio),<br/> new Pair&lt;&gt;(warpTransform , warpImageTransformRatio)<br/> );<br/> ImageTransform transform = new PipelineImageTransform(pipeline);<span><br/></span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, i<span>f we don't need a random flip but a specified mode of flip (deterministic), then we can do the following:</span></p>
<pre>int flipMode = 0;<br/>ImageTransform flipTransform = new FlipImageTransform(flipMode);</pre>
<p><span><kbd>flipMode</kbd> is the deterministic flip mode. </span></p>
<ul class="ul1">
<li><kbd>flipMode = 0</kbd>: Flips around the <em>x</em> axis</li>
<li><kbd>flipMode &gt; 0</kbd>: Flips around the <em>y</em> axis</li>
<li class="li2"><kbd>flipMode &lt; 0</kbd>: Fl<span class="s5">ips around both axes</span></li>
</ul>
<p><span class="s5">In step 2, we p</span>assed in two attributes: <kbd>Random(seed)</kbd> and <kbd>delta</kbd>. <kbd>delta</kbd> is the <span class="s5"><span>magnitude in which an image is warped. Check the following image sample for the demonstration of image warping:</span><br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1250 image-border" src="assets/770b3c00-d390-4d41-9b67-8cd238a57618.jpg" style="width:21.83em;height:29.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">(Image source: https://commons.wikimedia.org/wiki/File:Image_warping_example.jpg<br/>
License: CC BY-SA 3.0)</div>
<p><kbd>WarpImageTransform(new Random(seed),delta)</kbd> internally calls the following constructor:</p>
<pre>public WarpImageTransform(java.util.Random random,<br/> float dx1,<br/> float dy1,<br/> float dx2,<br/> float dy2,<br/> float dx3,<br/> float dy3,<br/> float dx4,<br/> float dy4</pre>
<p>It will assume<span> </span><kbd>dx1=dy1=dx2=dy2=dx3=dy3=dx4=dy4=delta</kbd>.</p>
<p>Here are the parameter<span> </span>descriptions:</p>
<ul>
<li><kbd>dx1</kbd>: Maximum warping in <kbd>x</kbd> for the top-left corner (pixels)</li>
<li><kbd>dy1</kbd>: <span>Maximum</span> warping in <kbd>y</kbd> for the top-left corner (pixels)</li>
<li><kbd>dx2</kbd>: <span>Maximum</span> warping in <kbd>x</kbd> for the top-right corner (pixels)</li>
<li><kbd>dy2</kbd>: <span>Maximum</span> warping in <kbd>y</kbd> for the top-right corner (pixels)</li>
<li><kbd>dx3</kbd>: <span>Maximum</span> warping in <kbd>x</kbd> for the bottom-right corner (pixels)</li>
<li><kbd>dy3</kbd>: <span>Maximum</span> warping in <kbd>y</kbd> for the bottom-right corner (pixels)</li>
<li><kbd>dx4</kbd>: <span>Maximum</span> warping in <kbd>x</kbd> for the bottom-left corner (pixels)</li>
<li><kbd>dy4</kbd>: <span>Maximum</span> warping in <kbd>y</kbd> for the bottom-left corner (pixels)</li>
</ul>
<p><span>The valu</span>e of delta will be auto adjusted as per the normalized width/<span>height while creating <kbd>ImageRecordReader</kbd>. This means that the given value</span> of delta will be <span>treated relative to the normalized width/height specified while creating <kbd>ImageRecordReader</kbd>. So, let's say we perform 10 pixels of warping across the <em>x</em>/<em>y</em> axis in an image with a size of 100 x 100. If the image is normalized to a size of 30 x 30, then 3 pixels of warping will happen across the <em>x</em>/<em>y</em> axis. You need to experiment with different values for</span> <kbd>delta</kbd> since there<span>'s n</span>o constant/min/max <kbd>delta</kbd> value that can solve all types of image classification problems.</p>
<p><span>In step 3, we used <kbd>RotateImageTransform</kbd> to perform rotational image transformations by rotating the image samples on the</span> angle <span>mentione</span><span>d</span>.</p>
<p><span>In step 4, we added </span><span class="s1">multiple image transformations with the help of <span><kbd>PipelineImageTransform</kbd> </span>into a pipeline to load them sequentially or randomly for training purposes. We have created a pipeline with the <span><kbd>List&lt;Pair&lt;ImageTransform,Double&gt;&gt;</kbd> type. The <kbd>Double</kbd> value in <kbd>Pair</kbd> is the <em>probability</em> that the particular element (<kbd>ImageTransform</kbd>) in the pipeline is executed.</span></span></p>
<p class="mce-root"/>
<div class="mce-root packt_infobox">Image transformations will help CNN to learn image patterns better. Training on top of transformed images will further avoid the chances of overfitting. </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p><kbd>WarpImageTransform</kbd><span> under the hood makes an internal call</span> to the JavaCPP method, <kbd>warpPerspective()</kbd>, with the given <span>properties, </span><kbd>interMode</kbd><span>, </span><kbd>borderMode</kbd><span>, and</span> <kbd>borderValue</kbd>. JavaCPP is <span>an API that parses native C/C++ files and generates Java interfaces to act as a wrapper. We ad</span>ded the JavaCPP dependency for OpenCV in <kbd>pom.xml</kbd> earlier. This will enable us to exploit OpenCV libraries for image t<span>ransformation.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image preprocessing and the design of input layers</h1>
                </header>
            
            <article>
                
<p>Normalization is a crucial preprocessing step for a CNN, just like for any feed forward networks. Image data is complex. Each image has several pixels of information. Also, each pixel is a source of information. We need to normalize this pixel value so that the neural network will not overfit/underfit while training. Convolution/subsampling layers <span>also need to be specified while designing input layers for CNN. In this recipe, we will normalize and then design input layers for the CNN.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Create <kbd>ImagePreProcessingScaler</kbd> for image normalization:</li>
</ol>
<pre style="padding-left: 60px">DataNormalization scaler = new ImagePreProcessingScaler(0,1);<br/><br/></pre>
<ol start="2">
<li>Create a neural network configuration and add default hyperparameters:</li>
</ol>
<pre style="padding-left: 60px">MultiLayerConfiguration.Builder builder = new NeuralNetConfiguration.Builder().weightInit(WeightInit.DISTRIBUTION)<br/> .dist(new NormalDistribution(0.0, 0.01))<br/> .activation(Activation.RELU)<br/> .updater(new Nesterovs(new StepSchedule(ScheduleType.ITERATION, 1e-2, 0.1, 100000), 0.9))<br/> .biasUpdater(new Nesterovs(new StepSchedule(ScheduleType.ITERATION, 2e-2, 0.1, 100000), 0.9))<br/> .gradientNormalization(GradientNormalization.RenormalizeL2PerLayer) // normalize to prevent vanishing or exploding gradients<br/> .l2(l2RegularizationParam)<br/> .list();</pre>
<ol start="3">
<li>Create convolution layers for a CNN using <kbd>ConvolutionLayer</kbd>: </li>
</ol>
<pre style="padding-left: 60px">builder.layer(new ConvolutionLayer.Builder(11,11)<br/> .nIn(channels)<br/> .nOut(96)<br/> .stride(1,1)<br/> .activation(Activation.RELU)<br/> .build());</pre>
<ol start="4">
<li>Configure subsampling layers using <kbd>SubsamplingLayer</kbd>: </li>
</ol>
<pre style="padding-left: 60px">builder.layer(new SubsamplingLayer.Builder(PoolingType.MAX)<br/> .kernelSize(kernelSize,kernelSize)<br/> .build());</pre>
<ol start="5">
<li>Normalize activation between layers using <kbd>LocalResponseNormalization</kbd>:</li>
</ol>
<pre style="padding-left: 60px"> builder.layer(1, new LocalResponseNormalization.Builder().name("lrn1").build());</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, <span><kbd>ImagePreProcessingScaler</kbd> normalizes the pixels in a specified range of values</span> (0, 1) . W<span>e will use this normalizer once we create iterators for the data. </span></p>
<p><span>In step 2, we have added hyperparameters such as an L2 regularization coefficient, a gradient normalization strategy, a gradient update algorithm, and an activation function globally (applicable for all layers).</span></p>
<p><span>In step 3, <kbd>ConvolutionLayer</kbd> requires you to mention the kernel dimensions (11*11 for the previous code). A kernel acts as a feature detector in the context of a CNN:</span></p>
<ul>
<li><kbd>stride</kbd>: Directs the space between each sample in an operation on a pixel grid.</li>
<li><kbd>channels</kbd>: The number of input neurons. We mention the number of color channels here (RGB: 3).</li>
<li><kbd>OutGoingConnectionCount</kbd>: The number of output neurons.</li>
</ul>
<p><span>In step 4, <strong><kbd>SubsamplingLayer</kbd></strong> is a downsampling layer to reduce the amount of data to be transmitted or stored, and, at the same time, keep the significant features intact. Max pooling is the most commonly used sampling method. </span><span class="s1"><kbd><span>ConvolutionLayer</span></kbd> is always followed by <strong><kbd>SubsamplingLayer</kbd></strong>.</span></p>
<p><span class="s1">E</span><span class="s2">fficiency is a challenging task in the case of a CNN. It requires a lot of images, along with transformations, to train better. In step 4, </span><kbd><span class="s3">LocalResponseNormalization</span></kbd><span class="s2"> improves the generalization power of a CNN. It performs a normalization operation right before performing ReLU activation</span></p>
<p><span class="s2">We add this as a separate layer placed between a convolution layer and a subsampling layer:</span></p>
<ul class="ul1">
<li class="li5"><span class="s2"><span><kbd>ConvolutionLayer</kbd> </span>is similar to a feed forward layer, but for performing two-dimensional convolution on images.</span></li>
<li class="li5"><span class="s2"><strong><kbd>SubsamplingLayer</kbd></strong> is required for pooling/downsampling in CNNs.</span></li>
<li class="li5"><span class="s2"><kbd><span>ConvolutionLayer</span></kbd> and <strong><kbd>SubsamplingLayer</kbd></strong> together form the input layers for a CNN and extract abstract features from images and pass them to the hidden layers for further processing.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Constructing hidden layers for a CNN</h1>
                </header>
            
            <article>
                
<p>The input layers of a CNN produce abstract images and pass them to hidden layers. The abstract image features are passed from input layers to the hidden layers. If there are multiple hidden layers in your CNN, then each of them will have unique responsibilities for the prediction. For example, one of them can detect lights and dark in the image, and the following layer can detect edges/shapes with the help of the preceding hidden layer. The next layer can then discern more complex objects from the edges/recipes from the preceding hidden layer, and so on.</p>
<p>In this recipe, we will design hidden layers for our image classification problem.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Build hidden layers using <kbd>DenseLayer</kbd>:</li>
</ol>
<pre style="padding-left: 60px">new DenseLayer.Builder()<br/> .nOut(nOut)<br/> .dist(new NormalDistribution(0.001, 0.005))<br/> .activation(Activation.RELU)<br/> .build();</pre>
<ol start="2">
<li>Add <kbd>AddDenseLayer</kbd> to the layer structure by calling <kbd>layer()</kbd>:</li>
</ol>
<pre style="padding-left: 60px">builder.layer(new DenseLayer.Builder()<br/> .nOut(500)<br/> .dist(new NormalDistribution(0.001, 0.005))<br/> .activation(Activation.RELU)<br/> .build());</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, hidden layers are created using <kbd>DenseLayer</kbd>, which are <span class="s1">preceded by convolution/subsampling layers.</span></p>
<p><span class="s1">In step 2, n</span><span class="s1">ote that we didn't mention the number of input neurons in hidden layers, since it would be same </span><span class="s2">as the preceding layer's (<kbd>SubSamplingLayer</kbd>) outgoing neurons.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Constructing output layers for output classification</h1>
                </header>
            
            <article>
                
<p><span>We need to perform image classification using logistic regressio</span>n (<kbd>SOFTMAX</kbd>), resu<span>lting in probabilities of occurrence for each of the image labels. Logistic regression is a predictive analysis algorithm and, hence, more suitable for prediction problems.</span> In this recipe, we will design output layers for the image classification problem.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Design the output layer using <kbd>OutputLayer</kbd>:</li>
</ol>
<pre style="padding-left: 60px">builder.layer(new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)<br/> .nOut(numLabels)<br/> .activation(Activation.SOFTMAX)<br/> .build());</pre>
<ol start="2">
<li>Set the input type using <kbd>setInputType()</kbd>:</li>
</ol>
<pre style="padding-left: 60px">builder.setInputType(InputType.convolutional(30,30,3));<span><br/></span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, <kbd>nOut()</kbd><span> </span>expects the number of image labels that we calculated using <kbd>FileSplit</kbd> in an earlier recipe.</p>
<p>In step 2, we have used <span><kbd>setInputType()</kbd> to set the convolutional input type. This will trigger computation/settings of the input neurons and add preprocessors (<kbd>LocalResponseNormalization</kbd>) to handle data flow from the convolutional/subsampling layers to the dense layers.</span></p>
<p><span class="s1">The </span><kbd><span class="s2">InputType</span></kbd><span class="s1"> class is used to track and define the types of activations. This is most useful for automatically adding preprocessors between layers, and automatically setting </span><kbd><span class="s2">nIn</span></kbd><span class="s1"> (</span>number of input neurons) value<span class="s1">s. That's how we skipped specif</span>ying <kbd>nIn</kbd> v<span class="s1">alues earlier when configuring the model. The convolutional input type is four-dimensional in shape <kbd>[miniBatchSize, channels, height, width]</kbd>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training images and evaluating CNN output</h1>
                </header>
            
            <article>
                
<p>We have layer configurations in place. Now, we need to train the CNN to make it suitable for predictions. In a CNN, filter values will be adjusted during the training process. The network will learn by itself how to choose proper filters (feature maps) to produce the best results. <span>We will also see that the efficiency and performance of the CNN becomes a challenging task because of the complexity involved in computation. In this recipe, we will train and evaluate our CNN model. </span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Load and initialize the training data using <kbd>ImageRecordReader</kbd>:</li>
</ol>
<pre style="padding-left: 60px">ImageRecordReader imageRecordReader = new ImageRecordReader(imageHeight,imageWidth,channels,parentPathLabelGenerator);<br/> imageRecordReader.initialize(trainData,null);</pre>
<ol start="2">
<li>Create a dataset iterator using <kbd>RecordReaderDataSetIterator</kbd>:</li>
</ol>
<pre style="padding-left: 60px">DataSetIterator dataSetIterator = new RecordReaderDataSetIterator(imageRecordReader,batchSize,1,numLabels);</pre>
<ol start="3">
<li>Add the normalizer to the dataset iterator:</li>
</ol>
<pre style="padding-left: 60px">DataNormalization scaler = new ImagePreProcessingScaler(0,1);<br/> scaler.fit(dataSetIterator);<br/> dataSetIterator.setPreProcessor(scaler);<br/> </pre>
<ol start="4">
<li>Train the model by calling <kbd>fit()</kbd>:</li>
</ol>
<pre style="padding-left: 60px">MultiLayerConfiguration config = builder.build();<br/> MultiLayerNetwork model = new MultiLayerNetwork(config);<br/> model.init();<br/> model.setListeners(new ScoreIterationListener(100));<br/> model.fit(dataSetIterator,epochs);</pre>
<ol start="5">
<li>Train the model again with image transformations:</li>
</ol>
<pre style="padding-left: 60px">imageRecordReader.initialize(trainData,transform);<br/> dataSetIterator = new RecordReaderDataSetIterator(imageRecordReader,batchSize,1,numLabels);<br/> scaler.fit(dataSetIterator);<br/> dataSetIterator.setPreProcessor(scaler);<br/> model.fit(dataSetIterator,epochs);</pre>
<ol start="6">
<li>Evaluate the model and observe the results:</li>
</ol>
<pre style="padding-left: 60px">Evaluation evaluation = model.evaluate(dataSetIterator);<br/> System.out.println(evaluation.stats());<span> <br/></span></pre>
<p style="padding-left: 60px">The evaluation metrics will appear as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1236 image-border" src="assets/ba5b0d89-60a4-4939-b38f-579bc1a573c0.png" style="width:42.33em;height:24.17em;"/></p>
<ol start="7">
<li>Add support for the GPU-accelerated environment by adding the following dependencies:</li>
</ol>
<pre style="padding-left: 60px">&lt;dependency&gt;<br/>  &lt;groupId&gt;org.nd4j&lt;/groupId&gt;<br/>  &lt;artifactId&gt;nd4j-cuda-9.1-platform&lt;/artifactId&gt;<br/>  &lt;version&gt;1.0.0-beta3&lt;/version&gt;<br/> &lt;/dependency&gt;<br/> <br/> &lt;dependency&gt;<br/>  &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;<br/>  &lt;artifactId&gt;deeplearning4j-cuda-9.1&lt;/artifactId&gt;<br/>  &lt;version&gt;1.0.0-beta3&lt;/version&gt;<br/> &lt;/dependency&gt;</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1, t<span class="s1">he parameters included are as follows:</span></p>
<ul class="ul1">
<li class="li2"><kbd><span class="s3">parentPathLabelGenerator</span></kbd><span class="s1">—created during the data extraction stage (see the <em>Extracting images from disk</em> recipe in this chapter).</span></li>
<li class="li2"><kbd><span class="s3">channels</span></kbd><span class="s1">—The number of color channels (default = </span><kbd><span class="s3">3</span></kbd><span class="s1"><span class="s1"> for RGB).</span></span></li>
<li class="li3"><span class="s1"><kbd><span>ImageRecordReader(imageHeight, imageWidth, channels, parentPathLabelGenerator)</span></kbd></span><span class="s1">—resize the actual image to the specified size <kbd>(<span class="s3">imageHeight, imageWidth</span>)</kbd> to reduce the data loading effort.</span></li>
<li class="li3"><span class="s1">The </span><span class="s3">null</span><span class="s1"> attribute in the </span><kbd><span class="s3">initialize()</span></kbd><span class="s1"> method is to indicate that we are not training transformed images.</span></li>
</ul>
<p>In step 3, we use <span><kbd>ImagePreProcessingScaler</kbd> for </span><span class="s1">min-max normalization. </span><span class="s1">Note that we need to use </span><span class="s1">both </span><kbd><span class="s2">fit()</span></kbd><span class="s1"> and </span><kbd>setPreProcessor()</kbd> <span class="s1">to apply normalization to the data.</span></p>
<p>For GPU-accelerated environments, we can use <kbd>PerformanceListener</kbd> instead of <kbd>ScoreIterationListener</kbd> in step 4 to optimize the training process further. <kbd>PerformanceListener</kbd> t<span>racks the time spent on training per iteration, while</span> <kbd>ScoreIterationListener</kbd><strong> </strong><span>reports the score of the network every <em>N</em> iterations during training. Make sure that GPU dependencies are added as per step 7.</span></p>
<p><span>In step 5, we have trained the model again with the image transformations that were created earlier. Make sure to apply normalization to the transformed images as well.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>Our CNN has an accuracy of around 50%. We trained our neural network using 396 images across 4 categories. For an i7 processor with 8 GB of RAM, it will take 15-30 minutes to complete the training. This can vary depending on the applications that are running parallel to the training instance. Training time can also change depending on the quality of the hardware. You will observe better evaluation metrics if you train with more images. More data will contribute toward better predictions. And, of course, it demands extended training time.</p>
<p>Another important aspect is to experiment with the number of hidden layers and subsampling/convolution layers to give you the optimal results. Too many layers could result in overfitting, hence, you really have to experiment by adding a different number of layers to your network configuration. Do not add large values for <kbd>stride</kbd><em>, </em><span>or overly small dimensions for the images. That may cause excessive downsampling and will result in feature loss.</span></p>
<p class="mce-root"/>
<p>We can also try different values for the weights or how weights are distributed across neurons and test different gradient normalization strategies, applying L2 regularization and dropouts. There is no rule of thumb to choose a constant value for L1/L2 regularization or for dropouts. However, the L2 regularization constant takes a smaller value as it<span> </span><span>forces the weights to decay toward zero. Neural networks can safely accommodate dropout of 10-20 percent, beyond which it can actually cause underfitting. There is no constant value that will apply in every instance, as it varies from case to case:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1413 image-border" src="assets/059cd7f6-e130-4234-97bb-71913a34d140.png" style="width:78.92em;height:32.42em;"/></p>
<p>A GPU-accelerated environment will help decrease the training time. DL4J supports CUDA, and it can be accelerated <span>further </span>using cuDNN. Most two-dimensional CNN layers (such as<span> </span><kbd>ConvolutionLayer</kbd> and<span> </span><kbd>SubsamplingLayer</kbd>) support cuDNN.</p>
<p>The NVIDIA <strong>CUDA Deep Neural Network</strong><span> (<strong>cuDNN</strong>) </span>library is a GPU-accelerated library of primitives for deep learning networks. You can read more about cuDNN here:<span> </span><a href="https://developer.nvidia.com/cudnn">https://developer.nvidia.com/cudnn</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating an API endpoint for the image classifier</h1>
                </header>
            
            <article>
                
<p>We want to leverage the image classifier as an API to use them in external applications. An API can be accessed externally, and prediction results can be obtained without setting up anything. In this recipe, we will create an API endpoint for the image classifier.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<ol>
<li>Persist the model using <kbd>ModelSerializer</kbd>:</li>
</ol>
<pre style="padding-left: 60px">File file = new File("cnntrainedmodel.zip");<br/> ModelSerializer.writeModel(model,file,true);<br/> ModelSerializer.addNormalizerToModel(file,scaler);</pre>
<ol start="2">
<li>Restore the trained model using <kbd>ModelSerializer</kbd> to perform predictions:</li>
</ol>
<pre style="padding-left: 60px">MultiLayerNetwork network = ModelSerializer.restoreMultiLayerNetwork(modelFile);<br/> NormalizerStandardize normalizerStandardize = ModelSerializer.restoreNormalizerFromFile(modelFile);</pre>
<ol start="3">
<li>Design an API method that accepts inputs from users and returns results. An example API method would look like the following:</li>
</ol>
<pre style="padding-left: 60px">public static INDArray generateOutput(File file) throws IOException, InterruptedException {<br/> final File modelFile = new File("cnnmodel.zip");<br/> final MultiLayerNetwork model = ModelSerializer.restoreMultiLayerNetwork(modelFile);<br/> final RecordReader imageRecordReader = generateReader(file);<br/> final NormalizerStandardize normalizerStandardize = ModelSerializer.restoreNormalizerFromFile(modelFile);<br/> final DataSetIterator dataSetIterator = new RecordReaderDataSetIterator.Builder(imageRecordReader,1).build();<br/> normalizerStandardize.fit(dataSetIterator);<br/> dataSetIterator.setPreProcessor(normalizerStandardize);<br/> return model.output(dataSetIterator);<br/> }<span><br/></span></pre>
<ol start="4">
<li>Create a URI mapping to service client requests, as shown in the following example:</li>
</ol>
<pre style="padding-left: 60px">@GetMapping("/")<br/> public String main(final Model model){<br/> model.addAttribute("message", "Welcome to Java deep learning!");<br/> return "welcome";<br/> }<br/> <br/> @PostMapping("/")<br/> public String fileUpload(final Model model, final @RequestParam("uploadFile")MultipartFile multipartFile) throws IOException, InterruptedException {<br/> final List&lt;String&gt; results = cookBookService.generateStringOutput(multipartFile);<br/> model.addAttribute("message", "Welcome to Java deep learning!");<br/> model.addAttribute("results",results);<br/> return "welcome";<br/> }</pre>
<ol start="5">
<li>Build a <kbd>cookbookapp-cnn</kbd> project and add the API dependency to your Spring project:</li>
</ol>
<pre style="padding-left: 60px">&lt;dependency&gt;<br/> &lt;groupId&gt;com.javadeeplearningcookbook.app&lt;/groupId&gt;<br/> &lt;artifactId&gt;cookbookapp-cnn&lt;/artifactId&gt;<br/> &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;<br/> &lt;/dependency&gt;</pre>
<ol start="6">
<li>Create the <kbd>generateStringOutput()</kbd> <span>method in the</span> service layer to serve API content:</li>
</ol>
<pre style="padding-left: 60px">@Override<br/> public List&lt;String&gt; generateStringOutput(MultipartFile multipartFile) throws IOException, InterruptedException {<br/> //TODO: MultiPartFile to File conversion (multipartFile -&gt; convFile)<br/> INDArray indArray = ImageClassifierAPI.generateOutput(convFile);<br/><br/> for(int i=0; i&lt;indArray.rows();i++){<br/>           for(int j=0;j&lt;indArray.columns();j++){<br/>                   DecimalFormat df2 = new DecimalFormat("#.####");<br/>                   results.add(df2.format(indArray.getDouble(i,j)*100)+"%"); <br/>                //Later add them from list to the model display on UI.<br/>            }            <br/>        }<br/>  convFile.deleteOnExit();<br/>   return results;<br/> }<span><br/> <br/></span></pre>
<ol start="7">
<li>Download and install the Google Cloud SDK: <a href="https://cloud.google.com/sdk/">https://cloud.google.com/sdk/</a>.</li>
</ol>
<ol start="8">
<li>Install the Cloud SDK <kbd>app-engine-java</kbd> component by running the following command on the Google Cloud console:</li>
</ol>
<pre style="padding-left: 60px"><strong>gcloud components install app-engine-java</strong></pre>
<ol start="9">
<li>Log in and configure Cloud SDK using the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>gcloud init</strong></pre>
<ol start="10">
<li>Add the following dependency for Maven App Engine in <kbd>pom.xml</kbd>:</li>
</ol>
<pre style="padding-left: 60px">&lt;plugin&gt;<br/> &lt;groupId&gt;com.google.cloud.tools&lt;/groupId&gt;<br/> &lt;artifactId&gt;appengine-maven-plugin&lt;/artifactId&gt;<br/> &lt;version&gt;2.1.0&lt;/version&gt;<br/> &lt;/plugin&gt;</pre>
<ol start="11">
<li>Create an <kbd>app.yaml</kbd> file in your project as per the Google Cloud documentation:<br/>
<a href="https://cloud.google.com/appengine/docs/flexible/java/configuring-your-app-with-app-yaml">https://cloud.google.com/appengine/docs/flexible/java/configuring-your-app-with-app-yaml</a>.</li>
<li>Navigate to Google App Engine and click on the <span class="packt_screen">Create Application</span> button:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1238 image-border" src="assets/03aa855c-b3a4-4bdc-9027-aad8828684f2.png" style="width:150.75em;height:65.83em;"/></p>
<ol start="13">
<li>Pick a region and click on <span class="packt_screen">Create app</span>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1239 image-border" src="assets/cf2b0a73-1c7b-4628-96c3-1c20f1cd9d75.png" style="width:39.58em;height:42.08em;"/></p>
<p class="mce-root"/>
<ol start="14">
<li>Select <span class="packt_screen">Java</span> and click the <span class="packt_screen">Next</span> button:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1240 image-border" src="assets/91c64ce6-3d2b-498b-9c14-3cb8453e924d.png" style="width:48.50em;height:26.50em;"/></p>
<p style="padding-left: 60px">Now, your app engine has been created at Google Cloud.</p>
<ol start="15">
<li>Build the spring boot application using Maven:</li>
</ol>
<pre style="padding-left: 60px"><strong>mvn clean install</strong></pre>
<ol start="16">
<li>Deploy the application using the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>mvn appengine:deploy</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In step 1 and step 2, we have persisted the model to reuse the model capabilities in API. </p>
<p>In step 3, an API method is created to accept user inputs and return the results from the image classifier. </p>
<p>In step 4, the URI mappings will accept client requests (GET/POST). A GET request will serve the home page at the very beginning. A POST request will serve the end user request for image classification.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In step 5, we added an API dependency to the <kbd>pom.xml</kbd> file. For demonstration purposes, we build the API JAR file and the JAR file is stored in the local Maven repository. For production, you need to submit your API (JAR file) in a private repository so that Maven can fetch it from there. </p>
<p>In step 6, we are calling the ImageClassifier API at our Spring Boot application service layer to retrieve the results and return them to the controller class. </p>
<p>In the previous chapter, we deployed the application locally for demonstration purposes. In this chapter, we have deployed the application in Google Cloud. Steps 7 to 16 are dedicated to deployment in Google Cloud.</p>
<p>We have used Google App Engine, although we can set up the same thing in more customized ways using Google Compute Engine or Dataproc. Dataproc is designed to deploy your application in a Spark distributed environment. </p>
<p><span>Once deployment is successful, you should see something like the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1414 image-border" src="assets/fa8bb5bd-d9d9-43ff-ae39-b882886b7cf4.png" style="width:106.67em;height:26.50em;"/></p>
<p>When you hit the URL (which starts with <kbd>https://xx.appspot.com</kbd>), you should be able to see the web page (the same as in the previous chapter) where end users can upload images for image classification. </p>


            </article>

            
        </section>
    </body></html>