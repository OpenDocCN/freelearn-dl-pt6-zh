["```py\n# Read data\nlibrary(keras)\ndata <- read.csv('~/Desktop/data/CTG.csv', header=T)\nstr(data)\n\nOUTPUT\n ## 'data.frame': 2126 obs. of 22 variables:\n ## $ LB : int 120 132 133 134 132 134 134 122 122 122 ...\n ## $ AC : num 0 0.00638 0.00332 0.00256 0.00651 ...\n ## $ FM : num 0 0 0 0 0 0 0 0 0 0 ...\n ##  $ UC      : num  0 0.00638 0.00831 0.00768 0.00814 ...\n ##  $ DL      : num  0 0.00319 0.00332 0.00256 0 ...\n ##  $ DS      : num  0 0 0 0 0 0 0 0 0 0 ...\n ##  $ DP      : num  0 0 0 0 0 ...\n ##  $ ASTV    : int  73 17 16 16 16 26 29 83 84 86 ...\n ##  $ MSTV    : num  0.5 2.1 2.1 2.4 2.4 5.9 6.3 0.5 0.5 0.3 ...\n ##  $ ALTV    : int  43 0 0 0 0 0 0 6 5 6 ...\n ##  $ MLTV    : num  2.4 10.4 13.4 23 19.9 0 0 15.6 13.6 10.6 ...\n ##  $ Width   : int  64 130 130 117 117 150 150 68 68 68 ...\n ##  $ Min     : int  62 68 68 53 53 50 50 62 62 62 ...\n ##  $ Max     : int  126 198 198 170 170 200 200 130 130 130 ...\n ##  $ Nmax    : int  2 6 5 11 9 5 6 0 0 1 ...\n ##  $ Nzeros  : int  0 1 1 0 0 3 3 0 0 0 ...\n ##  $ Mode    : int  120 141 141 137 137 76 71 122 122 122 ...\n ##  $ Mean    : int  137 136 135 134 136 107 107 122 122 122 ...\n ##  $ Median  : int  121 140 138 137 138 107 106 123 123 123 ...\n ##  $ Variance: int  73 12 13 13 11 170 215 3 3 1 ...\n ##  $ Tendency: int  1 0 0 1 1 0 0 1 1 1 ...\n ##  $ NSP     : int  2 1 1 1 1 3 3 3 3 3 ...\n```", "```py\n# Normalize data \ndata <- as.matrix(data)\n dimnames(data) <- NULL  \n data[,1:21] <- normalize(data[,1:21])\n data[,22] <- as.numeric(data[,22]) -1\n```", "```py\n# Data partition \nset.seed(1234)\n ind <- sample(2, nrow(data), replace = T, prob=c(.7, .3))\n training <- data[ind==1, 1:21]\n test <- data[ind==2, 1:21]\n trainingtarget <- data[ind==1, 22]\n testtarget <- data[ind==2, 22]\n```", "```py\n# One-hot encoding\n trainLabels <- to_categorical(trainingtarget)\n testLabels <- to_categorical(testtarget)\n print(testLabels[1:10,])\n\nOUTPUT\n ##        [,1] [,2] [,3]\n ##   [1,]    1    0    0\n ##   [2,]    1    0    0\n ##   [3,]    1    0    0\n ##   [4,]    0    0    1\n ##   [5,]    0    0    1\n ##   [6,]    0    1    0\n ##   [7,]    1    0    0\n ##   [8,]    1    0    0\n ##   [9,]    1    0    0\n ##  [10,]    1    0    0\n\n```", "```py\n# Initializing the model\n model <- keras_model_sequential()\n\n# Model architecture\n model %>% \n layer_dense(units = 8, activation = 'relu', input_shape = c(21)) %>% \n layer_dense(units = 3, activation = 'softmax')\n```", "```py\n# Model summary\n summary(model)\n\n OUTPUT\n ## ___________________________________________________________________________\n ## Layer    (type) Output Shape Param #\n ## ===========================================================================\n ## dense_1 (Dense) (None, 8)      176\n ## ___________________________________________________________________________\n ## dense_2 (Dense) (None, 3)       27\n ## ===========================================================================\n ## Total params: 203\n ## Trainable params: 203\n ## Non-trainable params: 0\n ## ___________________________________________________________________________\n```", "```py\n# Compile model \nmodel %>% \n   compile(loss = 'categorical_crossentropy', \n   optimizer = 'adam',\n   metrics = 'accuracy')\n```", "```py\n# Fit model\nmodel_one <- model %>%   \n fit(training, \n   trainLabels, \n   epochs = 200,\n   batch_size = 32, \n   validation_split = 0.2)\n\nOUTPUT (last 3 epochs)\nEpoch 198/200\n1218/1218 [==============================] - 0s 43us/step - loss: 0.3662 - acc: 0.8555 - val_loss: 0.5777 - val_acc: 0.8000\nEpoch 199/200\n1218/1218 [==============================] - 0s 41us/step - loss: 0.3654 - acc: 0.8530 - val_loss: 0.5763 - val_acc: 0.8000\nEpoch 200/200\n1218/1218 [==============================] - 0s 40us/step - loss: 0.3654 - acc: 0.8571 - val_loss: 0.5744 - val_acc: 0.8000\n```", "```py\n plot(model_one)\n```", "```py\n# Model evaluation\n model %>% \n evaluate(test, testLabels) \n\nOUTPUT\n ## $loss\n ## [1] 0.4439415\n ##\n ## $acc\n ## [1] 0.8424544\n```", "```py\n# Prediction and confusion matrix\n pred <- model %>%\n   predict_classes(test)\n table(Predicted=pred, Actual=testtarget)\n\nOUTPUT\n          Actual\n ## Predicted   0   1   2\n ##         0 435  41  11\n ##         1  24  51  16\n ##         2   1   2  22\n```", "```py\n# Prediction probabilities\nprob <- model %>%\n    predict_proba(test)\ncbind(prob, pred, testtarget)[1:7,]\n\nOUTPUT\n                                         pred testtarget\n[1,] 0.993281245 0.006415705 0.000302993    0          0\n[2,] 0.979825318 0.018759586 0.001415106    0          0\n[3,] 0.982333243 0.014519051 0.003147765    0          0\n[4,] 0.009040437 0.271216542 0.719743013    2          2\n[5,] 0.008850170 0.267527819 0.723622024    2          2\n[6,] 0.946622312 0.030137880 0.0232398603   0          1\n[7,] 0.986279726 0.012411724 0.0013086179   0          0\n```", "```py\n# Model architecture\nmodel <- keras_model_sequential()\nmodel %>% \n   layer_dense(units = 8, activation = 'relu', input_shape = c(21)) %>%   \n   layer_dense(units = 5, activation = 'relu') %>% \n   layer_dense(units = 3, activation = 'softmax')\n\nsummary(model)\n\nOUTPUT\n___________________________________________________________________________\nLayer (type)                   Output Shape                Param #     \n===========================================================================\ndense_1 (Dense)                (None, 8)                    176          \n___________________________________________________________________________\ndense_2 (Dense)                (None, 5)                    45           \n___________________________________________________________________________\ndense_3 (Dense)                (None, 3)                    18           \n===========================================================================\nTotal params: 239\nTrainable params: 239\nNon-trainable params: 0\n___________________________________________________________________________\n```", "```py\n# Compile and fit model\nmodel %>% \n compile(loss = 'categorical_crossentropy', \n optimizer = 'adam',\n metrics = 'accuracy')\nmodel_two <- model %>%   \n   fit(training, \n       trainLabels, \n       epochs = 200,\n       batch_size = 32,  \n       validation_split = 0.2)\n plot(model_two)\n```", "```py\n# Prediction and confusion matrix\npred <- model %>% \n    predict_classes(test)\ntable(Predicted=pred, Actual=testtarget)\n\nOUTPUT\n          Actual\n ## Predicted   0   1   2\n ##         0 429  38   4\n ##         1  29  54  33\n ##         2   2   2  12\n```", "```py\n# Model architecture\n model <- keras_model_sequential()\n model %>% \n   layer_dense(units = 30, activation = 'relu', input_shape = c(21)) %>% \n   layer_dense(units = 3, activation = 'softmax') \n\nsummary(model)\nOUTPUT\n__________________________________________________________________________\nLayer (type)                   Output Shape               Param #      \n==========================================================================\ndense_1 (Dense)                (None, 30)                  660          \n__________________________________________________________________________\ndense_2 (Dense)                (None, 3)                   93           \n==========================================================================\nTotal params: 753\nTrainable params: 753\nNon-trainable params: 0\n__________________________________________________________________________\n\n# Compile model\n model %>% \n   compile(loss = 'categorical_crossentropy', \n           optimizer = 'adam',\n           metrics = 'accuracy')\n\n# Fit model\nmodel_three <- model %>%\n   fit(training, \n       trainLabels, \n       epochs = 200,\n       batch_size = 32,\n       validation_split = 0.2)\n plot(model_three )\n```", "```py\n# Prediction and confusion matrix\n pred <- model %>%\n    predict_classes(test)\n table(Predicted=pred, Actual=testtarget)\n\nOUTPUT\n          Actual\n ## Predicted   0   1   2\n ##         0 424  35   5\n ##         1  28  55   5\n ##         2   8   4  39\n```", "```py\n# Model architecture\nmodel <- keras_model_sequential()\nmodel %>%\n         layer_dense(units = 40, activation = 'relu', input_shape = c(21)) %>%\n         layer_dropout(rate = 0.4) %>%\n         layer_dense(units = 30, activation = 'relu') %>%\n         layer_dropout(rate = 0.3) %>%\n         layer_dense(units = 20, activation = 'relu') %>%\n         layer_dropout(rate = 0.2) %>%\n         layer_dense(units = 3, activation = 'softmax')\nsummary(model)\n\nOUTPUT\n__________________________________________________________________________\nLayer (type)                  Output Shape                 Param #     \n==========================================================================\ndense_1 (Dense)                (None, 40)                   880          \n__________________________________________________________________________\ndropout_1 (Dropout)            (None, 40)                    0            \n__________________________________________________________________________\ndense_2 (Dense)                (None, 30)                   1230         \n__________________________________________________________________________\ndropout_2 (Dropout)            (None, 30)                    0            \n__________________________________________________________________________\ndense_3 (Dense)                (None, 20)                   620          \n__________________________________________________________________________\ndropout_3 (Dropout)            (None, 20)                    0            \n__________________________________________________________________________\ndense_4 (Dense)                (None, 3)                     63           \n==========================================================================\nTotal params: 2,793\nTrainable params: 2,793\nNon-trainable params: 0\n___________________________________________________________________________\n\n# Compile model\n model %>% \n   compile(loss = 'categorical_crossentropy', \n           optimizer = 'adam',\n           metrics = 'accuracy')\n\n# Fit model\nmodel_four <- model %>% \n fit(training, \n trainLabels, \n epochs = 200,\n batch_size = 32, \n validation_split = 0.2)\nplot(model_four)\n```", "```py\n# Predictions and confusion matrix\npred <- model %>% \n         predict_classes(test)\ntable(Predicted=pred, Actual=testtarget)\n\nOUTPUT\n         Actual\nPredicted   0   1   2\n        0 431  34   7\n        1  20  53   2\n        2   9   7  40\n```", "```py\n# Bar plot\nbarplot(prop.table(table(data$NSP)),\n        col = rainbow(3),\n        ylim = c(0, 0.8),\n        ylab = 'Proportion',\n        xlab = 'NSP',\n        cex.names = 1.5)\n```", "```py\n# Fit model\nmodel_five <- model %>% \n  fit(training, \n      trainLabels,\n      epochs = 200,\n      batch_size = 32,\n      validation_split = 0.2,\n      class_weight = list(\"0\"=1,\"1\"=5.6, \"2\" = 9.4))\nplot(model_five)\n```", "```py\n# Prediction and confusion matrix\npred <- model %>% \n  predict_classes(test)\ntable(Predicted=pred, Actual=testtarget)\n\nOUTPUT\n         Actual\nPredicted   0   1   2\n        0 358  12   3\n        1  79  74   5\n        2  23   8  41\n```", "```py\n# Save and reload model\nsave_model_hdf5(model, \n filepath, \n overwrite = TRUE,\n include_optimizer = TRUE)\nmodel_x <- load_model_hdf5(filepath, \n custom_objects = NULL, \n compile = TRUE)\n```"]