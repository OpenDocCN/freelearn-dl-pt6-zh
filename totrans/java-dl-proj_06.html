<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Real-Time Object Detection using YOLO, JavaCV, and DL4J</h1>
                </header>
            
            <article>
                
<pre><strong>Deep Convolutional Neural Networks</strong> (<strong>DCNN</strong>) have been used in computer vision—for example, image classification, image feature extraction, object detection, and semantic segmentation. Despite such successes of state-of-the-art approaches for object detection from still images, detecting objects in a video is not an easy job.</pre>
<p>Considering this drawback, in this chapter, we will develop an end-to-end project that will detect objects from video frames when a video clip plays continuously. We will be utilizing a trained YOLO model for transfer learning and JavaCV techniques on top of <strong>Deeplearning4j</strong> (<strong>DL4J</strong>) to do this. In short, the following topics will be covered throughout this end-to-end project:</p>
<ul>
<li>Object detection</li>
<li>Challenges in object detection from videos</li>
<li>Using YOLO with DL4J</li>
<li>Frequently asked questions (FAQs)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Object detection from images and videos</h1>
                </header>
            
            <article>
                
<p>Deep learning has been widely applied to various computer vision tasks such as image classification, object detection, semantic segmentation, and human pose estimation. When we intend to solve the object detection problem from images, the whole process starts from object classification. Then we perform object localization and finally, we perform the object detection.</p>
<div class="packt_infobox">This project is highly inspired by the Java Autonomous driving – Car detection article by Klevis Ramo (<a href="http://ramok.tech/">http://ramok.tech/</a>). Also, some theoretical concepts are used (but significantly extended for this need) with due permission from the author. </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Object classification, localization, and detection</h1>
                </header>
            
            <article>
                
<p>In an object classification problem, from a given image (or video clip), we're interested to know if it contains a region of interest (ROI) or object. More formally, "the image contains a car" versus "the image does not contain any car." To solve this problem, over the last few years, ImageNet and PASCAL VOC (see more at <a href="http://host.robots.ox.ac.uk/pascal/VOC/">http://host.robots.ox.ac.uk/pascal/VOC/</a>) have been in use and are based on deep CNN architectures .</p>
<p>Moreover, of course, the latest technological advances (that is, both software and hardware) have contributed to boosting the performance to the next level. Despite such success of state-of-the-art approaches for still images, detecting objects in videos is not easy. However, object detection from videos brings up new questions, possibilities, and challenges on how to solve the object detection problem for videos effectively and robustly.</p>
<p>Answering this question is not an easy job. First, let's try to solve the problem step by step. First let's try to answer for an still image. Well, when we want to use a CNN to predict if an image contains a particular object or not, we need to localize the object position in the image. To do that, we need to specify the position of the object in the image along with the classification task. This is usually done by marking the object with a rectangular box commonly known as a bounding box.</p>
<p>Now, the idea of a bounding box is that at each frame of a video, the algorithm is required to annotate bounding boxes and confidence scores on objects of each class. To grab the idea clearly, take a look at what a bounding box is. A bounding box is usually represented by the center (<em>b<sup>x</sup></em>, <em>b<sup>y</sup></em>), rectangle height (<em>b<sup>h</sup></em>), and rectangle width (<em>b<sup>w</sup></em>), as shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/518cdb57-d806-492e-9d1d-57fe445afe3d.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">A bounding box representation</div>
<p>Now that we know how to represent such a bounding box, we can perceive what we need to define these things in our training data for each of the objects in the images too. Only then will <span>the network </span>be able to output the following:</p>
<ul>
<li>The probability of an image's class number (for example, 20% probability of being a car, 60% probability of being a bus, 10% probability of being a truck, or 10% probability of being a train)</li>
<li>Also, the four variables defining the bounding box of the object</li>
</ul>
<p>Knowing only this information is not enough. Interestingly, with this minimum contextual information about bounding box points (that is, center, width, and height), our model is still able to predict and give us a more detailed view of the content. In other words, using this approach, we can solve the object localization problem, but still it is applicable only for a single object.</p>
<p>Therefore, we can even go a step further by localizing not just a single object but <span>multiple</span><span> </span><span>or</span> all objects in the image, which will help us move toward the object detection problem. Although the structure of the original image remains the same, we need to deal with multiple bounding boxes in a single image.</p>
<p>Now to crack this problem, a state-of-the-art technique is dividing the image into smaller rectangles. And we have the same additional five variables we have already seen (<em>P<sup>c</sup>, b<sup>x</sup>, b<sup>y</sup></em>, <em>b<sup>h</sup></em>, <em>b<sup>w</sup></em>) and, of course, the normal prediction probabilities at each bounding box.</p>
<p>The idea sounds easy, but how would it work in practice? If we just need to deal with a static image classification problem, things become easier. Using a Naïve approach would be cropping each car image from a collection of thousands of car images and then we training a convolution neural network (for example, VGG-19) to train the model with all these images (although, each image might have a different size).</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/182e01a5-2ec9-4892-8184-ef2d89b6918c.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Typical highway traffic</div>
<p>Now, to handle this scenario, we can scan the image with a sliding rectangle window and each time let our model predict if there is a car in it or not. As we can see by using different sizes of rectangles, we can figure out quite a different shape for cars and their positions.</p>
<p>Albeit, this works quite well in detecting only cars, but imagine a more practical problem such as developing an autonomous driving application. On a typical highway, in a city even in a suburb, there will be many cars, buses, trucks, motorbikes, bicycles, and other vehicles. Moreover, there will be other objects, such as pedestrians, traffic signs, bridges, dividers, and lamp-posts. These will make the scenario more complicated. Nevertheless, the size of real images will very different (that is, much bigger) compared to cropped ones. Also, in the front, many cars might be approaching, so manual resizing, feature extraction and then handcraft training would be necessary.</p>
<p>Another issue would be the slowness of the training algorithm, so it would not be used for real-time video object detection. This application will be built so that you folks can learn something useful so that the same knowledge can be extended and applied to emerging applications, such as autonomous driving.</p>
<p>Anyway, let's come to the original discussion. When moving the rectangle (right, left, up and down), many shared pixels may not be reused but they are just recalculated repeatedly. Even with very accurate and different bounding box sizes, this approach will fail to mark the object with a bounding box very precisely.</p>
<p>Consequently, the model may not output the class of the vehicle very accurately as if the box may include only a part of the object. This might lead an autonomous driving car to be accident-prone—that is, may collide with other vehicles or objects. Now to get rid of this limitation, one of the state-of-the-art approaches is using the <strong>Convolutional Sliding Window</strong> (<strong>CSW</strong>) solution, which is pretty much utilized in YOLO (we will see this later on).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional Sliding Window (CSW)</h1>
                </header>
            
            <article>
                
<p>In the previous subsection, we have seen that the Naïve sliding window-based approach has severe performance drawbacks since this type of approach is not able to reuse many of the values already computed.</p>
<p>Nevertheless, when each individual window moves, we need to execute millions of hyperparameters for all pixels in order to get a prediction. In reality, most of the computation could be reused by introducing convolution (refer to <a href="2d4fc6f2-3753-456c-8774-3f41dbe13cfb.xhtml" target="_blank">Chapter 5</a>, <em>Image Classification using Transfer Learning</em>, to get to know more on transfer learning using pre-trained DCNN architecture for image classification). This can be achieved in two incremental ways:</p>
<ul>
<li>By turning full-connected CNN layers into convolution</li>
<li><span class="ez-toc-section">Using CSW</span></li>
</ul>
<p>We have seen that whatever DCNN architectures people are using (for example, DarkNet, VGG-16, AlexNet, ImageNet, ResNet, and Inception), regardless of their size and configuration, in the end, they were used to feed fully connected neural nets with a different number of layers and output several predictions depending on classes.</p>
<p>In addition, these deep architectures typically have so many layers that it is difficult to interpret them well. Therefore, taking a smaller network sounds a reasonable choice. In the following diagram, the network takes a colored image (that is, RGB) of size 32 x 32 x 3 as input. It then uses the same convolution, which leaves the first two dimensions (that is, width x height) unchanged at 3 x 3 x 64 in order to get an output 32 x 32 x 64. This way, the 3rd dimension (that is, 64) remains the same as conv matrix.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/14c94add-449d-4c09-ab47-49d25838b4d0.png" style=""/></div>
<p>Then, a maximum pooling layer is placed to reduce the width and height but leaving the 3rd dimension unchanged at 16 x 16 x 64. After that, the reduced layer is fed into a dense layer having 2 hidden layers with 256 and 128 neurons each. In the end, the network outputs the probabilities of five classes using a Softmax layer.</p>
<p>Now, let's see how we can replace <strong>fully connected</strong> (<strong>FC</strong>) layers with conv layers while leaving the linear function of the input at 16 x 16 x 64, as illustrated by the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a8745d97-47a6-4e71-9f1b-3e913238ab0f.png" style=""/></div>
<p>In the preceding diagram, we just replaced the FC layers with conv filters. In reality, a 16 x 16 x 256 conv filter is equivalent to a 16 x 16 x 64 x 256 matrix. In that case, the third dimension, 64, is always the same as the third dimension input 16 x 16 x 64. Therefore, it can be written as a 16 x 16 x 256 conv filter by omitting the 64, which is actually equivalent to the corresponding FC layer. The following math provides the answer as to why:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><em>Out: 1 x 1 x 256 = in: [16 x 16 x 64] * conv: [16 x 16 x 64 x 256]</em></p>
<p>The preceding math signifies that every element of the output 1 x 1 x 256 is a linear function of a corresponding element of the input 16 x 16 x 64. The reason why we bothered to convert FC layers to convolution layers is that it will give us more flexibility in the way output is generated by the network: with an FC layer, we will always have the same output size, which is a number of classes.</p>
<p>Now, in order  to see the effectiveness of replacing the FC layers with a convolution filter, we have to take an input image having a bigger size, say 36 x 36 x 3. Now, if we use the Naïve sliding window technique, where stride is 2 and we have FC, we need to move the original image size nine times to cover all, therefore, executing the model nine times as well. Hence, it definitely does not make sense to adopt that approach. Instead, let us try now to apply this new bigger matrix as input for our new model with convolution layers only.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7025aace-6796-4bb6-bca7-0181884071df.png"/></div>
<p>Now, we can see that the output has changed from 1 x 1 x 5 to 3 x 3 x 5, which is in comparison to FC. Again, if we recall the CSW-based approach, where we had to move the sliding window nine times to cover all images, which is interestingly equal to the 3 x 3 output of conv, nonetheless, each 3 x 3 cell represents the probability prediction results of a sliding window of 1 x 1 x 5 class! Therefore, instead of having only one 1 x 1 x 5 with 9 lots of sliding window moves, the output now with one shot is 3 x 3 x 5.</p>
<p>Now, using the CSW-based approach, we have been able to solve the object detection problem from images. Yet, this approach is not so accurate but still, will produce an acceptable result with marginal accuracy. Nevertheless, when it comes to real-time video, things get much more complicated. We will see how YOLO has solved the remaining limitation later in this chapter. For the time being, let's try to understand the underlying complexity in detecting objects from video clips.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Object detection from videos</h1>
                </header>
            
            <article>
                
<p>Let's think of a simple scenario before digging down deeper. Suppose we have a video clip containing the movement of a cat or wolf in a forest. Now we want to detect this animal but while on the move at each timestep.</p>
<p>The following graph shows the challenges in such a scenario. The red boxes are ground truth annotations. The upper part of the figure (that is <strong>a</strong>) shows still-image object detection methods have large temporal fluctuations across frames, even on ground truth bounding boxes. The fluctuations may result from motion blur, video defocus, part occlusion and bad pose. Information from boxes of the same object on adjacent frames needs to be utilized for object detection in video.</p>
<p>On the other hand, (<strong>b</strong>) shows tracking is able to relate boxes of the same object. However, due to occlusions, appearance changes and pose variations, the tracked boxes may drift to non-target objects. Object detectors should be incorporated into tracking algorithms to constantly start new tracks when drifting occurs.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6e9d0c22-358d-4b9c-ad96-984e81c33202.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Challenges in object detection from the video (source: Kai Kang et al, Object Detection from Video Tubelets with Convolutional Neural Networks)</div>
<p>There are methods to tackle this problem. However, most of them focus on detecting one specific class of objects, such as pedestrians, cars, or humans with actions.</p>
<p>Fortunately, similar to object detection in still images being able to assist tasks including image classification, localization, and object detection, accurately detecting objects in videos could possibly boost the performance of video classification as well. By locating objects in the videos, the semantic meaning of a video could also be described more clearly, which results in more robust performance for video-based tasks.</p>
<p>In other words, existing methods for general object detection cannot be applied to solve this problem effectively. Their performance may suffer from large appearance changes of objects in videos. For instance, in the preceding graph (<strong>a</strong>), if the cat faces the camera at first and then turns back, it's back image cannot be effectively recognized as a cat because it contains little texture information and is not likely to be included in the training. Nevertheless, this is a simpler scenario where we have to detect a single object (that is, an animal).</p>
<p>When we want to develop an application for an autonomous driving car, we will have to deal with so many objects and considerations. Anyway, since we cannot cover all the aspects through this chapter, let's move to solving the problem with only a modicum of knowledge.</p>
<p>Furthermore, implementing and training these types of applications from scratch is time consuming and challenging too. Therefore, nowadays, transfer learning techniques are becoming popular and viable options. By utilizing a trained model, we can develop much more easily. One such trained object detection framework is YOLO, which is one of the state-of-the-art real-time object detection systems. These challenges and YOLO like frameworks have motivated me to develop this project with a minimum of effort.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">You Only Look Once (YOLO)</h1>
                </header>
            
            <article>
                
<p>Although we already addressed issues in object detection from static images by introducing convolution-sliding windows, our model still may not output very accurate bounding boxes, even with several bounding box sizes. Let's see how YOLO solves that problem well:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a8d5df7e-72c6-47c5-a951-4a387edd4f3b.jpg" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Using the bounding box specification, we go to each image and mark the objects we want to detect</div>
<p>We need to label our training data in some specific way so that the YOLO algorithm will work correctly. YOLO V2 format requires bounding box dimensions of <em>b<sup>x</sup></em>, <em>b<sup>y</sup></em> and <em>b<sup>h</sup></em>, <em>b</em><sup><em>w</em>  </sup>in order to be relative to the original image width and height.</p>
<p>First, we normally go to each image and mark the objects we want to detect. After that, each image is split into a smaller number of rectangles (boxes), usually, 13 x 13 rectangles, but here, for simplicity, we have 8 x 9. Both the bounding box (blue) and the object can be part of several boxes (green), so we can assign the object and the bounding box only to the box owning the centre of the object (yellow boxes).</p>
<p>This way, we train our model with four additional (besides identifying the object as a car) variables (<em>b<sup>x</sup></em>, <em>b<sup>y</sup></em>, <em>b<sup>h</sup></em>, and <em>b<sup>w</sup></em>) and assign those to the box owning the center <em>b<sup>x</sup></em>, <em>b<sup>y</sup></em>. Since the neural network is trained with this labeled data, it also predicts this four variables (besides what object is) values or bounding boxes.</p>
<p>Instead of scanning with predefined bounding box sizes and trying to fit the object, we let the model learn how to mark objects with bounding boxes. Therefore, bounding boxes are now flexible. This is definitely a better approach and the accuracy of the bounding boxes is much higher and more flexible.</p>
<p>Let us see how we can represent the output now that we have added four variables (<em>b<sup>x</sup></em>, <em>b<sup>y</sup></em>, <em>b<sup>h</sup></em>, and <em>b<sup>w</sup></em>) beside classes like 1-car, 2-pedestrian. In reality, another variable, <em>P<sup>c</sup></em>, is also added, which simply tells if the image has any of the objects we want to detect at all.</p>
<ul>
<li><strong>P<sup>c</sup> =1(red)</strong>: This means there is at least one of the objects, so it is worth looking at probabilities and bounding boxes</li>
<li><strong>P<sup>c</sup> =0(red)</strong>: The image has none of the objects we want so we do not care about probabilities or bounding box specifications</li>
</ul>
<p>The resultant predictions, <em>b<sub>w,</sub></em> and <em>b<sub>h</sub></em>, are normalized by the height and width of the image. (Training labels are chosen this way.) So, if the predictions <em>b<sub>x</sub></em> and <em>b<sub>y</sub></em> for the box containing the car are (0.3, 0.8), then the actual width and height on 13 x 13 feature maps are (13 x 0.3, 13 x 0.8).</p>
<p>A bounding box predicted by YOLO is defined relative to the box that owns the center of the object (yellow). The upper left corner of the box starts from (0, 0) and the bottom right (1, 1). Since the point is inside the box, in such a scenario, the sigmoid activation function makes sure that the center (<em>b<sub>x</sub></em> , <em>b<sub>y</sub></em>) is in the range 0-1, as outlined in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4ff68531-32ea-4470-8ffb-db6422a055b9.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Sigmoid activation function makes sure that the center (b<sub>x</sub> , b<sub>y</sub>)<sup> </sup>is in the range 0-1</div>
<p>While <em>b<sub>h</sub></em>, <em>b<sub><sup>w </sup></sub></em>are calculated in proportion to <em>w</em> and <em>h</em> values (yellow) of the box, values can be greater than 1 (exponential used for positive values). In the picture, we can see that the width <em>b<sub>w</sub></em> of the bounding box is almost 1.8 times the size of the box width <em>w</em>. Similarly, <em>b<sub>h</sub></em> is approximately 1.6 times the size of box height <em>h</em>, as outlined in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2925cbfc-4935-4abb-a3ab-5a696df75677.png" style=""/></div>
<p>Now, the question would be what is the probability that an object is contained in the bounding box? Well, to answer this question, we need to know the object score, which represents the probability that an object is contained inside a bounding box. It should be nearly 1 for the red and the neighboring grids, while almost 0 for, say, the grid at the corners. The following formulas describe how the network output is transformed so as to obtain bounding box predictions:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/320f4dc8-05b4-4843-9e59-cf0253df142b.png" style="width:66.50em;height:9.33em;"/></div>
<p>In the preceding formulas<em>, b<sub>x</sub>, b<sub>y</sub>, b<sub>w</sub>,</em> and <em>b<sub>h </sub></em>are the <em>x</em>, <em>y</em> center coordinates, width and height, respectively, of our prediction. On the other hand, <em>t<sub>x</sub>, t<sub>y</sub>, t<sub>w, </sub></em>and <em>t<sub>h</sub></em> are what the network outputs. Furthermore, <em>c<sub>x</sub></em> and <em>c<sub>y</sub></em> are the top-left coordinates of the grid. Finally, <em>p<sub>w</sub></em> and <em>p<sub>h</sub></em> are anchor dimensions for the box.</p>
<p>The abjectness score is also passed through a sigmoid, as it is to be interpreted as a probability. Then, the class confidences are used that represent the probabilities of the detected object belonging to a particular class. After prediction, we see how much the predicted box intersects with the real bounding box labeled at the beginning. We try to maximize the intersection between them so ideally, the predicted bounding box is fully intersecting to the labelled bounding box.</p>
<p>In short, we provide enough labeled data with bounding boxes (<em>b<sup>x</sup></em> , <em>b<sup>y</sup></em>, <em>b<sup>h</sup></em>, <em>b<sup>w</sup></em>), then we split the image and assign it to the box containing the center, train using CSW network and predict the object and its position. So first, we classify, and then localize the object and detect it.</p>
<p>Up to this point, we can see that we have been able to overcome most of the obstacles using YOLO. However, in reality, there are further two small problems to solve. Firstly, even though if the object is assigned to one box (one containing the center of the object) in the training time, during inferencing, the trained model assumes several boxes (that is, yellow) have the center of the object (with red). Therefore, this confusion introduces its own bounding boxes for the same object.</p>
<p>Fortunately, the non-max suppression algorithm can solve this: first, the algorithm chooses a prediction box with a maximum <em>P<sup>c</sup></em> probability so that it has a value between 0 and 1 instead of binary 0 or 1 values. Then, every intersecting box above a certain threshold with respect to that box is removed. The same logic is repeated until there are no more bounding boxes left. Secondly, since we are predicting multiple objects (car, train, bus, and so on), the center of two or more objects may lie in a single box. This issue can be solved by introducing anchor boxes:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/15810d89-ef14-47f1-bdfa-ee44007e1f80.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">An anchor box specification</div>
<p>With anchor boxes, we choose several shapes of bounding boxes we find frequently used for the object we want to detect. Then, the dimensions of the bounding box are predicted by applying a log-space transform to the output and then multiplying them by an anchor.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Developing a real-time object detection project</h1>
                </header>
            
            <article>
                
<p>In this section, we will be developing a video object classification application using pre-trained YOLO models (that is, transfer learning), DL4J, and OpenCV that can detect labels such as cars, and trees inside the video frame. To be frank, this application is also about extending an image detection problem to video detection. So let's get started.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 1 – Loading a pre-trained YOLO model</h1>
                </header>
            
            <article>
                
<p>Since Alpha release 1.0.0, DL4J provides a Tiny YOLO model via ZOO. For this, we need to add a dependency to your Maven friendly <kbd>pom.xml</kbd> file:</p>
<pre><strong>&lt;dependency&gt;</strong><br/>  &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;<br/>  &lt;artifactId&gt;deeplearning4j-zoo&lt;/artifactId&gt;<br/>  &lt;version&gt;${dl4j.version}&lt;/version&gt;<br/><strong>&lt;/dependency&gt;</strong></pre>
<p>Apart from this, if possible, make sure you utilize the CUDA and cuDNN by adding the following dependencies (see <a href="e27fb252-7892-4659-81e2-2289de8ce570.xhtml" target="_blank">Chapter 2</a>, <em>Cancer Types Prediction Using Recurrent Type Networks</em>, for more details):</p>
<pre><strong>&lt;dependency&gt;</strong><br/>  &lt;groupId&gt;org.nd4j&lt;/groupId&gt;<br/>  &lt;artifactId&gt;nd4j-cuda-9.0-platform&lt;/artifactId&gt;<br/>  &lt;version&gt;${nd4j.version}&lt;/version&gt;<br/><strong>&lt;/dependency&gt;</strong><br/><strong>&lt;dependency&gt;</strong><br/>  &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;<br/>  &lt;artifactId&gt;deeplearning4j-cuda-9.0&lt;/artifactId&gt;<br/>  &lt;version&gt;${dl4j.version}&lt;/version&gt;<br/><strong>&lt;/dependency&gt;</strong></pre>
<p>Then, we are ready to load the pre-trained Tiny YOLO model as a <kbd>ComputationGraph</kbd> with the following lines of code:</p>
<pre><strong>private ComputationGraph</strong> model; 
<strong>private</strong> TinyYoloModel() { 
        <strong>try</strong> { 
            model = (<strong>ComputationGraph</strong>) new <strong>TinyYOLO</strong>().initPretrained(); 
            createObjectLabels(); 
        } <strong>catch</strong> (<strong>IOException</strong> e) { 
            <strong>throw new RuntimeException</strong>(e); 
        } 
    }  </pre>
<p>In the preceding code segment, the <kbd>createObjectLabels()</kbd> method is referring to the labels from the PASCAL Visual Object Classes (PASCAL VOC) dataset that was used to train the YOLO 2 model. The signature of the method can be seen as follows:</p>
<pre><strong>private HashMap&lt;Integer, String&gt;</strong> labels;  
<strong>void</strong> createObjectLabels() { 
        <strong>if</strong> (labels == null) { 
            <strong>String</strong> label = "aeroplanen" + "bicyclen" + "birdn" + "boatn" + "bottlen" + "busn" + "carn" + 
                    "catn" + "chairn" + "cown" + "diningtablen" + "dogn" + "horsen" + "motorbiken" + 
                    "personn" + "pottedplantn" + "sheepn" + "sofan" + "trainn" + "tvmonitor"; 
            <strong>String</strong>[] split = label.split("\n"); 
            <strong>int</strong> i = 0; 
            labels = <strong>new HashMap</strong>&lt;&gt;(); 
            <strong>for</strong>(<strong>String</strong> label1 : split) { 
                labels.put(i++, label1); 
            } 
        } 
    } </pre>
<p>Now, let's create a Tiny YOLO model instance:</p>
<pre><strong>    static final TinyYoloModel</strong> yolo = <strong>new TinyYoloModel</strong>(); 
    <strong>public static TinyYoloModel</strong> getPretrainedModel() { 
        <strong>return</strong> yolo; 
    } </pre>
<p>Now, out of curiosity, let's take a look at the model architecture and the number of hyperparameters in each layer:</p>
<pre><strong>TinyYoloModel model</strong> = <strong>TinyYoloModel</strong>.getPretrainedModel(); 
System.out.println(<strong>TinyYoloModel</strong>.getSummary()); </pre>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f8ad0f0e-daa2-4a32-8575-441734544417.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Network summary and layer structure of a pre-trained Tiny YOLO model</div>
<p>Therefore, our Tiny YOLO model has around 1.6 million parameters across its 29-layer network. However, the original YOLO 2 model has more layers. Interested readers can look at the original YOLO 2 at <a href="https://github.com/yhcc/yolo2/blob/master/model_data/model.png">https://github.com/yhcc/yolo2/blob/master/model_data/model.png</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 2 – Generating frames from video clips</h1>
                </header>
            
            <article>
                
<p>Now, to deal with real-time video, we can use video processing tools or frameworks such as JavaCV frameworks that can split a video into individual frames, and we take the image height and width. For this, we have to include the following dependency in the <kbd>pom.xml</kbd> file:</p>
<pre><strong>&lt;dependency&gt;</strong><br/>  &lt;groupId&gt;org.bytedeco&lt;/groupId&gt;<br/>  &lt;artifactId&gt;javacv-platform&lt;/artifactId&gt;<br/>  &lt;version&gt;1.4.1&lt;/version&gt;<br/><strong>&lt;/dependency&gt;</strong></pre>
<p>JavaCV uses wrappers from the JavaCPP presets of <span>libraries </span><span>commonly used by researchers in the field of computer vision (for example, OpenCV and FFmpeg), and provides utility classes to make their functionality easier to use on the Java platform, including Android. More details can be found at</span> <a href="https://github.com/bytedeco/javacv">https://github.com/bytedeco/javacv</a><span>.</span></p>
<p>For this project, I have collected two video clips (each 1 minute long) that should give you a glimpse into an autonomous driving car. I have downloaded the dataset from YouTube from the following links:</p>
<ul>
<li class="title style-scope ytd-video-primary-info-renderer"><em>Building Self Driving Car - Local Dataset - Day</em>: <a href="https://www.youtube.com/watch?v=7BjNbkONCFw">https://www.youtube.com/watch?v=7BjNbkONCFw </a></li>
<li><em>Building Self Driving Car - Local Dataset - Night</em>: <a href="https://www.youtube.com/watch?v=ev5nddpQQ9I">https://www.youtube.com/watch?v=ev5nddpQQ9I</a></li>
</ul>
<p>After downloading them from YouTube downloader (or so), I renamed them as follows:</p>
<ul>
<li><kbd>SelfDrivingCar_Night.mp4</kbd></li>
<li><kbd>SelfDrivingCar_Day.mp4</kbd></li>
</ul>
<p>Now, if you play these clips, you will see how German people drive cars at 160 km/h or even faster. Now, let us parse the video (first we use day 1) and see some properties to get an idea of video quality hardware requirements:</p>
<pre><strong>String</strong> videoPath = "data/SelfDrivingCar_Day.mp4"; 
<strong>FFmpegFrameGrabber</strong> frameGrabber = new FFmpegFrameGrabber(videoPath); 
frameGrabber.start(); 
 
<strong>Frame</strong> frame; 
<strong>double</strong> frameRate = frameGrabber.getFrameRate(); 
System.out.println("The inputted video clip has " + frameGrabber.getLengthInFrames() + " frames"); 
System.out.println("Frame rate " + framerate + "fps"); </pre>
<pre><q>&gt;&gt;&gt;</q><br/> <span class="packt_screen">The inputted video clip has 1802 frames.<br/> The inputted video clip has frame rate of 29.97002997002997.</span></pre>
<p>We then grab each frame and use <kbd>Java2DFrameConverter</kbd>; it helps us to convert frames to JPEG images:</p>
<pre><strong>Java2DFrameConverter</strong> converter = new Java2DFrameConverter(); 
// grab the first frame 
frameGrabber.setFrameNumber(1); 
frame = frameGrabber.grab(); <br/><strong>BufferedImage</strong> bufferedImage = converter.convert(frame); 
System.out.println("First Frame" + ", Width: " + bufferedImage.getWidth() + ", Height: " + bufferedImage.getHeight()); 
 
// grab the second frame 
frameGrabber.setFrameNumber(2); 
frame = frameGrabber.grab(); 
bufferedImage = converter.convert(frame); 
System.out.println("Second Frame" + ", Width: " + bufferedImage.getWidth() + ", Height: " + bufferedImage.getHeight()); </pre>
<pre><q>&gt;&gt;&gt;</q><br/> First Frame: Width-640, Height-360<br/> Second Frame: Width-640, Height-360</pre>
<p>In this way, the preceding code will generate 1,802 JPEG images against an equal number of frames. Let's take a look at the generated images:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/66694124-6340-48c9-9cc2-2ace25135ed5.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">From video clip to video frame to image</div>
<p>Thus, the 1-minute long video clip has a fair number (that is, 1,800) of frames and is 30 frames per second. In short, this video clip has 720p video quality. So you can understand that processing this video should require good hardware; in particular, having a GPU configured should help.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 3 – Feeding generated frames into Tiny YOLO model</h1>
                </header>
            
            <article>
                
<p>Now that we know some properties of the clip, we can start generating the frames to be passed to the Tiny YOLO pre-trained model. First, let's look at a less important but transparent approach:</p>
<pre><strong>private volatile</strong> <strong>Mat[]</strong> v = <strong>new Mat</strong>[1]; 
<strong>private String</strong> windowName = "Object Detection from Video"; 
<strong>try</strong> { 
    <strong>for</strong>(<strong>int</strong> i = 1; i &lt; frameGrabber.getLengthInFrames();     
    i+ = (<strong>int</strong>)frameRate) { 
                frameGrabber.setFrameNumber(i); 
                frame = frameGrabber.grab(); 
                v[0] = new OpenCVFrameConverter.ToMat().convert(frame); 
                model.markObjectWithBoundingBox(v[0], frame.imageWidth, <br/>                                               frame.imageHeight, true, windowName); 
                imshow(windowName, v[0]); 
 
                <strong>char</strong> key = (<strong>char</strong>) waitKey(20); 
                // Exit on escape: 
                <strong>if</strong> (<strong>key</strong> == 27) { 
                    destroyAllWindows(); 
                    <strong>break</strong>; 
                } 
            } 
        } <strong>catch</strong> (<strong>IOException</strong> e) { 
            e.printStackTrace(); 
        } <strong>finally</strong> { 
            frameGrabber.stop(); 
        } 
        frameGrabber.close(); </pre>
<p>In the preceding code block, we send each frame to the model. Then we use the <kbd>Mat</kbd> class to represent each frame in an n-dimensional, dense, numerical multi-channel (that is, RGB) array.</p>
<div class="packt_tip">To know more, visit <a href="https://docs.opencv.org/trunk/d3/d63/classcv_1_1Mat.html#details">https://docs.opencv.org/trunk/d3/d63/classcv_1_1Mat.html#details</a>.</div>
<p>In other words, we split the video clip into multiple frames and pass into the Tiny YOLO model to process them one by one. This way, we apply a single neural network to the full image.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 4 – Object detection from image frames</h1>
                </header>
            
            <article>
                
<p>Tiny YOLO extracts the features from each frame as an n-dimensional, dense, numerical multi-channel array. Then each image is split into a smaller number of rectangles (boxes):</p>
<pre><strong>public void</strong> markObjectWithBoundingBox(<strong>Mat</strong> file, <strong>int</strong> imageWidth, <strong>int</strong> imageHeight, <strong>boolean</strong> newBoundingBOx,<br/>                                      <strong>String</strong> winName) throws Exception { 
        // parameters matching the pretrained TinyYOLO model<br/>        <strong>int</strong> W = 416; // width of the video frame  
        <strong>int</strong> H = 416; // Height of the video frame 
        <strong>int</strong> gW = 13; // Grid width 
        <strong>int</strong> gH = 13; // Grid Height 
        <strong>double</strong> dT = 0.5; // Detection threshold <br/><br/>        <strong>Yolo2OutputLayer</strong> outputLayer = (<strong>Yolo2OutputLayer</strong>) model.getOutputLayer(0); 
        <strong>if</strong> (newBoundingBOx) { 
            <strong>INDArray</strong> indArray = prepareImage(file, W, H); 
            <strong>INDArray</strong> results = model.outputSingle(indArray); 
            predictedObjects = outputLayer.getPredictedObjects(results, dT); <br/>            System.out.println("results = " + predictedObjects); 
            markWithBoundingBox(file, gW, gH, imageWidth, imageHeight); 
        } <strong>else</strong> { 
            markWithBoundingBox(file, gW, gH, imageWidth, imageHeight); 
        } 
        imshow(winName, file); 
    }</pre>
<p>In the preceding code, the <kbd>prepareImage()</kbd> method takes video frames as images, parses them using the <kbd>NativeImageLoader class</kbd>, does the necessary preprocessing, and extracts image features that are further converted into <kbd>INDArray</kbd> format, consumable by the model:</p>
<pre><strong>INDArray</strong> prepareImage(<strong>Mat</strong> file, <strong>int</strong> width, <strong>int</strong> height) <strong>throws</strong> IOException { 
        <strong>NativeImageLoader</strong> loader = <strong>new NativeImageLoader</strong>(height, width, 3); 
        <strong>ImagePreProcessingScaler</strong> imagePreProcessingScaler = <strong>new </strong>ImagePreProcessingScaler(0, 1); 
        <strong>INDArray</strong> indArray = loader.asMatrix(file); 
        imagePreProcessingScaler.transform(indArray); 
        <strong>return</strong> indArray; 
    } </pre>
<p>Then, the <kbd>markWithBoundingBox()</kbd> method is used for non-max suppression in the case of more than one bounding box.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 5 – Non-max suppression in case of more than one bounding box</h1>
                </header>
            
            <article>
                
<p>As YOLO predicts more than one bounding box per object, non-max suppression is implemented; it merges all detections that belong to the same object. Therefore, instead of using <em>b<sup>x</sup></em>, <em>b<sup>y</sup></em>, <em>b<sup>h</sup></em>, and <em>b<sup>w</sup></em><strong>,</strong> we can use the top-left and bottom-right points. <kbd>gridWidth</kbd> and <kbd>gridHeight</kbd> are the number of small boxes we split our image into. In our case, this is 13 x 13, where <kbd>w</kbd> and <kbd>h</kbd> are the original image frame dimensions:</p>
<pre><strong>void</strong> markObjectWithBoundingBox(<strong>Mat</strong> file, <strong>int</strong> gridWidth, <strong>int</strong> gridHeight, <strong>int</strong> w, <strong>int</strong> h, <strong>DetectedObject</strong> obj) {  
        <strong>double</strong>[] xy1 = obj.getTopLeftXY(); 
        <strong>double</strong>[] xy2 = obj.getBottomRightXY(); 
        <strong>int</strong> predictedClass = obj.getPredictedClass(); <br/>        <strong>int</strong> x1 = (int) Math.round(w * xy1[0] / gridWidth); 
        <strong>int</strong> y1 = (int) Math.round(h * xy1[1] / gridHeight); 
        <strong>int</strong> x2 = (int) Math.round(w * xy2[0] / gridWidth); 
        <strong>int</strong> y2 = (int) Math.round(h * xy2[1] / gridHeight); <br/>        rectangle(file, new Point(x1, y1), new Point(x2, y2), Scalar.RED); 
        putText(file, labels.get(predictedClass), new Point(x1 + 2, y2 - 2), <br/>                                 FONT_HERSHEY_DUPLEX, 1, Scalar.GREEN); 
    } </pre>
<p>Finally, we remove those objects that intersect with the max suppression, as follows:</p>
<pre><strong>static void</strong> removeObjectsIntersectingWithMax(<strong>ArrayList&lt;DetectedObject&gt;</strong> detectedObjects, <br/>                                             <strong>DetectedObject</strong> maxObjectDetect) { 
        <strong>double</strong>[] bottomRightXY1 = maxObjectDetect.getBottomRightXY(); 
        <strong>double</strong>[] topLeftXY1 = maxObjectDetect.getTopLeftXY(); 
        <strong>List&lt;DetectedObject&gt;</strong> removeIntersectingObjects = new ArrayList&lt;&gt;(); <br/>        <strong>for</strong>(<strong>DetectedObject</strong> detectedObject : detectedObjects) { 
            <strong>double</strong>[] topLeftXY = detectedObject.getTopLeftXY(); 
            <strong>double</strong>[] bottomRightXY = detectedObject.getBottomRightXY(); 
            <strong>double</strong> iox1 = <strong>Math</strong>.max(topLeftXY[0], topLeftXY1[0]); 
            <strong>double</strong> ioy1 = <strong>Math</strong>.max(topLeftXY[1], topLeftXY1[1]); 
 
            <strong>double</strong> iox2 = Math.min(bottomRightXY[0], bottomRightXY1[0]); 
            <strong>double</strong> ioy2 = Math.min(bottomRightXY[1], bottomRightXY1[1]); 
 
            <strong>double</strong> inter_area = (ioy2 - ioy1) * (iox2 - iox1); 
 
            <strong>double</strong> box1_area = (bottomRightXY1[1] - topLeftXY1[1]) * (bottomRightXY1[0] - topLeftXY1[0]); 
            <strong>double</strong> box2_area = (bottomRightXY[1] - topLeftXY[1]) * (bottomRightXY[0] - topLeftXY[0]); 
 
            <strong>double</strong> union_area = box1_area + box2_area - inter_area; 
            <strong>double</strong> iou = inter_area / union_area;  
 
            <strong>if</strong>(iou &gt; 0.5) { 
                removeIntersectingObjects.add(detectedObject); 
            } 
        } 
        detectedObjects.removeAll(removeIntersectingObjects); 
    } </pre>
<p>In the second block, we scaled each image into 416 x 416 x 3 (that is, W x H x 3 RGB channels). This scaled image is then passed to Tiny YOLO for predicting and marking the bounding boxes as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7b10e461-0e7c-42ed-b47f-bde5329922f3.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Our Tiny YOLO model predicts the class of an object detected in a bounding box</div>
<p>Once the <kbd>markObjectWithBoundingBox()</kbd> method is executed, the following logs containing the predicted class, <em>b<sup>x</sup></em>, <em>b<sup>y</sup></em>, <em>b<sup>h</sup></em>, <em>b<sup>w</sup></em>, and confidence (that is, the detection threshold) will be generated and shown on the console:</p>
<pre><span class="packt_screen">[4.6233e-11]], predictedClass=6),<br/></span><span class="packt_screen">DetectedObject(exampleNumber=0,<br/>centerX=3.5445247292518616, centerY=7.621537864208221,<br/>width=2.2568163871765137, height=1.9423424005508423,<br/>confidence=0.7954192161560059,<br/>classPredictions=[[ 1.5034e-7], [ 3.3064e-9]...</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 6 – wrapping up everything and running the application</h1>
                </header>
            
            <article>
                
<p>Since, up to this point, we know the overall workflow of our approach, we can now wrap up everything and see whether it <span>really </span>works. However, before that, let's take a look at the functionalities of different Java classes:</p>
<ul>
<li><kbd>FramerGrabber_ExplorartoryAnalysis.java</kbd>: This shows how to grab frames from the video clip and save each frame as a JPEG image. Besides, it also shows some exploratory properties of the video clip.</li>
<li><kbd>TinyYoloModel.java</kbd>: This instantiates the Tiny YOLO model and generates the label. It also creates and marks the object with the bounding box. Nonetheless, it shows how to handle non-max suppression for more than one bounding box per object.</li>
<li><kbd>ObjectDetectorFromVideo.java</kbd>: The main class. It continuously grabs the frames and feeds them to the Tiny YOLO model (that is, until the user presses the <em>Esc</em> key). Then it predicts the corresponding class of each object successfully detected inside the normal or overlapped bounding boxes with non-max suppression (if required).</li>
</ul>
<p>In short, <span>first,</span> <span>we</span> <span>create</span> <span>and instantiate the Tiny YOLO model. Then we grab the frames and treat each frame as a separate JPEG image. Next, we pass all the images to the model and the model does its trick as outlined previously. The whole workflow can </span><span>now</span><span> </span><span>be depicted with some Java code as follows:</span></p>
<pre>// ObjectDetectorFromVideo.java<br/><strong>public class</strong> ObjectDetectorFromVideo{ 
    <strong>private</strong> <strong>volatile Mat[]</strong> v = new Mat[1]; 
    <strong>private String</strong> windowName; 
 
    <strong>public static void main</strong>(<strong>String</strong>[] args) <strong>throws java.lang.Exception</strong> { 
        <strong>String</strong> videoPath = "data/SelfDrivingCar_Day.mp4"; 
        <strong>TinyYoloModel</strong> model = <strong>TinyYoloModel</strong>.getPretrainedModel(); 
         
        System.out.println(TinyYoloModel.getSummary()); 
        <strong>new</strong> ObjectDetectionFromVideo().startRealTimeVideoDetection(videoPath, model); 
    } 
 
    <strong>public void</strong> startRealTimeVideoDetection(<strong>String</strong> videoFileName, <strong>TinyYoloModel</strong> model) <br/>           <strong>throws</strong> <strong>java.lang.Exception</strong> { 
        windowName = "Object Detection from Video"; 
        <strong>FFmpegFrameGrabber</strong> frameGrabber = new FFmpegFrameGrabber(videoFileName); 
        frameGrabber.start(); 
 
        <strong>Frame</strong> frame; 
        <strong>double</strong> frameRate = frameGrabber.getFrameRate(); 
        System.out.println("The inputted video clip has " + frameGrabber.getLengthInFrames() + " frames"); 
        System.out.println("The inputted video clip has frame rate of " + frameRate); 
 
        <strong>try</strong> { 
            <strong>for</strong>(<strong>int</strong> i = 1; i &lt; frameGrabber.getLengthInFrames(); i+ = (int)frameRate) { 
                frameGrabber.setFrameNumber(i); 
                frame = frameGrabber.grab(); 
                v[0] = new OpenCVFrameConverter.ToMat().convert(frame); 
                model.markObjectWithBoundingBox(v[0], frame.imageWidth, frame.imageHeight, <br/>                                                true, windowName); 
                imshow(windowName, v[0]); 
 
                <strong>char</strong> key = (<strong>char</strong>) waitKey(20); 
                // Exit on escape: 
                <strong>if</strong>(key == 27) { 
                    destroyAllWindows(); 
                    <strong>break</strong>; 
                } 
            } 
        } <strong>catch</strong> (<strong>IOException</strong> e) { 
            e.printStackTrace(); 
        } <strong>finally</strong> { 
            frameGrabber.stop(); 
        } 
        frameGrabber.close(); 
    } 
} </pre>
<p class="mce-root CDPAlignLeft CDPAlign">Once the preceding class is executed, the application should load the pretrained model and the UI should be loaded, showing each object being classified:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/2cdfb2ce-0c92-46aa-93cb-9c2fd1af9a3b.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Our Tiny YOLO model can predict multiple cars simultaneously from a video clip (day)</div>
<p>Now, to see the effectiveness of our model even in night mode, we can perform a second experiment on the night dataset. To do this, just change one line in the <kbd>main()</kbd> method, as follows:</p>
<pre><strong>String</strong> videoPath = "data/SelfDrivingCar_Night.mp4";</pre>
<p>Once the preceding class is executed using this clip, the application should load the pretrained model and the UI should be loaded, showing each object being classified:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6f66961b-2b0a-4755-b510-9be340da10c5.png" style=""/></div>
<div>
<div class="packt_figref CDPAlignCenter CDPAlign">Our Tiny YOLO model can predict multiple cars simultaneously from a video clip (night)</div>
</div>
<p>Furthermore, to see the real-time output, execute the given screen recording clips showing the output of the application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Frequently asked questions (FAQs)</h1>
                </header>
            
            <article>
                
<p>In this section, we will see some frequently asked questions that might already be on your mind. Answers to these questions can be found in Appendix A.</p>
<ol>
<li>Can't we train YOLO from scratch?</li>
<li>I was wondering whether we could use the YOLO v3 model.</li>
</ol>
<ol start="3">
<li>What changes to the code do I need to make it work for my own video clip?</li>
<li>The application provided can detect cars and another vehicle in the video clip. However, the processing is not smooth. It seems to halt. How can I solve this issue?</li>
<li>Can I extend this app and make it work for a real-time video from a webcam?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we saw how to develop an end-to-end project that will detect objects from video frames when video clips play continuously. We saw how to utilize the pre-trained Tiny YOLO model, which is a smaller variant of the original YOLO v2 model.</p>
<p>Furthermore, we covered some typical challenges in object detection from both still images and videos, and how to solve them using bounding box and non-max suppression techniques. We learned how to process a video clip using the JavaCV library on top of DL4J. Finally, we saw some frequently asked questions that should be useful in implementing and extending this project.</p>
<p>In the next chapter, we will see how to develop anomaly detection, which is useful in fraud analytics in finance companies such as banks, and insurance and credit unions. It is an important task to grow the business. We will use unsupervised learning algorithms such as variational autoencoders and reconstructing probability.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Answers to questions</h1>
                </header>
            
            <article>
                
<p><strong>Answer</strong> <strong>to question 1</strong>: We can train a YOLO network from scratch, but that would take a lot of work (and costly GPU hours). As engineers and data scientists, we want to leverage as many prebuilt libraries and machine learning models as we can, so we are going to use a pre-trained YOLO model to get our application into production faster and more cheaply.</p>
<p><strong>Answer</strong> <strong>to question 2</strong>: Perhaps yes, but the latest DL4J release provides only YOLO v2. However, when I talked to their Gitter (see <a href="https://deeplearning4j.org/" target="_blank">https://deeplearning4j.org/</a>), they informed me that with some additional effort, you can make it <span>work</span>. I mean you can import YOLO v3 with Keras import. Unfortunately, I tried but could not make it <span>workfullly</span><span>.</span></p>
<p><strong>Answer</strong> <strong>to question 3</strong>: You should be able to directly feed your own video. However, if it does not work, or throws any unwanted exception, then video properties such as frame rate, width, and the height of each frame should be the same as the bounding box specifications.</p>
<p><strong>Answer</strong> <strong>to question 4</strong>: Well, I've already stated that your machine should have good hardware specifications and processing should not cause any delays. For example, my machine has 32 GB of RAM, a core i7 processor, and GeForce GTX 1050 GPU with 4 GB of main memory, and the apps run very smoothly.</p>
<p><strong>Answer</strong> <strong>to question 5:</strong> Perhaps, yes. In that case, the main source of the video should be from the webcam directly. According to the documentation provided at <a href="https://github.com/bytedeco/javacv">https://github.com/bytedeco/javacv</a>, JavaCV also comes with a hardware-accelerated full-screen image display, easy-to-use methods to execute code in parallel on multiple cores, user-friendly geometric and color calibration of cameras, projectors, and so on.</p>


            </article>

            
        </section>
    </body></html>