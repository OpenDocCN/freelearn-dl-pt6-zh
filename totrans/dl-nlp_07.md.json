["```py\n# Importing packages and setting the random seed to have a fixed output\nimport numpy as np\nnp.random.seed(0)\n# A sigmoid needs to be defined to be used later\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n# Simulating dummy values for the previous state and current input\nh_prev = np.random.randn(3, 1)\nx = np.random.randn(5, 1)\n```", "```py\n# Initialize W_f and U_f with dummy values\nW_f = np.random.randn(3, 5) # n_h = 3, n_x=5\nU_f = np.random.randn(3, 3) # n_h = 3\n```", "```py\nf = sigmoid(np.matmul(W_f, x) + np.matmul(U_f, h_prev)\n```", "```py\n# Initialize W_i and U_i with dummy values\nW_i = np.random.randn(3, 5) # n_h = 3, n_x=5\nU_i = np.random.randn(3, 3) # n_h = 3\n```", "```py\ni = sigmoid(np.matmul(W_i, x) + np.matmul(U_i, h_prev))\n```", "```py\n# Initialize W_c and U_c with dummy values\nW_c = np.random.randn(3, 5) # n_h = 3, n_x=5\nU_c = np.random.randn(3, 3) # n_h = 3\n```", "```py\nc_candidate = np.tanh(np.matmul(W_c, x) + np.matmul(U_c, h_prev))\n```", "```py\n# Initialize c_prev with dummy value\nc_prev = np.random.randn(3,1)\nc_new = np.multiply(f, c_prev) + np.multiply(i, c_candidate)\n```", "```py\n# Initialize dummy values for W_o and U_o\nW_o = np.random.randn(3, 5) # n_h = 3, n_x=5\nU_o = np.random.randn(3, 3) # n_h = 3\n```", "```py\no = np.tanh(np.matmul(W_o, x) + np.matmul(U_o, h_prev))\n```", "```py\nh_new = np.multiply(o, np.tanh(c_new))\n```", "```py\n    import pandas as pd\n    import numpy as np\n    from keras.models import Model, Sequential\n    from keras.layers import LSTM, Dense,Embedding\n    from keras.preprocessing.text import Tokenizer\n    from keras.preprocessing import sequence\n    ```", "```py\n    df = pd.read_csv(\"spam.csv\", encoding=\"latin\")\n    df.head() \n    ```", "```py\n    df = df[[\"v1\",\"v2\"]]\n    df.head()\n    ```", "```py\n    df[\"v1\"].value_counts()\n    ```", "```py\n    lab_map = {\"ham\":0, \"spam\":1}\n    Y = df[\"v1\"].map(lab_map).values\n    X = df[\"v2\"].values\n    ```", "```py\n    max_words = 100\n    mytokenizer = Tokenizer(nb_words=max_words,lower=True, split=\" \")\n    mytokenizer.fit_on_texts(X)\n    text_tokenized = mytokenizer.texts_to_sequences(X)\n    ```", "```py\n    max_len = 50\n    sequences = sequence.pad_sequences(text_tokenized, maxlen=max_len)\n    ```", "```py\n    model = Sequential()\n    model.add(Embedding(max_words, 20, input_length=max_len))\n    model.add(LSTM(64))\n    model.add(Dense(1, activation=\"sigmoid\"))\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    model.fit(sequences,Y,batch_size=128,epochs=10,\n              validation_split=0.2)\n    ```", "```py\ninp_test_seq = \"WINNER! U win a 500 prize reward & free entry to FA cup final tickets! Text FA to 34212 to receive award\"\ntest_sequences = mytokenizer.texts_to_sequences(np.array([inp_test_seq]))\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\nmodel.predict(test_sequences_matrix)\n```", "```py\nimport os\nimport re\nimport numpy as np\nwith open(\"deu.txt\", 'r', encoding='utf-8') as f:\n    lines = f.read().split('\\n')\nnum_samples = 20000 # Using only 20000 pairs for this example\nlines_to_use = lines[: min(num_samples, len(lines) - 1)]\nprint(lines_to_use)\n```", "```py\nfor l in range(len(lines_to_use)):\n    lines_to_use[l] = re.sub(\"\\d\", \" NUMBER_PRESENT \",lines_to_use[l])\ninput_texts = []\ntarget_texts = []\ninput_words = set()\ntarget_words = set()\nfor line in lines_to_use:\n    input_text, target_text = line.split('\\t')\n    target_text = 'BEGIN_ ' + target_text + ' _END'\n    input_texts.append(input_text)\n    target_texts.append(target_text)\n    for word in input_text.split():\n        if word not in input_words:\n            input_words.add(word)\n    for word in target_text.split():\n        if word not in target_words:\n            target_words.add(word)\n```", "```py\nmax_input_seq_length = max([len(i.split()) for i in input_texts])\nmax_target_seq_length = max([len(i.split()) for i in target_texts])\ninput_words = sorted(list(input_words))\ntarget_words = sorted(list(target_words))\nnum_encoder_tokens = len(input_words)\nnum_decoder_tokens = len(target_words)\n```", "```py\ninput_token_index = dict(\n    [(word, i) for i, word in enumerate(input_words)])\ntarget_token_index = dict([(word, i) for i, word in enumerate(target_words)])\n```", "```py\nencoder_input_data = np.zeros(\n    (len(input_texts), max_input_seq_length),\n    dtype='float32')\ndecoder_input_data = np.zeros(\n    (len(target_texts), max_target_seq_length),\n    dtype='float32')\ndecoder_target_data = np.zeros(\n    (len(target_texts), max_target_seq_length, num_decoder_tokens),\n    dtype='float32')\n```", "```py\nfor i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n    for t, word in enumerate(input_text.split()):\n        encoder_input_data[i, t] = input_token_index[word]\n    for t, word in enumerate(target_text.split()):\n        decoder_input_data[i, t] = target_token_index[word]\n        if t > 0:\n            # decoder_target_data is ahead of decoder_input_data by one timestep\n            decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n```", "```py\nfrom keras.layers import Input, LSTM, Embedding, Dense\nfrom keras.models import Model\nembedding_size = 50 # For embedding layer\n```", "```py\nencoder_inputs = Input(shape=(None,))\nencoder_after_embedding =  Embedding(num_encoder_tokens, embedding_size)(encoder_inputs)\nencoder_lstm = LSTM(50, return_state=True)\n_, state_h, state_c = encoder_lstm(encoder_after_embedding)\nencoder_states = [state_h, state_c]\n```", "```py\ndecoder_inputs = Input(shape=(None,))\ndecoder_after_embedding = Embedding(num_decoder_tokens, embedding_size)(decoder_inputs)\ndecoder_lstm = LSTM(50, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_after_embedding,\n                                     initial_state=encoder_states)\ndecoder_dense = Dense(num_decoder_tokens, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)\n```", "```py\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\nmodel.summary()\n```", "```py\nmodel.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n          batch_size=128,\n          epochs=20,\n          validation_split=0.05)\n```", "```py\nencoder_model = Model(encoder_inputs, encoder_states)\n```", "```py\ndecoder_state_input_h = Input(shape=(50,))\ndecoder_state_input_c = Input(shape=(50,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\ndecoder_outputs_inf, state_h_inf, state_c_inf = decoder_lstm(decoder_after_embedding, initial_state=decoder_states_inputs)\n```", "```py\ndecoder_states_inf = [state_h_inf, state_c_inf]\ndecoder_outputs_inf = decoder_dense(decoder_outputs_inf)\n# Multiple input, multiple output\ndecoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs_inf] + decoder_states_inf)\n```", "```py\n# Reverse-lookup token index to decode sequences\nreverse_input_word_index = dict(\n    (i, word) for word, i in input_token_index.items())\nreverse_target_word_index = dict(\n    (i, word) for word, i in target_token_index.items())\n```", "```py\ndef decode_sequence(input_seq):\nstates_value = encoder_model.predict(input_seq)\n```", "```py\n    target_seq = np.zeros((1,1))\n\n```", "```py\n    target_seq[0, 0] = target_token_index['BEGIN_']\n\n```", "```py\n    stop_condition = False\n    decoded_sentence = ''\n\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict(\n            [target_seq] + states_value)\n\n```", "```py\n        sampled_token_index = np.argmax(output_tokens)\n        sampled_word = reverse_target_word_index[sampled_token_index]\n        decoded_sentence += ' ' + sampled_word\n\n```", "```py\n        # or find stop character.\n        if (sampled_word == '_END' or\n           len(decoded_sentence) > 60):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_token_index\n```", "```py\n        states_value = [h, c]\n\n    return decoded_sentence\n```", "```py\ntext_to_translate = \"Where is my car?\"\nencoder_input_to_translate = np.zeros(\n    (1, max_input_seq_length),\n    dtype='float32')\nfor t, word in enumerate(text_to_translate.split()):\n    encoder_input_to_translate[0, t] = input_token_index[word]\ndecode_sequence(encoder_input_to_translate)\n```"]