- en: TensorFlow on Mobile with Speech-to-Text with the WaveNet Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to learn how to convert audio to text using the
    WaveNet model. We will then build a model that will take audio and convert it
    into text using an Android application.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is based on the *WaveNet: A Generative Model for Raw Audio* paper,
    by Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals,
    Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. You can find
    this paper at [https://arxiv.org/abs/1609.03499](https://arxiv.org/abs/1609.03499).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: WaveNet and how it works
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The WaveNet architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a model using WaveNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the WaveNet network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming a speech WAV file into English text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an Android application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's dig deeper into what Wavenet actually is.
  prefs: []
  type: TYPE_NORMAL
- en: WaveNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: WaveNet is a deep generative network that is used to generate raw audio waveforms.
    Sounds waves are generated by WaveNet to mimic the human voice. This generated
    sound is more natural than any of the currently existing text-to-speech systems,
    reducing the gap between system and human performance by 50%.
  prefs: []
  type: TYPE_NORMAL
- en: With a single WaveNet, we can differentiate between multiple speakers with equal
    fidelity. We can also switch between individual speakers based on their identity.
    This model is autoregressive and probabilistic, and it can be trained efficiently
    on thousands of audio samples per second. A single WaveNet can capture the characteristics
    of many different speakers with equal fidelity, and can switch between them by
    conditioning the speaker identity.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the movie *Her*, the long-standing dream of human-computer interaction
    is to allow people to talk to machines. The computer's ability to understand voices
    has increased tremendously over the past few years as a result of deep neural
    networks (for example, Google Assistant, Siri, Alexa, and Cortana). On the other
    hand, to generate speech with computers, a process referred to as speech synthesis
    or text to speech is followed. In the text-to-speech method, a large database
    of short sound fragments are recorded by a single speaker and then combined to
    form the required utterances. This process is very difficult because we can't
    change the speaker.
  prefs: []
  type: TYPE_NORMAL
- en: This difficulty has led to a great need for other methods of generating speech,
    where all the information that is needed for generating the data is stored in
    the parameters of the model. Additionally, using the inputs that are given to
    the model, we can control the contents and various attributes of speech. When
    speech is generated by adding sound fragments together, attribution graphs are
    generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the attribution graph of speech that is generated in **1 second**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b3c68bc-06ff-4998-9a13-f5d1541ce051.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the attribution graph of speech that is generated in **100
    milliseconds**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aa119a92-08a6-4354-820c-5b7a2b351b3e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the attribution graph of speech that is generated in **10
    milliseconds**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c40a3332-995c-4e3b-904f-0e5108ee9f50.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the attribution graph of speech that is generated in **1 millisecond**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/81b3eadd-6104-4a28-8e83-631eed845c03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The **Pixel Recurrent Neural Network** (**PixelRNN**) and **Pixel C****onvolutional
    Neural Network** (**PixelCNN**) models from Google ensure that it''s possible
    to generate images that include complex formations – not by generating one pixel
    at a time, but by an entire color channel altogether. At any one time, a color
    channel will need at least a thousand predictions per image. This way, we can
    alter a two-dimensional PixelNet into a one-dimensional WaveNet; this idea is
    shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8f342fa-40b2-47a9-bb24-c3764be2bacd.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram displays the structure of a WaveNet model. WaveNet is
    a full CNN, in which the convolutional layers include a variety of dilation factors.
    These factors help the receptive field of WaveNet to grow exponentially with depth,
    and it also helps to cover thousands of time steps.
  prefs: []
  type: TYPE_NORMAL
- en: During training, the human speaker records the input sequences to create waveforms.
    Once the training is complete, we generate synthetic utterances by sampling the
    network. A value is taken from the probability distribution which is computed
    by the network at each step of sampling. The value that's received is fed as the
    input for the next step, and then a new prediction is made. Building these samples
    at each step is expensive; however, it's necessary to generate complex and realistic-sounding
    audio.
  prefs: []
  type: TYPE_NORMAL
- en: More information about PixelRNN can be found at [https://arxiv.org/pdf/1601.06759.pdf](https://arxiv.org/pdf/1601.06759.pdf),
    while information about *Conditional Image Generation with PixelCNN Decoders*
    can be found at [https://arxiv.org/pdf/1606.05328.pdf](https://arxiv.org/pdf/1606.05328.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The architecture of WaveNet neural networks shows amazing outputs by generating
    audio and text-to-speech translations, since it directly produces a raw audio
    waveform.
  prefs: []
  type: TYPE_NORMAL
- en: When the previous samples and additional parameters are given as the input,
    the network produces the next sample in the form of an audio waveform using conditional
    probability.
  prefs: []
  type: TYPE_NORMAL
- en: The waveform that is given as the input is quantized to a fixed range of integers.
    This happens after the audio is preprocessed. The tensors are produced by one-hot
    encoding these integer amplitudes. Hence, the dimensions of the channel are reduced
    by the convolutional layer that only accesses the current and previous inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram displays the WaveNet architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf310496-903e-4995-8f65-fb92e47535cc.png)'
  prefs: []
  type: TYPE_IMG
- en: A stack of causal dilated layers is used to build the network core. Each layer
    is a dilated convolution with holes, and it accesses only the past and current
    audio samples.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the outputs that are received from all the layers are combined and, using
    an array of dense postprocessing layers, they are fed to the original channels.
    Later, the softmax function converts the output into a categorical distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The loss function is calculated as the cross entropy between the output for
    each time step and the input at the next time step.
  prefs: []
  type: TYPE_NORMAL
- en: Network layers in WaveNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we will focus on generating dilated causal convolution network layers
    with the filter size of two. Note that these ideas are relevant to larger filter
    sizes.
  prefs: []
  type: TYPE_NORMAL
- en: 'During this generation, the computational graph that''s used to compute a single
    output value can be seen as a binary tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0bf29096-a2fc-4312-94bf-6ff9d81ebe45.png)'
  prefs: []
  type: TYPE_IMG
- en: The **Input** nodes on the bottom layer of the diagram are the leaves of the
    tree, while the **Output** layer is the root. The intermediate computations are
    represented by the nodes above the **Input** layer. The edges of the graph correspond
    to multiple matrices. Since the computation is a binary tree, the overall computation
    time for the graph is *O(2^L)*. When *L* is large, the computation exponentially
    shoots up.
  prefs: []
  type: TYPE_NORMAL
- en: However, since this model is being applied repeatedly over time, there is a
    lot of redundant computation, which we can cache to increase the speed of generating
    a single sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key insight is this – given certain nodes in the graph, we have all the
    information that we need to compute the current output. We call these nodes **recurrent
    states** by using the analogy of RNNs. These nodes have already been computed,
    so all we need to do is cache them on the different layers, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b45b4536-8ba4-4a1d-a815-f6dfcca66d84.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that at the next time point, we will need a different subset of recurrent
    states. As a result, we will need to cache several recurrent states per layer.
    The number we need to keep is equal to the dilation of that layer, as shown in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86c05505-5b7f-42e1-9818-403ede26646a.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the preceding diagram with arrow marks, the number of recurrent
    states is the same as the dilation value in the layer.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm's components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm behind building a speech detector has two components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The generation model**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The convolution queues**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These two components are shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3654f9ba-3c2d-4ff7-8c5a-652e55f1b401.png)'
  prefs: []
  type: TYPE_IMG
- en: The generation model can be viewed as one step of an RNN. It takes the current
    observation and several recurrent states as input, and then computes the output
    prediction and new recurrent states.
  prefs: []
  type: TYPE_NORMAL
- en: The convolution queues store the new recurrent states that have been computed
    by the layer underneath it.
  prefs: []
  type: TYPE_NORMAL
- en: Let's jump into building the model.
  prefs: []
  type: TYPE_NORMAL
- en: Building the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will implement sentence-level English speech recognition using DeepMind''s
    WaveNet. However, we need to consider a number of data points before building
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: First, while the paper on WaveNet (provided at the beginning of this chapter)
    used the TIMIT dataset for the speech recognition experiment, we will use the
    free VCTK dataset instead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, the paper added a mean pooling layer after the dilated convolution layer
    for downsampling. We have extracted **mel-frequency cepstral coefficients** (**MFCC**)
    from the `.wav` files and removed the final mean pooling layer because the original
    setting is impossible to run on our TitanX **Graphics Processing Unit** **GPU**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Third, since the TIMIT dataset has phoneme labels, the paper trained the model
    with two loss terms: **phoneme classification** and **next phoneme prediction**.
    Instead, we will use a single **connectionist temporal classification** (**CTC**)
    loss because VCTK provides sentence-level labels. As a result, we only use dilated
    Conv1D layers without any dilated Conv1D layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we won't do quantitative analyses, such as the **bilingual evaluation
    understudy score** (**BLEU**) score and postprocessing by combining a language
    model, due to time constraints.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is a list of all the dependency libraries that will need to be installed
    first:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tensorflow`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sugartensor`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`librosa`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scikits.audiolab`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have problems with the `librosa` library, you can try installing `ffmpeg`
    using `pip`.
  prefs: []
  type: TYPE_NORMAL
- en: Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We used the VCTK, LibriSpeech, and TED-LIUM release 2 datasets. The total number
    of sentences in the training set are composed of the previous three datasets,
    which equals 240,612 sentences. The validation and test sets are built using only
    LibriSpeech and the TED-LIUM corpus, because the VCTK corpus does not have validation
    and test sets. After downloading each corpus, extract them in the `asset/data/VCTK-Corpus`,
    `asset/data/LibriSpeech`, and `asset/data/TEDLIUM_release2` directories.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the links to these datasets here: *CSTR VCTK Corpus:* [http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html](http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html)
    *LibriSpeech ASR corpus:* [http://www.openslr.org/12](http://www.openslr.org/12/)
    *TED-LIUM:* [http://www-lium.univ-lemans.fr/en/content/ted-lium-corpus](https://lium.univ-lemans.fr/en/ted-lium2/)'
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The TED-LIUM release 2 dataset provides audio data in the SPH format, so we
    should convert it into a format that the `librosa` library can handle. To do this,
    run the following command in the `asset/data` directory to convert the SPH format
    into the WAV format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you don't have `sox` installed,please install it first.
  prefs: []
  type: TYPE_NORMAL
- en: We found that the main bottleneck is the disk read time when training because
    of the size of the audio files. It is better to have smaller audio files before
    processing for faster execution. So, we have decided to preprocess the whole audio
    data into the MFCC feature files, which are much smaller. Additionally, we highly
    recommend using a **solid-state drive** (**SSD**) instead of a hard drive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command in the console to preprocess the whole dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: With the processed audio files, we can now train the network.
  prefs: []
  type: TYPE_NORMAL
- en: Training the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will start training the network by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are using a machine with CUDA enabled, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You can see the resulting `.ckpt` files and log files in the `asset/train` directory.
    Launch `tensorboard--logdir asset/train/log` to monitor the training process.
  prefs: []
  type: TYPE_NORMAL
- en: We've trained this model on a 3 Nvidia 1080 Pascal GPU for 40 hours until 50
    epochs were reached, and then we picked the epoch when the validation loss is
    at a minimum. In our case, it is epoch 40\. If you can see the out-of-memory error,
    reduce `batch_size` in the `train.py` file from 16 to 4.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CTC losses at each epoch are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Epoch** | **Train set** | **Valid set** | **Test set** |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 79.541500 | 73.645237 | 83.607269 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | 72.884180 | 69.738348 | 80.145867 |'
  prefs: []
  type: TYPE_TB
- en: '| 40 | 69.948266 | 66.834316 | 77.316114 |'
  prefs: []
  type: TYPE_TB
- en: '| 50 | 69.127240 | 67.639895 | 77.866674 |'
  prefs: []
  type: TYPE_TB
- en: Here, you can see the difference between the values from the training dataset
    and the testing dataset. The difference is largely due to the bigger volume of
    data in the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After training the network, you can check the validation or test set CTC loss
    by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `frac` option will be useful if you want to test only a fraction of the
    dataset for fast evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming a speech WAV file into English text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, you can convert the speech WAV file into English text by executing the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This will transform a speech WAV file into an English sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result will be printed on the console; try the following command as an
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*he hoped there would be stoo for dinner turnips and charrats and bruzed patatos
    and fat mutton pieces to be ladled out in th thick peppered flower fatan sauce
    stuffid into you his belly counsiled him after early night fall the yetl lampse
    woich light hop here and there on the squalled quarter of the browfles o berty
    and he god in your mind numbrt tan fresh nalli is waiting on nou cold nit husband*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The ground truth is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*HE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES
    AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOUR FATTENED SAUCE
    STUFF IT INTO YOU HIS BELLY COUNSELLED HIM AFTER EARLY NIGHTFALL THE YELLOW LAMPS
    WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS HELLO BERTIE
    ANY GOOD IN YOUR MIND NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND*'
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned earlier, there is no language model, so there are some cases
    where capital letters and punctuation are misused, or words are misspelled.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unlike image problems, it''s not easy to find a pretrained deep learning model
    for speech-to-text that gives out checkpoints. Luckily, I found the following
    WaveNet speech-to-text implementation. To export the model for compression, I
    ran the Docker image, loaded the checkpoint, and wrote it into a protocol buffers
    file. To run this, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We will build the graph for inference, load the checkpoint, and write it into
    a protocol buffer file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Next, we need to build a TensorFlow model and quantize the model so that it
    can be consumed in the mobile application.
  prefs: []
  type: TYPE_NORMAL
- en: Bazel build TensorFlow and quantizing the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To quantize the model with TensorFlow, you need to have Bazel installed and
    the cloned Tensorflow repository. I recommend creating a new virtual environment
    to install and build TensorFlow there. Once you''re done, you can run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You can check out the official quantization tutorial on the TensorFlow website
    for other options in transforms. After quantization, the model was downsized by
    75%, from 15.5 MB to 4 MB due to the 8-bit conversion. Due to the time limit,
    I haven't calculated the letter error rate with a test set to quantify the accuracy
    drop before and after quantization.
  prefs: []
  type: TYPE_NORMAL
- en: For a detailed discussion on neural network quantization, there is a great post
    by Pete Warden, called *Neural network quantization with TensorFlow* ([https://petewarden.com/2016/05/03/how-to-quantize-neural-networks-with-tensorflow/](https://petewarden.com/2016/05/03/how-to-quantize-neural-networks-with-tensorflow/)).
  prefs: []
  type: TYPE_NORMAL
- en: Note that you can also do a full 8-bit calculation graph transformation by following
    the instructions in this section.
  prefs: []
  type: TYPE_NORMAL
- en: The model's size is down to 5.9 MB after this conversion, and the inference
    time is doubled. This could be due to the fact that the 8-bit calculation is not
    optimized for the Intel i5 processor on the macOS platform, which was used to
    write the application.
  prefs: []
  type: TYPE_NORMAL
- en: So, now that we have a compressed pretrained model, let's see what else we need
    to deploy the model on Android.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow ops registration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we will build the TensorFlow model using Bazel to create a `.so` file
    that can be called by the **Java Native Interface** (**JNI**), and includes all
    the operation libraries that we need for the pretrained WaveNet model inference.
    We will use built model in the Android application.
  prefs: []
  type: TYPE_NORMAL
- en: To find out more about Bazel, you can refer the following link: [https://docs.bazel.build/versions/master/bazel-overview.html](https://docs.bazel.build/versions/master/bazel-overview.html).
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin by editing the WORKSPACE file in the cloned TensorFlow repository
    by uncommenting and updating the paths to **Software** **Development** **Kit**
    (**SDK**) and **Native Development Kit** (**NDK**).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to find out what ops were used in the pretrained model and generate
    a `.so` file with that piece of information.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: All the ops in the `.pb` file will be listed in `ops_to_register.h`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, move `op_to_register.h` to `/tensorflow/tensorflow/core/framework/` and
    run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Unfortunately, while I didn''t get any error message, the `.so` file still
    didn''t include all the ops listed in the header file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If you haven''t tried the first option and have got the list of ops in the
    model, you can get the ops by using the `tf.train.write_graph` command and typing
    the following into your Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, edit the BUILD file by adding the missing ops into `android_extended_ops_group1` or
    `android_extended_ops_group2` in the Android libraries section. You can also make
    the `.so` file smaller by removing any unnecessary ops. Now, run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'You''ll find the `libtensorflow_inference.so` file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note that while running this command on Android, we ran into an error with the
    `sparse_to_dense` op. If you'd like to repeat this work, add `REGISTER_KERNELS_ALL(int64);` to
    `sparse_to_dense_op.cc` on line 153, and compile again.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the `.so` file, we also need a JAR file. You can simply add
    this in the `build.gradle` file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, you can run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You''ll find the file, as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Now, move both files into your Android project.
  prefs: []
  type: TYPE_NORMAL
- en: Building an Android application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to build an Android application that will convert
    the user's voice input into text. Essentially, we are going to build a speech-to-text
    converter. We have modified the TensorFlow speech example in the TensorFlow Android
    demo repository for this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the TensorFlow Android demo application at [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android).
  prefs: []
  type: TYPE_NORMAL
- en: The `build.gradle` file in the demo actually helps you build the `.so` and JAR
    files. So, if you'd like to start the demo examples with your own model, you can
    simply get the list of your ops, modify the BUILD file, and let the `build.gradle`
    file take care of the rest. We will get into the details of setting up the Android
    application in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The requirements you will need to build the Android application are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow 1.13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy 1.15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: python-speech-features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow link: [https://github.com/tensorflow/tensorflow/releases](https://github.com/tensorflow/tensorflow/releases)
  prefs: []
  type: TYPE_NORMAL
- en: Python link: [https://pip.pypa.io/en/stable/installing/](https://pip.pypa.io/en/stable/installing/)
  prefs: []
  type: TYPE_NORMAL
- en: Numpy link: [https://docs.scipy.org/doc/numpy-1.13.0/user/install.html](https://docs.scipy.org/doc/numpy-1.13.0/user/install.html)
    Python-speech-features link: [https://github.com/jameslyons/python_speech_features](https://github.com/jameslyons/python_speech_features)
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's start building the Android application from scratch. In this application,
    we will record audio and then convert it into text.
  prefs: []
  type: TYPE_NORMAL
- en: Set up Android Studio based on your operating system by going to the following
    link: [https://developer.android.com/studio/install](https://developer.android.com/studio/install).The
    code repository that's used in this project has been modified from the TensorFlow
    example provided here: [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android.](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android)
  prefs: []
  type: TYPE_NORMAL
- en: We will use the TensorFlow sample application and edit it according to our needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add Application name and the Company domain name, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5683c7fa-8f1d-4492-87c0-0cd7db2e0d83.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the next step, select the Target Android Devices version. We will select
    the minimum version as API 15:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5033299-5f41-41d1-8d47-856fa7f342a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After this, we will add either Empty Activity or No Activity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43d69a73-8f73-4ac9-9c39-dbc1de2ae0d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s start adding the activity and use the generated TensorFlow model
    to get the result. We need to enable two permissions so that we can use them in
    our application, as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We will have a minimal UI for the application, with a couple of `TextView`
    components and a `Button`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/922ef678-9ff8-4124-ae40-3d2095533b4f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following XML layout mimics the UI in the preceding screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s add the steps for the speech recognizer activity, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are not going to discuss the basics of Android here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will launch the recorder, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code shows the implementation of the `record()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code shows the implementation of the audio recognizing method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The model is run through the `TensorFlowInferenceInterface`class, as shown in
    the preceding code.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the completed code running, run the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the first run, you will need to allow the application to use the phone''s
    internal microphone, as demonstrated in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8c35468a-ce89-472c-98d3-94da70ed4595.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Once we give permission to use the microphone, click on RECORD VOICE and give
    your voice input within 5 seconds. There are two attempts shown in the following
    screenshots for the `how are you` input keyword with an Indian accent. It works
    better with US and UK accents.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first attempt is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6434a237-92fa-4d77-9063-99252d3510ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The second attempt is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84aa3d2f-5f7f-4a8b-b273-5d3b7ad89d80.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You should try this with your own accent to get the correct output. This is
    a very simple way to start building your own speech detector that you can improve
    on even further.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to build a complete speech detector on your
    own. We discussed how the WaveNet model works in detail. With this application,
    we can make a simple speech-to-text converter work; however, a lot of improvements
    and updates need to be done to get perfect results. You can build the same application
    on the iOS platform as well by converting the model into CoreML.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will move on and build a handwritten digit classifier
    using the **Modified National Institute of Standards and Technology** (**MNIST**)
    model.
  prefs: []
  type: TYPE_NORMAL
