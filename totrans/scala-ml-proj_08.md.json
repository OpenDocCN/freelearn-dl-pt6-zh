["```py\n$ bin/zeppelin-daemon.sh start\n```", "```py\n$ binzeppelin.cmd \n```", "```py\n$ bin/zeppelin-daemon.sh stop\n```", "```py\nval trainDF = spark.read.option(\"inferSchema\", \"true\")\n            .format(\"com.databricks.spark.csv\")\n            .option(\"delimiter\", \";\")\n            .option(\"header\", \"true\")\n            .load(\"data/bank-additional-full.csv\")\ntrainDF.registerTempTable(\"trainData\")\n```", "```py\n%sql select y, count(1) from trainData group by y order by y\n>>>\n```", "```py\n%sql select job,y, count(1) from trainData group by job, y order by job, y\n```", "```py\n%sql select marital,y, count(1) from trainData group by marital,y order by marital,y\n>>>\n```", "```py\n%sql select education,y, count(1) from trainData group by education,y order by education,y\n```", "```py\n%sql select default,y, count(1) from trainData group by default,y order by default,y\n```", "```py\n%sql select housing,y, count(1) from trainData group by housing,y order by housing,y\n```", "```py\n%sql select loan,y, count(1) from trainData group by loan,y order by loan,y\n```", "```py\n%sql select contact,y, count(1) from trainData group by contact,y order by contact,y\n```", "```py\n%sql select month,y, count(1) from trainData group by month,y order by month,y\n```", "```py\n%sql select day_of_week,y, count(1) from trainData group by day_of_week,y order by day_of_week,y\n```", "```py\n%sql select poutcome,y, count(1) from trainData group by poutcome,y order by poutcome,y\n```", "```py\n%sql select age,y, count(1) from trainData group by age,y order by age,y\n```", "```py\n%sql select duration,y, count(1) from trainData group by duration,y order by duration,y\n```", "```py\n%sql select campaign, count(1), y from trainData group by campaign,y order by campaign,y\n```", "```py\n%sql select pdays, count(1), y from trainData group by pdays,y order by pdays,y\n```", "```py\n%sql select previous, count(1), y from trainData group by previous,y order by previous,y\n```", "```py\n%sql select emp_var_rate, count(1), y from trainData group by emp_var_rate,y order by emp_var_rate,y\n```", "```py\n%sql select cons_price_idx, count(1), y from trainData group by cons_price_idx,y order by cons_price_idx,y\n```", "```py\n%sql select cons_conf_idx, count(1), y from trainData group by cons_conf_idx,y order by cons_conf_idx,y\n```", "```py\n%sql select euribor3m, count(1), y from trainData group by euribor3m,y order by euribor3m,y\n```", "```py\n%sql select nr_employed, count(1), y from trainData group by nr_employed,y order by nr_employed,y\n```", "```py\nimport org.apache.spark.sql.types._\n\nval numericFeatures = trainDF.schema.filter(_.dataType != StringType)\nval description = trainDF.describe(numericFeatures.map(_.name): _*)\n\nval quantils = numericFeatures\n                .map(f=>trainDF.stat.approxQuantile(f.name,                 \n                Array(.25,.5,.75),0)).transposeval \n\nrowSeq = Seq(Seq(\"q1\"+:quantils(0): _*),\n            Seq(\"median\"+:quantils(1): _*),\n            Seq(\"q3\"+:quantils(2): _*))\n\nval rows = rowSeq.map(s=> s match{ \n    case Seq(a:String,b:Double,c:Double,d:Double,\n             e:Double,f:Double,g:Double,                                              \n             h:Double,i:Double,j:Double,k:Double)=> (a,b,c,d,e,f,g,h,i,j,k)})\n         val allStats = description.unionAll(sc.parallelize(rows).toDF)\n         allStats.registerTempTable(\"allStats\")\n\n%sql select * from allStats\n>>>\n```", "```py\nval spark = SparkSession.builder\n        .master(\"local[*]\")\n        .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\") // change accordingly\n        .appName(s\"OneVsRestExample\")\n        .getOrCreate()\n```", "```py\nspark.sqlContext.setConf(\"spark.sql.caseSensitive\", \"false\");\nval trainDF = spark.read.option(\"inferSchema\",\"true\")\n            .format(\"com.databricks.spark.csv\")\n            .option(\"delimiter\", \";\")\n            .option(\"header\", \"true\")\n            .load(\"data/bank-additional-full.csv\")\n```", "```py\nval withoutDuration = trainDF.drop(\"duration\")\n```", "```py\nimplicit val h2oContext = H2OContext.getOrCreate(spark.sparkContext)\nimport h2oContext.implicits._implicit \n\nval sqlContext = SparkSession.builder().getOrCreate().sqlContext\nimport sqlContext.implicits._\n```", "```py\nval H2ODF: H2OFrame = withoutDuration.orderBy(rand())\n```", "```py\nH2ODF.types.zipWithIndex.foreach(c=> if(c._1.toInt== 2) toCategorical(H2ODF,c._2))\n```", "```py\ndef toCategorical(f: Frame, i: Int): Unit = {f.replace(i,f.vec(i).toCategoricalVec)f.update()}\n```", "```py\nval sf = new FrameSplitter(H2ODF, Array(0.6, 0.2), \n                            Array(\"train.hex\", \"valid.hex\", \"test.hex\")\n                            .map(Key.make[Frame](_)), null)\n\nwater.H2O.submitTask(sf)\nval splits = sf.getResultval (train, valid, test) = (splits(0), splits(1), splits(2))\n```", "```py\nval dlModel = buildDLModel(train, valid)\n```", "```py\ndef buildDLModel(train: Frame, valid: Frame,epochs: Int = 10, \n                l1: Double = 0.001,l2: Double = 0.0,\n                hidden: Array[Int] = Array[Int](256, 256, 256)\n               )(implicit h2oContext: H2OContext): \n     DeepLearningModel = {import h2oContext.implicits._\n                // Build a model\n    val dlParams = new DeepLearningParameters()\n        dlParams._train = traindlParams._valid = valid\n        dlParams._response_column = \"y\"\n        dlParams._epochs = epochsdlParams._l1 = l2\n        dlParams._hidden = hidden\n\n    val dl = new DeepLearning(dlParams, water.Key.make(\"dlModel.hex\"))\n    dl.trainModel.get\n    }\n```", "```py\nval auc = dlModel.auc()println(\"Train AUC: \"+auc)\nprintln(\"Train classification error\" + dlModel.classification_error())\n>>>\nTrain AUC: 0.8071186909427446\nTrain classification error: 0.13293674881631662\n```", "```py\nval result = dlModel.score(test)('predict)\n```", "```py\nresult.add(\"actual\",test.vec(\"y\"))\n```", "```py\nval predict_actualDF = h2oContext.asDataFrame(result)predict_actualDF.groupBy(\"actual\",\"predict\").count.show\n>>>\n```", "```py\nVegas().withDataFrame(predict_actualDF)\n    .mark(Bar)\n     .encodeY(field=\"*\", dataType=Quantitative, AggOps.Count, axis=Axis(title=\"\",format=\".2f\"),hideAxis=true)\n    .encodeX(\"actual\", Ord)\n    .encodeColor(\"predict\", Nominal, scale=Scale(rangeNominals=List(\"#FF2800\", \"#1C39BB\")))\n    .configMark(stacked=StackOffset.Normalize)\n    .show()\n>>>\n```", "```py\nval trainMetrics = ModelMetricsSupport.modelMetrics[ModelMetricsBinomial](dlModel, test)println(trainMetrics)\n>>>\n```", "```py\nval auc = trainMetrics._auc//tp,fp,tn,fn\nval metrics = auc._tps.zip(auc._fps).zipWithIndex.map(x => x match { \n    case ((a, b), c) => (a, b, c) })\n\nval fullmetrics = metrics.map(_ match { \n    case (a, b, c) => (a, b, auc.tn(c), auc.fn(c)) })\n\nval precisions = fullmetrics.map(_ match {\n     case (tp, fp, tn, fn) => tp / (tp + fp) })\n\nval recalls = fullmetrics.map(_ match { \n    case (tp, fp, tn, fn) => tp / (tp + fn) })\n\nval rows = for (i <- 0 until recalls.length) \n    yield r(precisions(i), recalls(i))\n\nval precision_recall = rows.toDF()\n\n//precision vs recall\nVegas(\"ROC\", width = 800, height = 600)\n    .withDataFrame(precision_recall).mark(Line)\n    .encodeX(\"re-call\", Quantitative)\n    .encodeY(\"precision\", Quantitative)\n    .show()\n>>>\n```", "```py\nval sensitivity = fullmetrics.map(_ match { \n    case (tp, fp, tn, fn) => tp / (tp + fn) })\n\nval specificity = fullmetrics.map(_ match {\n    case (tp, fp, tn, fn) => tn / (tn + fp) })\nval rows2 = for (i <- 0 until specificity.length) \n    yield r2(sensitivity(i), specificity(i))\nval sensitivity_specificity = rows2.toDF\n\nVegas(\"sensitivity_specificity\", width = 800, height = 600)\n    .withDataFrame(sensitivity_specificity).mark(Line)\n    .encodeX(\"specificity\", Quantitative)\n    .encodeY(\"sensitivity\", Quantitative).show()\n>>>\n```", "```py\nval withTh = auc._tps.zip(auc._fps).zipWithIndex.map(x => x match {\n    case ((a, b), c) => (a, b, auc.tn(c), auc.fn(c), auc._ths(c)) })\n\nval rows3 = for (i <- 0 until withTh.length) \n    yield r3(withTh(i)._1, withTh(i)._2, withTh(i)._3, withTh(i)._4, withTh(i)._5)\n```", "```py\nVegas(\"tp\", width = 800, height = 600).withDataFrame(rows3.toDF)\n    .mark(Line).encodeX(\"th\", Quantitative)\n    .encodeY(\"tp\", Quantitative)\n    .show\n>>>\n```", "```py\nVegas(\"fp\", width = 800, height = 600)\n    .withDataFrame(rows3.toDF).mark(Line)\n    .encodeX(\"th\", Quantitative)\n    .encodeY(\"fp\", Quantitative)\n    .show\n>>>\n```", "```py\nVegas(\"tn\", width = 800, height = 600)\n    .withDataFrame(rows3.toDF).mark(Line)\n    .encodeX(\"th\", Quantitative)\n    .encodeY(\"tn\", Quantitative)\n    .show\n>>>\n```", "```py\nVegas(\"fn\", width = 800, height = 600)\n    .withDataFrame(rows3.toDF).mark(Line)\n    .encodeX(\"th\", Quantitative)\n    .encodeY(\"fn\", Quantitative)\n    .show\n>>>\n```", "```py\ncase class r(precision: Double, recall: Double)\ncase class r2(sensitivity: Double, specificity: Double)\ncase class r3(tp: Double, fp: Double, tn: Double, fn: Double, th: Double)\n```", "```py\nh2oContext.stop(stopSparkContext = true)\nspark.stop()\n```"]