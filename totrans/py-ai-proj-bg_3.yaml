- en: Applications for Comment Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll overview the bag-of-words model for text classification.
    We will look at predicting YouTube comment spam with the bag-of-words and the
    random forest techniques. Then we'll look at the Word2Vec models and prediction
    of positive and negative reviews with the Word2Vec approach and the k-nearest
    neighbor classifier.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will particularly focus on text and words and classify internet
    comments as spam or not spam or to identify internet reviews as positive or negative.
    We will also have an overview for bag of words for text classification and prediction
    model to predict YouTube comments are spam or not using bag of words and random
    forest techniques. We will also look at Word2Vec models an k-nearest neighbor
    classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'But, before we start, we''ll answer the following question: *w**hat makes text
    classification an interesting problem?*'
  prefs: []
  type: TYPE_NORMAL
- en: Text classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To find the answer to our question, we will consider the famous iris flower
    dataset as an example dataset. The following image is of iris versicolor species.
    To identify the species, we need some more information other than just an image
    of the species, such as the flower''s **Petal length**, **Petal width**, **Sepal
    length**, and **Sepal width **would help us identify the image better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00064.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The dataset not only contains examples of versicolor but also contains examples
    of setosa and virginica as well. Every example in the dataset contains these four
    measurements. The dataset contains around 150 examples, with 50 examples of each
    species. We can use a decision tree or any other model to predict the species
    of a new flower, if provided with the same four measurements. As we know same
    species will have almost similar measurements. Since similarity has different
    definition all together but here we consider similarity as the closeness on a
    graph, if we consider each point is a flower. The following graph is a comparison
    between sepal width versus petal width:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00065.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: If we had no way of measuring similarity, if, say, every flower had different
    measurements, then there'd be no way to use machine learning to build a classifier.
  prefs: []
  type: TYPE_NORMAL
- en: As we are aware of the fact that flowers of same species have same measurement
    and that helps us to distinguish different species. Consider what if every flower
    had different measurement, it would of no use to build classifier using machine
    learning to identify images of species.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before to that we considered images, let''s now consider text. For example,
    consider the following sentences and try to find what makes the first pair of
    phrases similar to the second pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00066.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: I hope you got the answer to that question, otherwise we will not be able to
    build a decision tree, a random forest or anything else to predict the model.
    To answer the question, notice that the top pair of phrases are similar as they
    contain some words in common, such as **subscribe** and **channel**, while the
    second pair of sentences have fewer words in common, such as **to **and **the**.
    Consider the each phrase representing vector of numbers in a way that the top
    pair is similar to the numbers in the second pair. Only then we will be able to
    use random forest or another technique for classification, in this case, to detect
    YouTube comment spam. To achieve this, we need to use the bag-of-words model.
  prefs: []
  type: TYPE_NORMAL
- en: Bag of words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The bag-of-words model does exactly we want that is to convert the phrases or
    sentences and counts the number of times a similar word appears. In the world
    of computer science, a bag refers to a data structure that keeps track of objects
    like an array or list does, but in such cases the order does not matter and if
    an object appears more than once, we just keep track of the count rather we keep
    repeating them.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider the first phrase from the previous diagram, it has a bag
    of words that contents words such as **channel**, with one occurrence, **plz**,
    with one occurrence, **subscribe**, two occurrences, and so on. Then, we would
    collect all these counts in a vector, where one vector per phrase or sentence
    or document, depending on what you are working with. Again, the order in which
    the words appeared originally doesn't matter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The vector that we created can also be used to sort data alphabetically, but
    it needs to be done consistently for all the different phrases. However, we still
    have the same problem. Each phrase has a vector with different columns, because
    each phrase has different words and a different number of columns, as shown in
    the following two tables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00067.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we make a larger vector with all the unique words across both phrases, we
    get a proper matrix representation. With each row representing a different phrase,
    notice the use of 0 to indicate that a phrase doesn''t have a word:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00068.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: If you want to have a bag of words with lots of phrases, documents, or  we would
    need to collect all the unique words that occur across all the examples and create
    a huge matrix, *N* x *M*, where *N* is the number of examples and *M* is the number
    of occurrences. We could easily have thousands of dimensions compared in a four-dimensional
    model for the iris dataset. The bag of words matrix is likely to be sparse, meaning
    mostly zeros, since most phrases don't have most words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start building our bag of words model, we need to take care of a
    few things, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Lowercase every word
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drop punctuation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drop very common words (stop words)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove plurals (for example, bunnies => bunny)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform lemmatization (for example, reader => read, reading = read)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use n-grams, such as bigrams (two-word pairs) or trigrams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep only frequent words (for example, must appear in >10 examples)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep only the most frequent *M* words (for example, keep only 1,000)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Record binary counts (*1* = present, *0* = absent) rather than true counts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many other combinations for best practice, and finding the best that
    suits the particular data needs some research.
  prefs: []
  type: TYPE_NORMAL
- en: The problem that we face with long documents is that they will have higher word
    counts generally, but we may still want to consider long documents about some
    topic to be considered, similar to a short document about the same topic, even
    though the word counts will differ significantly.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, if we still wanted to reduce very common words and highlight the
    rare ones, what we would need to do is record the relative importance of each
    word rather than its raw count. This is known as **term frequency inverse document
    frequency** (**TF-IDF**), which measures how common a word or term is in the document.
  prefs: []
  type: TYPE_NORMAL
- en: We use logarithms to ensure that long documents with many words are very similar
    to short documents with similar words. TF-IDF has two components that multiply,
    that is when TF is high, the result is high but IDF measures how common the word
    is among all the documents and that will affect the common words. So, a word that
    is common in other documents will have a low score, regardless of how many times
    it appeared.
  prefs: []
  type: TYPE_NORMAL
- en: 'If a document has a low score which means the word appeared rarely and if the
    score is high it means the word appears frequently in the document. But if the
    word is quite common in all the documents then it becomes irrelevant to score
    on this document. It is anyhow considered to have low score. This shows that the
    formula for TF-IDF exhibits in a way we want our model to be. The following graph
    explains our theory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00069.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We will be using the bag-of-words method to detect whether YouTube comments
    are spam or .
  prefs: []
  type: TYPE_NORMAL
- en: Detecting YouTube comment spam
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we're going to look at a technique for detecting YouTube comment
    spam using bags of words and random forests. The dataset is pretty straightforward.
    We'll use a dataset that has about 2,000 comments from popular YouTube videos
    ([https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection](https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection)).
    The dataset is formatted in a way where each row has a comment followed by a value
    marked as 1 or 0 for spam or not spam.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will import a single dataset. This dataset is actually split into
    four different files. Our set of comments comes from the PSY-Gangnam Style video:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00070.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then we will print a few comments as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00071.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here we are able to see that there are more than two columns, but we will only
    require the content and the class columns. The content column contains the comments
    and the class column contains the values 1 or 0 for spam or not spam. For example,
    notice that the first two comments are marked as not spam, but then the comment
    **subscribe to me for call of duty vids** is spam and **hi guys please my android
    photo editor download yada yada** is spam as well. Before we start sorting comments,
    let''s look at the count of how many rows in the dataset are spam and how many
    are not spam. The result we acquired is 175 and 175 respectively, which sums up
    to 350 rows overall in this file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00072.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'In scikit-learn, the bag of words technique is actually called `CountVectorizer`,
    which means counting how many times each word appears and puts them into a vector.
    To create a vector, we need to make an object for `CountVectorizer`, and then
    perform the fit and transform simultaneously:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00073.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This performed in two different steps. First comes the fit step, where it discovers
    which words are present in the dataset, and second is the transform step, which
    gives you the bag of words matrix for those phrases. The result obtained in that
    matrix is 350 rows by 1,418 columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00074.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There are 350 rows, which means we have 350 different comments and 1,418 words.
    1418 word apparently are word that appear across all of these phrases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s print a single comment and then run the analyzer on that comment
    so that we can see how well the phrases breaks it apart. As seen in the following
    screenshot, the comment has been printed first and then we are analyzing it below,
    which is just to see how it broke it into words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00075.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can use the vectorizer feature to find out which word the dataset found
    after vectorizing. The following is the result found after vectorizing where it
    starts with numbers and ends with regular words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00076.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Execute the following command to shuffle the dataset with fraction 100% that
    is adding `frac=1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00077.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we will split the dataset into training and testing sets. Let''s assume
    that the first 300 will be for training, while the latter 50 will be for testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00078.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding code, `vectorizer.fit_transform(d_train['CONTENT'])` is an
    important step. At that stage, you have a training set that you want to perform
    a fit transform on, which means it will learn the words and also produce the matrix.
    However, for the testing set, we don't perform a fit transform again, since we
    don't want the model to learn different words for the testing data. We will use
    the same words that it learned on the training set. Suppose that the testing set
    has different words out of which some of them are unique to the testing set that
    might have never appeared in the training set. That's perfectly fine and anyhow
    we are going to ignore it. Because we are using the training set to build a random
    forest or decision tree or whatever would be the case, we have to use a certain
    set of words, and those words will have to be the same words, used on the testing
    set. We cannot introduce new words to the testing set since the random forest
    or any other model would not be able to gauge the new words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we perform the transform on the dataset, and later we will use the answers
    for training and testing. The training set now has 300 rows and 1,287 different
    words or columns, and the testing set has 50 rows, but we have the same 1,287
    columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00079.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Even though the testing set has different words, we need to make sure it is
    transformed in the same way as the training set with the same columns. Now we
    will begin with the building of the random forest classifier. We will be converting
    this dataset into 80 different trees and we will fit the training set so that
    we can score its performance on the testing set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00080.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The output of the score that we received is 98%; that''s really good. Here
    it seems it got confused between spam and not-spam. We need be sure that the accuracy
    is high; for that, we will perform a cross validation with five different splits.
    To perform a cross validation, we will use all the training data and let it split
    it into four different groups: 20%, 80%, and 20% will be testing data, and 80%
    will be the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00081.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We will now perform an average to the scores that we just obtained, which comes
    to about 95% accuracy. Now we will print all the data as seen in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00082.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'The entire dataset has five different videos with comments, which means all
    together we have around 2,000 rows. On checking all the comments, we noticed that
    there are `1005` spam comments and `951` not-spam comments, that quite close enough
    to split it in to even parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00083.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Here we will shuffle the entire dataset and separate the comments and the answers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00084.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We need to perform a couple of steps here with `CountVectorizer` followed by
    the random forest. For this, we will use a feature in scikit-learn called a **Pipeline**.
    Pipeline is really convenient and will bring together two or more steps so that
    all the steps are treated as one. So, we will build a pipeline with the bag of
    words, and then use `countVectorizer` followed by the random forest classifier.
    Then we will print the pipeline, and it the steps required:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00085.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can let the pipeline name of each step by itself by adding `CountVectorizer`
    in our `RandomForestClassifier` and it will name them `CountVectorizer` and `RandomForestclassifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00086.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the pipeline is created you can just call it fit and it will perform the
    rest that is first it perform the fit and then transform with the `CountVectorizer`,
    followed by a fit with the `RandomForest` classifier. That''s the benefit of having
    a pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00087.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now you call score so that it knows that when we are scoring it will to run
    it through the bag of words `countVectorizer`, followed by predicting with the
    `RandomForestClassifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00088.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This whole procedure will produce a score of about 94\. We can only predict
    a single example with the pipeline. For example, imagine we have a new comment
    after the dataset has been trained, and we want to know whether the user has just
    typed this comment or whether it''s spam:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00089.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As seen, it''s detected correctly; but what about the following comment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00090.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'To overcome this and deploy this classifier into an environment and predict
    whether it is a `spm` or not when someone types a new comment. We will use our
    pipeline to figure out how accurate our cross-validation was. We find in this
    case that the average accuracy was about 94:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00091.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s pretty good. Now let''s add TF-IDF to our model to make it more precise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00092.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This will be placed after `countVectorizer`. After we have produced the counts,
    we can then produce a TF-IDF score for these counts. Now we will add this in the
    pipeline and perform another cross-validation check with the same accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00093.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This show the steps required for the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00094.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The following output got us `CountVectorizer`, a TF-IDF transformer, and `RandomForestClassifier`.
    Notice that `countvectorizer` can be lower case or upper case in the dataset;
    it is on us to decide how many words you want to have. We can either use single
    words or bigrams, which would be pairs of words, or trigrams, which can be triples
    of words. We can also remove stop words, which are really common English words
    such as **and**, **or**, and **the**. With TF-IDF, you can turn off the `idf`
    component and just keep the `tf` component, which would just be a log of the count.
    You can use `idf` as well. With random forests, you've got a choice of how many
    trees you use, which is the number of estimators.
  prefs: []
  type: TYPE_NORMAL
- en: 'There''s another feature of scikit-learn available that allows us to search
    all of these parameters. For that, it finds out what the best parameters are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00095.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We can make a little dictionary where we say the name of the pipeline step and
    then mention what the parameter name would be and this gives us our options. For
    demonstration, we are going to try maximum number of words or maybe just a maximum
    of 1,000 or 2,000 words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `ngrams`, we can mention just single words or pairs of words that are
    stop words, use the English dictionary of stop words, or don''t use stop words,
    which means in the first case we need to get rid of common words, and in the second
    case we do not get rid of common words. Using TF-IDF, we use `idf` to state whether
    it''s yes or no. The random forest we created uses 20, 50, or 100 trees. Using
    this, we can perform a grid search, which runs through all of the combinations
    of parameters and finds out what the best combination is. So, let''s give our
    pipeline number 2, which has the TF-IDF along with it. We will use `fit` to perform
    the search and the outcome can be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00096.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Since there is a large number of words, it takes a little while, around 40
    seconds, and ultimately finds the best parameters. We can get the best parameters
    out of the grid search and print them to see what the score is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00097.gif)'
  prefs: []
  type: TYPE_IMG
- en: So, we got nearly 96% accuracy. We used around 1,000 words, only single words,
    used yes to get rid of stop words, had 100 trees in the random forest, and used
    yes and the IDF and the TF-IDF computation. Here we've demonstrated not only bag
    of words, TF-IDF, and random forest, but also the pipeline feature and the parameter
    search feature known as grid search.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll learn about Word2Vec, a modern and popular technique
    for working with text. Usually, Word2Vec performs better than simple bag of words
    models. A bag of words model only counts how many times each word appears in each
    document. Given two such bag of words vectors, we can compare documents to see
    how similar they are. This is the same as comparing the words used in the documents.
    In other words, if the two documents have many similar words that appear a similar
    number of times, they will be considered similar.
  prefs: []
  type: TYPE_NORMAL
- en: But bag of words models have no information about how similar the words are.
    So, if two documents do not use exactly the same words but do use synonyms, such
    as **please** and **plz**, they're not regarded as similar for the bag of words
    model. Word2Vec can figure out that some words are similar to each other and we
    can exploit that fact to get better performance when doing machine learning with
    text.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Word2Vec, each word itself is a vector, with perhaps 300 dimensions. For
    example, in a pre-trained Google Word2Vec model that examined millions or billions
    of pages of text, we can see that cat, dog, and spatula are 300-dimensional vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: Cat = <0.012, 0.204, ..., -0.275, 0.056> (300 dimensions)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dog = <0.051, -0.022, ..., -0.355, 0.227>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spatula = <-0.191, -0.043, ..., -0.348, 0.398>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarity (distance) between cat and dog—0.761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarity between cat and spatula—0.124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we compare the similarity of the dog and cat vectors, we will get 0.761 or
    76% of similarity. If we do the same with cat and spatula, we get 0.124\. It's
    clear that Word2Vec learned that dog and cat are similar words but cat and spatula
    are not. Word2Vec uses neural networks to learn these word vectors. At a high
    level, a neural network is similar to random forest or a decision tree and other
    machine learning techniques because they're given a bunch of inputs and a bunch
    of outputs, and they learn how to predict the outputs from the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: For Word2Vec, the input is a single word, the word whose vector we want to learn,
    and the output is its nearby words from the text. Word2Vec also supports the reverse
    of this input-output configuration. Thus, Word2Vec learns the word vectors by
    remembering its context words. So, dog and cat will have similar word vectors
    because these two words are used in similar ways, like *she pet the dog* and *she
    pet the cat*. Neural networking with Word2Vec can take one of two forms because
    Word2Vec supports two different techniques for training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first technique is known as continuous bag of words, where the context
    words are the input, leaving out the middle word and the word whose vector we''re
    learning, the middle word, is the output. In the following diagram, you can see
    three words before and after the word **channel**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00098.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Those are the context words. The continuous bag of words model slides over the
    whole sentence with every word acting as a center word in turn. The neural network
    learns the 300-dimensional vectors for each word so that the vector can predict
    the center word given the context words. In other words, it can predict the output
    given its inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second technique, we''re going to flip this. This is known as **skip-gram**,
    and the center word is the input and the context words are the outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00099.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In this technique, the center word vector is used to predict the context words
    given that center word.
  prefs: []
  type: TYPE_NORMAL
- en: Both of these techniques perform well for most situations. They each have minor
    pros and cons that will not be important for our use case.
  prefs: []
  type: TYPE_NORMAL
- en: Doc2Vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''re going to use Word2Vec to detect positive and negative product, restaurant,
    and movie reviews. We will do so with a slightly different form of Word2Vec known
    as **Doc2Vec**. In this case, the input is a document name, such as the filename,
    and the output is the sliding window of the words from the document. This time,
    we will not have a center word:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00100.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In this case, as a vector that helps us predict the words, from knowing the
    filename. In fact, the input is not very important, which in this case is the
    filename. We just need to keep track of the words on the right side, and that
    they all came from the same document. So, all of those words will be connected
    to that filename, but the actual content of that filename is not important. Since
    we can predict the document's words based on its filename, we can effectively
    have a model that knows which words go together in a document. In other words,
    that documents usually talk about just one thing, for example, learning that a
    lot of different positive words are used in positive reviews and a lot of negative
    words are used in negative reviews.
  prefs: []
  type: TYPE_NORMAL
- en: Document vector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After training, we have a new document and we want to find its document vector.
    We''ll use the word similarities learned during training to construct a vector
    that will predict the words in the new document. We will use a dummy filename
    since the actual name is not important. What''s important is that it''s just one
    name. So, all of these words get connected together under that one name:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00101.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once we get that new document vector, we can compare it with other document
    vectors and find which known document from the past is the most similar, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00102.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Thus, we can use `Doc2Vec` to find which documents are most similar to each
    other. This will help us detect positive and negative reviews because, ideally,
    the positive reviews will have document vectors that are similar to each other
    and this will be the same for negative reviews. We expect `Doc2Vec` to perform
    better than bag of words because `Doc2Vec` learns the words that are used together
    in the same document, so those words that are similar to bag of words never actually
    learned any information about how similar the words are different.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting positive or negative sentiments in user reviews
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''re going to look at detecting positive and negative sentiments
    in user reviews. In other words, we are going to detect whether the user is typing
    a positive comment or a negative comment about the product or service. We''re
    going to use `Word2Vec` and `Doc2Vec` specifically and the `gensim` Python library
    for those services. There are two categories, which are positive and negative,
    and we have over 3,000 different reviews to look at. These come from Yelp, IMDb,
    and Amazon. Let''s begin the code by importing the `gensim` library, which provides
    `Word2Vec` and `Doc2Vec` for logging to note status of the messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00103.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'First, we will see how to load a pre-built `Word2Vec` model, provided by Google,
    that has been trained on billions of pages of text and has ultimately produced
    300-dimensional vectors for all the different words. Once the model is loaded,
    we will look at the vector for `cat`. This shows that the model is a 300-dimensional
    vector, as represented by the word `cat`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00104.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows the 300-dimensional vector for the word `dog`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00105.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows the 300-dimensional vector for the word `spatula`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00106.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We obtain a result of 76% when computing the similarity of dog and cat, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00107.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The similarity between cat and spatula is 12%; it is a bit lower, as it should
    be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00108.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here we train our `Word2Vec` and `Doc2Vec` model using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00109.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We are using `Doc2Vec` because we want to determine a vector for each document,
    not necessarily for each word in the document, because our documents are reviews
    and we want to see whether these reviews are positive or negative, which means
    it's similar to positive reviews or similar to negative reviews. `Doc2Vec` is
    provided by `gensim` and the library has a class called `TaggedDocument` that
    allows us to use "`these are the words in the document, and Doc2Vec is the model`".
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we create a utility function that will take a sentence or a whole paragraph
    and lowercase it and remove all the HTML tags, apostrophes, punctuation, spaces,
    and repeated spaces, and then ultimately break it apart by words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00110.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now it's time for our training set. We are not going to use the 3,000 Yelp,
    IMDb, and Amazon reviews because there's simply not enough data to train for a
    good `Doc2Vec` model. If we had millions reviews, then we could take a good portion
    of that to train with and use the rest for testing, but with just 3,000 reviews
    it's not enough. So, instead, I've gathered reviews from IMDb and other places,
    including Rotten Tomato. This will be enough to train a `Doc2Vec` model, but none
    of these are actually from the dataset that we're going to use for our final prediction.
    These are simply reviews. They're positive; they're negative. I don't know which,
    as I'm not keeping track of which. What matters is that we have enough text to
    learn how words are used in these reviews. Nothing records whether the review
    is positive or negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, `Doc2Vec` and `Word2Vec` are actually being used for unsupervised training.
    That means we don''t have any answers. We simply learn how words are used together.
    Remember the context of words, and how a word is used according to the words nearby:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00111.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, in each case, in each file, we simply make a `TaggedDocument` object with
    the words from that document or that review plus a tag, which is simply the filename.
    This is important so that it learns that all these words go together in the same
    document, and that these words are somehow related to each other. After loading,
    we have 175,000 training examples from different documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00112.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s have a look at the first 10 sentences in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00113.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We shuffle these documents and then feed them into our `Doc2Vec` trainer, using `Doc2Vec(permuter,
    dm=0, hs=1, size=50)`, where we finally do the training of the `Doc2Vec` model
    and where it learns the document vectors for all the different documents. `dm=0` and `hs=1` are
    just parameters to say how to do the training. These are just things that I found
    were the most accurate. `dm=0` is where we are using the model that was shown
    in the last section, which means it receives a filename and it predicts the words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00114.gif)'
  prefs: []
  type: TYPE_IMG
- en: Here `size=50` means that we found that 50-dimensional vectors for each document
    was best, and 300-dimensional vectors are optimal, because we don't have enough
    training examples. Since we don't have millions or billions of data. This is a
    good 300 dimensional vector, and 50 seemed to work better. Running this code uses
    the processor and all the cores you have, so it will takes some time to execute.
    You will see that it's going through all the percentages of how much it got through.
    Ultimately, it takes 300 seconds to get this information in my case, which is
    definitely not bad. That's pretty fast, but if you have millions or billions of
    training documents, it could take days.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the training is complete, we can delete some stuff to free up some memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00115.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We do need to keep the inference data, which is enough to bind a new document
    vector for new documents, but we don't need it to keep all the data about all
    the different words.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can save the model and then load it later with the `model = Doc2Vec.Load(''reviews.d2v'')`
    command, if you want to put it in a product and deploy it, or put it on a server:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00116.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'After the model''s been trained, you can infer a vector, which is regarding
    what the document vector is for this new document. So, let''s extract the words
    with the utility function. Here we are using an example phrase that was found
    in a review. This is the 50-dimensional vector it learned for that phrase:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00117.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now the question that rises is what about a negative phrase? And another negative
    phrases. Are they considered similar? Well, they''re considered 48% similar, as
    seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00118.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'What about different phrases? `Highly recommended` and `Service sucks`. They''re
    less similar:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00119.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The model learned about how words are used together in the same review and that
    these words go together in one way and that other words go together in a different
    way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we are ready to load our real dataset for prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00120.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: To summarize, we used Yelp, Amazon, and IMDb reviews. We loaded different files
    and in each file, each line had a review. As a result, we get the words from the
    line and found out what the vector was for that document. We put that in a list,
    shuffle, and finally built a classifier. In this case, we're going to use k-nearest
    neighbors, which is a really simple technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s just a technique that says *find all the similar documents*, in this
    case, the nine closest documents to the one that we''re looking at, and count
    votes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00121.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We will be using nine reviews for the purposes of this example, and if you have
    a majority, let's say of positive reviews, then we will say that this is a positive
    review too. If the majority says negative, then this is a negative too. We don't
    want a tie regarding the reviews, which is why we say that there's nine instead
    of eight.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will compare the outcome with a random forest:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00122.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we need to perform cross-validation with the 9 nearest neighbors; we get
    76% accuracy for detecting positive/negative reviews with `Doc2Vec`. For experimental
    purposes, if we use a random forest without really trying to choose an amount
    of trees, we just get an accuracy of 70%:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00123.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In such cases, k-nearest neighbors is both simpler and more accurate. Ultimately,
    is it all worth it? Well, let''s comparing it to the bag of words model. Let''s
    make a little pipeline with `CountVectorizer`, TF-IDF, and random forest, and
    at the end, do cross-validation on the same data, which in this case is the reviews.
    Here, we get 74%, as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00124.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The outcome that we found after executing the model build we found `Doc2Vec`
    was better. `Doc2Vec` can be a lot more accurate than bag of words if we add a
    lot of training examples that are of the same style as the testing set. Hence,
    in our case, the testing set was pretty much the Yelp, Amazon, and IMDb reviews,
    which are all one sentence or one line of text and are pretty short. However,
    the training set that we found came from different reviews from different places,
    and we got about 175,000 examples. Those were often like paragraphs or just written
    in different ways.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, we will train a `Doc2Vec` or `Word2Vec` model on examples that are
    similar to what we're going to predict on later, but it can be difficult to find
    enough examples, as it was here so we did our best. Even so, it still turned out
    better than bag of words.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced text processing and the bag of words technique.
    We then used this technique to build a spam detector for YouTube comments. Next,
    we learned about the sophisticated Word2Vec model and put it to task with a coding
    project that detects positive and negative product, restaurant, and movie reviews.
    That's the end of this chapter about text.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to look at deep learning, which is a popular
    technique that's used in neural networks.
  prefs: []
  type: TYPE_NORMAL
