- en: Image Classification Using Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Convolutional neural networks** (**CNNs**) are popular deep neural networks
    and are considered to be the gold standard for large-scale image classification
    tasks. Applications involving CNNs include image recognition and classification,
    natural language processing, medical image classification, and many others. In
    this chapter, we will continue with supervised learning situations where a response
    variable exists. This chapter provides steps for applying image classification
    and recognition using convolutional neural networks with an easy-to-follow practical
    example involving fashion-related **Modified National Institute of Standards and
    Technology** (**MNIST**) data. We also make use of images of fashion items downloaded
    from the internet to explore the generalization potential of the classification
    model that we develop.'
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically in this chapter, we cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layers in convolutional neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fitting the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model evaluation and prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance optimization tips and best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will make use of the Keras and EBImage libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s get started by looking at some images downloaded from the internet.
    There are 20 images that include fashion articles such as shirts, bags, sandals,
    dresses, and others. These images were obtained using a Google search. We will
    try to develop an image recognition and classification model that recognizes these
    images and classifies them in appropriate categories. And to develop such a model,
    we will make use of the fashion-MNIST database of fashion articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The 20 images of fashion items downloaded from the internet are shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/909db000-7ce0-40ca-9380-3972d160ed9a.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, let's look at the fashion-MNIST data that contains a much larger collection
    of images of such fashion items.
  prefs: []
  type: TYPE_NORMAL
- en: Fashion-MNIST data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can access fashion-MNIST data from Keras using the`dataset_fashion_mnist`
    function. Take a look at the following code and its output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Looking at the structure of the preceding data, we see that it contains train
    data with 60,000 images and test data with 10,000 images. All these images are
    28 x 28 grayscale images. We know from the previous chapter that images can be
    represented as numeric data based on color and intensity. The independent variable
    x contains the intensity values, and the dependent variable y contains labels
    from 0 to 9.
  prefs: []
  type: TYPE_NORMAL
- en: 'The 10 different fashion items in the fashion-MNIST dataset are labelled from
    0 to 9, as shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Label | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | T-shirt/Top |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Trouser |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Pullover |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Dress |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Coat |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Sandal |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Shirt |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Sneaker |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | Bag |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | Ankle Boot |'
  prefs: []
  type: TYPE_TB
- en: Looking at the preceding table, we may observe that developing a classification
    model for these images will be challenging as some categories will be difficult
    to differentiate.
  prefs: []
  type: TYPE_NORMAL
- en: Train and test data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We extract train image data, store it in `trainx`, and store the respective
    labels in `trainy`. In a similar fashion, we create `testx` and `testy` from the
    test data. The table based on `trainy` indicates that there are exactly 6,000
    images each for the 10 different fashion articles in the train data, while in
    the test data, there are exactly 1,000 images of each fashion article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We next plot the first 64 images in the train data. Note that these are grayscale
    image data and that each image has a black background. Since our image classification
    model will be based on this data, the color images that we started with will have
    to be converted into grayscale images too. In addition, images of shirts, coats,
    and dresses are somewhat challenging to differentiate and this may impact the
    accuracy of our model. Let''s have a look at the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the output of the first 64 images in the train data as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08e58ae2-975a-47d4-afba-8d1169790f2a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A histogram based on the first image (an ankle boot) in the train data is shown
    in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6a41ac4-0875-4150-bd49-e01e44962d9d.png)'
  prefs: []
  type: TYPE_IMG
- en: The highest bar on the left is from the low intensity data points that capture
    the black background in the image. The higher intensity values representing the
    lighter color of the ankle boot are reflected in the higher bars toward the right.
    These intensity values in the histogram range from 0 to 255.
  prefs: []
  type: TYPE_NORMAL
- en: Reshaping and resizing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we reshape, train, and test data. We also divide the train and the test
    data by 255 to change the range of values from 0-255 to 0-1\. The codes used are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The structure of the preceding `trainx `shows that after reshaping the train
    data, we now have data with 60,000 rows and 784 (28 x 28) columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'We get the output of the histogram based on the first image (an ankle boot)
    in the train data after dividing the data by 255, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8732ae1c-7c1b-460f-af40-9c69b7ec14e2.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding histogram shows that the range of data points has now changed
    to values between 0 and 1\. However, the shape observed in the previous histogram
    has not changed.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we do one-hot encoding of labels stored in `trainy` and `testy` using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: After one-hot encoding, the first row for the train data indicates a value of
    1 for the tenth category (ankle boot). Similarly, the second row for the train
    data indicates a value of 1 for the first category (t-shirt/top). After completing
    the changes mentioned previously, now the fashion-MNIST data is ready for developing
    an image recognition and classification model.
  prefs: []
  type: TYPE_NORMAL
- en: Layers in the convolutional neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will develop model architecture and then compile the model.
    We will also carry out calculations to compare the convolutional network with
    a fully connected network. Let's get started by specifying the model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Model architecture and related calculations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We start by creating a model using the `keras_model_sequential` function. The
    codes used for the model architecture are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code, we add various layers to develop a CNN model.
    The input layer in this network has 28 x 28 x 1 dimensions based on the height
    and width of the images, which are 28 each. And since we are using grayscale images,
    the color channel is one. We use two-dimensional convolutional layers here as
    we are building a deep learning model with gray-scale images.
  prefs: []
  type: TYPE_NORMAL
- en: Note that when developing an image recognition and classification model with
    gray scale image data, we make use of a 2D convolutional layer, and with color
    images, we make use of a 3D convolutional layer.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at some calculations involving the first convolutional layer of the
    network, which will help us to appreciate the use of such layers compared to a
    densely connected layer. In a CNN, neurons in a layer are not connected to all
    the neurons in the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the input layer has an image with dimensions of 28 x 28 x 1\. To obtain
    the output shape, we subtract three (from `kernel_size`) from 28 (height of the
    input image) and add one. This gives us 26\. The final dimension for the output
    shape becomes 26 x 26 x 32, where 32 is the number of output filters. Thus, the
    output shape has reduced height and width, but it has a greater depth. To arrive
    at the number of parameters, we use 3 x 3 x 1 x 32 + 32 = 320, where 3 x 3 is
    the `kernel_size`, 1 is the number of channels for the image, 32 is the number
    of output filters, and to this we add 32 bias terms.
  prefs: []
  type: TYPE_NORMAL
- en: If we compare this to a fully connected neural network, we will obtain a much
    larger number of parameters. In a fully connected network, 28 x 28 x 1 = 784 neurons
    will be connected to 26 x 26 x 32 = 21,632 neurons. So, the total number of parameters
    will be 784 x 21,632 + 21,632 = 16,981,120\. This is more than 53,000 times the
    number of parameters for a densely connected layer compared to what we get for
    a convolutional layer. This, in turn, helps to significantly reduce the processing
    time and thereby the processing cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of parameters for each layer is indicated in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The output shape for the second convolutional network is 24 x 24 x 64, where
    64 is the number of output filters. Here too, the output shape has a reduced height
    and width, but it has a greater depth. To arrive at a number of parameters, we
    use 3 x 3 x 32 x 64 + 64 = 18,496, where 3 x 3 is the `kernel_size`, 32 is the
    number of filters in the previous layer, and 64 is the number of output filters,
    and to this, we add 64 bias terms.
  prefs: []
  type: TYPE_NORMAL
- en: The next layer is the pooling layer, which is usually placed after the convolutional
    layer and performs a down-sampling operation. This helps to reduce processing
    time and also helps to reduce overfitting. To obtain output shape, we can divide
    24 by 2, where 2 comes from the pool size that we have specified. The output shape
    here is 12 x 12 x 64 and no new parameters are added. The pooling layer is followed
    by a dropout layer with the same output shape and, once again, no new parameters
    are added.
  prefs: []
  type: TYPE_NORMAL
- en: In the flattened layer, we go from 3 dimensions (12 x 12 x 64) to one dimension
    by multiplying the three numbers to obtain 9,216\. This is followed by a densely
    connected layer with 64 units. The number of parameters here can be obtained as
    9216 x 64 + 64 = 589,888\. This is followed by another dropout layer to avoid
    the overfitting problem and no parameters are added here. And finally, we have
    the last layer, which is a densely connected layer with 10 units representing
    10 fashion items. The number of parameters here is 64 x 10 + 10 = 650\. The total
    number of parameters is thus 609,354\. In the CNN architecture that we have, we
    are using the relu activation function for the hidden layers and softmax for the
    output layer.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we compile the model using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, loss is specified as `categorical_crossentropy` as there
    are 10 categories of fashion items. For the optimizer, we make use of `optimizer_adadelta`
    with its recommended default settings. Adadelta is an adaptive learning rate method
    for gradient descent. As the name suggests, it dynamically adapts over time and
    doesn't require manual tuning of the learning rate. We have also specified `accuracy`
    for the metrics.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will fit the model for image recognition and classification.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For fit the model, we will continue with the format that we have been using
    in the earlier chapters. The following code is used to fit the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are using 20 epochs, a batch size of 128, and 20% of the training data
    is reserved for validation. Since the neural network used here is more complex
    than the previous chapters, each run is likely to take relatively more time.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy and loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After fitting the model, accuracy and loss values for the 15 epochs are plotted
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee1fd997-7042-4d80-bc61-e1b1badabac2.png)'
  prefs: []
  type: TYPE_IMG
- en: We can observe from the preceding plot that training accuracy continues to increase,
    whereas validation accuracy for the last few epochs is more or less flat. A similar
    pattern in the opposite direction is observed for the loss values. However, we
    do not observe any major overfitting problem.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now evaluate this model and see how predictions with this model perform.
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation and prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After fitting the model, we will evaluate its performance in terms of loss and
    accuracy. We will also create a confusion matrix to assess classification performance
    across all 10 types of fashion items. We will perform a model evaluation and prediction
    for both train and test data. We will also obtain images of fashion items that
    do not belong to the MNIST fashion data and explore how well the performance of
    the model can be generalized to new images.
  prefs: []
  type: TYPE_NORMAL
- en: Training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Loss and accuracy based on training data are obtained as 0.115 and 0.960, respectively,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a confusion matrix based on the predicted and actual values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding confusion matrix, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The correct classifications shown on the diagonal for all 10 categories have
    large values, with the lowest being 5,263 out of 6,000 for item 6 (shirt).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The best classification performance is seen for item 8 (bag), where this model
    correctly classifies 5,974 bag images out of 6,000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Among off-diagonal numbers representing misclassifications by the model, the
    highest value is 359, where item 6 (shirt) is mistaken for item 0 (t-shirt/top).
    There are 230 occasions when item-0 (t-shirt/top) is misclassified as item 6 (shirt).
    So, this model certainly has some difficulty differentiating between item 0 and
    item 6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s also look deeper by calculating prediction probabilities for the first
    five items, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We can observe from the preceding output that all five fashion items are correctly
    classified. The correct classification probabilities range from 0.656 (item 0
    in the fifth row) to 1.000 (item 0 in the second row). These probabilities are
    significantly high to effect correct classification without any confusion.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see whether this performance is replicated with test data.
  prefs: []
  type: TYPE_NORMAL
- en: Test data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We start by looking at loss and accuracy values based on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We observe that loss is higher and accuracy is lower compared to the values
    obtained from the train data. This is as expected considering a similar situation
    with validation data that we observed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Confusion matrix for test data is provided as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding confusion matrix, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: This model is most confused regarding item 6 (shirt), with 91 instances where
    it classifies fashion items as item-0 (t-shirt/top).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The best image recognition and classification performance is for item-5 (sandal),
    with 988 correct predictions out of 1,000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, the confusion matrix exhibits a similar pattern to the one we observed
    with the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Looking at prediction probabilities for the first five items in the test data,
    we observe that all five predictions are correct. Prediction probability for all
    five items is quite high:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Now, with sufficiently high classification performances with both the training
    and testing data in terms of accuracy, let's see if we can do the same with the
    20 images of fashion items with which we started this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 20 fashion items from the internet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We read the 20 colored images from the desktop and change them to gray to maintain
    compatibility with the data and model that we have used so far. Take a look at
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'As seen previously, we also resize all 20 images to 28 x 28, and the resulting
    20 images to be classified are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b80c10e-8cee-4778-901e-047fd020ab1a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can observe from the preceding plot, there are two fashion items belonging
    to each of the 10 categories of the fashion-MNIST data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We reshape images in the required dimensions and then row-bind them. Looking
    at the structure of `new`, we see a 20 x 784 matrix. However, to get an appropriate,
    structure we will reshape it further to 20 x 28 x 28 x 1, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We reshape `new` to get the appropriate format, and save the result in `newx`.
    We use `newy` to store the actual labels for the 20 fashion items.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are ready to use the prediction model, and create a confusion matrix
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We observe from the numbers on the diagonal that only 10 items are correctly
    classified out of 20\. This translates to a low accuracy of only 50%, compared
    to over 90% accuracy observed for the train and test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we summarize these predictions in the form of a plot that includes the
    prediction probabilities, predicted class, and actual class, using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding plot summarizes the performance of the classification model with
    the help of prediction probabilities, predicted class, and actual class (`model-one`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/281a96f9-b448-475d-bea5-c3c323e6f39b.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding plot, the first number in the top-left position is the prediction
    probability, the second number in the top-middle position is the predicted class,
    and the third number in the top-right position is the actual class. Looking at
    some of these misclassifications, what stands out is the fact that surprisingly,
    all images of the sandal (item 5), sneakers (item 7), and ankle boots (item 9)
    are incorrectly classified. These categories of images were classified with high
    accuracy in the training as well as test data. These six misclassifications have
    contributed to the significantly low accuracy value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two key aspects of what we have done so far can now be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The first one is what we would generally expect—model performance with the test
    data is usually lower compared to what is observed with the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second one is a bit of an unexpected outcome. The 20 fashion item images
    that were downloaded from the internet had significantly reduced accuracy with
    the same model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's see whether we can devise a strategy or make changes to the model to obtain
    better performance. We plan to have a closer look at the data and find a way to
    translate the performance that we saw with training and test data, if possible,
    to the 20 new images.
  prefs: []
  type: TYPE_NORMAL
- en: Performance optimization tips and best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In any data analysis task, it is important to understand how the data was collected.
    With the model that we developed in the previous section, the accuracy dropped
    from over 90% for the test data to 50% for the 20 fashion item images that were
    downloaded from the internet. If this difference is not addressed, it will be
    difficult for this model to generalize well with any fashion items that are not
    part of the training or test data, and therefore will not be of much practical
    use. In this section, we will explore improvements to the classification performance
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Image modification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Looking at the 64 images at the beginning of this chapter reveals some clues
    as to what''s going on. We notice that images of the sandals, sneakers, and ankle
    boots seem to have a specific pattern. In all pictures involving these fashion
    items, the toe has always been pictured pointing in the left direction. On the
    other hand, in the images downloaded from the internet for the three footwear
    fashion items, we notice that the toe has been pictured pointing in the right
    direction. To address this, let''s modify images of the 20 fashion items with
    a `flop` function that will make the toes point in the left direction, and then
    we can again assess the classification performance of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the prediction probabilities, predicted class,
    and actual class after applying the flop (`model-one`) function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8438c6a4-c1fe-4b8c-9964-92537cc9a22e.png)'
  prefs: []
  type: TYPE_IMG
- en: As observed from the preceding plot, after changing the orientation of the images
    of the fashion items, we now get correct classifications by the model for sandals,
    sneakers, and ankle boots. With 16 correct classifications out of 20, the accuracy
    improves to 80%, compared to a figure of 50% that we obtained earlier. Note that
    this improvement in accuracy comes from the same model. The only thing that we
    did here was to observe how the original data was collected and then maintain
    consistency with the new image data being used. Next, let's work on the modification
    of the deep network architecture and see whether we can improve results further.
  prefs: []
  type: TYPE_NORMAL
- en: Before prediction models are used for generalizing the results to new data,
    it is a good idea to review how data was originally collected and then maintain
    consistency in terms of the format for the new data.
  prefs: []
  type: TYPE_NORMAL
- en: We encourage you to experiment a bit further to explore and see what happens
    if certain percentages of images in the fashion-MNIST data are changed to their
    mirror images. Can this help to generalize even better without a need to make
    changes to the new data?
  prefs: []
  type: TYPE_NORMAL
- en: Changes to the architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We modify the architecture of the CNN by adding more convolutional layers to
    illustrate how such layers can be added. Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, for the first two convolutional layers, we use 32 filters
    each, and for the next set of convolutional layers, we use 64 filters each. After
    each pair of convolutional layers, as done earlier, we add pooling and dropout
    layers. Another change carried out here is the use of 512 units in the dense layer.
    Other settings are similar to the earlier network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows accuracy and loss for training and validation
    data (`model_two`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea618897-a548-4815-a69e-d52c9bf5036d.png)'
  prefs: []
  type: TYPE_IMG
- en: The plot based on `model_two` shows closer performance between training and
    validation data for loss and accuracy compared to `model_one`. In addition, a
    flattening of the lines toward the fifteenth epoch also suggests that increasing
    the number of epochs is not likely to help much in improving the classification
    performance further.
  prefs: []
  type: TYPE_NORMAL
- en: 'Loss and accuracy values for the training data are obtained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The loss and accuracy values based on this model do not show a major improvement,
    with the loss value being slightly higher and accuracy values being slightly lower.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following confusion matrix summarizes the predicted and actual classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'From the confusion matrix, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: It shows that the model has maximum confusion (456 misclassifications) between
    item 6 (shirt) and item 0 (t-shirt/top). And this confusion is observed in both
    directions, where item 6 is confused for item 0, and item-0 being confused for
    item 6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Item 8 (bag) has been classified most accurately, with 5,974 instances out of
    a total of 6,000 (about 99.6% accuracy).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Item-6 (shirt) has been classified with the lowest accuracy out of 10 categories,
    with 4,700 instances out of 6,000 (about 78.3% accuracy).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the test data loss, the accuracy and confusion matrices are provided as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding output, we observe that loss is lower than we obtained with
    the earlier model, while accuracy is slightly lower than the earlier performance.
    From the confusion matrix, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: It shows that the model has the maximum confusion (104 misclassifications) between
    item 6 (shirt) and item 0 (t-shirt/top).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Item 8 (bag) has been classified most accurately, with 989 instances out of
    a total of 1,000 (about 98.9% accuracy).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Item 6 (shirt) has been classified with the lowest accuracy out of 10 categories,
    with 720 instances out of 1,000 (about 72.0% accuracy).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, overall, we observe a similar performance to the one that we observed
    with the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the 20 images of fashion items downloaded from the internet, the following
    screenshot summarizes the performance of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dfb34d18-652e-478c-a156-d52b01e6da0a.png)'
  prefs: []
  type: TYPE_IMG
- en: As seen from the preceding plot, this time, we have 17 out 20 images correctly
    classified. Although this is a slightly better performance, it is still a little
    lower than the figure in the region of 92% accuracy for the test data. In addition,
    note that due to a much smaller sample, the accuracy values can fluctuate significantly.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we made modifications to the 20 new images and made some changes
    to the CNN model architecture to obtain a better classification performance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we showed how to use a **convolutional neural network** (**CNN**)
    deep learning model for image recognition and classification. We made use of the
    popular fashion-MNIST data for training and testing the image classification model.
    We also went over calculations involving a number of parameters, and were able
    to contrast this with the number of parameters that would have been needed by
    a densely connected neural network. CNN models help to significantly reduce the
    number of parameters needed and thus result in significant savings in computing
    time and resources. We also used images of fashion items downloaded from the internet
    to see whether a classification model based on fashion-MNIST data can be generalized
    to similar items. We did notice that it is important to maintain consistency in
    the way images are laid out in the training data. Additionally, we also showed
    how we can add more convolutional layers in the model architecture to develop
    a deeper CNN model.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have gradually progressed from not-so-deep neural network models
    to more complex and deeper neural network models. We also mainly covered such
    applications that are categorized under supervised learning methods. In the next
    chapter, we will go over another interesting class of deep neural network models
    called autoencoders. We will cover applications involving autoencoder networks
    that can be classified under unsupervised learning approaches.
  prefs: []
  type: TYPE_NORMAL
