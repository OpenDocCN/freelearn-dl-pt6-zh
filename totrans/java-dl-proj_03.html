<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Multi-Label Image Classification Using Convolutional Neural Networks</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we developed a project that accurately classifies cancer patients based on cancer types using an LSTM network. This is a challenging problem in biomedical informatics. Unfortunately, when it comes to classifying multimedia objects such as images, audio, or videos, linear ML models and other regular <strong>deep neural network</strong> (<strong>DNN</strong>) models, such as <strong>Multilayer Perceptron</strong> (<strong>MLP</strong>) or <strong>Deep Belief Networks</strong> (<strong>DBN</strong>), often fail to learn or model non-linear features from images.</p>
<p>On the other hand, <strong>convolutional neural networks</strong> (<strong>CNNs</strong>) can be utilized to overcome these limitations. In CNNs, the connectivity pattern between neurons is inspired by the human visual cortex, which more accurately resembles human vision, so it is perfect for image processing-related tasks. Consequently, CNNs have shown outstanding successes in numerous domains: computer vision, NLP, multimedia analytics, image searches, and so on.</p>
<p>Considering this motivation, in this chapter, we will see how to develop an end-to-end project for handling multi-label (that is, each entity can belong to multiple classes) image classification problems using CNNs based on the Scala and <strong>Deeplearning4j</strong> (<strong>DL4J</strong>) frameworks on real Yelp image datasets. We will also discuss some theoretical aspects of CNNs before getting started. Nevertheless, we will discuss how to tune hyperparameters for better classification results. Concisely, we will learn the following topics throughout our end-to-end project:</p>
<ul>
<li>Drawbacks of regular DNNs</li>
<li>CNN architectures: convolution operations and pooling layers</li>
<li>Large-scale image classification using CNNs</li>
<li>Frequently asked questions (FAQs)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image classification and drawbacks of DNNs</h1>
                </header>
            
            <article>
                
<p>In this project, we will show a step-by-step example of developing real-life ML projects for image classification using Scala and CNN. One such image data source is Yelp, where there are many photos and many users uploading photos. These photos provide rich local business information across categories. Thus, using these photos, developing an ML application by understanding the context of these photos is not an easy task. We will see how to use the DL4j platform to do so using Java. However, some theoretical background is a prior mandate before we start formally.</p>
<p>Before we start developing the end-to-end project for image classification using CNN, let's take a look at the drawbacks of regular DNNs. Although regular DNNs work fine for small images (for example, MNIST and CIFAR-10), it breaks down for large-scale and high-quality images because of the huge number of hyperparameters it requires. For example, a 200 Ã— 200 image has 40,000 pixels, and if the first layer has just 2,000 neurons, this means there will have 80 million different connections just in the first layer. Thus, if your network is very deep, there might be even billions of parameters.</p>
<p>CNNs solve this problem using partially connected layers. Because consecutive layers are only partially connected and because it heavily reuses its weights, a CNN has far fewer parameters than a fully connected DNN, which makes it much faster to train, reduces the risk of overfitting, and requires much less training data.</p>
<p>Moreover, when a CNN has learned a kernel that can detect a particular feature, it can detect that feature anywhere on the image. In contrast, when a DNN learns a feature in one location, it can detect it only in that particular location. Since images typically have very repetitive features, CNNs are able to generalize much better than DNNs for image processing tasks such as classification, using fewer training examples.</p>
<p>Importantly, DNN has no prior knowledge of how pixels are organized: it does not know that nearby pixels are close. A CNN's architecture embeds this prior knowledge. Lower layers typically identify features in small areas of the images, while higher layers combine the lower-level features into larger features. This works well with most natural images, giving CNNs a decisive head start compared to DNNs:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-641 image-border" src="assets/f6a9a8ed-d7aa-4f6e-9d9e-854ca4fd78b9.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Regular DNN versus CNN where each layer has neurons arranged in 3D</div>
<p>For example, in the preceding diagram, on the left, you can see a regular three-layer neural network. On the right, a ConvNet arranges its neurons into three dimensions (width, height, and depth), as visualized in one of the layers. Every layer of a <kbd>CNN</kbd> transforms the 3D structure into a 3D output structure of neuron activations. The red input layer holds the image, so its width and height would be the dimensions of the image, and the depth would be three (red, green, and blue channels).</p>
<p>Therefore, all the multilayer neural networks we looked at had layers composed of a long line of neurons, and we had to flatten input images to 1D before feeding them to the network. However, feeding 2D images directly to CNNs is possible since each layer in CNN is represented in 2D, which makes it easier to match neurons with their corresponding inputs. We will see examples of this in the upcoming sections.</p>
<p>Another important fact is that all the neurons in a feature map share the same parameters, so it dramatically reduces the number of parameters in the model. Also, more importantly, once a CNN has learned to recognize a pattern in one location, it can do the same for other locations as well.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CNN architecture</h1>
                </header>
            
            <article>
                
<p>In CNN networks, the way connectivity is defined among layers is significantly different compared to MLP or DBN. The <strong>convolutional</strong> (<strong>conv</strong>) layer is the main type of layer in a CNN, where each neuron is connected to a certain region of the input image, which is called a <strong>receptive field</strong>.</p>
<p>To be more specific, in a CNN architecture, a few conv layers are connected in a cascade style: each layer is followed by a <strong>rectified linear unit</strong> (<strong>ReLU</strong>) layer, then a pooling layer, then a few more conv layers (+ReLU), then another pooling layer, and so on. The output from each conv layer is a set of objects called feature maps, which are generated by a single kernel filter. Then, the feature maps are fed to the next layer as a new input. In the fully connected layer, each neuron produces an output followed by an activation layer (that is, the Softmax layer):</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-642 image-border" src="assets/50345908-1f46-4a96-801b-e62f4648cf8b.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">A conceptual architecture of CNN</div>
<p>As you can see in the preceding diagram, the pooling layers are usually placed after <span>the convolutional layers </span>(that is, between two such layers). A pooling layer into sub-regions then divides the convolutional region. Then, a single representative value is selected using either a max-pooling or an average pooling technique to reduce the computational time of subsequent layers. This way, a CNN can be thought of as a feature extractor. To understand this more clearly, refer to the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-643 image-border" src="assets/f436e2e8-09eb-4a3b-8910-1f3389993208.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">A CNN is an end-to-end network that acts as both a feature extractor and a classifier. This way, it can accurately identify (under the given condition that it gets sufficient training data) the label of a given input image. For example, it can classify that the input image is really a tiger.</div>
<p>The robustness of the feature with respect to its spatial position is increased too. To be more specific, when feature maps are used as image properties and pass through the grayscale image, it gets smaller and smaller as it progresses through the network, but it also typically gets deeper and deeper since more feature maps will be added. The convolution operation brings a solution to this problem as it reduces the number of free parameters, allowing the network to be deeper with fewer parameters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional operations</h1>
                </header>
            
            <article>
                
<p>A convolution is a mathematical operation that slides one function over another and measures the integrity of their pointwise multiplication. Convolutional layers are probably the most important building blocks in a CNN. For the first conv layer, neurons are not connected to every single pixel in the input image, but only to pixels in their receptive fields (refer to the preceding diagram), whereas each neuron in the second conv layer is only connected to neurons located within a small rectangle in the first layer:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-644 image-border" src="assets/48f7701f-6398-490e-ba5d-65ac689f0d22.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Each convolutional neuron only processes data for its receptive field</div>
<p>In <a href="e27fb252-7892-4659-81e2-2289de8ce570.xhtml" target="_blank">Chapter 2</a>, <em><span class="item-title">Cancer Types Prediction Using Recurrent Type Networks,</span></em> we have seen that all multilayer neural networks (for example, MLP) have layers composed of so many neurons, and we had to flatten input images to 1D before feeding them to the network. Instead, in a CNN, each layer is represented in 2D, which makes it easier to match neurons with their associated inputs.</p>
<div class="packt_infobox">The receptive field is used to exploit spatial locality by enforcing a local connectivity pattern between neurons of adjacent layers.</div>
<p>This architecture allows the network to concentrate on low-level features in the first hidden layer, and then assemble them into higher-level features in the next hidden layer, and so on. This hierarchical structure is common in real-world images, which is one of the reasons why CNNs work so well for image recognition.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pooling and padding operations</h1>
                </header>
            
            <article>
                
<p>Once you understand how convolutional layers work, pooling layers are quite easy to grasp. A pooling layer typically works on every input channel independently, so the output depth is the same as the input depth. Alternatively, you may pool over the depth dimension, as we will see next, in which case the image's spatial dimensions (for example, height and width) remain unchanged, but the number of channels is reduced. Let's see a formal definition of pooling layers from TensorFlow API documentation (see more at <a href="https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/python/ops/nn.py">https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/python/ops/nn.py</a>):</p>
<div class="packt_quote">"The pooling ops sweep a rectangular window over the input tensor, computing a reduction operation for each window (average, max, or max with argmax). Each pooling op uses rectangular windows of size called ksize separated by offset strides. For example, if strides are all ones, every window is used, if strides are all twos, every other window is used in each dimension, and so on."</div>
<p>Similar to a conv layer, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer that are located within a small rectangular receptive field. However, the size, the stride, and the padding type have to be defined. So, in summary, the output from a pooling layer can be computed as follows:</p>
<pre>output[i] = reduce(value[strides * i:strides * i + ksize])  </pre>
<p>where indices are also taken into consideration along with the padding values. In other words, the goal of using pooling is to subsample the input image in order to reduce the computational load, the memory usage, and the number of parameters. This helps to avoid overfitting in the training stage.</p>
<div class="packt_infobox">A pooling neuron has no weights. Therefore, it only aggregate the inputs using an aggregation function such as the max or mean.</div>
<p>The spatial semantics of the convolution ops depend on the padding scheme chosen. Padding is an operation to increase the size of the input data:</p>
<ul>
<li><strong>For 1D input</strong>: Just an array is appended with a constant, say, <kbd>c</kbd></li>
<li><strong>For a 2D input</strong>: A matrix that is surrounded with <kbd>c</kbd></li>
<li><strong>For a milt-dimensional (that is, nD) input</strong>: The nD hypercube is surrounded with <kbd>c</kbd></li>
</ul>
<p>Now, the question is, what's this constant <kbd>c</kbd>? Well, in most of the cases (but not always), <kbd>c</kbd> is zero called <strong>zero padding</strong>. This concept can be further broken down into two types of padding called <kbd>VALID</kbd> and <kbd>SAME</kbd>, which are outlined as follows:</p>
<ul>
<li><strong>VALID padding</strong>: Only drops the right-most columns (or bottom-most rows).</li>
<li><strong>SAME padding</strong>: In this scheme, padding is applied evenly or both left and right. However, if the number of columns to be added is odd, then an extra column is added to the right.</li>
</ul>
<p>We've explained the previous definition graphically in the following diagram. If we want a layer to have the same height and width as the previous layer, it is common to add zeros around the inputs. This is called <kbd>SAME</kbd> or zero padding.</p>
<div class="packt_tip">The term <kbd>SAME</kbd> means that the output feature map has the same spatial dimensions as the input feature map.</div>
<p>On the other hand, zero padding is introduced to make the shapes match as needed, equally on every side of the input map. On the other hand, <kbd>VALID</kbd> means no padding and only drops the right-most columns (or bottom-most rows):</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-645 image-border" src="assets/b37960ea-1ba3-4df2-9d86-9017f1424776.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">SAME versus VALID padding with CNN</div>
<p>In the following diagram, we use a 2 Ã— 2 pooling kernel, a stride of 2 with no padding. Only the max input value in each kernel makes it to the next layer since the other inputs are dropped (we will see this later on):</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-646 image-border" src="assets/ed02b9c1-62ad-479e-8ec0-a2fd75b91cb7.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">An example using max pooling, that is, subsampling</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fully connected layer (dense layer)</h1>
                </header>
            
            <article>
                
<p>At the top of the stack, a regular fully connected layer (feedforward neural network or dense layer) is added, which acts similar to an MLP that might be composed of a few fully connected layers (+ReLUs), and the final layer outputs the prediction: typically, a Softmax layer is used that outputs estimated class probabilities for a multiclass classification.</p>
<p>Well, up to this point, we have minimum theoretical knowledge about CNNs and their architectures for image classification. Now, it is time to do a hands-on project, which is about classifying large-scale Yelp images. At Yelp, there are many photos and many users uploading photos. These photos provide rich local business information across categories. Teaching a computer to understand the context of these photos is not an easy task.</p>
<div class="packt_infobox">Yelp engineers work on deep learning-based image classification projects in-house (see more at <a href="https://engineeringblog.yelp.com/2015/10/how-we-use-deep-learning-to-classify-business-photos-at-yelp.html">https://engineeringblog.yelp.com/2015/10/how-we-use-deep-learning-to-classify-business-photos-at-yelp.html</a>).</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multi-label image classification using CNNs</h1>
                </header>
            
            <article>
                
<p>In this section, we will show you a systematic example of developing real-life ML projects for image classification. However, we need to know the problem description first so as to know what sort of image classification needs to be done. Moreover, knowledge about the dataset is a mandate before getting started.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem description</h1>
                </header>
            
            <article>
                
<p>Nowadays, food selfies and photo-centric social storytelling are becoming social trends. Consequently, an enormous amount of selfies that include foods and a picture of the restaurant are being uploaded on social media and websites. In many instances, food lovers also provide the written reviews that can significantly boost the popularity of a business (for example, a restaurant).</p>
<p>For example, millions of unique visitors have visited the Yelp website and have written more than 135 million reviews. Besides, many photos and users are uploading photos. Nevertheless, business owners can post photos and message their customers. This way, Yelp makes money by <strong>selling ads</strong> to those local businesses.</p>
<p>An interesting fact is that these photos provide rich local business information across categories. Thus, developing deep learning applications to understand the context of these photos would be a useful task. Take a look at the following screenshot to get an insight:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-647 image-border" src="assets/954a1298-4086-4644-9a69-ae3a20bf1ebb.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Mining some insights about a business from a Yelp dataset</div>
<p>Thus, if we're given photos that belong to a business, we need to build a model so that it can tag restaurants with multiple labels of the user-submitted photos automatically in order to predict business attributes. Eventually, the goal of this project is to turn Yelp pictures into words.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Description of the dataset</h1>
                </header>
            
            <article>
                
<p>The Yelp dataset for this fun project was downloaded from <a href="https://www.kaggle.com/c/yelp-restaurant-photo-classification">https://www.kaggle.com/c/yelp-restaurant-photo-classification</a>. We got permission from the Yelp guys under the condition that the images won't be redistributed. However, you need to get usage permission from <a href="https://www.yelp.com/dataset">https://www.yelp.com/dataset</a>.</p>
<p>Submitting a review is tricky. When Yelp users want to submit a review, they have to select the labels of the restaurants manually from nine different labels that are annotated by the Yelp community, which are associated with the dataset. These are as follows:</p>
<ul>
<li><kbd>0</kbd>: <kbd>good_for_lunch</kbd></li>
<li><kbd>1</kbd>: <kbd>good_for_dinner</kbd></li>
<li><kbd>2</kbd>: <kbd>takes_reservations</kbd></li>
<li><kbd>3</kbd>: <kbd>outdoor_seating</kbd></li>
<li><kbd>4</kbd>: <kbd>restaurant_is_expensive</kbd></li>
<li><kbd>5</kbd>: <kbd>has_alcohol</kbd></li>
<li><kbd>6</kbd>: <kbd>has_table_service</kbd></li>
<li><kbd>7</kbd>: <kbd>ambience_is_classy</kbd></li>
<li><kbd>8</kbd>: <kbd>good_for_kids</kbd></li>
</ul>
<p>Thus, this is a multiple label multiclass classification problem, where each business can have one or more of the nine characteristics listed previously. Therefore, we have to predict these labels as accurately as possible. There are six files in the dataset, as follows:</p>
<ul>
<li><kbd>train_photos.tgz</kbd>: Photos to be used as the training set (234,842 images)</li>
<li><kbd>test_photos.tgz</kbd>: Photos to be used as the test set (237,152 images)</li>
<li><kbd>train_photo_to_biz_ids.csv</kbd>: Provides the mapping between the photo ID and the business ID (234,842 rows)</li>
<li><kbd>test_photo_to_biz_ids.csv</kbd>: Provides the mapping between the photo ID and business the ID (1,190,225 rows)</li>
<li><kbd>train.csv</kbd>: This is the main training dataset, which includes business IDs and their corresponding labels (2,000 rows)</li>
<li><kbd>sample_submission.csv</kbd>: A sample submissionâ€”reference the correct format for your predictions including <kbd>business_id</kbd> and the corresponding predicted labels</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Removing invalid images</h1>
                </header>
            
            <article>
                
<p>I do not know why, but each image folder (train and test) also contains some temporary images with the <kbd>_*.jpg</kbd> name pattern, but not actual images. Therefore, I removed them using a UNIX command as follows:</p>
<pre>$ find . -type f -name "._*.jpg" -exec rm -f {} ;</pre>
<p>Then, I unzipped and copied each <kbd>.csv</kbd> file into a folder called <kbd>label</kbd>. Additionally, I moved the training and test images into the <kbd>train</kbd> and <kbd>test</kbd> folders (that is, inside the <kbd>images</kbd> folder), respectively. In short, after extraction and copying, the following folder structure is used in our projects. Therefore, the resulting structure will be as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-648 image-border" src="assets/84b1fa36-fa32-4d88-9743-09c6f5da2f10.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Folder structure in the Large Movie Review Dataset</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Workflow of the overall project</h1>
                </header>
            
            <article>
                
<p>Since we already know that this is a multi-label multiclass image classification problem, we have to deal with the multiple-instance issue<strong>.</strong> Since DL4J does not provide an example of how to solve a<strong> </strong>multi-label multiclass image classification problem, I found <span class="fn">Andrew Brooks's blog article (see</span> <a href="http://brooksandrew.github.io/simpleblog/articles/convolutional-neural-network-training-with-dl4j/">http://brooksandrew.github.io/simpleblog/articles/convolutional-neural-network-training-with-dl4j/</a>) motivation for this project<strong>.</strong></p>
<p>I simply applied the labels of the restaurant to all of the images associated with it and treated each image as a separate record. To be more technical, I handled each class as a separate binary classification problem. Nevertheless, at the beginning of this project, we will see how to read images from <kbd>.jpg</kbd> format into a matrix representation in Java. Then, we will further process and prepare those images so that they are feedable by the CNNs. Also, since images do not come with uniform shapes and sizes, we need to apply several rounds of image preprocessing ops, such as squaring and resizing every image to the uniform dimensions, before we apply a grayscale filter to the image:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/04a01dab-a68c-4ec0-83ba-c393c91dc1ea.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">A conceptualized view of a CNN for image classification</div>
<p class="mce-root">Then, we train nine CNNs on training data for each class. Once the training is complete, we save the trained model, CNN configurations, and parameters so that they can be restored later on. Then, we apply a simple aggregate function to assign classes to each restaurant, where each one has multiple images associated with it, each with its own vector of probabilities for each of the nine classes. Then, we score test data and finally, we evaluate the model using test images.</p>
<p class="mce-root CDPAlignLeft CDPAlign">Now, let's see the structure of each CNN. Well, each network will have two convolutional layers, two subsampling layers, one dense layer, and the output layer as the dense layer. The first layer is a conv layer, followed by a subsampling layer, which is again followed by another conv layer, then a subsampling layer, then a dense layer, which is followed by an output layer. We will see each layer's structure later on. In short, the Java class (<kbd>YelpImageClassifier.java</kbd>) has the following workflow:</p>
<ol>
<li>We read all the business labels from the <kbd>train.csv</kbd> file</li>
<li>We then read and create a map from the image ID to the business ID as <span class="packt_screen">imageID | busID</span></li>
<li>Then, we generate a list of images from the <kbd>photoDir</kbd> directory to load and process, which helps us to retrieve image IDs of a certain number of images</li>
<li>We then read and process images into a <span class="packt_screen">photoID | vector map</span></li>
<li>We chain the output of step 3 and step 4 to align the business feature, image IDs, and label IDs to extract image features</li>
<li>Then, we construct nine CNNs for nine possible labels in a multi-label setting</li>
</ol>
<ol start="7">
<li>We then train all the CNNs and specify the model savings locations</li>
<li>S<em>teps 2</em> to<em> 6</em> are repeated several times to extract the features from the test set as well</li>
<li>Finally, we evaluate the model and save the prediction in a CSV file</li>
</ol>
<p>Now, let's see what the preceding steps would look like in a high-level diagram, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-649 image-border" src="assets/6ee535a5-af3c-4880-a53f-6d5477b45aff.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">DL4j image processing pipeline for image classification</div>
<p>Too much of a mouthful? Don't worry; we will now see each step in detail. If you look at the previous steps carefully, you will see that steps 1 to 5 are image processing and feature constructions. Then, step 6 is training nine CNNs and then, in step 7, we save the trained CNNs so that we can restore them during result submission.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image preprocessing</h1>
                </header>
            
            <article>
                
<p class="mce-root">When I tried to develop this application, I found that the photos are different shapes and sizes: some images are tall, some of them are wide, some of them are outside, some images are inside, and most of them are food. Also, images come in different shapes (most were roughly square, though), of pixel and many of them are exactly 500 x 375 in dimension:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-650 image-border" src="assets/b9b618f0-ad81-48e5-8a5e-b75997f17044.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Resized figure (left, the original and tall one, right, the squared one)</div>
<p class="mce-root">We have already seen that CNN cannot work with images with heterogeneous shapes and sizes. There are many robust and efficient image processing techniques to extract only the <strong>region of interes</strong>t (<strong>ROI</strong>), but honestly, I am not an image-processing expert, so I decided to keep this resizing step simple. In simple terms, I made all the images square but still, I tried to preserve their quality. The thing is that ROIs are cantered in most cases. So, capturing only the middle-most square of each image is not a trivial task. Nevertheless, we also need to convert each image into a grayscale image. Let's make irregularly shaped images square. Look at the preceding image, where the one on the left is the original one and the one on the right is the squared one.</p>
<p>We have generated a square image, but how did we achieve this? Well, I first checked whether the height and the width were the same, and then I resized the image. In the other two cases, I cropped the central region. The following method does the trick (but feel free to execute the <kbd>SquaringImage.java</kbd> script to see the output):</p>
<pre><strong>private static</strong> <strong>BufferedImage</strong> makeSquare(<strong>BufferedImage</strong> img) {<br/>        <strong>int</strong> w = img.getWidth();<br/>        <strong>int</strong> h = img.getHeight();<br/>        <strong>int</strong> dim = Math.min(w, h);<br/><br/>        <strong>if</strong> (w == h) {<br/>            <strong>return</strong> img;<br/>        } <strong>else</strong> <strong>if</strong> (w &gt; h) {<br/>            <strong>return</strong> Scalr.crop(img, (w - h) / 2, 0, dim, dim);<br/>        } <strong>else</strong> {<br/>            <strong>return</strong> Scalr.crop(img, 0, (h - w) / 2, dim, dim);<br/>        }<br/>    }</pre>
<p>Well done! Now that all of our training images are squared, the next step is to use the import-preprocessing task to resize them all. I decided to make all the images 128 x 128 in size. Let's see what the previous image (the original one) looks like after resizing:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/13daa415-e364-4988-b0c9-35c650fd0b93.jpg" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Image resizing (256 x 256, 128 x 128, 64 x 64 and 32 x 32, respectively)</div>
<p>The following method does this trick (but feel free to execute the <kbd>imageUtils.java</kbd> script to see a demo):</p>
<pre>// resize pixels<br/>    <strong>public</strong> <strong>static</strong> <strong>BufferedImage</strong> resizeImg(<strong>BufferedImage</strong> img, <strong>int</strong> width, <strong>int</strong> height) {<br/>        <strong>return</strong> Scalr.resize(img, Scalr.Method.BALANCED, width, height);<br/>    }</pre>
<p>By the way, for the image resizing and squaring, I used some built-in package for image reading and some third-party packages for processing:</p>
<pre><strong>import</strong> javax.imageio.ImageIO;<br/><strong>import</strong> org.imgscalr.Scalr;</pre>
<p>To use the previous packages, add the following dependencies in a Maven-friendly <kbd>pom.xml</kbd> file (for the complete list of dependencies, refer to the <kbd>pom.xml</kbd> file provided for this chapter):</p>
<pre><strong>&lt;dependency&gt;</strong><br/>      &lt;groupId&gt;org.imgscalr&lt;/groupId&gt;<br/>      &lt;artifactId&gt;imgscalr-lib&lt;/artifactId&gt;<br/>      &lt;version&gt;4.2&lt;/version&gt;<br/><strong>&lt;/dependency&gt;</strong><br/><strong>&lt;dependency&gt;</strong><br/>      &lt;groupId&gt;org.datavec&lt;/groupId&gt;<br/>      &lt;artifactId&gt;datavec-data-image&lt;/artifactId&gt;<br/>      &lt;version&gt;${dl4j.version}&lt;/version&gt;<br/><strong>&lt;/dependency&gt;</strong></pre>
<p>Processing color images is more exciting and effective, and DL4J-based CNNs can handle color images, too. However, it's better to simplify the computation with the grayscale images. Nevertheless, this way, we can make the overall representation simpler and space efficient.</p>
<p>Let's give an example for our previous step; we resized each 256 x 256 pixel imageâ€”which is represented by 16,384 features rather than 16,384 x 3 for a color image having three RGB channels (execute <kbd>GrayscaleConverter.java</kbd> to see a demo). Let's see what the converted image would look like:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9e7e7c72-9b24-4331-8382-6dc595c1d4d6.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">On the leftâ€”the original image, on the rightâ€”the grayscale one with RGB averaging</div>
<p>The previous conversion is done using two methods called <kbd>pixels2Gray()</kbd> and <kbd>makeGray()</kbd>. The former converts RGB pixels into corresponding grayscale ones. Let's see the signature for this:</p>
<pre><strong>private static int</strong> pixels2Gray(int R, int G, int B) {<br/>        return (R + G + B) / 3;<br/>    }<br/><strong>private static BufferedImage</strong> makeGray(<strong>BufferedImage</strong> testImage) {<br/>        <strong>int</strong> w = testImage.getWidth();<br/>        <strong>int</strong> h = testImage.getHeight();<br/>        <strong>for</strong> (<strong>int</strong> w1 = 0; w1 &lt; w; w1++) {<br/>            for (<strong>int</strong> h1 = 0; h1 &lt; h; h1++) {<br/>                <strong>int</strong> col = testImage.getRGB(w1, h1);<br/>                <strong>int</strong> R = (col &amp; 0xff0000) / 65536;<br/>                <strong>int</strong> G = (col &amp; 0xff00) / 256;<br/>                <strong>int</strong> B = (col &amp; 0xff);<br/>                <strong>int</strong> graycol = pixels2Gray(R, G, B);<br/>                testImage.setRGB(w1, h1, new Color(graycol, graycol, graycol).getRGB());<br/>            }<br/>        }<br/>        <strong>return</strong> testImage;<br/>    }</pre>
<p>So, what happens under the hood? We chain all the previous three steps: make all the images square, then convert all of them to 256 x 256, and finally convert the resized image into a grayscale one (I assume that <kbd>x</kbd> is the image to be converted):</p>
<pre>convertedImage = ImageIO.read(<strong>new</strong> File(x))<br/>          .makeSquare()<br/>          .resizeImg(resizeImgDim, resizeImgDim) // (128, 128)<br/>         .image2gray();</pre>
<p class="mce-root CDPAlignLeft CDPAlign">Therefore, in summary, now all of the images are in grey, but only after squaring and resizing. The following image gives some sense of the conversion step:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fee1cc5f-9747-4161-837c-20e182d2f710.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Resized figure (left, the original and tall one, right, the squared one)</div>
<p>The previous chaining also comes with some additional effort. Now, putting all these three coding steps together, we can finally prepare all of our images:</p>
<pre>//imageUtils.java<br/><strong>public class</strong> imageUtils {<br/>    // image 2 vector processing<br/>    <strong>private static</strong> Integer pixels2gray(Integer red, Integer green, Integer blue){<br/>        <strong>return</strong> (red + green + blue) / 3;<br/>    }<br/>    <strong>private static List&lt;Integer&gt;</strong> pixels2color(Integer red, Integer green, Integer blue) {<br/>        <strong>return</strong> Arrays.asList(red, green, blue);<br/>    }<br/><br/><strong>private static &lt;T&gt; List&lt;T&gt;</strong> image2vec(<strong>BufferedImage</strong> img, <strong>Function&lt;Triple&lt;Integer, Integer, Integer&gt;</strong>, T&gt; f) {<br/>        <strong>int</strong> w = img.getWidth();<br/>        <strong>int</strong> h = img.getHeight();<br/><br/>        <strong>ArrayList&lt;T&gt;</strong> result = new ArrayList&lt;&gt;();<br/>        <strong>for</strong> (<strong>int</strong> w1 = 0; w1 &lt; w; w1++ ) {<br/>            <strong>for</strong> (<strong>int</strong> h1 = 0; h1 &lt; h; h1++) {<br/>                <strong>int</strong> col = img.getRGB(w1, h1);<br/>                <strong>int</strong> red =  (col &amp; 0xff0000) / 65536;<br/>                <strong>int</strong> green = (col &amp; 0xff00) / 256;<br/>                <strong>int</strong> blue = (col &amp; 0xff);<br/>                result.add(f.apply(new Triple&lt;&gt;(red, green, blue)));<br/>            }<br/>        }<br/>        <strong>return</strong> result;<br/>    }<br/><br/>    <strong>public static List&lt;Integer&gt;</strong> image2gray(<strong>BufferedImage</strong> img) {<br/>        <strong>return</strong> image2vec(img, t -&gt; pixels2gray(t.getFirst(), t.getSecond(), t.getThird()));<br/>    }<br/><br/>    <strong>public static List&lt;Integer&gt;</strong> image2color(<strong>BufferedImage</strong> img) {<br/>        <strong>return</strong> image2vec(img, t -&gt; pixels2color(t.getFirst(), t.getSecond(), t.getThird()))<br/>                .stream()<br/>                .flatMap(l -&gt; l.stream())<br/>                .collect(Collectors.toList());<br/>    }<br/><br/>    // make image square<br/>   <strong> public static</strong> BufferedImage makeSquare(BufferedImage img) {<br/>        <strong>int</strong> w = img.getWidth();<br/>        <strong>int</strong> h = img.getHeight();<br/>        <strong>int</strong> dim = Math.min(w, h);<br/><br/>        <strong>if</strong> (w == h) {<br/>            return img;<br/>        } <strong>else if</strong> (w &gt; h) {<br/>            <strong>return</strong> Scalr.crop(img, (w-h)/2, 0, dim, dim);<br/>        } <strong>else </strong> {<br/>            <strong>return</strong> Scalr.crop(img, 0, (h-w)/2, dim, dim);<br/>        }<br/>    }<br/><br/>    // resize pixels<br/><strong>public static</strong> BufferedImage resizeImg(BufferedImage img, int width, int height) {<br/>        <strong>return</strong> Scalr.resize(img, Scalr.Method.BALANCED, width, height);<br/>    }<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extracting image metadata</h1>
                </header>
            
            <article>
                
<p>Up to this point, we have loaded and pre-processed raw images. However, we have no idea about the image metadata that is added, which is needed so that our CNNs can learn. Thus, it's time to load those CSV files containing metadata about each image.</p>
<p>I wrote a method called <kbd>readMetadata()</kbd> to read such metadata in CSV format so that it can be used by two other methods called <kbd>readBusinessLabels</kbd> and <kbd>readBusinessToImageLabels</kbd>. These three methods are defined in the <kbd>CSVImageMetadataReader.java</kbd> script. Here's the signature of the <kbd>readMetadata()</kbd> method:</p>
<pre><strong>public static List&lt;List&lt;String&gt;&gt;</strong> readMetadata(String csv, List&lt;Integer&gt; rows) throws IOException {<br/>        <strong>boolean</strong> defaultRows = rows.size() == 1 &amp;&amp; rows.get(0) == -1;<br/>        <strong>LinkedList&lt;Integer&gt;</strong> rowsCopy = null;<br/>        <strong>if</strong> (!defaultRows) {<br/>            rowsCopy = new LinkedList&lt;&gt;(rows);<br/>        }<br/>        <strong>try</strong>(<strong>BufferedReader</strong> bufferedReader = new BufferedReader(<strong>new InputStreamReader</strong>(<strong>new FileInputStream</strong>(new File(csv))))) {<br/>            <strong>ArrayList&lt;List&lt;String&gt;&gt;</strong> arrayList = new ArrayList&lt;&gt;();<br/>            <strong>String</strong> line = bufferedReader.readLine();<br/>            <strong>int</strong> i = 0;<br/>            <strong>while</strong> (line != null) {<br/>                <strong>if</strong> (defaultRows || rowsCopy.getFirst() == i) {<br/>                    <strong>if</strong> (!defaultRows) {<br/>                        rowsCopy.removeFirst();<br/>                    }<br/>                    arrayList.add(Arrays.asList(line.split(",")));<br/>                }<br/>                line = bufferedReader.readLine();<br/>                i++;<br/>            }<br/>            <strong>return</strong> arrayList;<br/>        }<br/>    }</pre>
<p>The <kbd>readBusinessLabels()</kbd> method maps from the business ID to labels of the form <span class="packt_screen">businessID | Set(labels)</span>:</p>
<pre><strong>public static Map&lt;String, Set&lt;Integer&gt;&gt;</strong> readBusinessLabels(String csv) throws IOException {<br/>        <strong>return</strong> readBusinessLabels(csv, DEFAULT_ROWS);<br/>    }<br/><br/><strong>public static Map&lt;String, Set&lt;Integer&gt;&gt;</strong> readBusinessLabels(String csv, List&lt;Integer&gt; rows) throws IOException {<br/>        <strong>return</strong> readMetadata(csv, rows).stream()<br/>                .skip(1)<br/>                .map(l -&gt; parseBusinessLabelsKv(l))<br/>                .collect(Collectors.toMap(e -&gt; e.getKey(), e -&gt; e.getValue()));<br/>    }</pre>
<p>The <kbd>readBusinessToImageLabels()</kbd> method maps from the image ID to the business ID of the form <span class="packt_screen">imageID | businessID</span>:</p>
<pre><strong>public static Map&lt;Integer, String&gt;</strong> readBusinessToImageLabels(String csv) throws IOException {<br/>        <strong>return</strong> readBusinessToImageLabels(csv, DEFAULT_ROWS);<br/>    }<br/><br/><strong>public static Map&lt;Integer, String&gt;</strong> readBusinessToImageLabels(String csv, List&lt;Integer&gt; rows) throws IOException {<br/>        <strong>return</strong> readMetadata(csv, rows).stream()<br/>                .skip(1)<br/>                .map(l -&gt; parseBusinessToImageLabelsKv(l))<br/>                .collect(Collectors.toMap(e -&gt; e.getKey(), e -&gt; e.getValue(), useLastMerger()));<br/>    }</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image feature extraction</h1>
                </header>
            
            <article>
                
<p>So far, we have seen how to preprocess images and extract image metadata by linking them with the original images. Now, we need to extract features from those preprocessed images so that they can be fed into CNNs.</p>
<p>We need the map operations for feature extractions for business, data, and labels. These three operations will ensure that we don't lose any image provenance (see the <kbd>imageFeatureExtractor.java</kbd> script):</p>
<ul>
<li>Business mapping with the form <span class="packt_screen">imageID | businessID</span></li>
<li>Data map of the form <span class="packt_screen">imageID | image data</span></li>
<li>Label map of the form <span class="packt_screen">businessID | labels</span></li>
</ul>
<p>First, we must define a regular expression pattern to extract a jpg name from the <kbd>CSVImageMetadataReaderclass</kbd>, which is used to match against training labels:</p>
<pre><strong>public static Pattern</strong> patt_get_jpg_name = Pattern.compile("[0-9]");</pre>
<p>Then, we extract all of the image IDs associated with their respective business IDs:</p>
<pre><strong>public static List&lt;Integer&gt;</strong> getImgIdsFromBusinessId(Map&lt;Integer, String&gt; bizMap, List&lt;String&gt; businessIds) {<br/>        <strong>return</strong> bizMap.entrySet().stream().filter(x -&gt; <br/>                 businessIds.contains(x.getValue())).map(Map.Entry::getKey)<br/>                .collect(Collectors.toList());<br/>    }</pre>
<p>Now, we need to load and process all the images that are already preprocessed to extract the image IDs by mapping them with the IDs extracted from the business IDs in the earlier examples:</p>
<pre><strong>public static List&lt;String&gt;</strong> getImageIds(String photoDir, Map&lt;Integer, String&gt; businessMap, <br/>                                       List&lt;String&gt; businessIds) {<br/>        <strong>File</strong> d = new File(photoDir);<br/>        <strong>List&lt;String&gt;</strong> imgsPath = Arrays.stream(d.listFiles()).map(f -&gt; <br/>                                f.toString()).collect(Collectors.toList());<br/>        <strong>boolean</strong> defaultBusinessMap = businessMap.size() == 1 &amp;&amp; businessMap.get(-1).equals("-1");<br/>        <strong>boolean</strong> defaultBusinessIds = businessIds.size() == 1 &amp;&amp; businessIds.get(0).equals("-1");<br/>        <strong>if</strong> (defaultBusinessMap || defaultBusinessIds) {<br/>            <strong>return</strong> imgsPath;<br/>        } <strong>else</strong> {<br/>            <strong>Map&lt;Integer, String&gt;</strong> imgsMap = imgsPath.stream()<br/>                    .map(x -&gt; new AbstractMap.SimpleEntry&lt;Integer, String&gt;(extractInteger(x), x))<br/>                    .collect(Collectors.toMap(e -&gt; e.getKey(), e -&gt; e.getValue()));<br/>            <strong>List&lt;Integer&gt;</strong> imgsPathSub = imageFeatureExtractor.getImgIdsFromBusinessId(<br/>                                        businessMap, businessIds);<br/>            <strong>return</strong> imgsPathSub.stream().filter(x -&gt; imgsMap.containsKey(x)).map(x -&gt; imgsMap.get(x))<br/>                    .collect(Collectors.toList());<br/>        }<br/>    }</pre>
<p>In the preceding code block, we get a list of images from the photoDir directory (which is where the raw images reside). The <kbd>ids</kbd> parameter is an optional parameter to subset the images loaded from the photoDir. So far, we have been able to extract all the image IDs that are somehow associated with at least one business. The next move will be to read and process the images into an imageID â†’ vector map:</p>
<pre><strong>public static Map&lt;Integer, List&lt;Integer&gt;&gt;</strong> processImages(<strong>List&lt;String&gt;</strong> imgs, int resizeImgDim, int nPixels) {<br/>        <strong>Function&lt;String, AbstractMap.Entry&lt;Integer, List&lt;Integer&gt;&gt;&gt;</strong> handleImg = x -&gt; {<br/>            <strong>BufferedImage</strong> img = null;<br/>            <strong>try</strong> {<br/>                img = <strong>ImageIO</strong>.read(<strong>new File</strong>(x));<br/>            } <strong>catch</strong> (<strong>IOException</strong> e) {<br/>                e.printStackTrace();<br/>            }<br/>            img = makeSquare(img);<br/>            img = resizeImg(img, resizeImgDim, resizeImgDim);<br/>            <strong>List&lt;Integer&gt;</strong> value = image2gray(img);<br/>            <strong>if</strong>(nPixels != -1) {<br/>                value = value.subList(0, nPixels);<br/>            }<br/>            <strong>return</strong> new <strong>AbstractMap.SimpleEntry&lt;Integer, List&lt;Integer&gt;&gt;</strong>(extractInteger(x), value);<br/>        };<br/><br/>        <strong>return</strong> imgs.stream().map(handleImg).filter(e -&gt; !e.getValue().isEmpty())<br/>                .collect(Collectors.toMap(e -&gt; e.getKey(), e -&gt; e.getValue()));<br/>    }</pre>
<p>In the preceding code block, we read and processed the images into a photoID â†’ vector map. The <kbd>processImages()</kbd> method takes the following parameters:</p>
<ul>
<li><kbd>images</kbd>: A list of images in the <kbd>getImageIds()</kbd> method</li>
<li><kbd>resizeImgDim</kbd>: Dimension to rescale square images</li>
<li><kbd>nPixels</kbd>: Number of pixels used to sample the image to drastically reduce runtime while testing features</li>
</ul>
<p>Well done! We are just one step away from extracting the data that is required to train our CNNs. The final step in feature extraction is to extract the pixel data, which consists of four objects to keep track of each image -- that is, imageID, businessID, labels, and pixel data:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-651 image-border" src="assets/1f48580d-b6fc-44aa-ba0a-f40ea4946ae7.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Image data representation</div>
<p>Thus, as shown in the preceding diagram, the primary data structure is constructed with four data types (that is, four tuples) -- <kbd>imgID</kbd>, <kbd>businessID</kbd>, <kbd>pixel data vector</kbd>, and <kbd>labels</kbd>:</p>
<p>Thus, we should have a class containing all of the parts of these objects. Don't worry; all we need is defined in the <kbd>FeatureAndDataAligner.java</kbd> script. Once we have the instantiated instance of <kbd>FeatureAndDataAligner</kbd> using the following line of code in the <kbd>YelpImageClassifier.java</kbd> script (under the main method), <kbd>businessMap</kbd>, <kbd>dataMap</kbd>, and <kbd>labMap</kbd> are provided:</p>
<pre>FeatureAndDataAligner alignedData = <strong>new</strong> FeatureAndDataAligner(dataMap, businessMap, Optional.<em>of</em>(labMap));</pre>
<p>Here, the option type for <kbd>labMap</kbd> is used since we don't have this information when we score on test data -- that is, it is optional for invocation. Now, let's see how I did this. We start from the constructor of the class that is being used, initializing the preceding data structure:</p>
<pre><strong>private Map&lt;Integer, List&lt;Integer&gt;&gt;</strong> dataMap;<br/><strong>private Map&lt;Integer, String&gt;</strong> bizMap;<br/><strong>private Optional&lt;Map&lt;String, Set&lt;Integer&gt;&gt;&gt;</strong> labMap;<br/><strong>private List&lt;Integer&gt;</strong> rowindices;<br/><br/><strong>public</strong> FeatureAndDataAligner(<strong>Map&lt;Integer, List&lt;Integer&gt;&gt;</strong> dataMap, <strong>Map&lt;Integer, String&gt;</strong> bizMap, <strong>Optional&lt;Map&lt;String, Set&lt;Integer&gt;&gt;&gt;</strong> labMap) {<br/>        <strong>this</strong>(dataMap, bizMap, labMap, dataMap.keySet().stream().collect(Collectors.toList()));<br/>    }</pre>
<p>Now, we initialize the values through the constructor of the <kbd>FeatureAndDataAligner.java</kbd> class as follows:</p>
<pre><strong>public</strong> FeatureAndDataAligner(<strong>Map&lt;Integer, List&lt;Integer&gt;&gt;</strong> dataMap, <strong>Map&lt;Integer, String&gt;</strong> bizMap, <strong>Optional&lt;Map&lt;String, Set&lt;Integer&gt;&gt;&gt;</strong> labMap,<strong>List&lt;Integer&gt;</strong> rowindices) {<br/>        <strong>this</strong>.dataMap = dataMap;<br/>        <strong>this</strong>.bizMap = bizMap;<br/>        <strong>this</strong>.labMap = labMap;<br/>        <strong>this</strong>.rowindices = rowindices;<br/>    }</pre>
<p>Now, when aligning the data, if <kbd>labMap</kbd> is empty -- which is not provided with the training data -- the following can be used too:</p>
<pre><strong>public</strong> FeatureAndDataAligner(<strong>Map&lt;Integer, List&lt;Integer&gt;&gt;</strong> dataMap, <strong>Map&lt;Integer, String&gt;</strong> bizMap) {<br/>        <strong>this</strong>(dataMap, bizMap, Optional.empty(), dataMap.keySet().stream().collect(Collectors.toList()));<br/>    }</pre>
<p>Now, we have to align the image IDs and image data with the business IDs. For this, I have written the <kbd>BusinessImgageIds()</kbd> method:</p>
<pre><strong>public List&lt;Triple&lt;Integer, String, List&lt;Integer&gt;&gt;&gt;</strong> alignBusinessImgageIds(<strong>Map&lt;Integer, List&lt;Integer&gt;&gt;</strong> dataMap, <strong>Map&lt;Integer, String&gt;</strong> bizMap) {<br/>        <strong>return</strong> alignBusinessImgageIds(dataMap, bizMap, dataMap.keySet().stream().collect(Collectors.toList()));<br/>    }   </pre>
<p>The actual implementation lies in the following overloaded method, which returns optional if an image does not have a business ID:</p>
<pre><strong>public List&lt;Triple&lt;Integer, String, List&lt;Integer&gt;&gt;&gt;</strong> alignBusinessImgageIds(Map&lt;Integer, List&lt;Integer&gt;&gt; dataMap, <strong>Map&lt;Integer, String&gt;</strong> bizMap, <strong>List&lt;Integer&gt;</strong> rowindices) {<br/>        <strong>ArrayList&lt;Triple&lt;Integer, String, List&lt;Integer&gt;&gt;&gt;</strong> result = <strong>new ArrayList&lt;&gt;</strong>();<br/>        <strong>for</strong> (<strong>Integer</strong> pid : rowindices) {<br/>            <strong>Optional&lt;String&gt;</strong> imgHasBiz = <strong>Optional</strong>.ofNullable(bizMap.get(pid));<br/>            <strong>String</strong> bid = imgHasBiz.orElse("-1");<br/>            <strong>if</strong> (dataMap.containsKey(pid) &amp;&amp; imgHasBiz.isPresent()) {<br/>               result.add(new ImmutableTriple&lt;&gt;(pid, bid, dataMap.get(pid)));<br/>            }<br/>        }<br/>        <strong>return</strong> result;<br/>    }</pre>
<p>Finally, as shown in the preceding diagram, we now need to align the labels, which is a four-tuple list compromising of <kbd>dataMap</kbd>, <kbd>bizMap</kbd>, <kbd>labMap</kbd>, and <kbd>rowindices</kbd>:</p>
<pre><strong>private List&lt;Quarta&lt;Integer, String, List&lt;Integer&gt;, Set&lt;Integer&gt;&gt;&gt;</strong> alignLabels(<strong>Map&lt;Integer, List&lt;Integer&gt;&gt;</strong>   <br/>                                                                   dataMap, <strong>Map&lt;Integer, String&gt;</strong>             <br/>                                                                   bizMap,<strong>Optional&lt;Map&lt;String,  </strong><br/><strong>                                                                   Set&lt;Integer&gt;&gt;&gt;</strong> labMap,  <br/>                                                                   <strong>List&lt;Integer&gt;</strong> rowindices) {<br/>        <strong>ArrayList&lt;Quarta&lt;Integer, String, List&lt;Integer&gt;, Set&lt;Integer&gt;&gt;&gt;</strong> result = <strong>new ArrayList&lt;&gt;()</strong>;<br/>        <strong>List&lt;Triple&lt;Integer, String, List&lt;Integer&gt;&gt;&gt;</strong> a1 = alignBusinessImgageIds(dataMap, <br/>                                                                                 bizMap, rowindices);<br/>        <strong>for</strong> (<strong>Triple&lt;Integer, String, List&lt;Integer&gt;&gt;</strong> p : a1) {<br/>            <strong>String</strong> bid = p.getMiddle();<br/>            <strong>Set&lt;Integer&gt;</strong> labs = <strong>Collections</strong>.emptySet();<br/>            <strong>if</strong> (labMap.isPresent() &amp;&amp; labMap.get().containsKey(bid)) {<br/>                 labs = labMap.get().get(bid);<br/>            }<br/>            result.add(new Quarta&lt;&gt;(p.getLeft(), p.getMiddle(), p.getRight(), labs));<br/>        }<br/>        <strong>return</strong> result;<br/>    }</pre>
<p>In the previous code block, <kbd>Quarta</kbd> is a case class that help us to maintain our desired data structure, shown as follows:</p>
<pre><strong>public static class Quarta</strong> &lt;A, B, C, D&gt; {<br/>        <strong>public final</strong> A a;<br/>        <strong>public final</strong> B b;<br/>        <strong>public final</strong> C c;<br/>        <strong>public final</strong> D d;<br/><br/>        <strong>public</strong> Quarta(A a, B b, C c, D d) {<br/>            <strong>this</strong>.a = a;<br/>            <strong>this</strong>.b = b;<br/>            <strong>this</strong>.c = c;<br/>            <strong>this</strong>.d = d;<br/>        }<br/>    }</pre>
<p>Finally, we pre-compute and save the data so that the method does not need to re-compute each time it is called:</p>
<pre> <strong>private volatile List&lt;Quarta&lt;Integer, String, List&lt;Integer&gt;, Set&lt;Integer&gt;&gt;&gt;</strong> _data = null;<br/>// pre-computing and saving data as a val so method does not need to re-compute each time it is called.<br/><strong>public List&lt;Quarta&lt;Integer, String, List&lt;Integer&gt;, Set&lt;Integer&gt;&gt;&gt;</strong> data() {<br/>        <strong>if</strong> (_data == null) {<br/>            <strong>synchronized</strong> (<strong>this</strong>) {<br/>                if (_data == <strong>null</strong>) {<br/>                    _data = alignLabels(dataMap, bizMap, labMap, rowindices);<br/>                }<br/>            }<br/>        }<br/>        <strong>return</strong> _data;<br/>    }</pre>
<p>Finally, as used in the preceding code block, we now create some getter methods so that in each invocation, we can retrieve the <kbd>image id</kbd>, <kbd>business id</kbd>, business label, and image for each business easily:</p>
<pre>// getter functions<br/><strong>public List&lt;Integer&gt;</strong> getImgIds() {<br/>        <strong>return</strong> data().stream().map(e -&gt; e.a).collect(Collectors.toList());<br/>    }<br/><strong>public List&lt;String&gt;</strong> getBusinessIds() {<br/>        <strong>return</strong> data().stream().map(e -&gt; e.b).collect(Collectors.toList());<br/>    }<br/><strong>public List&lt;List&lt;Integer&gt;&gt;</strong> getImgVectors() {<br/>        <strong>return</strong> data().stream().map(e -&gt; e.c).collect(Collectors.toList());<br/>    }<br/><strong>public List&lt;Set&lt;Integer&gt;&gt;</strong> getBusinessLabels() {<br/>        <strong>return</strong> data().stream().map(e -&gt; e.d).collect(Collectors.toList());<br/>    }<br/><strong>public Map&lt;String, Integer&gt;</strong> getImgCntsPerBusiness() {<br/>        <strong>return</strong> getBusinessIds().stream().collect(<strong>Collectors</strong>.groupingBy(<strong>Function</strong>.identity())).entrySet()<br/>                .stream().map(e -&gt; <strong>new AbstractMap.SimpleEntry</strong>&lt;&gt;(e.getKey(), e.getValue().size()))<br/>                .collect(<strong>Collectors</strong>.toMap(e -&gt; e.getKey(), e -&gt; e.getValue()));<br/>    }</pre>
<p>Excellent! Up to this point, we have managed to extract the features to train our CNNs. However, the thing is that the feature in its current form is still not suitable to feed into the CNNs. This is because we only have the feature vectors without labels. Thus, it needs another intermediate conversion.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the ND4J dataset</h1>
                </header>
            
            <article>
                
<p>As I said previously, we need an intermediate conversion to prepare the training set containing feature vectors and labels: features from images, but labels from the business labels.</p>
<p>For this, we have the <kbd>makeND4jDataSets</kbd> class (see <kbd>makeND4jDataSets.java</kbd> for details). The class creates an ND4J dataset object from the data structure from the <kbd>alignLables</kbd> function in the <kbd>List[(imgID, bizID, labels, pixelVector)]</kbd> form. First, we prepare the dataset using the <kbd>makeDataSet()</kbd> method as follows:</p>
<pre><strong>public static DataSet</strong> makeDataSet(<strong>FeatureAndDataAligner</strong> alignedData, <strong>int</strong> bizClass) {<br/>        <strong>INDArray</strong> alignedXData = makeDataSetTE(alignedData);<br/>        <strong>List&lt;Set&lt;Integer&gt;&gt;</strong> labels = alignedData.getBusinessLabels();<br/>        <strong>float</strong>[][] matrix2 = labels.stream().map(x -&gt; (x.contains(bizClass) ? <strong>new float</strong>[]{1, 0} <br/>                             : <strong>new float</strong>[]{0, 1})).toArray(float[][]::new);<br/>        <strong>INDArray</strong> alignedLabs = toNDArray(matrix2);<br/>        <strong>return</strong> new DataSet(alignedXData, alignedLabs);<br/>    }</pre>
<p>Then, we further need to convert the preceding data structure into an <kbd>INDArray</kbd>, which can then be consumed by the CNNs:</p>
<pre><strong>public static INDArray</strong> makeDataSetTE(FeatureAndDataAligner alignedData) {<br/>        <strong>List&lt;List&lt;Integer&gt;&gt;</strong> imgs = alignedData.getImgVectors();<br/>        <strong>double</strong>[][] matrix = <strong>new double</strong>[imgs.size()][];<br/>        <strong>for</strong> (<strong>int</strong> i = 0; i &lt; matrix.length; i++) {<br/>            <strong>List&lt;Integer&gt;</strong> img = imgs.get(i);<br/>            matrix[i] = img.stream().mapToDouble(Integer::doubleValue).toArray();<br/>        }<br/>        <strong>return</strong> toNDArray(matrix);<br/>    }</pre>
<p>In the preceding code block, the <kbd>toNDArray()</kbd> method is used to convert the double or float matrix into <kbd>INDArray</kbd> format:</p>
<pre>// For converting floar matrix to INDArray<br/><strong>private</strong> <strong>static</strong> INDArray toNDArray(<strong>float</strong>[][] matrix) {<br/>          <strong>return</strong> Nd4j.<em>create</em>(matrix);<br/>             }<br/>// For converting double matrix to INDArray<br/><strong>private</strong> <strong>static</strong> INDArray toNDArray(<strong>double</strong>[][] matrix) {<br/>            <strong>return</strong> Nd4j.<em>create</em>(matrix);<br/>                  }</pre>
<p>Fantastic! We were able to extract all the metadata and features from the images and prepared the training data in an ND4J format that can now be consumed by the DL4J-based model. However, since we will be using CNN as our model, we still need to convert this 2D object into 4D by using the <kbd>convolutionalFlat</kbd> operation during network construction. Anyway, we will see this in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training, evaluating, and saving the trained CNN models</h1>
                </header>
            
            <article>
                
<p>So far, we have seen how to prepare the training set. Now that we have, a more challenging part lies ahead as we have to train our CNNs with 234,545 images, although the testing phase could be less exhaustive with a limited number of images, for example, 500 images. Therefore, it is better to train each CNN involving batchmode using DL4j's <kbd>MultipleEpochsIterator</kbd>, which is a dataset iterator for doing multiple passes over a dataset.</p>
<div class="packt_infobox"><kbd>MultipleEpochsIterator</kbd> is a dataset iterator for doing multiple passes over a dataset. See more at <a href="https://deeplearning4j.org/doc/org/deeplearning4j/datasets/iterator/MultipleEpochsIterator.html">https://deeplearning4j.org/doc/org/deeplearning4j/datasets/iterator/MultipleEpochsIterator.html</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Network construction</h1>
                </header>
            
            <article>
                
<p>The following is a list of important hyperparameters and their details. Here, I will try to construct a five-layered CNN, as follows:</p>
<ul>
<li>Layer 0 has a <kbd>ConvolutionLayer</kbd> having a 6 x 6 kernel, one channel (since they are grayscale images), a stride of 2 x 2, and 20 feature maps where ReLU is the activation function:</li>
</ul>
<pre style="padding-left: 60px"><strong>ConvolutionLayer</strong> layer_0 = <strong>new</strong> <strong>ConvolutionLayer</strong>.Builder(6,6)<br/>            .nIn(nChannels)<br/>            .stride(2,2) // default stride(2,2)<br/>            .nOut(20) // # of feature maps<br/>            .dropOut(0.7) // dropout to reduce overfitting<br/>            .activation(Activation.<strong><em>RELU</em></strong>) // Activation: rectified linear units<br/>            .build();</pre>
<ul>
<li>Layer 1 has <kbd>SubsamplingLayer</kbd> max pooling, and a stride of 2x2. Thus, by using stride, we down sample by a factor of 2. Note that only MAX, AVG, SUM, and PNORM are supported. Here, the kernel size will be the same as the filter size from the last <kbd>ConvolutionLayer</kbd>. Therefore, we do not need to define the kernel size explicitly:</li>
</ul>
<pre style="padding-left: 60px"><strong>SubsamplingLayer</strong> layer_1 = <strong>new</strong> SubsamplingLayer<br/>                .Builder(SubsamplingLayer.PoolingType.<strong><em>MAX</em></strong>)<br/>                .stride(2, 2)<br/>                .build();</pre>
<ul>
<li>Layer 2 has a <kbd>ConvolutionLayer</kbd> having a 6 x 6 kernel, one channel (since they are grayscale images), a stride of 2 x 2, and 20 output neurons where RELU is the activation function. We will use Xavier for network weight initialization:</li>
</ul>
<pre style="padding-left: 60px"><strong>ConvolutionLayer</strong> layer_2= <strong>new</strong> ConvolutionLayer.Builder(6, 6)<br/>            .stride(2, 2) // nIn need not specified in later layers<br/>            .nOut(50)<br/>            .activation(Activation.<strong><em>RELU</em></strong>) // Activation: rectified linear units<br/>            .build();</pre>
<ul>
<li>Layer 3 has <kbd>SubsamplingLayer</kbd> max pooling and a stride of 2 x 2. Thus, by using stride, we down sample by a factor of 2. Note that only MAX, AVG, SUM, and PNORM are supported. Here, the kernel size will be the same as the filter size from the last <kbd>ConvolutionLayer</kbd>. Therefore, we do not need to define the kernel size explicitly:</li>
</ul>
<pre style="padding-left: 60px"><strong>SubsamplingLayer</strong> layer_3 = <strong>new</strong> SubsamplingLayer<br/>           .Builder(SubsamplingLayer.PoolingType.<strong><em>MAX</em></strong>)<br/>           .stride(2, 2)<br/>           .build();</pre>
<ul>
<li>Layer 4 has a <kbd>DenseLayer</kbd>, that is, a fully connected feed forward layer trainable by backpropagation with 50 neurons and ReLU as an activation function. It should be noted that we do not need to specify the number of input neurons as it assumes the input from the previous <kbd>ConvolutionLayer</kbd>:</li>
</ul>
<pre style="padding-left: 60px"><strong>DenseLayer</strong> layer_4 = <strong>new</strong> DenseLayer.Builder() // Fully connected layer<br/>               .nOut(500)<br/>               .dropOut(0.7) // dropout to reduce overfitting<br/>              .activation(Activation.<strong><em>RELU</em></strong>) // Activation: rectified linear units <br/>             .build();</pre>
<ul>
<li>Layer 5 is an <kbd>OutputLayer</kbd> having two output neurons driven by the softmax activation (that is, probability distribution over the classes). We compute the loss using XENT (that is, cross entropy for binary classification) as the loss function:</li>
</ul>
<pre style="padding-left: 60px"><strong>OutputLayer</strong> layer_5 = <strong>new</strong> <strong>OutputLayer</strong>.Builder(LossFunctions.LossFunction.<strong><em>XENT</em></strong>)<br/>          .nOut(outputNum) // number of classes to be predicted<br/>          .activation(Activation.<strong><em>SOFTMAX</em></strong>)<br/>          .build();</pre>
<p>Apart from these layers, we also need to perform image flatteningâ€”that is, converting a 2D object into a 4D consumable using CNN layers by invoking the following method:</p>
<pre>convolutionalFlat(numRows, numColumns, nChannels))</pre>
<p>Therefore, in summary, using DL4J, our CNN will be as follows:</p>
<pre><strong>MultiLayerConfiguration</strong> conf = <strong>new</strong> <strong>NeuralNetConfiguration</strong>.Builder()<br/>           .seed(seed)a<br/>           .miniBatch(<strong>true</strong>) // for MultipleEpochsIterator<br/>           .optimizationAlgo(OptimizationAlgorithm.<strong><em>STOCHASTIC_GRADIENT_DESCENT</em></strong>)<br/>           .updater(<strong>new</strong> Adam(0.001)) // Aama for weight updater<br/>           .weightInit(WeightInit.<strong><em>XAVIER</em></strong>) //Xavier weight init<br/>           .list()<br/>                    .layer(0, layer_0)<br/>                    .layer(1, layer_1)<br/>                    .layer(2, layer_2)<br/>                    .layer(3, layer_3)<br/>                    .layer(4, layer_4)<br/>                   .layer(5, layer_5)<br/>            .setInputType(InputType.<em>convolutionalFlat</em>(numRows, numColumns, nChannels))<br/>            .backprop(<strong>true</strong>).pretrain(<strong>false</strong>)<br/>            .build();</pre>
<p>The other important aspects related to the training are described as follows:</p>
<ul>
<li><strong>The number of samples</strong>: If you were training all the images other than GPU, that is, CPU, it would take days. When I tried with 50,000 images, it took one whole day with a machine having a core i7 processor and 32 GB of RAM. Now, you can imagine how long it would take for the whole dataset. In addition, it will require at least 256 GB of RAM even if you do the training in batch mode.</li>
<li><strong>Number of epochs</strong>: This is the number of iterations through all the training records. I iterated for 10 epochs due to time constraints.</li>
<li><strong>Number of batches</strong>: This is the number of records in each batch, for example, 32, 64, and 128. I used 128.</li>
</ul>
<p>Now, with the preceding hyperparameters, we can start training our CNNs. The following code does the trick. The thing is that at first, we prepare the training set, then we define the required hyperparameters, and then we normalize the dataset so the ND4j data frame is encoded so that any labels that are considered true are ones and the rest zeros. Then, we shuffle both the rows and labels of the encoded dataset.</p>
<p>Then, we need to create epochs for the dataset iterator using <kbd>ListDataSetIterator</kbd> and <kbd>MultipleEpochsIterator</kbd>, respectively. Once the dataset is converted into the batchmodel, we are then ready to train the constructed CNNs:</p>
<pre>log.info("Train model....");<br/><strong>for</strong>( <strong>int</strong> i=0; i&lt;nepochs; i++ ){<br/>      model.fit(epochitTr);<br/>}</pre>
<p>Once we finish the training, we can evaluate the model on the test set:</p>
<pre>log.info("Evaluate model....");<br/><strong>Evaluation</strong> eval = <strong>new</strong> Evaluation(outputNum)<br/><br/><strong>while</strong> (epochitTe.hasNext()) {<br/>       <strong>DataSet</strong> testDS = epochitTe.next(nbatch);<br/>       <strong>INDArray</strong> output = model.output(testDS.getFeatureMatrix());<br/>       eval.eval(testDS.getLabels(), output);<br/>}</pre>
<p>When the evaluation is finished, we can now inspect the result of each CNN (run the <kbd>YelpImageClassifier.java</kbd> script):</p>
<pre>System.<strong><em>out</em></strong>.println(eval.stats())</pre>
<pre>&gt;&gt;&gt;<br/> <span class="packt_screen">==========================Scores========================================<br/> Accuracy: 0.5600<br/> Precision: 0.5584<br/> Recall: 0.5577<br/> F1 Score: 0.5926<br/> Precision, recall &amp; F1: reported for positive class (class 1 - "1") only<br/> ========================================================================</span></pre>
<p>Oops! Unfortunately, we have not seen good accuracy. However, do not worry, since in the FAQ section, we will see how to improve upon this. Finally, we can save the layer-wise network configuration and network weights to be used later on (that is, scoring before submission):</p>
<pre><strong>if</strong> (!saveNN.isEmpty()) {<br/>      // model config<br/>      <strong>FileUtils</strong>.write(<strong>new</strong> File(saveNN + ".json"), model.getLayerWiseConfigurations().toJson());<br/>      // model parameters<br/>      <strong>DataOutputStream</strong> dos = <strong>new</strong> <strong>DataOutputStream</strong>(Files.<em>newOutputStream</em>(Paths.<em>get</em>(saveNN + ".bin")));<br/>      Nd4j.<em>write</em>(model.params(), dos);<br/>         }<br/>    log.info("****************Example finished********************");<br/>}</pre>
<p>In the previous code, we also saved a JSON file containing all the network configurations and a binary file for holding all the weights and parameters of all the CNNs. This is done using two methods, namely <kbd>saveNN()</kbd> and <kbd>loadNN()</kbd>, which are defined in the <kbd>NetwokSaver.java</kbd> script. First, let's look at the signature of the <kbd>saveNN()</kbd> method, as follows:</p>
<pre><strong>public</strong> <strong>void</strong> saveNN(<strong>MultiLayerNetwork</strong> model, <strong>String</strong> NNconfig, <strong>String</strong> NNparams) <strong>throws</strong> IOException {<br/>       // save neural network config<br/>       <strong>FileUtils</strong>.write(<strong>new</strong> <strong>File</strong>(NNconfig), model.getLayerWiseConfigurations().toJson());<br/><br/>       // save neural network parms<br/>      <strong>DataOutputStream</strong> dos = <strong>new</strong>  <strong>DataOutputStream</strong>(Files.<em>newOutputStream</em>(Paths.<em>get</em>(NNparams)));        <br/>      Nd4j.<em>write</em>(model.params(), dos);<br/>  }</pre>
<p>The idea is visionary as well as important since, as I said earlier, you would not train your whole network for the second time to evaluate a new test set. For example, suppose you want to test just a single image. The thing is, we also have another method named <kbd>loadNN()</kbd> that reads back the <kbd>.json</kbd> and <kbd>.bin</kbd> files we created earlier to a <kbd>MultiLayerNetwork</kbd>, which can be used to score new test data. This method is as follows:</p>
<pre><strong>public static MultiLayerNetwork</strong> loadNN(String NNconfig, String NNparams) <strong>throws</strong> <strong>IOException</strong> {<br/>        // get neural network config<br/>        <strong>MultiLayerConfiguration</strong> confFromJson = <strong>MultiLayerConfiguration</strong><br/>                .fromJson(FileUtils.readFileToString(new File(NNconfig)));<br/><br/>        // get neural network parameters<br/>        <strong>DataInputStream</strong> dis = new DataInputStream    (new FileInputStream(NNparams));<br/>        <strong>INDArray</strong> newParams = Nd4j.read(dis);<br/><br/>        // creating network object<br/>        <strong>MultiLayerNetwork</strong> savedNetwork = new MultiLayerNetwork(confFromJson);<br/>        savedNetwork.init();<br/>        savedNetwork.setParameters(newParams);<br/><br/>        <strong>return</strong> savedNetwork;<br/>    }</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scoring the model</h1>
                </header>
            
            <article>
                
<p>The scoring approach that we are going to use is simple. It assigns business-level labels by averaging the image-level predictions. I did this in a simplistic manner, but you can try using a better approach. What I did is assign a business as label <kbd>0</kbd> if the average of the probabilities across all of its images belonging to class <kbd>0</kbd> are greater than a certain threshold, say, 0.5:</p>
<pre><strong>public static INDArray</strong> scoreModel(<strong>MultiLayerNetwork</strong> model, <strong>INDArray</strong> ds) {<br/>        <strong>return</strong> model.output(ds);<br/>    }</pre>
<p>Then, I collected the model predictions from the <kbd>scoreModel()</kbd> method and merged them with <kbd>alignedData</kbd>:</p>
<pre>/** Take model predictions from scoreModel and merge with alignedData*/<br/><strong>public static List&lt;Pair&lt;String, Double&gt;&gt;</strong> aggImgScores2Business(<strong>INDArray</strong> scores,<br/>                                         <strong>FeatureAndDataAligner</strong> alignedData) {<br/>        <strong>assert</strong>(scores.size(0) == alignedData.data().size());<br/>       <strong> ArrayList&lt;Pair&lt;String, Double&gt;&gt;</strong> result = <strong>new ArrayList&lt;Pair&lt;String, Double&gt;&gt;</strong>();<br/><br/>        <strong>for</strong> (<strong>String</strong> x : alignedData.getBusinessIds().stream().distinct().collect(Collectors.toList())) {<br/>            //R irows = getRowIndices4Business(alignedData.getBusinessIds(), x);<br/>           <strong> List&lt;String&gt;</strong> ids = alignedData.getBusinessIds();<br/>            <strong>DoubleStream</strong> ret = IntStream.range(0, ids.size())<br/>                    .filter(i -&gt; ids.get(i).equals(x))<br/>                    .mapToDouble(e -&gt; scores.getRow(e).getColumn(1).getDouble(0,0));<br/>            <strong>double</strong> mean = ret.sum() / ids.size();<br/>            result.add(new ImmutablePair&lt;&gt;(x, mean));<br/>        }<br/>        <strong>return</strong> result;<br/>    }</pre>
<p>Finally, we can restore the trained and saved models, restore them back, and generate the submission file for Kaggle. The thing is that we need to aggregate image predictions to business scores for each model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Submission file generation</h1>
                </header>
            
            <article>
                
<p>For this, I wrote a class called <kbd>ResultFileGenerator.java</kbd>. According to the Kaggle web page, we will have to write the result in the <kbd>business_ids, labels</kbd> format. Here, <kbd>business_id</kbd> is the ID for the corresponding business, and the label is the multi-label prediction. Let's see how easily we can do that.</p>
<p>First, we aggregate image predictions to business scores for each model. Then, we transform the preceding data structure into a list for each <kbd>bizID</kbd> containing a Tuple (<kbd>bizid</kbd>, <kbd>List[Double]</kbd>) where the <kbd>Vector[Double]</kbd> is the vector of probabilities:</p>
<pre><strong>public static List&lt;Pair&lt;String, List&lt;Double&gt;&gt;&gt;</strong> SubmitObj(<strong>FeatureAndDataAligner</strong> alignedData,<br/>                                               <strong>String</strong> modelPath,<br/>                                               <strong>String</strong> model0,<br/>                                               <strong>String</strong> model1,<br/>                                               <strong>String</strong> model2,<br/>                                               <strong>String</strong> model3,<br/>                                               <strong>String</strong> model4,<br/>                                               <strong>String</strong> model5,<br/>                                               <strong>String</strong> model6,<br/>                                               <strong>String</strong> model7,<br/>                                               <strong>String</strong> model8) throws IOException {<br/>        <strong>List&lt;String&gt;</strong> models = Arrays.asList(model0, model1, <br/>                                            model2, model3, <br/>                                            model4, model5, <br/>                                            model6, model7, model8);<br/>       <strong> ArrayList&lt;Map&lt;String, Double&gt;&gt;</strong> big = <strong>new ArrayList</strong>&lt;&gt;();<br/>        <strong>for</strong> (<strong>String</strong> m : models) {<br/>            <strong>INDArray</strong> ds = makeND4jDataSets.makeDataSetTE(alignedData);<br/>            <strong>MultiLayerNetwork</strong> model = <strong>NetworkSaver</strong>.loadNN(modelPath + m + ".json", <br/>                                                          modelPath + m + ".bin");<br/>            <strong>INDArray</strong> scores = ModelEvaluation.scoreModel(model, ds);<br/>            <strong>List&lt;Pair&lt;String, Double&gt;&gt;</strong> bizScores = ModelEvaluation.<br/>                                                   aggImgScores2Business(scores, alignedData);<br/>            <strong>Map&lt;String, Double&gt;</strong> map = bizScores.stream().collect(Collectors.toMap(<br/>                                                                 e -&gt; e.getKey(), e -&gt; e.getValue()));<br/>            big.add(map);<br/>              }<br/><br/>        // transforming the data structure above into a List for each bizID containing a Tuple (bizid, <br/>           List[Double]) where the Vector[Double] is the the vector of probabilities: <br/>        <strong>List&lt;Pair&lt;String, List&lt;Double&gt;&gt;&gt;</strong> result = <strong>new ArrayList&lt;&gt;</strong>();<br/>        <strong>Iterator&lt;String&gt;</strong> iter = alignedData.data().stream().map(e -&gt; e.b).distinct().iterator();<br/>        <strong>while</strong> (iter.hasNext()) {<br/>            <strong>String</strong> x = iter.next();<br/>            result.add(new MutablePair(x, big.stream().map(x2 -&gt; <br/>                                       x2.get(x)).collect(Collectors.toList())));<br/>        }<br/>        <strong>return</strong> result;<br/>    }</pre>
<p>Therefore, once we have the result aggregated from each model, we then need to generate the submission file:</p>
<pre><strong>public static void</strong> writeSubmissionFile(<strong>String outcsv, List&lt;Pair&lt;String, List&lt;Double&gt;&gt;&gt;</strong> phtoObj, <strong>double</strong> thresh) <strong>throws</strong> <strong>FileNotFoundException</strong> {<br/>        <strong>try</strong> (<strong>PrintWriter</strong> writer = <strong>new PrintWriter</strong>(outcsv)) {<br/>            writer.println("business_ids,labels");<br/>            <strong>for</strong> (<strong>int</strong> i = 0; i &lt; phtoObj.size(); i++) {<br/>                <strong>Pair&lt;String, List&lt;Double&gt;&gt;</strong> kv = phtoObj.get(i);<br/>                <strong>StringBuffer</strong> sb = new StringBuffer();<br/>                <strong>Iterator&lt;Double&gt;</strong> iter = kv.getValue().stream().filter(x -&gt; x &gt;= thresh).iterator();<br/>                <strong>for</strong> (<strong>int</strong> idx = 0; iter.hasNext(); idx++) {<br/>                    iter.next();<br/>                    if (idx &gt; 0) {<br/>                        sb.append(' ');<br/>                    }<br/>                    sb.append(Integer.toString(idx));<br/>                }<br/>                <strong>String</strong> line = kv.getKey() + "," + sb.toString();<br/>                writer.println(line);<br/>            }<br/>        }<br/>    }</pre>
<p>Now that we have managed to do everything up to this point, we can now wrap this up and generate a sample prediction and submission file for Kaggle. For simplicity, I randomly sliced this to only 20,000 images to save time. Interested readers can try building CNNs for all the images, too. However, it might take days. Nevertheless, we will look at some performance tuning tips in the FAQ section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Wrapping everything up by executing the main() method</h1>
                </header>
            
            <article>
                
<p>Let's wrap the overall discussion by watching the performance of our model programmatically (see the main <kbd>YelpImageClassifier.java</kbd> <span>class</span>):</p>
<pre><strong>public class</strong> YelpImageClassifier {<br/>    <strong>public static void main</strong>(String[] args) <strong>throws</strong> <strong>IOException</strong> {<br/>        <strong>Map&lt;String, Set&lt;Integer&gt;&gt;</strong> labMap = readBusinessLabels("Yelp/labels/train.csv");        <br/>        <strong>Map&lt;Integer, String&gt;</strong> businessMap = readBusinessToImageLabels("Yelp/labels<br/>                                                                      /train_photo_to_biz_ids.csv");<br/>        <strong>List&lt;String&gt;</strong> businessIds = businessMap.entrySet().stream().map(e -&gt; <br/>                                                    e.getValue()).distinct().collect(Collectors.toList());<br/>        // 100 images<br/>        <strong>List&lt;String&gt;</strong> imgs = getImageIds("Yelp/images/train/", businessMap, businessIds).subList(0, 100); <br/>        System.out.println("Image ID retreival done!");<br/><br/>        <strong>Map&lt;Integer, List&lt;Integer&gt;&gt;</strong> dataMap = processImages(imgs, 64);<br/>        System.out.println("Image processing done!");<br/><br/>        <strong>FeatureAndDataAligner</strong> alignedData = new FeatureAndDataAligner(dataMap, <br/>                                                                      businessMap, Optional.of(labMap));<br/>        //System.out.println(alignedData.data());<br/>        System.out.println("Feature extraction done!");<br/><br/>        // Training one model for one class at a time<br/>        <strong>CNNEpochs</strong>.trainModelEpochs(alignedData, 0, "results/models/model0"); <br/>        <strong>CNNEpochs</strong>.trainModelEpochs(alignedData, 1, "results/models/model1");<br/>        <strong>CNNEpochs</strong>.trainModelEpochs(alignedData, 2, "results/models/model2");<br/>        <strong>CNNEpochs</strong>.trainModelEpochs(alignedData, 3, "results/models/model3");<br/>        <strong>CNNEpochs</strong>.trainModelEpochs(alignedData, 4, "results/models/model4");<br/>        <strong>CNNEpochs</strong>.trainModelEpochs(alignedData, 5, "results/models/model5");<br/>        <strong>CNNEpochs</strong>.trainModelEpochs(alignedData, 6, "results/models/model6");<br/>        <strong>CNNEpochs</strong>.trainModelEpochs(alignedData, 7, "results/models/model7");<br/>        <strong>CNNEpochs</strong>.trainModelEpochs(alignedData, 8, "results/models/model8");<br/><br/>        // processing test data for scoring<br/>        <strong>Map&lt;Integer, String&gt;</strong> businessMapTE = readBusinessToImageLabels("Yelp/labels<br/>                                                                        /test_photo_to_biz.csv");<br/>        <strong>List&lt;String&gt;</strong> imgsTE = getImageIds("Yelp/images/test/", businessMapTE,                                     <br/>                                  businessMapTE.values().stream()<br/>                                  .distinct().collect(Collectors.toList()))<br/>                                  .subList(0, 100);<br/><br/>        <strong>Map&lt;Integer, List&lt;Integer&gt;&gt;</strong> dataMapTE = processImages(imgsTE, 64); // make them 64x64<br/>        <strong>FeatureAndDataAligner</strong> alignedDataTE = <strong>new FeatureAndDataAligner</strong>(dataMapTE, <br/>                                                  businessMapTE, Optional.empty());<br/><br/>        // creating csv file to submit to kaggle (scores all models)<br/>        <strong>List&lt;Pair&lt;String, List&lt;Double&gt;&gt;&gt;</strong> Results = SubmitObj(alignedDataTE, "results/models/", <br/>                                                             "model0", "model1", "model2", <br/>                                                             "model3", "model4", "model5", <br/>                                                             "model6", "model7", "model8");<br/>        writeSubmissionFile("results/kaggleSubmission/kaggleSubmitFile.csv", Results, 0.50);<br/>        <br/>       // example of how to score just model<br/>        <strong>INDArray</strong> dsTE = makeND4jDataSets.makeDataSetTE(alignedDataTE);<br/>        <strong>MultiLayerNetwork</strong> model = NetworkSaver.loadNN("results/models/model0.json", <br/>                                                      "results/models/model0.bin");<br/>        <strong>INDArray</strong> predsTE = ModelEvaluation.scoreModel(model, dsTE);<br/>        <strong>List&lt;Pair&lt;String, Double&gt;&gt;</strong> bizScoreAgg = ModelEvaluation<br/>                                                .aggImgScores2Business(predsTE, alignedDataTE);<br/>        System.out.println(bizScoreAgg);<br/>    }<br/>}<strong>     <br/></strong></pre>
<p>It's true that we haven't achieved outstanding classification accuracy. Nevertheless, we can still try this with tuned hyperparameters. The following sections provide some insight.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Frequently asked questions (FAQs)</h1>
                </header>
            
            <article>
                
<p>Although we have been able to solve this multi-label classification problem, the accuracy we experienced was below par. Therefore, in this section, we will see some <strong>frequently asked questions</strong> (<strong>FAQs</strong>) that might already be on your mind. Knowing the answers to these questions might help you to improve the accuracy of the CNNs we trained. Answers to these questions can be found in the Appendix:</p>
<ol>
<li>What are the hyperparameters that I can try tuning while implementing this project?</li>
<li>My machine is getting OOP while running this project. What should I do?</li>
<li>While training the networks with full images, my GPU is getting OOP. What should I do?</li>
<li>I understand that the predictive accuracy using CNN in this project is still very low. Did our network under or overfit? Is there any way to observe how the training went?</li>
<li>I am very interested in implementing the same project in Scala. How can I do that?</li>
<li>Which optimizer should I use for this type of project where we need to process large-scale images?</li>
<li>How many hyperparameters do we have? I also want to see them for each layer.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have seen how to develop a real-life application using CNNs on the DL4J framework. We have seen how to solve a multi-label classification problem through nine CNNs and a series of complex feature engineering and image manipulation operations. Albeit, we couldn't achieve higher accuracy, but readers are encouraged to tune hyperparameters in the code and try the same approach with the same dataset.</p>
<p>Also, training the CNNs with all the images is recommended so that networks can get enough data to learn the features from Yelp images. One more suggestion is improving the feature extraction process so that the CNNs can have more quality features.</p>
<p>In the next chapter, we will see how to implement and deploy a hands-on deep learning project that classifies review texts as either positive or negative based on the words they contain. A large-scale movie review dataset that contains 50,000 reviews (training plus testing) will be used.</p>
<p>A combined approach using Word2Vec (that is, a widely used word embedding technique in NLP) and the LSTM network for modeling will be applied: the pre-trained Google news vector model will be used as the neural word embeddings. Then, the training vectors, along with the labels, will be fed into the LSTM network to classify them as negative or positive sentiments. This will evaluate the trained model on the test set.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Answers to questions</h1>
                </header>
            
            <article>
                
<p><strong>Answer</strong> <strong>to question 1</strong>: The following hyperparameters are very important and must be tuned to achieve optimized results:</p>
<ul>
<li>Dropout is used to randomly off certain neurons (that is, feature detectors) to prevent overfitting</li>
<li>Learning rate optimizationâ€”Adagrad can be used for feature-specific learning rate optimization</li>
<li>Regularizationâ€”L1 and/or L2 regularization</li>
<li>Gradient normalization and clipping</li>
<li>Finally, apply batch normalization to reduce internal covariate shift in training</li>
</ul>
<p>Now, for dropout, we can add dropout in each convolutional and dense layer and in case of overfitting, the model is specifically adjusted to the training dataset, so it will not be used for generalization. Therefore, although it performs well on the training set, its performance on the test dataset and subsequent tests is poor because it lacks the generalization property.</p>
<p>Anyway, we can apply dropout on a CNN and DenseLayer. Now, for better learning rate optimization, Adagrad can be used for feature-specific learning rate optimization. Then, for better regularization, we can use either L1 and/or L2. Thus, considering this, our network configuration should look as follows:</p>
<pre><strong>ConvolutionLayer</strong> layer_0 = new ConvolutionLayer.Builder(6, 6)<br/>                .nIn(nChannels)<br/>                .stride(2, 2) // default stride(2,2)<br/>                .nOut(20) // # of feature maps<br/>                .dropOut(0.7) // dropout to reduce overfitting<br/>                .activation(Activation.RELU) // Activation: rectified linear units<br/>                .build();<br/>        <strong>SubsamplingLayer</strong> layer_1 = new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)<br/>                .stride(2, 2)<br/>                .build();<br/>        <strong>ConvolutionLayer</strong> layer_2 = new ConvolutionLayer.Builder(6, 6)<br/>                .stride(2, 2) // nIn need not specified in later layers<br/>                .nOut(50)<br/>                .activation(Activation.RELU) // Activation: rectified linear units<br/>                .build();<br/>        <strong>SubsamplingLayer</strong> layer_3 = new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)<br/>                .stride(2, 2)<br/>                .build();<br/>        <strong>DenseLayer</strong> layer_4 = new DenseLayer.Builder() // Fully connected layer<br/>                .nOut(500)<br/>                .dropOut(0.7) // dropout to reduce overfitting<br/>                .activation(Activation.RELU) // Activation: rectified linear units<br/>                .gradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue)<br/>                .gradientNormalizationThreshold(10)<br/>                .build();<br/>        <strong>OutputLayer</strong> layer_5 = new OutputLayer.Builder(LossFunctions.LossFunction.XENT)<br/>                .nOut(outputNum) // number of classes to be predicted<br/>                .gradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue)<br/>                .gradientNormalizationThreshold(10)<br/>                .activation(Activation.SOFTMAX)<br/>                .build();<br/>        <strong>MultiLayerConfiguration</strong> conf = new NeuralNetConfiguration.Builder().seed(seed).miniBatch(true)<br/>                .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT<br/>                .l2(0.001) // l2 reg on all layers<br/>                .updater(new AdaGrad(0.001))<br/>                .weightInit(WeightInit.XAVIER) // Xavier weight init<br/>                .list()<br/>                        .layer(0, layer_0)<br/>                        .layer(1, layer_1)<br/>                        .layer(2, layer_2)<br/>                        .layer(3, layer_3)<br/>                        .layer(4, layer_4)<br/>                         .layer(5, layer_5)<br/>                .setInputType(InputType.convolutionalFlat(numRows, numColumns, nChannels))<br/>                .backprop(true).pretrain(false) // Feedforward hence no pre-train.<br/>                .build();</pre>
<p><strong>Answer to question 2</strong>: Due to the layering architecture's perspective and convolutional layers, training a CNN requires a huge amount of RAM. This is because the reverse pass of backpropagation requires all the intermediate values computed during the forward pass. Fortunately, during the inferencing stage, memory occupied by one layer is released as soon as the computation is completed when the next layer has been computed.</p>
<p>Also, as stated earlier, DL4J is built upon ND4J and ND4J utilizes off-heap memory management. This enables us to control the maximum amount of off-heap memory. We can set the <kbd>org.bytedeco.javacpp.maxbytes</kbd> system property. For example, for a single JVM run, you can pass <kbd>-Dorg.bytedeco.javacpp.maxbytes=1073741824</kbd> to limit the off-heap memory to 1 GB.</p>
<p><strong>Answer to question 3</strong>: As I mentioned previously, training a CNN with Yelp's 50,000 images took one whole day with a machine with a core i7 processor and 32 GB of RAM. Naturally, performing this on all of the images can take a week. Therefore, in such cases, training on GPU makes much more sense.</p>
<p>Fortunately, we have already seen that DL4J works on distributed GPUs, as well as on native. For this, it has what we call <strong>backbends</strong>, or different types of hardware that it works on. Finally, a funny question would be: What should we do if our GPU runs out of memory? Well, if your GPU runs out of memory while training a CNN, here are five things you could do in order to try to solve the problem (other than purchasing a GPU with more RAM):</p>
<ul>
<li>Reduce the mini-batch size</li>
<li>Reduce dimensionality using a larger stride in one or more layers, but don't go with PCA or SVD</li>
<li>Remove one or more layers unless it's strictly essential to have a very deep network</li>
<li>Use 16-bit floats instead of 32-bit (but precision has to be compromised)</li>
<li>Distribute the CNN across multiple devices (that is, GPUs/CPUs)</li>
</ul>
<div class="packt_tip">For more on distributed training on GPUs using DL4J, refer to <a href="a59fb1f2-b585-44f5-a467-903b8c25867b.xhtml" target="_blank">Chapter 8</a>, <em>Distributed Deep Learning â€“ Video Classification Using Convolutional LSTM Networks</em>.</div>
<p><strong>Answer</strong> <strong>to question 4</strong>: It is true that we did not experience good accuracy. However, there are several reasons as to why we have not performed hyperparameter tuning. Secondly, we have not trained our network with all the images, so our network does not have enough data to learn the Yelp images. Finally, we can still see the model versus iteration score and other parameters from the following graph, so we can see that our model was not overfitted:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-652 image-border" src="assets/36122f86-763f-4da4-8628-de1c9a84a1c2.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Model versus iteration score and other parameters of the LSTM sentiment analyzer</div>
<p><strong>Answer</strong> <strong>to question 5</strong>: Yes, it is possible since Scala is also a JVM language, so it would not be that difficult to convert this Java project into Scala. Nevertheless, one of my previous books solves this same problem in Scala.</p>
<p>Here is the reference: Md. Rezaul Karim, <em>Scala Machine Learning Projects</em>, Packt Publishing Ltd., January 2018. Note that in that book, I used an old version of ND4J and DL4J, but I believe you can upgrade it by following this project.</p>
<p><strong>Answer</strong> <strong>to question 6</strong>: Since, in CNN, one of the objective functions is to minimize the evaluated cost, we must define an optimizer. DL4j supports the following optimizers:</p>
<ul>
<li>SGD (learning rate only)</li>
<li>Nesterovs momentum</li>
<li>Adagrad</li>
<li>RMSProp</li>
<li>Adam</li>
<li>AdaDelta</li>
</ul>
<p>For more information, interested readers can refer to the DL4J page on available updaters at <a href="https://deeplearning4j.org/updater">https://deeplearning4j.org/updater</a>.</p>
<p><strong>Answer to question 7</strong>: Just use the following code immediately after network initialization:</p>
<pre>//Print the number of parameters in the network (and for each layer)<br/>Layer[] layers = model.getLayers();<br/><strong>int</strong> totalNumParams = 0;<br/><strong>for</strong>( <strong>int</strong> i=0; i&lt;layers.length; i++ ){<br/>          <strong>int</strong> nParams = layers[i].numParams();<br/>          System.<strong><em>out</em></strong>.println("Number of parameters in layer " + i + ": " + nParams);<br/>          totalNumParams += nParams;<br/>        }<br/>System.<strong><em>out</em></strong>.println("Total number of network parameters: " + totalNumParams);</pre>
<pre class="mce-root"><q>&gt;&gt;&gt;</q><br/> <span class="packt_screen">Number of parameters in layer 0: 740<br/> Number of parameters in layer 1: 0<br/> Number of parameters in layer 2: 36050<br/> Number of parameters in layer 3: 0<br/> Number of parameters in layer 4: 225500<br/> Number of parameters in layer 5: 1002<br/> Total number of network parameters: 263292</span></pre>
<p>This also tell us that the subsampling layers do not have any hyperparameters. Nevertheless, if you want to create an MLP or DBN, we will require millions of hyperparameters. However, here, we can see that we only need 263,000 hyperparameters.</p>


            </article>

            
        </section>
    </body></html>