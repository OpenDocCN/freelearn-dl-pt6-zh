- en: Creating New Images Using Generative Adversarial Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter illustrates the application of **generative adversarial networks**
    (**GANs**) for generating new images using a practical example. So far in this
    book, using image data, we have illustrated the use of deep networks for image
    classification tasks. However, in this chapter, we will explore an interesting
    and popular approach that helps create new images. Generative adversarial networks
    have been applied for generating new images, improving image quality, and generating
    new text and new music. Another interesting application of GANs is in the area
    of anomaly detection. Here, a GAN is trained to generate data that is considered
    normal. When this network is used for reconstructing data that is considered not
    normal or anomalous, the differences in results can help us detect the presence
    of an anomaly. We will look at an example of generating new images in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, in this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial network overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing MNIST image data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing the generator network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing the discriminator network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance optimization tips and best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative adversarial network overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'GANs make use of two networks:'
  prefs: []
  type: TYPE_NORMAL
- en: Generator network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discriminator network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the generator network, noisy data, which is usually random numbers that
    have been generated from a standard normal distribution are provided as input.
    A flow chart showing an overview of a generative adversarial network is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02dd32ee-b66b-462b-9b75-8fc5ec56a546.png)'
  prefs: []
  type: TYPE_IMG
- en: As indicated in the preceding flowchart, the generator network uses noisy data
    as input and tries to create an image that we can label as fake. These fake images,
    along with the labels representing them as fake, are provided as input to the
    discriminator network. Along with the labeled fake images, we can also provide
    real images with labels as input to the discriminator network.
  prefs: []
  type: TYPE_NORMAL
- en: During the training process, the discriminator network tries to differentiate
    between a fake image created by the generator network and a real image. While
    developing a generative adversarial network, this process continues so that a
    generator network tries its best to generate an image that a discriminator network
    cannot classify as fake. At the same time, the discriminator network gets better
    and better at correctly discriminating between a fake and a real image.
  prefs: []
  type: TYPE_NORMAL
- en: Success is achieved when the generator network learns to consistently produce
    images that are not available in the training data and the discriminator network
    is unable to classify them as fake. For the real images in this chapter, we will
    make use of MNIST train data that contains images of handwritten digits.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming sections, we will illustrate the steps we need to follow in
    order to develop a generative adversarial network for the handwritten digit five,
    which is available in the MNIST data.
  prefs: []
  type: TYPE_NORMAL
- en: Processing MNIST image data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, will use the Keras library, which also includes MNIST data.
    We will also make use of the EBImage library, which is useful for processing image
    data. MNIST data contains handwritten images from 0 to 9\. Let''s take a look
    at the following code to understand this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the structure of this data, we can see that there are 60,000 images
    in the training data and 10,000 images in the test data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These handwritten images are 28 x 28 in size and are black and white in color.
    This means that there's one channel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will only make use of digit five from the training data
    for training the generative adversarial network and for generating new images
    of digit five.
  prefs: []
  type: TYPE_NORMAL
- en: Digit five from the training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although a generative adversarial network can be developed to generate all
    10 digits, for someone just getting started, it is advisable to get started with
    just one digit. Let''s take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As seen in the preceding code, we are selecting images that contain digit five
    and are saving them in `trainx`. The structure of `trainx` shows us that there
    are 5,421 such images and they all have dimensions of 28 x 28\. The summary function
    shows that the values in `trainx` range from 0 to 255\. The first 64 images of
    the handwritten digit five from the train data can be seen in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b27ce764-d851-46b1-a947-b51b206bc129.png)'
  prefs: []
  type: TYPE_IMG
- en: These handwritten images show a high amount of variability. Such variability
    is expected since different people have different handwriting styles. Although
    most of these digits are clearly written and easy to recognize, there are some
    that are somewhat less clear.
  prefs: []
  type: TYPE_NORMAL
- en: Data processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To prepare our data for the steps that follow, we''ll reshape `trainx` so that
    its dimensions are 5,421 x 28 x 28 x 1, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here, we also divide the values in `trainx` by 255 to obtain a range of values
    between 0 and 1\. With the data processed in the required format, we can move
    on and develop the architecture for the generator network.
  prefs: []
  type: TYPE_NORMAL
- en: Developing the generator network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The generator network will be used for generating fake images from data that's
    provided in the form of noise. In this section, we will develop the architecture
    of the generator network and look at the parameters that are involved by summarizing
    the network.
  prefs: []
  type: TYPE_NORMAL
- en: Network architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at the code for developing the generator network architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We have specified height (h), width (w), number of channels (c), and the latent
    dimension (l) as 28, 28, 1, and 28, respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have specified the input shape for the generator input (gi) as 28\. At the
    time of training, the generator network will be provided an input of 28 random
    numbers that have been obtained from a standard normal distribution which is simply
    noise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we have specified the architecture for the generator network's output
    (go).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last layer is a convolutional 2D layer with a `tanh` activation function.
    In the last layer, we have set the filter as 1 since we will not be using color
    images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that `layer_conv_2d_transpose` is required to be 28 x 28 in size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output dimensions from the generator output will be 28 x 28 x 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The other values that were used, such as the number of filters, `kernel_size`,
    or strides can be experimented with later if you wish to explore improving the
    results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gi` and `go` are used for the generator network (g).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's look at the summary of this network.
  prefs: []
  type: TYPE_NORMAL
- en: Summary of the generator network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A summary of the generator network is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The summary of the generator network shows the output's shape and the number
    of parameters for each layer. Note that the final output shape is 28 x 28 x 1\.
    The fake images that will be generated will have these dimensions. Overall, for
    this network, we have 224,737 parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've specified the structure of the generator network, we can develop
    the architecture for the discriminator network.
  prefs: []
  type: TYPE_NORMAL
- en: Developing the discriminator network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The discriminator network will be used for classifying fake and real images.
    The architecture and summary of the network will be discussed in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code that''s used for developing the discriminator network architecture
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We provided an input shape (di) with h = 28, w = 28, and c = 1\. This is the
    dimension of fake and real images that will be used at the time of training the
    network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the last layer of the discriminator output (do), we have specified the activation
    function as `sigmoid` and the units as 1, since an image is differentiated as
    either real or fake.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`di` and `do` are used for the discriminator network model (d).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary of the discriminator network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The summary of the discriminator network shows the output shape and number
    of parameters for each layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here, the output of the first layer is 28 x 28 x 1 in size, which matches the
    dimensions of the fake and real images. The total number of parameters is 41,089.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can compile the discriminator network model using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have compiled the discriminator network using the `rmsprop` optimizer.
    For the loss, we have specified `binary_crossentropy`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we freeze the weight of the discriminator network. Note that we freeze
    these weights after compiling the discriminator network so that it applies them
    to the `gan` model only:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here, the generative adversarial network's output (gano) uses the generator
    network and the discriminator network with frozen weights. The generative adversarial
    network (gan) is based on `gani` and `gano`. The network is then compiled with
    the `rmsprop` optimizer and with the loss specified as `binary_crossentropy`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are ready to train the network.
  prefs: []
  type: TYPE_NORMAL
- en: Training the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will out training of the network. While training the network,
    we will save fake images and store loss values to review the training progress.
    They will help us assess the effectiveness of the network when creating realistic
    fake images.
  prefs: []
  type: TYPE_NORMAL
- en: Initial setup for saving fake images and loss values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will start by specifying a few things that we will need for the training
    process. Let''s take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We will use a batch size (b) of 50.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will save fake images in the `FakeImages` directory, which is created on
    the desktop of our computer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will also make use of discriminator loss values (dloss) and GAN loss values
    (gloss), which are initialized with `NULL`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we will train the model. Here, we will be using 100 iterations. Let''s
    go over the code for this, which has been summarized into five points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We start by simulating random data points from the standard normal distribution
    and the save results as noise. Then, we use the generator network `g` to create
    fake images from this data containing random noise. Note that `noise` is 50 x
    28 in size and that `fake` is 50 x 28 x 28 x 1 in size and contains 50 fake images
    in each iteration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We update the values of start and stop based on the batch size. For the first
    iteration, start and stop have values of 1 and 50, respectively. For the second
    iteration, start and stop have values of 51 and 100, respectively. Similarly,
    for the 100th iteration, start and stop have values of 4,951 and 5,000, respectively.
    Since `trainx`, which contains the handwritten digit five, has more than 5,000
    images, none of the images are repeated during these 100 iterations. Thus, in
    each iteration, 50 real images are selected and stored in `real`, which is 50
    x 28 x 28 in size. We use reshape to change the dimensions to 50 x 28 x 28 x 1,
    so that they match the dimensions of the fake images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we create an empty array called `both` that's 100 x 28 x 28 x 1 in size
    to store real and fake image data. The first 50 images in `both` contain fake
    data while the next 50 images contain real images. We also generate 50 random
    numbers between 0.9 and 1 using uniform distribution to use as labels for fake
    images and similar random numbers between 0 and 0.1 to use as labels for real
    images. Note that we do not use 0 to represent real and 1 to represent fake images
    and instead introduce some randomness or noise. Artificially introducing some
    noise in the values of labels helps at the time of training the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We train the discriminator network using image data contained in `both` and
    the correct category information contained in `labels`. We also store the discriminator
    loss values in `dloss` for all 100 iterations. If the discriminator network learns
    to do well in classifying fake and real images, then this loss value will be low.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We try to fool the network by labeling the noise containing random values between
    0 and 0.1, which we had used for real images. The resulting loss values are stored
    in `gloss` for all 100 iterations. If the network learns to do well in presenting
    fake images and makes the network classify them as real, then this loss value
    will be low.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We save the first fake image from each of the 100 iterations so that we can
    review it and observe the impact of the training process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that, usually, the training process for generative adversarial networks
    requires a significant amount of computational resources. However, the example
    we are using here is meant to quickly illustrate how this process works and complete
    the training process in a reasonable amount of time. For 100 iterations and a
    computer with 8 GB of RAM, it should take less than a minute to run all the code.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will review the network losses that were obtained from 100
    iterations. We will also take a look at the progress of using fake images from
    iteration 1 to 100.
  prefs: []
  type: TYPE_NORMAL
- en: Discriminator and GAN losses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The discriminator and GAN loss values that were obtained from our 100 iterations
    can be plotted as follows. The discriminator loss is based on the loss values
    for the fake and real images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92ead08f-6541-4a19-8d5c-04d2fb0da7ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding plot, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The loss values for the discriminator network and the GAN show high variability
    during the first 20 iterations. This variability is an outcome of the learning
    process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The discriminator and generator networks are competing against each other and
    trying to do better than one another. When one network performs better, it is
    at the cost of the other network. This is the reason why, if `dloss` and `gloss`
    were plotted on a scatter plot, we would expect to see some amount of negative
    correlation between them. The correlation is not expected to be perfectly negative,
    but the overall pattern is expected to indicate a negative relationship. In the
    long run, both loss values are expected to converge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss values that were obtained from the GAN show higher fluctuations compared
    to the loss values that are obtained from the discriminator network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After about 50 iterations, we notice that the discriminator loss values show
    a small but gradual increase. This suggests that the discriminator network is
    finding it increasingly difficult to differentiate between the real and fake images
    that are being generated by the generator network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that an increase in loss values is not necessarily a negative outcome.
    In this case, this is positive feedback and it indicates that pitting the generator
    network against the discriminator network is yielding results. This means that
    the generator network is able to create fake images that increasingly look like
    real images and helps us achieve our main objective.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fake images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the following code to read fake images and then plot them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we have made use of the EBImage library to process fake
    image data. We have read all 100 images that are saved in the `FakeImages` directory.
    Now, we can plot all the images in a 10 x 10 grid, as shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9b410e9-63fd-494b-8546-e948b8536f4e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding image, the first fake image from each of the 100 iterations
    is shown. From this, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The first ten images in the first row represent the first 10 iterations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first image simply reflects random noise. As we reach 10 iterations, the
    image begins to capture the essence of the handwritten digit five.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the time the network training goes through iterations 91 to 100, digit five
    becomes visually more clear.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will carry out an experiment by making some changes
    in the network and observing its impact on the network's training process.
  prefs: []
  type: TYPE_NORMAL
- en: Performance optimization tips and best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will carry out an experiment by inserting an additional
    convolutional layer into the generator network, as well as in the discriminator
    network. Through this experiment, we will convey performance optimization tips
    and best practices.
  prefs: []
  type: TYPE_NORMAL
- en: Changes in the generator and discriminator network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The changes in the generator network are shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that, in the generator network, we are adding the `layer_conv_2d`
    and `layer_activation_leaky_relu` layers just before the last layer. The total
    number of parameters for the generator network has increased to 276,801.
  prefs: []
  type: TYPE_NORMAL
- en: 'The changes in the discriminator network are shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have added the `layer_conv_2d` and `layer_activation_leaky_relu` layers
    before the flattening layer in the discriminator network. The number of parameters
    in the discriminator network has increased to 148,866\. We have kept everything
    else the same and then trained the network again for 100 iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can assess the impact of these changes.
  prefs: []
  type: TYPE_NORMAL
- en: Impact of these changes on the results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The discriminator and GAN loss values for 100 iterations can be plotted as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0b7934c-683d-488a-b9de-b4375b35f698.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding plot, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: By increasing the number of layers, the fluctuation in the loss values for the
    discriminator and GAN network has reduced compared to the results we obtained
    earlier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The spikes or high loss values that have been observed for some of the iterations
    indicate the corresponding network struggling, while competing against the other
    network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The variability in the GAN loss values continues to be higher compared to those
    for discriminator network-related loss.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following plot is of the first fake image in each of the 100 iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ea51275-7d28-4f89-ae28-e8054420bf78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding images, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: With additional convolutional layers in the generator and discriminator networks,
    the network begins to generate images replicating the handwritten digit five much
    earlier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous network, fake images that consistently looked like handwritten
    digit five did not appear until about 70-80 iterations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to the use of additional layers, we can see the digit five being formed
    more or less consistently after about 20-30 iterations, which suggests an improvement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will try to use this network to generate another handwritten digit.
  prefs: []
  type: TYPE_NORMAL
- en: Generating a handwritten image of digit eight
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this experiment, we will make use of the same network architecture as the
    previous one. However, we will use it for generating a handwritten image of digit
    eight. The discriminator and GAN loss values for 100 iterations for this experiment
    can be plotted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69db0b7a-6a39-40c3-906a-54311552e074.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding plot, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The discriminator and GAN loss values show variability that tends to reduce
    as the number of iterations goes from 1 to 100.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High spikes at certain intervals for the GAN loss are diminishing as the network's
    training proceeds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A plot of the first fake image from each iteration is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/28729cfc-3050-4fc5-bfcc-bd9a0f23691e.png)'
  prefs: []
  type: TYPE_IMG
- en: Compared to digit five, digit eight takes more iterations before it starts to
    form a recognizable pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we experimented with additional convolutional layers in the
    generator and the discriminator networks. Due to this, we can make the following
    observations:'
  prefs: []
  type: TYPE_NORMAL
- en: Additional convolutional layers seem to have a positive impact on the generation
    of fake images that began to look like handwritten images of digit five much quicker.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although the results for the data that we referred to in this chapter were decent,
    for other data, we may have to make other changes to the model architecture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also used the network with the same architecture to generate realistic-looking
    fake images of handwritten digit eight. It was observed that, for digit eight,
    it took more iterations of training the network before a recognizable pattern
    started to emerge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that a network for generating all 10 handwritten digits at the same time
    can be more complex and is likely to require many more iterations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, if we have color images that have significantly larger dimensions
    than 28 x 28, which is what we used for this chapter, we will need more computational
    resources and the task will be even more challenging.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we used a generative adversarial network to illustrate how
    to generate images of a single handwritten digit. Generative adversarial networks
    make use of two networks: generator and discriminator networks. Generator networks
    create fake images from data containing random noise, while discriminator networks
    are trained to differentiate between fake images and real images. These two networks
    compete against each other so that realistic-looking fake images can be created.
    Although in this chapter we provided an example of using a generative adversarial
    network to generate new images, these networks are also known to have applications
    in generating new text or new music, as well as in anomaly detection.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we went over various deep learning networks that are useful
    for dealing with image data. In the next section, we will go over deep learning
    networks for natural language processing.
  prefs: []
  type: TYPE_NORMAL
