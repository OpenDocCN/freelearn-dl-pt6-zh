<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Neural Network and Artificial Intelligence Concepts</h1>
                
            
            <article>
                
<p class="calibre2">From the scientific and philosophical studies conducted over the centuries, special mechanisms have been identified that are the basis of human intelligence. Taking inspiration from their operations, it was possible to create machines that imitate part of these mechanisms. The problem is that they have not yet succeeded in imitating and integrating all of them, so the <strong class="calibre1">Artificial Intelligence</strong> (<strong class="calibre1">AI</strong>) systems we have are largely incomplete.</p>
<p class="calibre2">A decisive step in the improvement of such machines came from the use of so-called <strong class="calibre1">Artificial Neural Networks</strong> (<strong class="calibre1">ANNs</strong>) that, starting from the mechanisms regulating natural neural networks, plan to simulate human thinking. S<span>oftware can now imitate the mechanisms needed to win a chess match or to translate text into a different language in accordance with its grammatical rules.</span></p>
<p class="calibre2">This chapter introduces the basic theoretical concepts of ANN and AI. Fundamental understanding of the following is expected:</p>
<ul class="calibre16">
<li class="calibre17">Basic high school mathematics; differential calculus and functions such as <em class="calibre14">sigmoid</em></li>
<li class="calibre17">R programming and usage of R libraries</li>
</ul>
<p class="calibre2">We will go through the basics of neural networks and try out one model using R. This chapter is a foundation for neural networks and all the subsequent chapters.</p>
<p class="calibre2">We will cover the following topics in this chapter:</p>
<ul class="calibre16">
<li class="calibre17">ANN concepts</li>
<li class="calibre17">Neurons, perceptron, and multilayered neural networks</li>
<li class="calibre17">Bias, weights, activation functions, and hidden layers</li>
<li class="calibre17">Forward and backpropagation methods</li>
<li class="calibre17">Brief overview of <strong class="calibre1">Graphics Processing Unit</strong> (<strong class="calibre1">GPU</strong>)</li>
</ul>
<p class="calibre2">At the end of the chapter, you will be able to recognize the different neural network algorithms and tools which R provides to handle them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Introduction</h1>
                
            
            <article>
                
<p class="calibre2">The brain is the most important organ of the human body. It is the central processing unit for all the functions performed by us. Weighing only 1.5 kilos, it has around 86 billion neurons. A neuron is defined as a cell transmitting nerve impulses or electrochemical signals. The brain is a complex network of neurons which process information through a system of several interconnected neurons. It has always been challenging to understand the brain functions; however, due to advancements in computing technologies, we can now program neural networks artificially.</p>
<p class="calibre2">The discipline of ANN arose from the thought of mimicking the functioning of the same human brain that was trying to solve the problem. The drawbacks of conventional approaches and their successive applications have been overcome within well-defined technical environments.</p>
<p class="calibre2">AI or machine intelligence is a field of study that aims to give cognitive powers to computers to program them to learn and solve problems. Its objective is to simulate computers with human intelligence. AI cannot imitate human intelligence completely; computers can only be programmed to do some aspects of the human brain.</p>
<p class="calibre2">Machine learning is a branch of AI which helps computers to program themselves based on the input data. Machine learning gives AI the ability to do data-based problem solving. ANNs are an example of machine learning algorithms.</p>
<p class="calibre2"><strong class="calibre1">Deep learning</strong> (<strong class="calibre1">DL</strong>) is complex set of neural networks with more layers of processing, which develop high levels of abstraction. They are typically used for complex tasks, such as image recognition, image classification, and hand writing identification.</p>
<p class="calibre2">Most of the audience think that neural networks are difficult to learn and use it as a black box. This book intends to open the black box and help one learn the internals with implementation in R. With the working knowledge, we can see many use cases where neural networks can be made tremendously useful seen in the following image:</p>
<div class="cdpaligncenter"><strong class="calibre1"><img class="image-border" src="../images/00005.jpeg"/></strong></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Inspiration for neural networks</h1>
                
            
            <article>
                
<p class="calibre2">Neural networks are inspired by the way the human brain works. A human brain can process huge amounts of information using data sent by human senses (especially vision). The processing is done by neurons, which work on electrical signals passing through them and applying flip-flop logic, like opening and closing of the gates for signal to transmit through. The following images shows the structure of a neuron:</p>
<div class="cdpaligncenter"><img class="alignnone" src="../images/00006.jpeg"/><br class="title-page-name"/></div>
<p class="calibre2">The major components of each neuron are:</p>
<ul class="calibre16">
<li class="calibre17"><strong class="calibre1">Dendrites</strong>: Entry points in each neuron which take input from other neurons in the network in form of electrical impulses</li>
<li class="calibre17"><strong class="calibre1">Cell Body</strong>: It generates inferences from the dendrite inputs and decides what action to take</li>
<li class="calibre17"><strong class="calibre1">Axon terminals</strong>: They transmit outputs in form of electrical impulses to next neuron</li>
</ul>
<p class="calibre2">Each neuron processes signals only if it exceeds a certain threshold. Neurons either fire or do not fire; it is either <em class="calibre14">0</em> or <em class="calibre14">1</em>.</p>
<p class="calibre2">AI has been a domain for sci-fi movies and fiction books. ANNs within AI have been around since the 1950s, but we have made them more dominant in the past 10 years due to advances in computing architecture and performance. There have been major advancements in computer processing, leading to:</p>
<ul class="calibre16">
<li class="calibre17">Massive parallelism</li>
<li class="calibre17">Distributed representation and computation</li>
<li class="calibre17">Learning and generalization ability</li>
<li class="calibre17">Fault tolerance</li>
<li class="calibre17">Low energy consumption</li>
</ul>
<p class="calibre2">In the domain of numerical computations and symbol manipulation, solving problems on-top of centralized architecture, modern day computers have surpassed humans to a greater extent. Where they actually lag behind with such an organizing structure is in the domains of pattern recognition, noise reduction, and optimizing. A toddler can recognize his/her mom in a huge crowd, but a computer with a centralized architecture wouldnâ€™t be able to do the same.</p>
<p class="calibre2">This is where the biological neural network of the brain has been outperforming machines, and hence the inspiration to develop an alternative loosely held, decentralized architecture mimicking the brain.</p>
<p class="calibre2">ANNs are massively parallel computing systems consisting of an extremely large number of simple processors with many interconnections.</p>
<p class="calibre2">One of the leading global news agencies, Guardian, used big data in digitizing the archives by uploading the snapshots of all the archives they had had. However, for a user to copy the content and use it elsewhere is the limitation here. To overcome that, one can use an ANN for text pattern recognition to convert the images to text file and then to any format according to the needs of the end-users.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">How do neural networks work?</h1>
                
            
            <article>
                
<p class="calibre2">Similar to the biological neuron structure, ANNs define the neuron as a central processing unit, which performs a mathematical operation to generate one output from a set of inputs. The output of a neuron is a function of the weighted sum of the inputs plus the bias. Each neuron performs a very simple operation that involves activating if the total amount of signal received exceeds an activation threshold, as shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border1" src="../images/00007.gif"/></div>
<p class="calibre2">The function of the entire neural network is simply the computation of the outputs of all the neurons, which is an entirely deterministic calculation. Essentially, ANN is a set of mathematical function approximations. We would now be introducing new terminology associated with ANNs:</p>
<ul class="calibre16">
<li class="calibre17">Input layer</li>
<li class="calibre17">Hidden layer</li>
<li class="calibre17">Output layer</li>
<li class="calibre17">Weights</li>
<li class="calibre17">Bias</li>
<li class="calibre17">Activation functions</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Layered approach</h1>
                
            
            <article>
                
<p class="calibre2">Any neural network processing a framework has the following architecture:</p>
<div class="cdpaligncenter"><img class="image-border2" src="../images/00008.gif"/></div>
<p class="calibre2">There is a set of inputs, a processor, and a set of outputs. This layered approach is also followed in neural networks. The inputs form the <strong class="calibre1">input layer</strong>, the <strong class="calibre1">middle layer(s)</strong> which performs the processing is called the <strong class="calibre1">hidden layer(s)</strong>, and the <strong class="calibre1">output(s)</strong> forms the output layer.</p>
<p class="calibre2">Our neural network architectures are also based on the same principle. The hidden layer has the magic to convert the input to the desired output. The understanding of the hidden layer requires knowledge of weights, bias, and activation functions, which is our next topic of discussion.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Weights and biases</h1>
                
            
            <article>
                
<p class="calibre2">Weights in an ANN are the most important factor in converting an input to impact the output. This is similar to slope in linear regression, where a weight is multiplied to the input to add up to form the output. Weights are numerical parameters which determine how strongly each of the neurons affects the other.</p>
<p class="calibre2">For a typical neuron, if the inputs are <em class="calibre14">x<sub class="calibre25">1</sub></em>, <em class="calibre14">x<sub class="calibre25">2</sub></em>, and <em class="calibre14">x<sub class="calibre25">3</sub></em>, then the <span>synaptic</span> weights to be applied to them are denoted as <em class="calibre14">w<sub class="calibre25">1</sub></em>, <em class="calibre14">w<sub class="calibre25">2</sub></em>, and <em class="calibre14">w<sub class="calibre25">3</sub></em>.</p>
<p class="calibre2">Output is</p>
<div class="calibre26"><img src="../images/00009.jpeg" class="calibre27"/></div>
<p class="calibre2">Â </p>
<p class="calibre2">where <em class="calibre14">i</em> is <em class="calibre14">1</em> to the number of inputs.</p>
<p class="calibre2">Simply, this is a matrix multiplication to arrive at the weighted sum.</p>
<p class="calibre2">Bias is like the intercept added in a linear equation. It is an additional parameter which is used to adjust the output along with the weighted sum of the inputs to the neuron.</p>
<p class="calibre2">The processing done by a neuron is thus denoted as :</p>
<div class="calibre28"><img src="../images/00010.jpeg" class="calibre29"/></div>
<p class="calibre2">Â </p>
<p class="calibre2">A function is applied on this output and is called an <strong class="calibre1">activation function</strong>. The input of the next layer is the output of the neurons in the previous layer, as shown in the following image:</p>
<div class="cdpaligncenter"><img class="alignnone1" src="../images/00011.gif"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Training neural networks</h1>
                
            
            <article>
                
<p class="calibre2">Training is the act of presenting the network with some sample data and modifying the weights to better approximate the desired function.</p>
<p class="calibre2">There are two main types of training: supervised learning and unsupervised learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Supervised learning</h1>
                
            
            <article>
                
<p class="calibre2">We supply the neural network with inputs and the desired outputs. Response of the network to the inputs is measured. The weights are modified to reduce the difference between the actual and desired outputs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Unsupervised learning</h1>
                
            
            <article>
                
<p class="calibre2">We only supply inputs. The neural network adjusts its own weights, so that similar inputs cause similar outputs. The network identifies the patterns and differences in the inputs without any external assistance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Epoch</h1>
                
            
            <article>
                
<p class="calibre2">One iteration or pass through the process of providing the network with an input and updating the network's weights is called an <strong class="calibre1">epoch</strong>. It is a full run of feed-forward and backpropagation for update of weights. It is also one full read through of the entire dataset.</p>
<p class="calibre2">Typically, many epochs, in the order of tens of thousands at times, are required to train the neural network efficiently. We will see more about epochs in the forthcoming chapters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Activation functions</h1>
                
            
            <article>
                
<p class="calibre2">The abstraction of the processing of neural networks is mainly achieved through the activation functions. An activation function is a mathematical function which converts the input to an output, and adds the magic of neural network processing. Without activation functions, the working of neural networks will be like linear functions. A linear function is one where the output is directly proportional to input, for example:</p>
<div class="calibre30"><img src="../images/00012.jpeg" class="calibre31"/></div>
<div class="calibre30"><img src="../images/00013.jpeg" class="calibre32"/></div>
<p class="calibre2">Â </p>
<p class="calibre2">A linear function is a polynomial of one degree. Simply, it is a straight line without any curves.</p>
<p class="calibre2">However, most of the problems the neural networks try to solve are nonlinear and complex in nature. To achieve the nonlinearity, the activation functions are used. Nonlinear functions are high degree polynomial functions, for example:</p>
<div class="calibre30"><img src="../images/00014.jpeg" class="calibre33"/></div>
<div class="calibre30"><img src="../images/00015.jpeg" class="calibre34"/></div>
<p class="calibre2">Â </p>
<p class="calibre2">The graph of a nonlinear function is curved and adds the complexity factor.</p>
<p class="calibre2">Activation functions give the nonlinearity property to neural networks and make them true universal function approximators.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Different activation functions</h1>
                
            
            <article>
                
<p class="calibre2">There are many activation functions available for a neural network to use. We shall see a few of them here.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Linear function</h1>
                
            
            <article>
                
<p class="calibre2">The simplest activation function, one that is commonly used for the output layer activation function in neural network problems, is the linear activation function represented by the following formula:</p>
<div class="calibre26"><img src="../images/00016.jpeg" class="calibre35"/></div>
<p class="calibre2">Â </p>
<p class="calibre2"><span>The output is same as the input and the function is defined in the</span> range (<em class="calibre14">-infinity, +infinity</em>). <span>In the following figure, a linear activation function is shown:</span></p>
<div class="cdpaligncenter"><img class="image-border3" src="../images/00017.gif"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Unit step activation function</h1>
                
            
            <article>
                
<p class="calibre2">A unit step activation function is a much-used feature in neural networks. The output assumes value <em class="calibre14">0</em> for negative argument and <em class="calibre14">1</em> for positive argument. The function is as follows:</p>
<div class="mce-root1">Â <img src="../images/00018.jpeg" class="calibre36"/></div>
<p class="calibre2">Â </p>
<p class="calibre2">The range is between <em class="calibre14">(0,1)</em> and the output is binary in nature. These types of activation functions are useful for binary schemes. When we want to classify an input model in one of two groups, we can use a binary compiler with a unit step activation function. A <span>unit step activation function is shown in the following figure:</span></p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00019.gif"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Sigmoid</h1>
                
            
            <article>
                
<p class="calibre2">The <em class="calibre14">sigmoid</em> function is a mathematical function that produces a sigmoidal curve; a characteristic curve for its <em class="calibre14">S</em> shape. This is the earliest and often used activation function. This squashes the input to any value between <em class="calibre14">0</em> and <em class="calibre14">1</em>, and makes the model logistic in nature. <span>This function refers to a special case of logistic function defined by the following formula:</span></p>
<div class="calibre26"><img src="../images/00020.jpeg" class="calibre37"/></div>
<p class="calibre2">Â </p>
<p class="calibre2">In the following figure is shown a sigmoid curve with an <em class="calibre14">S</em> shape:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00021.gif"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Hyperbolic tangent</h1>
                
            
            <article>
                
<p class="calibre2">Another very popular and widely used activation feature is the <em class="calibre14">tanh</em> function. If you look at the figure that follows, you can notice that it looks very similar to <em class="calibre14">sigmoid</em>; in fact, it is a scaled <em class="calibre14">sigmoid</em> function. This is a nonlinear function, defined in the range of values <em class="calibre14">(-1, 1)</em>, so you need not worry about activations blowing up. One thing to clarify is that the gradient is stronger for <em class="calibre14">tanh</em> than <em class="calibre14">sigmoid</em> (the derivatives are more steep). Deciding between <em class="calibre14">sigmoid</em> and <em class="calibre14">tanh</em> will depend on your gradient strength requirement. Like the <em class="calibre14">sigmoid</em>, <em class="calibre14">tanh</em> also has the missing slope problem. The function is defined by the following formula:</p>
<div class="calibre30"><img src="../images/00022.jpeg" class="calibre38"/></div>
<p class="calibre2">Â </p>
<p class="calibre2"><span>In the following figure is shown a hyberbolic tangent activation function:</span></p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00023.gif"/></div>
<p class="calibre2">This looks very similar to <em class="calibre14">sigmoid</em>; in fact, it is a scaled <em class="calibre14">sigmoid</em> function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Rectified Linear Unit</h1>
                
            
            <article>
                
<p class="calibre2"><strong class="calibre1">Rectified Linear Unit</strong> (<strong class="calibre1">ReLU</strong>) is the most used activation function since 2015. It is a simple condition and has advantages over the other functions. <span>The function is</span> <span>defined by the following formula:</span></p>
<div class="mce-root1"><img src="../images/00024.jpeg" class="calibre36"/></div>
<p class="calibre2">Â </p>
<p class="calibre2"><span>In the following figure is shown a ReLU activation function:</span></p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00025.gif"/></div>
<p class="calibre2">The range of output is between <em class="calibre14">0</em> and infinity. <span>ReLU</span> finds applications in computer vision and speech recognition using deep neural nets. There are various other activation functions as well, but we have covered the most important ones here.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Which activation functions to use?</h1>
                
            
            <article>
                
<p class="calibre2">Given that neural networks are to support nonlinearity and more complexity, the activation function to be used has to be robust enough to have the following:</p>
<ul class="calibre16">
<li class="calibre17">It should be differential; we will see why we need differentiation in backpropagation. It should not cause gradients to vanish.</li>
<li class="calibre17">It should be simple and fast in processing.</li>
<li class="calibre17">It should not be zero centered.</li>
</ul>
<p class="calibre2">The <em class="calibre14">sigmoid</em> is the most used activation function, but it suffers from the following setbacks:</p>
<ul class="calibre16">
<li class="calibre17">Since it uses logistic model, the computations are time consuming and complex</li>
<li class="calibre17">It cause gradients to vanish and no signals pass through the neurons at some point of time</li>
<li class="calibre17">It is slow in convergence</li>
<li class="calibre17"><span>It is</span> not zero centered</li>
</ul>
<p class="calibre2">These drawbacks are solved by ReLU. ReLU is simple and is faster to process. It does not have the vanishing gradient problem and has shown vast improvements compared to the <em class="calibre14">sigmoid</em> and <em class="calibre14">tanh</em> functions. ReLU is the most preferred activation function for neural networks and DL problems.</p>
<p class="calibre2">ReLU is used for hidden layers, while the output layer can use a <kbd class="calibre13">softmax</kbd> function for logistic problems and a linear function of regression problems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Perceptron and multilayer architectures</h1>
                
            
            <article>
                
<p class="calibre2">A perceptron is a single neuron that classifies a set of inputs into one of two categories (usually <em class="calibre14">1</em> or <em class="calibre14">-1</em>). If the inputs are in the form of a grid, a perceptron can be used to recognize visual images of shapes. The perceptron usually uses a step function, which returns <em class="calibre14">1</em> if the weighted sum of the inputs exceeds a threshold, and <em class="calibre14">0</em> otherwise.</p>
<p class="calibre2">When layers of perceptron are combined together, they form a multilayer architecture, and this gives the required complexity of the neural network processing. <strong class="calibre1">Multi-Layer Perceptrons</strong> (<strong class="calibre1">MLPs</strong>) are the most widely used architecture for neural networks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Forward and backpropagation</h1>
                
            
            <article>
                
<p class="calibre2">The processing from input layer to hidden layer(s) and then to the output layer is called <strong class="calibre1">forward propagation</strong>. The <em class="calibre14">sum(input*weights)+bias</em> is applied at each layer and then the activation function value is propagated to the next layer. The next layer can be another hidden layer or the output layer. The construction of neural networks uses large number of hidden layers to give rise to <strong class="calibre1">Deep Neural Network</strong> (<strong class="calibre1">DNN</strong>).</p>
<p class="calibre2">Once the output is arrived at, at the last layer (the output layer), we compute the error (the predicted output minus the original output). This error is required to correct the weights and biases used in forward propagation. Here is where the derivative function is used. The amount of weight that has to be changed is determined by <strong class="calibre1">gradient descent</strong>.</p>
<p class="calibre2">The backpropagation process uses the partial derivative of each neuron's activation function to identify the slope (or gradient) in the direction of each of the incoming weights. The gradient suggests how steeply the error will be reduced or increased for a change in the weight. The backpropagation keeps changing the weights until there is greatest reduction in errors by an amount known as the <strong class="calibre1">learning rate</strong>.</p>
<p class="calibre2">Learning rate is a scalar parameter, analogous to step size in numerical integration, used to set the rate of adjustments to reduce the errors faster. Learning rate is used in backpropagation during adjustment of weights and bias.</p>
<p class="calibre2">More the learning rate, the faster the algorithm will reduce the errors and faster will be the training process:</p>
<div class="cdpaligncenter"><img class="image-border5" src="../images/00026.jpeg"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Step-by-step illustration of a neuralnet and an activation function</h1>
                
            
            <article>
                
<p class="calibre2">We shall take a step-by-step approach to understand the forward and reverse pass with a single hidden layer. The input layer has one neuron and the output will solve a binary classification problem (predict 0 or 1). In the following figure is shown a <span>forward and reverse pass with a single hidden layer:</span></p>
<div class="cdpaligncenter"><img class="image-border6" src="../images/00027.jpeg"/></div>
<p class="calibre2">Next, let us analyze in detail, step by step, all the operations to be done for network training:</p>
<ol class="calibre19">
<li value="1" class="calibre17">Take the input as a matrix.</li>
<li value="2" class="calibre17">Initialize the weights and biases with random values. This is one time and we will keep updating these with the error propagation process.</li>
<li value="3" class="calibre17">Repeat the steps 4 to 9 f<span>or each training pattern (presented in random order), until the error is minimized.</span></li>
<li value="4" class="calibre17"><span>Apply the inputs to the network.</span></li>
<li value="5" class="calibre17">Calculate the output for every neuron from the input layer, through the hidden layer(s), to the output layer.</li>
<li value="6" class="calibre17">Calculate the error at the outputs: actual minus predicted.</li>
</ol>
<p class="calibre2">Â </p>
<ol start="7" class="calibre19">
<li value="7" class="calibre17">Use the output error to compute error signals for previous layers. The partial derivative of the activation function is used to compute the error signals.</li>
<li value="8" class="calibre17">Use the error signals to compute weight adjustments.</li>
<li value="9" class="calibre17">Apply the weight adjustments.</li>
</ol>
<p class="calibre2">Steps 4 and 5 are forward propagation and steps 6 through 9 are backpropagation.</p>
<p class="calibre2"><span>The learning rate is t</span><span>he amount that weights are updated is controlled by a configuration</span> <span>parameter.</span></p>
<p class="calibre2">The complete pass back and forth is called a <strong class="calibre1">training cycle</strong> or <strong class="calibre1">epoch</strong>. The updated weights and biases are used in the next cycle. We keep recursively training until the error is very minimal.</p>
<p class="calibre2">We shall cover more about the forward and backpropagation in detail throughout this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Feed-forward and feedback networks</h1>
                
            
            <article>
                
<p class="calibre2">The flow of the signals in neural networks can be either in only one direction or in recurrence. In the first case, we call the neural network architecture feed-forward, since the input signals are fed into the input layer, then, after being processed, they are forwarded to the next layer, just as shown in the following figure. MLPs and radial basis functions are also good examples of feed-forward networks. <span>In the following figure is shown an MLPs architecture:</span></p>
<div class="cdpaligncenter"><img class="image-border7" src="../images/00028.gif"/></div>
<p class="calibre2">When the neural network has some kind of internal recurrence, meaning that the signals are fed back <span>to</span> a neuron or layer that has already received and processed that signal, the network is of the type feedback, as shown in the following image:</p>
<div class="cdpaligncenter"><img class="image-border8" src="../images/00029.jpeg"/></div>
<p class="mce-root">The special reason to add recurrence in a network is the production of a dynamic behavior, particularly when the network addresses problems involving time series or pattern recognition, that require an internal memory to reinforce the learning process. However, such networks are particularly difficult to train, eventually failing to learn. Most of the feedback networks are single layer, such as the <strong class="calibre1">Elman</strong> and <strong class="calibre1">Hopfield</strong> networks, but it is possible to build a recurrent multilayer network, such as echo and recurrent MLP networks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Gradient descent</h1>
                
            
            <article>
                
<p class="calibre2">Gradient descent is an iterative approach for error correction in any learning model. For neural networks during backpropagation, the process of iterating the update of weights and biases with the error times derivative of the activation function is the gradient descent approach. The steepest descent step size is replaced by a similar size from the previous step. Gradient is basically defined as the slope of the curve and is the derivative of the activation function:</p>
<div class="cdpaligncenter"><img class="image-border9" src="../images/00030.jpeg"/></div>
<p class="calibre2">The objective of deriving gradient descent at each step is to find the global cost minimum, where the error is the lowest. And this is where the model has a good fit for the data and predictions are more accurate.</p>
<p class="calibre2">Gradient descent can be performed either for the full batch or stochastic. In full batch gradient descent, the gradient is computed for the full training dataset, whereas <strong class="calibre1">Stochastic Gradient Descent</strong> (<strong class="calibre1">SGD</strong>) takes a single sample and performs gradient calculation. It can also take mini-batches and perform the calculations. One advantage of SGD is faster computation of gradients.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Taxonomy of neural networks</h1>
                
            
            <article>
                
<p class="calibre2">The basic foundation for ANNs is the same, but various neural network models have been designed during its evolution. The following are a few of the ANN models:</p>
<ul class="calibre16">
<li class="calibre17"><strong class="calibre1">Adaptive Linear Element</strong> (<span><strong class="calibre1">ADALINE</strong>),</span> is a simple perceptron which can solve only linear problems. Each neuron takes the weighted linear sum of the inputs and passes it to a bi-polar function, which either produces a <em class="calibre14">+1</em> or <em class="calibre14">-1</em> depending on the sum. The function checks the sum of the inputs passed and if the net is <em class="calibre14">&gt;= 0</em>, it is <em class="calibre14">+1</em>, else it is <em class="calibre14">-1</em>.</li>
<li class="calibre17"><strong class="calibre1">Multiple ADALINEs</strong> (<strong class="calibre1">MADALINE</strong>), is a multilayer network of ADALINE units.</li>
<li class="calibre17">Perceptrons are single layer neural networks (single neuron or unit), where the input is multidimensional (vector) and the output is a function on the weight sum of the inputs.</li>
<li class="calibre17">Radial basis function network is an ANN where a radial basis function is used as an activation function. The network output is a linear combination of radial basis functions of the inputs and some neuron parameters.</li>
<li class="calibre17">Feed-forward is the simplest form of neural networks. The data is processed across layers without any loops are cycles. We will study the following feed- forward networks in this book:
<ul class="calibre39">
<li class="calibre17">Autoencoder</li>
<li class="calibre17">Probabilistic</li>
<li class="calibre17">Time delay</li>
<li class="calibre17">Covolutional</li>
</ul>
</li>
<li class="calibre17"><strong class="calibre1">Recurrent Neural Networks</strong> (<strong class="calibre1">RNNs</strong>), unlike feed-forward networks, <span>propagate data forward and also backwards from later processing stages to earlier stages. The following are the types of RNNs; we shall study them in our later chapters:</span>
<ul class="calibre39">
<li class="calibre17">Hopfield networks</li>
<li class="calibre17">Boltzmann machine</li>
<li class="calibre17"><strong class="calibre1">Self Organizing Maps</strong> (<strong class="calibre1">SOMs</strong>)</li>
<li class="calibre17"><strong class="calibre1">Bidirectional Associative Memory</strong> (<strong class="calibre1">BAM</strong>)</li>
<li class="calibre17"><strong class="calibre1">Long Short Term Memory</strong> (<strong class="calibre1">LSTM</strong>)</li>
</ul>
</li>
</ul>
<p class="calibre2">The following images depict <strong class="calibre1">(a) Recurrent neural network</strong> and <strong class="calibre1">(b) Forward neural network</strong>:</p>
<div class="cdpaligncenter"><img class="alignnone2" src="../images/00031.gif"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Simple example using R neural net library - neuralnet()</h1>
                
            
            <article>
                
<p class="calibre2">Consider a simple dataset of a square of numbers, which will be used to train a <kbd class="calibre13">neuralnet</kbd> function in R and then test the accuracy of the built neural network:</p>
<table class="calibre40">
<tbody class="calibre6">
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><strong class="calibre1">INPUT</strong></p>
</td>
<td class="calibre8">
<p class="calibre2"><strong class="calibre1">OUTPUT</strong></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">0</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">0</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">1</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">1</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">2</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">4</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">3</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">9</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">4</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">16</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">5</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">25</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">6</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">36</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">7</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">49</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">8</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">64</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">9</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">81</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">10</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">100</kbd></p>
</td>
</tr>
</tbody>
</table>
<p class="calibre2">Â </p>
<p class="calibre2">Our objective is to set up the weights and bias so that the model can do what is being done here. The output needs to be modeled on a function of input and the function can be used in future to determine the output based on an input:</p>
<pre class="calibre24"><strong class="calibre1">######################################################################### </strong><br class="title-page-name"/><strong class="calibre1">###Chapter 1 - Introduction to Neural Networks - using R ################ </strong><br class="title-page-name"/><strong class="calibre1">###Simple R program to build, train and test neural Networks############# </strong><br class="title-page-name"/><strong class="calibre1">######################################################################### </strong><br class="title-page-name"/><strong class="calibre1"><br class="title-page-name"/>#Choose the libraries to use</strong><br class="title-page-name"/><strong class="calibre1">library("neuralnet")</strong><br class="title-page-name"/> <br class="title-page-name"/><strong class="calibre1">#Set working directory for the training data</strong><br class="title-page-name"/><strong class="calibre1">setwd("C:/R")</strong><br class="title-page-name"/><strong class="calibre1">getwd()</strong><br class="title-page-name"/> <br class="title-page-name"/><strong class="calibre1">#Read the input file</strong><br class="title-page-name"/><strong class="calibre1">mydata=read.csv('Squares.csv',sep=",",header=TRUE)</strong><br class="title-page-name"/><strong class="calibre1">mydata</strong><br class="title-page-name"/><strong class="calibre1">attach(mydata)</strong><br class="title-page-name"/><strong class="calibre1">names(mydata)</strong><br class="title-page-name"/> <br class="title-page-name"/><strong class="calibre1">#Train the model based on output from input</strong><br class="title-page-name"/><strong class="calibre1">model=neuralnet(formula = Output~Input, </strong><br class="title-page-name"/><strong class="calibre1">                data = mydata, </strong><br class="title-page-name"/><strong class="calibre1">                hidden=10, </strong><br class="title-page-name"/><strong class="calibre1">                threshold=0.01 )</strong><br class="title-page-name"/><strong class="calibre1">print(model)</strong><br class="title-page-name"/> <br class="title-page-name"/><strong class="calibre1">#Lets plot and see the layers</strong><br class="title-page-name"/><strong class="calibre1">plot(model)</strong><br class="title-page-name"/> <br class="title-page-name"/><strong class="calibre1">#Check the data - actual and predicted</strong><br class="title-page-name"/><strong class="calibre1">final_output=cbind (Input, Output, </strong><br class="title-page-name"/><strong class="calibre1">                    as.data.frame(model$net.result) )</strong><br class="title-page-name"/><strong class="calibre1">colnames(final_output) = c("Input", "Expected Output", </strong><br class="title-page-name"/><strong class="calibre1">                           "Neural Net Output" )</strong><br class="title-page-name"/><strong class="calibre1">print(final_output)</strong><br class="title-page-name"/><strong class="calibre1">#########################################################################</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Let us go through the code line-by-line</h1>
                
            
            <article>
                
<p class="calibre2">To understand all the steps in the code just proposed, we will look at them in detail. Do not worry if a few steps seem unclear at this time, you will be able to look into it in the following examples. First, the code snippet will be shown, and the explanation will follow:</p>
<pre class="calibre24"><strong class="calibre1">library("neuralnet")</strong></pre>
<p class="calibre2">The line in R includes the library <kbd class="calibre13">neuralnet()</kbd> in our program. <kbd class="calibre13">neuralnet()</kbd> is part of <strong class="calibre1">Comprehensive R Archive Network</strong> (<strong class="calibre1">CRAN</strong>), which contains numerous R libraries for various applications.</p>
<pre class="calibre24"><strong class="calibre1">mydata=read.csv('Squares.csv',sep=",",header=TRUE)</strong><br class="title-page-name"/><strong class="calibre1">mydata</strong><br class="title-page-name"/><strong class="calibre1">attach(mydata)</strong><br class="title-page-name"/><strong class="calibre1">names(mydata)</strong></pre>
<p class="calibre2">This reads the CSV file with separator <kbd class="calibre13">,</kbd>(comma), and header is the first line in the file. <kbd class="calibre13">names()</kbd> would display the header of the file.</p>
<pre class="calibre24"><strong class="calibre1">model=neuralnet(formula = Output~Input, </strong><br class="title-page-name"/><strong class="calibre1">               data = mydata, </strong><br class="title-page-name"/><strong class="calibre1">               hidden=10, </strong><br class="title-page-name"/><strong class="calibre1">               threshold=0.01 )</strong></pre>
<p class="calibre2">The training of the output with respect to the input happens here. The <kbd class="calibre13">neuralnet()</kbd> library is passed the output and input column names (<kbd class="calibre13">ouput~input</kbd>), the dataset to be used, the number of neurons in the hidden layer, and the stopping criteria (<kbd class="calibre13">threshold</kbd>).</p>
<p class="calibre2">A brief description of the <kbd class="calibre13">neuralnet</kbd> package, extracted from the official documentation, is shown in the following table:</p>
<table class="calibre41">
<tbody class="calibre6">
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><strong class="calibre1">neuralnet-package</strong>:</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><strong class="calibre1">Description</strong>:</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2">Training of neural networks using the backpropagation, resilient backpropagation with (Riedmiller, 1994) or without weight backtracking (Riedmiller, 1993), or the modified globally convergent version by Anastasiadis et al. (2005). The package allows flexible settings through custom-choice of error and activation function. Furthermore, the calculation of generalized weights (Intrator O &amp; Intrator N, 1993) is implemented.</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><strong class="calibre1">Details</strong>:</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2">Package: <kbd class="calibre13">neuralnet</kbd></p>
<p class="calibre2">Type: Package</p>
<p class="calibre2">Version: 1.33</p>
<p class="calibre2">Date: 2016-08-05</p>
<p class="calibre2">License: GPL (&gt;=2)</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><strong class="calibre1">Authors</strong>:</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2">Stefan Fritsch, Frauke Guenther (email: <kbd class="calibre13">guenther@leibniz-bips.de</kbd>)</p>
<p class="calibre2">Maintainer: Frauke Guenther (email: <kbd class="calibre13">guenther@leibniz-bips.de</kbd>)</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><strong class="calibre1">Usage</strong>:</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">neuralnet(formula, data, hidden = 1, threshold = 0.01, stepmax = 1e+05, rep = 1, startweights = NULL, learningrate.limit = NULL, learningrate.factor = list(minus = 0.5, plus = 1.2), learningrate=NULL, lifesign = "none", lifesign.step = 1000, algorithm = "rprop+", err.fct = "sse", act.fct = "logistic", linear.output = TRUE, exclude = NULL,</kbd><br class="title-page-name"/>
<kbd class="calibre13">constant.weights = NULL, likelihood = FALSE)</kbd></td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><strong class="calibre1">Meaning of the arguments</strong>:</p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">formula</kbd>: A symbolic description of the model to be fitted.</p>
<p class="calibre2"><kbd class="calibre13">data</kbd>: A dataframe containing the variables specified in formula.</p>
<p class="calibre2"><kbd class="calibre13">hidden</kbd>: A vector of integers specifying the number of hidden neurons (vertices) in each layer.</p>
<p class="calibre2"><kbd class="calibre13">threshold</kbd>: A numeric value specifying the threshold for the partial derivatives of the error function as stopping criteria.</p>
<p class="calibre2"><kbd class="calibre13">stepmax</kbd>: The maximum steps for the training of the neural network. Reaching this maximum leads to a stop of the neural network's training process.</p>
<p class="calibre2"><kbd class="calibre13">rep</kbd>: The number of repetitions for the neural network's training.</p>
<p class="calibre2"><kbd class="calibre13">startweights</kbd>: A vector containing starting values for the weights. The weights will not be randomly initialized.</p>
<p class="calibre2"><kbd class="calibre13">learningrate.limit</kbd>: A vector or a list containing the lowest and highest limit for the learning rate. Used only for <kbd class="calibre13">RPROP</kbd> and <kbd class="calibre13">GRPROP</kbd>.</p>
<p class="calibre2"><kbd class="calibre13">learningrate.factor</kbd>: A vector or a list containing the multiplication factors for the upper and lower learning rate, used only for <kbd class="calibre13">RPROP</kbd> and <kbd class="calibre13">GRPROP</kbd>.</p>
<p class="calibre2"><kbd class="calibre13">learningrate</kbd>: A numeric value specifying the learning rate used by traditional backpropagation. Used only for traditional backpropagation.</p>
<p class="calibre2"><kbd class="calibre13">lifesign</kbd>: A string specifying how much the function will print during the calculation of the neural network-<kbd class="calibre13">'none'</kbd>, <kbd class="calibre13">'minimal'</kbd>, or <kbd class="calibre13">'full'</kbd>.</p>
<p class="calibre2"><kbd class="calibre13">lifesign.step</kbd>: An integer specifying the step size to print the minimal threshold in full lifesign mode.</p>
<p class="calibre2"><kbd class="calibre13">algorithm</kbd>: A string containing the algorithm type to calculate the neural network.</p>
<p class="calibre2"><kbd class="calibre13">err.fct</kbd>: A differentiable function that is used for the calculation of the error.</p>
<p class="calibre2"><kbd class="calibre13">act.fct</kbd>: A differentiable function that is used for smoothing the result of the cross product of the covariate or neurons and the weights.</p>
<p class="calibre2"><kbd class="calibre13">linear.output</kbd>: Logical. If <kbd class="calibre13">act.fct</kbd> should not be applied to the output neurons set linear output to <kbd class="calibre13">TRUE</kbd>, otherwise to <kbd class="calibre13">FALSE</kbd>.</p>
<p class="calibre2"><kbd class="calibre13">exclude</kbd>: A vector or a matrix specifying the weights that are excluded from the calculation.</p>
<p class="calibre2"><kbd class="calibre13">constant.weights</kbd>: A vector specifying the values of the weights that are excluded from the training process and treated as fix.</p>
<p class="calibre2"><kbd class="calibre13">likelihood</kbd>: Logical. If the error function is equal to the negative log-likelihood function, the information criteria AIC and BIC will be calculated. Furthermore the usage of confidence. interval is meaningful.</p>
</td>
</tr>
</tbody>
</table>
<p class="calibre2">Â </p>
<p class="calibre2"><span>After giving a brief glimpse into the package documentation, let's review the remaining lines of the proposed code sample:</span></p>
<pre class="calibre24"><strong class="calibre1"> print(model)</strong></pre>
<p class="calibre2">This command prints the model that has just been generated, as follows:</p>
<pre class="calibre24"><strong class="calibre1">$result.matrix</strong><br class="title-page-name"/><strong class="calibre1">                                            1</strong><br class="title-page-name"/><strong class="calibre1">error                          0.001094100442</strong><br class="title-page-name"/><strong class="calibre1">reached.threshold              0.009942937680</strong><br class="title-page-name"/><strong class="calibre1">steps                      34563.000000000000</strong><br class="title-page-name"/><strong class="calibre1">Intercept.to.1layhid1         12.859227998180</strong><br class="title-page-name"/><strong class="calibre1">Input.to.1layhid1             -1.267870997079</strong><br class="title-page-name"/><strong class="calibre1">Intercept.to.1layhid2         11.352189417430</strong><br class="title-page-name"/><strong class="calibre1">Input.to.1layhid2             -2.185293148851</strong><br class="title-page-name"/><strong class="calibre1">Intercept.to.1layhid3          9.108325110066</strong><br class="title-page-name"/><strong class="calibre1">Input.to.1layhid3             -2.242001064132</strong><br class="title-page-name"/><strong class="calibre1">Intercept.to.1layhid4        -12.895335140784</strong><br class="title-page-name"/><strong class="calibre1">Input.to.1layhid4              1.334791491801</strong><br class="title-page-name"/><strong class="calibre1">Intercept.to.1layhid5         -2.764125889399</strong><br class="title-page-name"/><strong class="calibre1">Input.to.1layhid5              1.037696638808</strong><br class="title-page-name"/><strong class="calibre1">Intercept.to.1layhid6         -7.891447011323</strong><br class="title-page-name"/><strong class="calibre1">Input.to.1layhid6              1.168603081208</strong><br class="title-page-name"/><strong class="calibre1">Intercept.to.1layhid7         -9.305272978434</strong><br class="title-page-name"/><strong class="calibre1">Input.to.1layhid7              1.183154841948</strong><br class="title-page-name"/><strong class="calibre1">Intercept.to.1layhid8         -5.056059256828</strong><br class="title-page-name"/><strong class="calibre1">Input.to.1layhid8              0.939818815422</strong><br class="title-page-name"/><strong class="calibre1">Intercept.to.1layhid9         -0.716095585596</strong><br class="title-page-name"/><strong class="calibre1">Input.to.1layhid9             -0.199246231047</strong><br class="title-page-name"/><strong class="calibre1">Intercept.to.1layhid10        10.041789457410</strong><br class="title-page-name"/><strong class="calibre1">Input.to.1layhid10            -0.971900813630</strong><br class="title-page-name"/><strong class="calibre1">Intercept.to.Output           15.279512257145</strong><br class="title-page-name"/><strong class="calibre1">1layhid.1.to.Output          -10.701406269616</strong><br class="title-page-name"/><strong class="calibre1">1layhid.2.to.Output           -3.225793088326</strong><br class="title-page-name"/><strong class="calibre1">1layhid.3.to.Output           -2.935972228783</strong><br class="title-page-name"/><strong class="calibre1">1layhid.4.to.Output           35.957437333162</strong><br class="title-page-name"/><strong class="calibre1">1layhid.5.to.Output           16.897986621510</strong><br class="title-page-name"/><strong class="calibre1">1layhid.6.to.Output           19.159646982676</strong><br class="title-page-name"/><strong class="calibre1">1layhid.7.to.Output           20.437748965610</strong><br class="title-page-name"/><strong class="calibre1">1layhid.8.to.Output           16.049490298968</strong><br class="title-page-name"/><strong class="calibre1">1layhid.9.to.Output           16.328504039013</strong><br class="title-page-name"/><strong class="calibre1">1layhid.10.to.Output          -4.900353775268</strong></pre>
<p class="calibre2">Let's go back to the code analysis:</p>
<pre class="calibre24"><strong class="calibre1">plot(model)</strong></pre>
<p class="calibre2">This preceding command plots the neural network for us, as follows:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00032.gif"/></div>
<pre class="mce-root2"><strong class="calibre1">final_output=cbind (Input, Output, </strong><br class="title-page-name"/><strong class="calibre1">                    as.data.frame(model$net.result) )</strong><br class="title-page-name"/><strong class="calibre1">colnames(final_output) = c("Input", "Expected Output", </strong><br class="title-page-name"/><strong class="calibre1">                     "Neural Net Output" )</strong><br class="title-page-name"/><strong class="calibre1">print(final_output)</strong></pre>
<p class="calibre2">This preceding code prints the final output, comparing the output predicted and actual as:</p>
<pre class="calibre24"><strong class="calibre1">&gt; print(final_output)</strong><br class="title-page-name"/><strong class="calibre1"> Input Expected Output Neural Net Output</strong><br class="title-page-name"/><strong class="calibre1">1    0               0     -0.0108685813</strong><br class="title-page-name"/><strong class="calibre1">2    1               1      1.0277796553</strong><br class="title-page-name"/><strong class="calibre1">3    2               4      3.9699671691</strong><br class="title-page-name"/><strong class="calibre1">4    3               9      9.0173879001</strong><br class="title-page-name"/><strong class="calibre1">5    4              16     15.9950295615</strong><br class="title-page-name"/><strong class="calibre1">6    5              25     25.0033272826</strong><br class="title-page-name"/><strong class="calibre1">7    6              36     35.9947137155</strong><br class="title-page-name"/><strong class="calibre1">8    7              49     49.0046689369</strong><br class="title-page-name"/><strong class="calibre1">9    8              64     63.9972090104</strong><br class="title-page-name"/><strong class="calibre1">10   9              81     81.0008391011</strong><br class="title-page-name"/><strong class="calibre1">11  10             100     99.9997950184 </strong>   <strong class="calibre1"> </strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Implementation using nnet() library</h1>
                
            
            <article>
                
<p class="calibre2">To improve our practice with the <kbd class="calibre13">nnet</kbd> library, we look at another example. This time we will use the data collected at a restaurant through customer interviews. The customers were asked to give a score to the following aspects: service, ambience, and food. They were also asked whether they would leave the tip on the basis of these scores. In this case, the number of inputs is <kbd class="calibre13">2</kbd> and the output is a categorical value (<kbd class="calibre13">Tip=1</kbd> and <kbd class="calibre13">No-tip=0</kbd>).</p>
<p class="calibre2">The input file to be used is shown in the following table:</p>
<table class="calibre40">
<tbody class="calibre6">
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><strong class="calibre1">No</strong></p>
</td>
<td class="calibre8">
<p class="calibre2"><strong class="calibre1">CustomerWillTip</strong></p>
</td>
<td class="calibre8">
<p class="calibre2"><strong class="calibre1">Service</strong></p>
</td>
<td class="calibre8">
<p class="calibre2"><strong class="calibre1">Ambience</strong></p>
</td>
<td class="calibre8">
<p class="calibre2"><strong class="calibre1">Food</strong></p>
</td>
<td class="calibre8">
<p class="calibre2"><strong class="calibre1">TipOrNo</strong></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">1</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">1</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">4</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">4</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">5</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">Tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">2</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">1</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">6</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">4</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">4</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">Tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">3</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">1</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">5</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">2</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">4</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">Tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">4</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">1</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">6</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">5</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">5</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">Tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">5</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">1</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">6</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">3</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">4</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">Tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">6</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">1</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">3</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">4</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">5</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">Tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">7</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">1</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">5</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">5</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">5</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">Tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">8</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">1</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">5</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">4</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">4</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">Tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">9</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">1</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">7</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">6</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">4</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">Tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">10</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">1</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">7</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">6</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">4</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">Tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">11</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">1</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">6</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">7</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">2</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">Tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">12</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">1</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">5</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">6</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">4</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">Tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">13</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">1</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">7</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">3</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">3</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">Tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">14</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">1</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">5</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">1</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">4</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">Tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">15</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">1</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">7</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">5</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">5</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">Tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">16</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">0</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">3</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">1</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">3</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">No-tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">17</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">0</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">4</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">6</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">2</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">No-tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">18</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">0</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">2</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">5</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">2</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">No-tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">19</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">0</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">5</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">2</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">4</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">No-tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">20</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">0</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">4</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">1</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">3</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">No-tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">21</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">0</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">3</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">3</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">4</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">No-tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">22</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">0</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">3</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">4</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">5</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">No-tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">23</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">0</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">3</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">6</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">3</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">No-tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">24</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">0</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">4</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">4</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">2</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">No-tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">25</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">0</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">6</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">3</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">6</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">No-tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">26</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">0</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">3</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">6</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">3</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">No-tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">27</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">0</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">4</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">3</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">2</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">No-tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">28</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">0</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">3</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">5</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">2</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">No-tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">29</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">0</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">5</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">5</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">3</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">No-tip</kbd></p>
</td>
</tr>
<tr class="calibre7">
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">30</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">0</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">1</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">3</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">2</kbd></p>
</td>
<td class="calibre8">
<p class="calibre2"><kbd class="calibre13">No-tip</kbd></p>
</td>
</tr>
</tbody>
</table>
<p class="calibre2">Â </p>
<p class="calibre2">This is a classification problem with three inputs and one categorical output. We will address the problem with the following code:</p>
<pre class="calibre24"><strong class="calibre1">######################################################################## </strong><br class="title-page-name"/><strong class="calibre1">##Chapter 1 - Introduction to Neural Networks - using R ################ </strong><br class="title-page-name"/><strong class="calibre1">###Simple R program to build, train and test neural networks ########### </strong><br class="title-page-name"/><strong class="calibre1">### Classification based on 3 inputs and 1 categorical output ########## </strong><br class="title-page-name"/><strong class="calibre1">######################################################################## </strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">###Choose the libraries to use</strong><br class="title-page-name"/><strong class="calibre1">library(NeuralNetTools)</strong><br class="title-page-name"/><strong class="calibre1">library(nnet)</strong><br class="title-page-name"/> <br class="title-page-name"/><strong class="calibre1">###Set working directory for the training data</strong><br class="title-page-name"/><strong class="calibre1">setwd("C:/R")</strong><br class="title-page-name"/><strong class="calibre1">getwd()</strong><br class="title-page-name"/> <br class="title-page-name"/><strong class="calibre1">###Read the input file</strong><br class="title-page-name"/><strong class="calibre1">mydata=read.csv('RestaurantTips.csv',sep=",",header=TRUE)</strong><br class="title-page-name"/><strong class="calibre1">mydata</strong><br class="title-page-name"/><strong class="calibre1">attach(mydata)</strong><br class="title-page-name"/><strong class="calibre1">names(mydata)</strong><br class="title-page-name"/> <br class="title-page-name"/><strong class="calibre1">##Train the model based on output from input</strong><br class="title-page-name"/><strong class="calibre1">model=nnet(CustomerWillTip~Service+Ambience+Food, </strong><br class="title-page-name"/><strong class="calibre1">           data=mydata, </strong><br class="title-page-name"/><strong class="calibre1">           size =5, </strong><br class="title-page-name"/><strong class="calibre1">           rang=0.1, </strong><br class="title-page-name"/><strong class="calibre1">           decay=5e-2, </strong><br class="title-page-name"/><strong class="calibre1">           maxit=5000)</strong><br class="title-page-name"/><strong class="calibre1">print(model)</strong><br class="title-page-name"/><strong class="calibre1">plotnet(model)</strong><br class="title-page-name"/><strong class="calibre1">garson(model)</strong><br class="title-page-name"/> <br class="title-page-name"/><strong class="calibre1">########################################################################</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Let us go through the code line-by-line</h1>
                
            
            <article>
                
<p class="calibre2"><span>To understand all the steps in the code just proposed, we will look at them in detail. First, the code snippet will be shown, and the explanation will follow.</span></p>
<pre class="calibre24"><strong class="calibre1">library(NeuralNetTools)</strong><br class="title-page-name"/><strong class="calibre1">library(nnet)</strong></pre>
<p class="calibre2">This includes the libraries <kbd class="calibre13">NeuralNetTools</kbd> and <kbd class="calibre13">nnet()</kbd> for our program.</p>
<pre class="calibre24"><strong class="calibre1">###Set working directory for the training data</strong><br class="title-page-name"/><strong class="calibre1">setwd("C:/R")</strong><br class="title-page-name"/><strong class="calibre1">getwd()</strong><br class="title-page-name"/><strong class="calibre1">###Read the input file</strong><br class="title-page-name"/><strong class="calibre1">mydata=read.csv('RestaurantTips.csv',sep=",",header=TRUE)</strong><br class="title-page-name"/><strong class="calibre1">mydata</strong><br class="title-page-name"/><strong class="calibre1">attach(mydata)</strong><br class="title-page-name"/><strong class="calibre1">names(mydata)</strong></pre>
<p class="calibre2">This sets the working directory and reads the input CSV file.</p>
<pre class="calibre24"><strong class="calibre1">##Train the model based on output from input</strong><br class="title-page-name"/><strong class="calibre1">model=nnet(CustomerWillTip~Service+Ambience+Food, </strong><br class="title-page-name"/><strong class="calibre1"> data=mydata, </strong><br class="title-page-name"/><strong class="calibre1"> size =5, </strong><br class="title-page-name"/><strong class="calibre1"> rang=0.1, </strong><br class="title-page-name"/><strong class="calibre1"> decay=5e-2, </strong><br class="title-page-name"/><strong class="calibre1"> maxit=5000)</strong><br class="title-page-name"/><strong class="calibre1">print(model)</strong></pre>
<p class="calibre2">This calls the <kbd class="calibre13">nnet()</kbd> function with the arguments passed. The output is as follows. <kbd class="calibre13">nnet()</kbd> processes the forward and backpropagation until convergence:</p>
<pre class="calibre24"><strong class="calibre1">&gt; model=nnet(CustomerWillTip~Service+Ambience+Food,data=mydata, size =5, rang=0.1, decay=5e-2, maxit=5000)</strong><br class="title-page-name"/><strong class="calibre1"># weights:  26</strong><br class="title-page-name"/><strong class="calibre1">initial  value 7.571002 </strong><br class="title-page-name"/><strong class="calibre1">iter  10 value 5.927044</strong><br class="title-page-name"/><strong class="calibre1">iter  20 value 5.267425</strong><br class="title-page-name"/><strong class="calibre1">iter  30 value 5.238099</strong><br class="title-page-name"/><strong class="calibre1">iter  40 value 5.217199</strong><br class="title-page-name"/><strong class="calibre1">iter  50 value 5.216688</strong><br class="title-page-name"/><strong class="calibre1">final  value 5.216665 </strong><br class="title-page-name"/><strong class="calibre1">converged</strong></pre>
<p class="calibre2">A brief description of the <kbd class="calibre13">nnet</kbd> package, extracted from the official documentation, is shown in the following table:</p>
<table class="calibre40">
<tbody class="calibre6">
<tr class="calibre7">
<td class="calibre8"><strong class="calibre1">nnet-package</strong>: Feed-forward neural networks and multinomial log-linear models</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><strong class="calibre1">Description</strong>:</td>
</tr>
<tr class="calibre7">
<td class="calibre8">Software for feed-forward neural networks with a single hidden layer, and for multinomial log-linear models.</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><strong class="calibre1">Details</strong>:</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><span>Package: <kbd class="calibre13">nnet</kbd></span><br class="title-page-name"/>
<span>Type: Package</span><br class="title-page-name"/>
<span>Version: 7.3-12</span><br class="title-page-name"/>
<span>Date: 2016-02-02</span><br class="title-page-name"/>
<span>License: GPL-2 | GPL-3</span></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><strong class="calibre1">Author(s)</strong>:</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">Brian Ripley</em><br class="title-page-name"/>
<em class="calibre14">William Venables</em></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><strong class="calibre1">Usage</strong>:</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">nnet(formula, data, weights,subset, na.action, contrasts = NULL)</kbd></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><strong class="calibre1">Meaning of the arguments</strong>:</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13"><span><span>Formula</span></span></kbd>: A formula of the form class <em class="calibre14">~ x1 + x2 + ...</em><br class="title-page-name"/>
<kbd class="calibre13">data</kbd>: Dataframe from which variables specified in formula are preferentially to be taken<br class="title-page-name"/>
<kbd class="calibre13">weights</kbd>: (Case) weights for each example; if missing, defaults to <em class="calibre14">1</em><br class="title-page-name"/>
<kbd class="calibre13">subset</kbd>: An index vector specifying the cases to be used in the training sample<br class="title-page-name"/>
<kbd class="calibre13">na.action</kbd>: A function to specify the action to be taken if NAs are found<br class="title-page-name"/>
<kbd class="calibre13">contrasts:</kbd> A list of contrasts to be used for some or all of the factors appearing as variables in the model formula</td>
</tr>
</tbody>
</table>
<p class="calibre2">Â </p>
<p class="calibre2"><span>After giving a brief glimpse into the package documentation, let's review the remaining lines of the proposed in the following code sample:</span></p>
<pre class="calibre24"><strong class="calibre1">print(model)</strong> </pre>
<p class="calibre2">This command prints the details of the <kbd class="calibre13">net()</kbd> as follows:</p>
<pre class="calibre24"><strong class="calibre1">&gt; print(model)</strong><br class="title-page-name"/><strong class="calibre1">a 3-5-1 network with 26 weights</strong><br class="title-page-name"/><strong class="calibre1">inputs: Service Ambience Food </strong><br class="title-page-name"/><strong class="calibre1">output(s): CustomerWillTip </strong><br class="title-page-name"/><strong class="calibre1">options were - decay=0.05</strong></pre>
<p class="calibre2">To plot the <kbd class="calibre13">model</kbd>, use the following command:</p>
<pre class="calibre24"><strong class="calibre1">plotnet(model)</strong></pre>
<p class="calibre2">The plot of the <kbd class="calibre13">model</kbd> is as follows; there are five nodes in the single hidden layer:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00033.jpeg"/></div>
<p class="calibre2">Using <kbd class="calibre13">NeuralNetTools</kbd>, it's possible to obtain the relative importance of input variables in neural networks using <kbd class="calibre13">garson</kbd> algorithm:</p>
<pre class="calibre24"><strong class="calibre1">garson(model)</strong></pre>
<p class="calibre2">This command prints the various input parameters and their importance to the output prediction, as shown in the following figure:</p>
<div class="cdpaligncenter"><strong class="calibre1"><img class="image-border4" src="../images/00034.jpeg"/></strong></div>
<p class="calibre2">From the chart obtained from the application of the Garson algorithm, it is possible to note that, in the decision to give the tip, the service received by the customers has the greater influence.</p>
<p class="calibre2">We have seen two neural network libraries in R and used them in simple examples. We would deep dive with several practical use cases throughout this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Deep learning</h1>
                
            
            <article>
                
<p class="calibre2">DL forms an advanced neural network with numerous hidden layers. DL is a vast subject and is an important concept for building AI. It is used in various applications, such as:</p>
<ul class="calibre16">
<li class="calibre17">Image recognition</li>
<li class="calibre17">Computer vision</li>
<li class="calibre17">Handwriting detection</li>
<li class="calibre17">Text classification</li>
<li class="calibre17">Multiclass classification</li>
<li class="calibre17">Regression problems, and more</li>
</ul>
<p class="calibre2">We would see more about DL with R in the future chapters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Pros and cons of neural networks</h1>
                
            
            <article>
                
<p class="calibre2">Neural networks form the basis of DL, and applications are enormous for DL, ranging from voice recognition to cancer detection. The pros and cons of neural networks are described in this section. The pros outweigh the cons and give neural networks as the preferred modeling technique for data science, machine learning, and predictions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Pros</h1>
                
            
            <article>
                
<p class="calibre2">The following are some of the advantages of neural networks:</p>
<ul class="calibre16">
<li class="calibre17">Neural networks are flexible and can be used for both regression and classification problems. Any data which can be made numeric can be used in the model, as neural network is a mathematical model with approximation functions.</li>
<li class="calibre17">Neural networks are good to model with nonlinear data with large number of inputs; for example, images. It is reliable in an approach of tasks involving many features. It works by splitting the problem of classification into a layered network of simpler elements.</li>
<li class="calibre17">Once trained, the predictions are pretty fast.</li>
<li class="calibre17">Neural networks can be trained with any number of inputs and layers.</li>
<li class="calibre17">Neural networks work best with more data points.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Cons</h1>
                
            
            <article>
                
<p class="calibre2">Let us take a look at some of the cons of neural networks:</p>
<ul class="calibre16">
<li class="calibre17">Neural networks are black boxes, meaning we cannot know how much each independent variable is influencing the dependent variables.</li>
<li class="calibre17">It is computationally very expensive and time consuming to train with traditional CPUs.</li>
<li class="calibre17">Neural networks depend a lot on training data. This leads to the problem of over-fitting and generalization. The mode relies more on the training data and may be tuned to the data.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Best practices in neural network implementations</h1>
                
            
            <article>
                
<p class="calibre2">The following are some best practices that will help in the implementation of neural network:</p>
<ul class="calibre16">
<li class="calibre17">Neural networks are best implemented when there is good training data</li>
<li class="calibre17">More the hidden layers in an MLP, the better the accuracy of the model for predictions</li>
<li class="calibre17">It is best to have five nodes in the hidden layer</li>
<li class="calibre17">ReLU and <strong class="calibre1">Sum of Square of Errors</strong> (<strong class="calibre1">SSE</strong>) are respectively best techniques for activation function and error deduction</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Quick note on GPU processing</h1>
                
            
            <article>
                
<p class="calibre2">The increase in processing capabilities has been a tremendous booster for usage of neural networks in day-to-day problems. GPU is a specialized processor designed to perform graphical operations (for example, gaming, 3D animation, and so on). They perform mathematically intensive tasks and are additional to the CPU. The CPU performs the operational tasks of the computer, while the GPU is used to perform heavy workload processing.</p>
<p class="calibre2">The neural network architecture needs heavy mathematical computational capabilities and GPU is the preferred candidate here. The vectorized dot matrix product between the weights and inputs at every neuron can be run in parallel through GPUs. The advancements in GPUs is popularizing neural networks. The applications of DL in image processing, computer vision, bioinformatics, and weather modeling are benefiting through GPUs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we saw an overview of ANNs. Neural networks implementation is simple, but the internals are pretty complex. We can summarize neural network as a universal mathematical function approximation. Any set of inputs which produce outputs can be made a black box mathematical function through a neural network, and the applications are enormous in the recent years.</p>
<p class="calibre2">We saw the following in this chapter:</p>
<ul class="calibre16">
<li class="calibre17">Neural network is a machine learning technique and is data-driven</li>
<li class="calibre17">AI, machine learning, and neural networks are different paradigms of making machines work like humans</li>
<li class="calibre17">Neural networks can be used for both supervised and unsupervised machine learning</li>
<li class="calibre17">Weights, biases, and activation functions are important concepts in neural networks</li>
<li class="calibre17">Neural networks are nonlinear and non-parametric</li>
<li class="calibre17">Neural networks are very fast in prediction and are most accurate in comparison with other machine learning models</li>
<li class="calibre17">There are input, hidden, and output layers in any neural network architecture</li>
<li class="calibre17">Neural networks are based on building MLP, and we understood the basis for neural networks: weights, bias, activation functions, feed-forward, and backpropagation processing</li>
<li class="calibre17">Forward and backpropagation are techniques to derive a neural network model</li>
</ul>
<p class="calibre2">Neural networks can be implemented through many programming languages, namely Python, R, MATLAB, C, and Java, among others. The focus of this book will be building applications using R. DNN and AI systems are evolving on the basis of neural networks. In the forthcoming chapter, we will drill through different types of neural networks and their various applications.</p>
<p class="calibre2"/>


            </article>

            
        </section>
    </body></html>