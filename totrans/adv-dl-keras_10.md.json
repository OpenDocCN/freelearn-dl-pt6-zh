["```py\nreward-= math.pow(action[0],2)*0.1\n```", "```py\n# autoencoder to convert states into features\ndef build_autoencoder(self):\n    # first build the encoder model\n    inputs = Input(shape=(self.state_dim, ), name='state')\n    feature_size = 32\n    x = Dense(256, activation='relu')(inputs)\n    x = Dense(128, activation='relu')(x)\n    feature = Dense(feature_size, name='feature_vector')(x)\n\n    # instantiate encoder model\n   self.encoder = Model(inputs, feature, name='encoder')\n    self.encoder.summary()\n    plot_model(self.encoder, to_file='encoder.png', show_shapes=True)\n\n    # build the decoder model\n    feature_inputs = Input(shape=(feature_size,), name='decoder_input')\n    x = Dense(128, activation='relu')(feature_inputs)\n    x = Dense(256, activation='relu')(x)\n    outputs = Dense(self.state_dim, activation='linear')(x)\n\n    # instantiate decoder model\n    self.decoder = Model(feature_inputs, outputs, name='decoder')\n    self.decoder.summary()\n    plot_model(self.decoder, to_file='decoder.png', show_shapes=True)\n\n    # autoencoder = encoder + decoder\n    # instantiate autoencoder model\n    self.autoencoder = Model(inputs, self.decoder(self.encoder(inputs)), name='autoencoder')\n    self.autoencoder.summary()\n    plot_model(self.autoencoder, to_file='autoencoder.png', show_shapes=True)\n\n    # Mean Square Error (MSE) loss function, Adam optimizer\n    self.autoencoder.compile(loss='mse', optimizer='adam')\n\n# training the autoencoder using randomly sampled\n# states from the environment\ndef train_autoencoder(self, x_train, x_test):\n    # train the autoencoder\n    batch_size = 32\n    self.autoencoder.fit(x_train,\n                         x_train,\n                         validation_data=(x_test, x_test),\n                         epochs=10,\n                         batch_size=batch_size)\n```", "```py\n    # given mean and stddev, sample an action, clip and return\n    # we assume Gaussian distribution of probability of selecting an\n    # action given a state\n    def action(self, args):\n        mean, stddev = args\n        dist = tf.distributions.Normal(loc=mean, scale=stddev)\n        action = dist.sample(1)\n        action = K.clip(action,\n                        self.env.action_space.low[0],\n                        self.env.action_space.high[0])\n        return action\n```", "```py\n# some implementations use a modified softplus to ensure that\n# the stddev is never zero\ndef softplusk(x):\n    return K.softplus(x) + 1e-10\n```", "```py\ndef build_actor_critic(self):\n    inputs = Input(shape=(self.state_dim, ), name='state')\n    self.encoder.trainable = False\n    x = self.encoder(inputs)\n    mean = Dense(1,\n                 activation='linear',\n                 kernel_initializer='zero',\n                 name='mean')(x)\n    stddev = Dense(1,\n                   kernel_initializer='zero',\n                   name='stddev')(x)\n    # use of softplusk avoids stddev = 0\n    stddev = Activation('softplusk', name='softplus')(stddev)\n    action = Lambda(self.action,\n                    output_shape=(1,),\n                    name='action')([mean, stddev])\n    self.actor_model = Model(inputs, action, name='action')\n    self.actor_model.summary()\n    plot_model(self.actor_model, to_file='actor_model.png', show_shapes=True)\n\n    logp = Lambda(self.logp,\n                  output_shape=(1,),\n                  name='logp')([mean, stddev, action])\n    self.logp_model = Model(inputs, logp, name='logp')\n    self.logp_model.summary()\n    plot_model(self.logp_model, to_file='logp_model.png', show_shapes=True)\n\n    entropy = Lambda(self.entropy,\n                     output_shape=(1,),\n                     name='entropy')([mean, stddev])\n    self.entropy_model = Model(inputs, entropy, name='entropy')\n    self.entropy_model.summary()\n    plot_model(self.entropy_model, to_file='entropy_model.png', show_shapes=True)\n    value = Dense(1,\n                  activation='linear',\n                  kernel_initializer='zero',\n                  name='value')(x)\n    self.value_model = Model(inputs, value, name='value')\n    self.value_model.summary()\n```", "```py\n    # given mean, stddev, and action compute\n    # the log probability of the Gaussian distribution\n    def logp(self, args):\n        mean, stddev, action = args\n        dist = tf.distributions.Normal(loc=mean, scale=stddev)\n        logp = dist.log_prob(action)\n        return logp\n```", "```py\n    # given the mean and stddev compute the Gaussian dist entropy\n    def entropy(self, args):\n        mean, stddev = args\n        dist = tf.distributions.Normal(loc=mean, scale=stddev)\n        entropy = dist.entropy()\n        return entropy\n```", "```py\ninputs = Input(shape=(self.state_dim, ), name='state')\nself.encoder.trainable = False\nx = self.encoder(inputs)\n\nvalue = Dense(1,\n              activation='linear',\n              kernel_initializer='zero',\n              name='value')(x)\nself.value_model = Model(inputs, value, name='value')\n```", "```py\n# logp loss, the 3rd and 4th variables (entropy and beta) are needed\n# by A2C so we have a different loss function structure\ndef logp_loss(self, entropy, beta=0.0):\n    def loss(y_true, y_pred):\n        return -K.mean((y_pred * y_true) + (beta * entropy), axis=-1)\n\n    return loss\n\n# typical loss function structure that accepts 2 arguments only\n# this will be used by value loss of all methods except A2C\ndef value_loss(self, y_true, y_pred):\n    return -K.mean(y_pred * y_true, axis=-1)\n```", "```py\n# train by episode (REINFORCE, REINFORCE with baseline\n# and A2C use this routine to prepare the dataset before\n# the step by step training)\ndef train_by_episode(self, last_value=0):\n    if self.args.actor_critic:\n        print(\"Actor-Critic must be trained per step\")\n        return\n    elif self.args.a2c:\n        # implements A2C training from the last state\n        # to the first state\n        # discount factor\n        gamma = 0.95\n        r = last_value\n        # the memory is visited in reverse as shown\n        # in Algorithm 10.5.1\n        for item in self.memory[::-1]:\n            [step, state, next_state, reward, done] = item\n            # compute the return\n            r = reward + gamma*r\n            item = [step, state, next_state, r, done]\n            # train per step\n            # a2c reward has been discounted\n            self.train(item)\n\n        return\n\n    # only REINFORCE and REINFORCE with baseline\n    # use the ff codes\n    # convert the rewards to returns\n    rewards = []\n    gamma = 0.99\n    for item in self.memory:\n       [_, _, _, reward, _] = item\n        rewards.append(reward)\n\n    # compute return per step\n    # return is the sum of rewards from t til end of episode\n    # return replaces reward in the list\n    for i in range(len(rewards)):\n        reward = rewards[i:]\n        horizon = len(reward)\n        discount =  [math.pow(gamma, t) for t in range(horizon)]\n        return_ = np.dot(reward, discount)\n        self.memory[i][3] = return_\n\n    # train every step\n    for item in self.memory:\n        self.train(item, gamma=gamma)\n```", "```py\n# main routine for training as used by all 4 policy gradient\n# methods\ndef train(self, item, gamma=1.0):\n    [step, state, next_state, reward, done] = item\n\n    # must save state for entropy computation\n    self.state = state\n\n    discount_factor = gamma**step\n\n    # reinforce-baseline: delta = return - value\n    # actor-critic: delta = reward - value + discounted_next_value\n    # a2c: delta = discounted_reward - value\n    delta = reward - self.value(state)[0]\n\n    # only REINFORCE does not use a critic (value network)\n    critic = False\n    if self.args.baseline:\n        critic = True\n    elif self.args.actor_critic:\n        # since this function is called by Actor-Critic\n        # directly, evaluate the value function here\n        critic = True\n        if not done:\n            next_value = self.value(next_state)[0]\n            # add  the discounted next value\n            delta += gamma*next_value\n    elif self.args.a2c:\n        critic = True\n    else:\n        delta = reward\n\n    # apply the discount factor as shown in Algortihms\n    # 10.2.1, 10.3.1 and 10.4.1\n    discounted_delta = delta * discount_factor\n    discounted_delta = np.reshape(discounted_delta, [-1, 1]) \n    verbose = 1 if done else 0\n\n    # train the logp model (implies training of actor model\n    # as well) since they share exactly the same set of\n    # parameters\n    self.logp_model.fit(np.array(state),\n                        discounted_delta,\n                        batch_size=1,\n                        epochs=1,\n                        verbose=verbose)\n\n    # in A2C, the target value is the return (reward\n    # replaced by return in the train_by_episode function)\n    if self.args.a2c:\n        discounted_delta = reward\n        discounted_delta = np.reshape(discounted_delta, [-1, 1])\n\n    # train the value network (critic)\n    if critic:\n        self.value_model.fit(np.array(state),\n                             discounted_delta,\n                             batch_size=1,\n                             epochs=1,\n                             verbose=verbose)\n```", "```py\n    # only REINFORCE and REINFORCE with baseline\n    # use the ff codes\n    # convert the rewards to returns\n    rewards = []\n    gamma = 0.99\n    for item in self.memory:\n        [_, _, _, reward, _] = item\n        rewards.append(reward)\n\n    # compute return per step\n    # return is the sum of rewards from t til end of episode\n    # return replaces reward in the list\n    for i in range(len(rewards)):\n        reward = rewards[i:]\n        horizon = len(reward)\n        discount =  [math.pow(gamma, t) for t in range(horizon)]\n        return_ = np.dot(reward, discount)\n        self.memory[i][3] = return_\n```", "```py\n        # the memory is visited in reverse as shown\n        # in Algorithm 10.5.1\n        for item in self.memory[::-1]:\n            [step, state, next_state, reward, done] = item\n            # compute the return\n            r = reward + gamma*r\n            item = [step, state, next_state, r, done]\n            # train per step\n            # a2c reward has been discounted\n            self.train(item)\n```", "```py\nv = 0 if reward > 0 else agent.value(next_state)[0]\n```", "```py\n# sampling and fitting\nfor episode in range(episode_count):\n    state = env.reset()\n    # state is car [position, speed]\n    state = np.reshape(state, [1, state_dim])\n    # reset all variables and memory before the start of\n    # every episode\n    step = 0 \n    total_reward = 0 \n    done = False\n    agent.reset_memory()\n   while not done:\n        # [min, max] action = [-1.0, 1.0]\n        # for baseline, random choice of action will not move\n        # the car pass the flag pole\n        if args.random:\n            action = env.action_space.sample()\n        else:\n            action = agent.act(state)\n        env.render()\n        # after executing the action, get s', r, done\n        next_state, reward, done, _ = env.step(action)\n        next_state = np.reshape(next_state, [1, state_dim])\n        # save the experience unit in memory for training\n        # Actor-Critic does not need this but we keep it anyway.\n        item = [step, state, next_state, reward, done]\n        agent.remember(item)\n\n        if args.actor_critic and train:\n            # only actor-critic performs online training\n            # train at every step as it happens\n            agent.train(item, gamma=0.99)\n        elif not args.random and done and train:\n            # for REINFORCE, REINFORCE with baseline, and A2C\n            # we wait for the completion of the episode before \n            # training the network(s)\n            # last value as used by A2C\n            v = 0 if reward > 0 else agent.value(next_state)[0]\n            agent.train_by_episode(last_value=v)\n\n        # accumulate reward\n        total_reward += reward\n        # next state is the new state\n        state = next_state\n        step += 1\n```", "```py\n$ python3 policygradient-car-10.1.1.py\n--encoder_weights=encoder_weights.h5 --actor_weights=actor_weights.h5\n\n```", "```py\npython3 policygradient-car-10.1.1.py\n--encoder_weights=encoder_weights.h5\n\n```", "```py\npython3 policygradient-car-10.1.1.py\n--encoder_weights=encoder_weights.h5 -b\n\n```", "```py\npython3 policygradient-car-10.1.1.py\n--encoder_weights=encoder_weights.h5 -a\n\n```", "```py\npython3 policygradient-car-10.1.1.py\n--encoder_weights=encoder_weights.h5 -c\n\n```", "```py\npython3 policygradient-car-10.1.1.py\n--encoder_weights=encoder_weights.h5\n--actor_weights=actor_weights.h5 --train\n\n```", "```py\npython3 policygradient-car-10.1.1.py\n--encoder_weights=encoder_weights.h5\n--actor_weights=actor_weights.h5\n--value_weights=value_weights.h5 -b --train\n\n```", "```py\npython3 policygradient-car-10.1.1.py\n--encoder_weights=encoder_weights.h5\n--actor_weights=actor_weights.h5\n--value_weights=value_weights.h5 -a --train\n\n```", "```py\npython3 policygradient-car-10.1.1.py\n--encoder_weights=encoder_weights.h5\n--actor_weights=actor_weights.h5\n--value_weights=value_weights.h5 -c --train\n\n```"]