<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">PyTorch Experiments on NLP and RNN</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we are going to deep dive into the PyTorch library on <strong>natural language processing</strong> (<strong>NLP</strong>) and other experiments. Then, we will convert the developed model into a format that can be used in an Android or iOS application using TensorFlow and CoreML.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Introduction to PyTorch features and installation</li>
<li>Using variables in PyTorch</li>
<li>Building our own model network</li>
<li>Classifying <strong>recurrent neural networks</strong> (<strong>RNN</strong>)</li>
<li>Natural language processing</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">PyTorch</h1>
                </header>
            
            <article>
                
<p>PyTorch is a Python-based library that's used to perform scientific computing operations with GPUs. It helps by performing faster experimentation to run production-grade ecosystems and distribute the training of libraries. It also provides two high-level features: tensor computations and building neural networks on tape-based autograd systems. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The features of PyTorch</h1>
                </header>
            
            <article>
                
<p>PyTorch provides an end-to-end deep learning system. Its features are as follows:</p>
<ul>
<li class="mce-root"><strong>Python utilization</strong>: PyTorch is not simply a Python binding to a C++ framework. It is deeply integrated in Python so that it can be used with other <span>popular</span><span> </span>libraries and frameworks.</li>
</ul>
<ul>
<li class="mce-root"><strong>Tools and libraries</strong>: It has an active community of researchers and developers in the areas of computer vision and reinforcement learning.</li>
<li><strong>Flexible frontend</strong>: This includes ease of use and hybrid in eager mode, accelerate speeds and seamless transitions to graph mode, and functionality and optimization in C++ runtime.</li>
<li class="mce-root"><strong>Cloud support</strong>: This is supported on all of the major cloud platforms, allowing for seamless development and scaling with prebuilt images, so that it is able to run as a production-grade application.</li>
<li class="mce-root"><strong>Distributed training</strong>: This includes performance optimization with the advantage of native support for the asynchronous execution of operations and peer-to-peer (p2p) communications, so that we can access both C++ and Python.</li>
<li class="mce-root"><strong>Native support for ONNX</strong><span>: We can export models into the standard</span> <strong>Open Neural Network Exchange</strong> <span>(</span><strong>ONNX</strong><span>) format for access to other platforms, runtimes, and visualizers.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing PyTorch</h1>
                </header>
            
            <article>
                
<p>There is a stable version of PyTorch available at the time of writing this book, that is, 1.0. There is also a nightly preview build available if you want to have a hands-on look at the latest code repository. You need to have the dependencies installed based on your package manager. <strong>Anaconda</strong> is the recommended package manager, and it installs all the dependencies automatically. <strong>LibTorch</strong> is only available for C++. Here is a grid showing the installation options that are available for installing PyTorch:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-782 image-border" src="assets/17ee8f45-5596-405d-9255-31d5f2fc0fd4.png" style="width:141.00em;height:35.33em;"/></p>
<p>The preceding screenshot specifies the package grid that was used while this book was being written. You can pick any package grid as per your hardware configuration availability. </p>
<p>To install PyTorch, and to start Jupyter Notebook, run the following command:</p>
<pre class="p1"><span class="s1">python --version<br/></span><span class="s1">sudo brew install python3<br/></span><span class="s1">brew install python3<br/></span><span class="s1">pip3 install --upgrade pip<br/></span><span class="s1">pip3 install jupyter<br/></span><span class="s1">jupyter notebook</span></pre>
<p>The installation of PyTorch is shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-783 image-border" src="assets/c6d1e025-b6a4-4a84-b77c-fc04da37914e.png" style="width:123.83em;height:80.83em;"/></p>
<p>When you initiate the Jupyter Notebook, a new browser session opens up with an empty notebook, as shown here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-784 image-border" src="assets/63b602b7-9b98-4f82-b22f-ed7fe087acb3.png" style="width:126.00em;height:44.33em;"/></p>
<p>Let's look at the basics of PyTorch.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">PyTorch basics</h1>
                </header>
            
            <article>
                
<p>Now that PyTorch has been installed, we can start experimenting with it. We will start with <kbd>torch</kbd> and <kbd>numpy</kbd>.</p>
<p>From the top menu, create a new notebook and include the following code:</p>
<pre class="mce-root"># first basic understanding on PyTorch <br/># book: AI for Mobile application projects<br/><br/>import torch<br/>import numpy as np<br/><br/># convert numpy to tensor or vise versa<br/>numpy_data = np.arange(8).reshape((2, 4))<br/>torch_data = torch.from_numpy(numpy_data)<br/>#convert tensor to array<br/>tensor2array = torch_data.numpy()<br/><br/>#Print the results<br/>print<br/>(<br/> '\nnumpy array:', numpy_data, # [[0 1 2 3], [4 5 6 7]]<br/> '\ntorch tensor:', torch_data, # 0 1 2 3\n 4 5 6 7 [torch.LongTensor of size 2x3]<br/> '\ntensor to array:', tensor2array, # [[0 1 2 3], [4 5 6 7]]<br/>)</pre>
<p>Now, let's do some mathematical operations:</p>
<pre># abs method on numpy<br/>numpy_data = [-1, -2, 1, 2]<br/>tensor = torch.FloatTensor(numpy_data) # 32-bit floating point<br/><br/>#print the results<br/>print<br/>(<br/> '\nabs',<br/> '\nnumpy: ', np.abs(numpy_data), # [1 2 1 2]<br/> '\ntorch: ', torch.abs(tensor) # [1 2 1 2]<br/>)<br/><br/># sin method on numpy<br/>#print the results<br/>print<br/>(<br/> '\nsin',<br/> '\nnumpy: ', np.sin(numpy_data), # [-0.84147098 -0.90929743 0.84147098 0.90929743]<br/> '\ntorch: ', torch.sin(tensor) # [-0.8415 -0.9093 0.8415 0.9093]<br/>)</pre>
<p>Let's calculate the mean method and print the results:</p>
<pre><br/>#print the results<br/>print<br/>(<br/> '\nmean',<br/> '\nnumpy: ', np.mean(data), # 0.0<br/> '\ntorch: ', torch.mean(tensor) # 0.0<br/>)<br/><br/># matrix multiplication with numpy<br/>numpy_data = [[1,2], [3,4]]<br/>tensor = torch.FloatTensor(numpy_data) # 32-bit floating point<br/># correct method and print the results<br/>print(<br/> '\nmatrix multiplication (matmul)',<br/> '\nnumpy: ', np.matmul(numpy_data, numpy_data), # [[7, 10], [15, 22]]<br/> '\ntorch: ', torch.mm(tensor, tensor) # [[7, 10], [15, 22]]<br/>)</pre>
<p class="mce-root"/>
<p>The following code shows the output of the mathematical operations:</p>
<pre>numpy array: [[0 1 2 3]
 [4 5 6 7]] 
torch tensor: tensor([[0, 1, 2, 3],
        [4, 5, 6, 7]]) 
tensor to array: [[0 1 2 3]
 [4 5 6 7]]

abs 
numpy:  [1 2 1 2] 
torch:  tensor([1., 2., 1., 2.])

sin 
numpy:  [-0.84147098 -0.90929743  0.84147098  0.90929743] 
torch:  tensor([-0.8415, -0.9093,  0.8415,  0.9093])

mean 
numpy:  0.0 
torch:  tensor(0.)

matrix multiplication (matmul) 
numpy:  [[ 7 10]
 [15 22]] 
torch:  tensor([[ 7., 10.],
        [15., 22.]])</pre>
<p>Now, let's look at how to use different variables in PyTorch.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using variables in PyTorch</h1>
                </header>
            
            <article>
                
<p>Variables in <kbd>torch</kbd> are used to build a computational graph. Whenever a variable is calculated, it builds a computational graph. This computational graph is used to connect all the calculation steps (nodes), and finally, when the error is reversed, the modification range (gradient) in all the variables is calculated at once. In comparison, <kbd>tensor</kbd> does not have this ability. We will look into this difference with a simple example:</p>
<pre>import torch<br/>from torch.autograd import Variable<br/><br/># Variable in torch is to build a computational graph,<br/># So torch does not have placeholder, torch can just pass variable to the computational graph.<br/><br/>tensor = torch.FloatTensor([[1,2,3],[4,5,6]]) # build a tensor<br/>variable = Variable(tensor, requires_grad=True) # build a variable, usually for compute gradients<br/><br/>print(tensor) # [torch.FloatTensor of size 2x3]<br/>print(variable) # [torch.FloatTensor of size 2x3]<br/><br/># till now the tensor and variable looks similar.<br/># However, the variable is a part of the graph, it's a part of the auto-gradient.<br/><br/>#Now we will calculate the mean value on tensor(X^2)<br/>t_out = torch.mean(tensor*tensor)<br/><br/>#Now we will calculate the mean value on variable(X^2)<br/>v_out = torch.mean(variable*variable) </pre>
<p>Now, we will be printing the results for all parameters:</p>
<pre><br/>#print the results<br/>print(t_out)<br/>print(v_out) <br/>#result will be 7.5<br/><br/>v_out.backward() # backpropagation from v_out<br/># v_out = 1/4 * sum(variable*variable)<br/># the gradients with respect to the variable, <br/><br/>#Let's print the variable gradient<br/><br/>print(variable.grad)<br/>'''<br/> 0.5000 1.0000<br/> 1.5000 2.0000<br/>'''<br/><br/>print("Resultant data in the variable: "+str(variable)) # this is data in variable<br/><br/>"""<br/>Variable containing:<br/> 1 2<br/> 3 4<br/>We will consider the variable as a FloatTensor<br/>[torch.FloatTensor of size 2x2]<br/>"""<br/><br/>print(variable.data) # this is data in tensor format<br/>"""<br/> 1 2<br/> 3 4<br/>We will consider the variable as FloatTensor<br/>[torch.FloatTensor of size 2x2]<br/>"""<br/><br/>#we will print the result in the numpy format<br/>print(variable.data.numpy()) <br/>"""<br/>[[ 1. 2.]<br/> [ 3. 4.]]<br/>"""</pre>
<p>Here is the output of the preceding code block:</p>
<pre>tensor([[1., 2., 3.],
        [4., 5., 6.]])
tensor([[1., 2., 3.],
        [4., 5., 6.]], requires_grad=True)
tensor(15.1667)
tensor(15.1667, grad_fn=&lt;MeanBackward1&gt;)
tensor([[0.3333, 0.6667, 1.0000],
        [1.3333, 1.6667, 2.0000]])
Data in the variabletensor([[1., 2., 3.],
        [4., 5., 6.]], requires_grad=True)
tensor([[1., 2., 3.],
        [4., 5., 6.]])
[[1. 2. 3.]
 [4. 5. 6.]]</pre>
<p>Now, let's try plotting data on a graph using <kbd>matplotlib</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Plotting values on a graph</h1>
                </header>
            
            <article>
                
<p>Let's work on one simple program to plot values on a graph. To do this, use the following code:</p>
<pre>#This line is necessary to print the output inside jupyter notebook<br/>%matplotlib inline<br/><br/>import torch<br/>import matplotlib.pyplot as plt<br/>import torch.nn.functional as F<br/>from torch.autograd import Variable<br/><br/><br/># dummy data for the example<br/>#lets declare linspace<br/>x = torch.linspace(-5, 5, 200) # x data (tensor), shape=(100, 1)<br/>x = Variable(x)<br/>#call numpy array to plot the results <br/>x_np = x.data.numpy() </pre>
<p>Following code block lists down a few of the activation methods:</p>
<pre><br/>#RelU function<br/>y_relu = torch.relu(x).data.numpy()<br/>#sigmoid method<br/>y_sigmoid = torch.sigmoid(x).data.numpy()<br/>#tanh method<br/>y_tanh = torch.tanh(x).data.numpy()<br/>#softplus method<br/>y_softplus = F.softplus(x).data.numpy() # there's no softplus in torch<br/># y_softmax = torch.softmax(x, dim=0).data.numpy() softmax is an activation function and it deals with probability</pre>
<p>Using <kbd>matplotlib</kbd> to activate the functions:</p>
<pre><br/>#we will plot the activation function with matplotlib<br/>plt.figure(1, figsize=(8, 6))<br/>plt.subplot(221)<br/>plt.plot(x_np, y_relu, c='red', label='relu')<br/>plt.ylim((-1, 5))<br/>plt.legend(loc='best')<br/><br/>#sigmoid activation function<br/>plt.subplot(222)<br/>plt.plot(x_np, y_sigmoid, c='red', label='sigmoid')<br/>plt.ylim((-0.2, 1.2))<br/>plt.legend(loc='best')<br/><br/>#tanh activation function<br/>plt.subplot(223)<br/>plt.plot(x_np, y_tanh, c='red', label='tanh')<br/>plt.ylim((-1.2, 1.2))<br/>plt.legend(loc='best')<br/><br/>#softplus activation function<br/>plt.subplot(224)<br/>plt.plot(x_np, y_softplus, c='red', label='softplus')<br/>plt.ylim((-0.2, 6))<br/>plt.legend(loc='best')<br/><br/>#call the show method to draw the graph on screen<br/>plt.show()</pre>
<p>Let's plot the values on the graph, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-785 image-border" src="assets/8b0e9e83-cf7f-4146-b058-5e9f304ce5e4.png" style="width:52.00em;height:38.42em;"/></p>
<p><span>Note that the first line in the preceding code is required to draw the graph inside Jupyter </span>Notebook. If you are running the Python file directly from the Terminal, you can omit the first line of the code.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building our own model network</h1>
                </header>
            
            <article>
                
<p>In this section, we will work on building our own network using PyTorch with a step-by-step example.</p>
<p>Let's begin by looking at linear regression as a starting point.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear regression</h1>
                </header>
            
            <article>
                
<p>Linear regression is probably the first method that anyone will learn in terms of machine learning. The objective of linear regression is<span> to find a relationship between one or more features (independent variables) and a continuous target variable (the dependent variabl</span>e), which can be seen in the following code.</p>
<p>Import all the necessary libraries and declare all the necessary variables:</p>
<pre>%matplotlib inline<br/><br/>#Import all the necessary libraries<br/>import torch<br/>import torch.nn.functional as F<br/>import matplotlib.pyplot as plt<br/><br/><br/>#we will define data points for both x-axis and y-axis<br/># x data (tensor), shape=(100, 1)<br/>x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1) <br/># noisy y data (tensor), shape=(100, 1)<br/>y = x.pow(2) + 0.2*torch.rand(x.size()) <br/><br/># torch can only train on Variable, so convert them to Variable<br/># x, y = Variable(x), Variable(y)<br/><br/># plt.scatter(x.data.numpy(), y.data.numpy())<br/># plt.show()</pre>
<p>We will define the linear regression class and run a simple <kbd>nn</kbd> to explain regression: </p>
<pre><br/>class Net(torch.nn.Module):<br/> def __init__(self, n_feature, n_hidden, n_output):<br/> super(Net, self).__init__()<br/> self.hidden = torch.nn.Linear(n_feature, n_hidden) # hidden layer<br/> self.predict = torch.nn.Linear(n_hidden, n_output) # output layer<br/><br/>def forward(self, x):<br/> x = F.relu(self.hidden(x)) # activation function for hidden layer<br/> x = self.predict(x) # linear output<br/> return x<br/><br/>net = Net(n_feature=1, n_hidden=10, n_output=1) # define the network<br/>print(net) # net architecture<br/><br/>optimizer = torch.optim.SGD(net.parameters(), lr=0.2)<br/>loss_func = torch.nn.MSELoss() # this is for regression mean squared loss<br/><br/>plt.ion() # something about plotting<br/><br/>for t in range(200):<br/> prediction = net(x) # input x and predict based on x<br/> loss = loss_func(prediction, y) # must be (1. nn output, 2. target)<br/> optimizer.zero_grad() # clear gradients for next train<br/> loss.backward() # backpropagation, compute gradients<br/> optimizer.step() # apply gradients<br/> if t % 50 == 0:</pre>
<p>Now we will see how to plot the graphs and display the process of learning:</p>
<pre>     plt.cla()<br/>     plt.scatter(x.data.numpy(), y.data.numpy())<br/>     plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)<br/>     plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color': 'black'})<br/>     plt.pause(0.1)<br/><br/>plt.ioff()<br/>plt.show()</pre>
<p>Let's plot the output of this code on the graph, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-786 image-border" src="assets/61351240-2e18-4920-986a-c70d07c5c39a.png" style="width:28.50em;height:35.08em;"/></p>
<p>The final plot looks as follows, with the loss (meaning <span>the deviation between the predicted output and the actual output)</span> equaling 0.01:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-787 image-border" src="assets/2f453783-0ea5-4706-b70b-d9108fe28426.png" style="width:28.08em;height:35.25em;"/></p>
<p>Now, we will start working toward deeper use cases using PyTorch.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classification</h1>
                </header>
            
            <article>
                
<p>A classification problem runs a neural network model to classify the inputs. For example, it classifies images of clothing into trousers, tops, and shirts. When we provide more inputs to the classification model, it will predict the value of the outcomes. </p>
<p>A simple example would be filtering an email as <em>spam</em> or <em>not spam</em>. <span>Classification either predicts categorical class labels based on the training set or the values (class labels) when classifying attributes that are used in classifying new data. There are many classification models, such as Naive Bayes, random forests, decision tress, and logistic regression.</span></p>
<p>Here, we will work on a simple classification problem. To do this, use the following this code:</p>
<pre class="mce-root">%matplotlib inline<br/><br/>import torch<br/>import torch.nn.functional as F<br/>import matplotlib.pyplot as plt<br/><br/># torch.manual_seed(1) # reproducible<br/><br/># make fake data<br/>n_data = torch.ones(100, 2)<br/>x0 = torch.normal(2*n_data, 1) # class0 x data (tensor), shape=(100, 2)<br/>y0 = torch.zeros(100) # class0 y data (tensor), shape=(100, 1)<br/>x1 = torch.normal(-2*n_data, 1) # class1 x data (tensor), shape=(100, 2)<br/>y1 = torch.ones(100) # class1 y data (tensor), shape=(100, 1)<br/>x = torch.cat((x0, x1), 0).type(torch.FloatTensor) # shape (200, 2) FloatTensor = 32-bit floating<br/>y = torch.cat((y0, y1), ).type(torch.LongTensor) # shape (200,) LongTensor = 64-bit integer<br/><br/>class Net(torch.nn.Module):<br/> def __init__(self, n_feature, n_hidden, n_output):<br/> super(Net, self).__init__()<br/> self.hidden = torch.nn.Linear(n_feature, n_hidden) # hidden layer<br/> self.out = torch.nn.Linear(n_hidden, n_output) # output layer<br/><br/>def forward(self, x):<br/> x = F.relu(self.hidden(x)) # activation function for hidden layer<br/> x = self.out(x)<br/> return x<br/><br/>net = Net(n_feature=2, n_hidden=10, n_output=2) # define the network<br/>print(net) # net architecture<br/><br/>optimizer = torch.optim.SGD(net.parameters(), lr=0.02)<br/>loss_func = torch.nn.CrossEntropyLoss() # the target label is NOT an one-hotted<br/><br/>plt.ion() # something about plotting<br/><br/>for t in range(100):<br/> out = net(x) # input x and predict based on x<br/> loss = loss_func(out, y) # must be (1. nn output, 2. target), the target label is NOT one-hotted<br/><br/>optimizer.zero_grad() # clear gradients for next train<br/> loss.backward() # backpropagation, compute gradients<br/> optimizer.step() # apply gradients<br/><br/>if t % 10 == 0:</pre>
<p class="mce-root">Now, let's plot the graphs and display the learning processes:</p>
<pre class="mce-root"> plt.cla()<br/> prediction = torch.max(out, 1)[1]<br/> pred_y = prediction.data.numpy()<br/> target_y = y.data.numpy()<br/> plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=pred_y, s=100, lw=0, cmap='RdYlGn')<br/> accuracy = float((pred_y == target_y).astype(int).sum()) / float(target_y.size)<br/> plt.text(1.5, -4, 'Accuracy=%.2f' % accuracy, fontdict={'size': 20, 'color': 'red'})<br/> plt.pause(0.1)<br/><br/>plt.ioff()<br/>plt.show()</pre>
<p>The output of the preceding code is as follows: </p>
<pre>Net(
  (hidden): Linear(in_features=2, out_features=10, bias=True)
  (out): Linear(in_features=10, out_features=2, bias=True)
)<br/><br/></pre>
<p>We will pick only a few plots from the output, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-788 image-border" src="assets/521a242d-fe44-4f20-ab09-3e8314c22e47.png" style="width:26.67em;height:32.42em;"/></p>
<p>You can see that the accuracy levels have increased with the increased number of steps in the iteration:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-789 image-border" src="assets/6356bef0-a6b8-4200-9586-b482aedd91a3.png" style="width:27.50em;height:34.00em;"/></p>
<p>We can reach an accuracy level of 1.00 in the final step of our execution:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-790 image-border" src="assets/eab78e53-d218-4ff3-b0bf-898dc7358898.png" style="width:25.25em;height:31.33em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Simple neural networks with torch</h1>
                </header>
            
            <article>
                
<p>Neural networks are necessary when<span> a heuristic approach is required to solve a problem. </span>Let's explore a basic neural network using the following example: </p>
<pre>import torch<br/>import torch.nn.functional as F<br/><br/># replace following class code with an easy sequential network<br/>class Net(torch.nn.Module):<br/> def __init__(self, n_feature, n_hidden, n_output):<br/> super(Net, self).__init__()<br/> self.hidden = torch.nn.Linear(n_feature, n_hidden) # hidden layer<br/> self.predict = torch.nn.Linear(n_hidden, n_output) # output layer<br/><br/>def forward(self, x):<br/> x = F.relu(self.hidden(x)) # activation function for hidden layer<br/> x = self.predict(x) # linear output<br/> return x<br/><br/>net1 = Net(1, 10, 1)</pre>
<p>Following is the easiest and fastest way to build your network:</p>
<pre> net2 = torch.nn.Sequential(<br/> torch.nn.Linear(1, 10),<br/> torch.nn.ReLU(),<br/> torch.nn.Linear(10, 1)<br/>)<br/><br/>print(net1) # net1 architecture<br/>"""<br/>Net (<br/> (hidden): Linear (1 -&gt; 10)<br/> (predict): Linear (10 -&gt; 1)<br/>)<br/>"""<br/><br/>print(net2) # net2 architecture<br/>"""<br/>Sequential (<br/> (0): Linear (1 -&gt; 10)<br/> (1): ReLU ()<br/> (2): Linear (10 -&gt; 1)<br/>)<br/>"""</pre>
<p>The output of the preceding code is as follows:</p>
<div class="output_area">
<div class="output_subarea output_text output_stream output_stdout">
<pre>Net(
  (hidden): Linear(in_features=1, out_features=10, bias=True)
  (predict): Linear(in_features=10, out_features=1, bias=True)
)
Sequential(
  (0): Linear(in_features=1, out_features=10, bias=True)
  (1): ReLU()
  (2): Linear(in_features=10, out_features=1, bias=True)
)<br/><br/>Out[1]:<br/><span>'\nSequential (\n  (0): Linear (1 -&gt; 10)\n  (1): ReLU ()\n  (2): Linear (10 -&gt; 1)\n)\n'</span></pre></div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Saving and reloading data on the network</h1>
                </header>
            
            <article>
                
<p>Let's look at one example of how to save data on the network and then restore the data:</p>
<pre>%matplotlib inline<br/><br/>import torch<br/>import matplotlib.pyplot as plt<br/><br/># torch.manual_seed(1) # reproducible<br/><br/># fake data<br/>x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1) # x data (tensor), shape=(100, 1)<br/>y = x.pow(2) + 0.2*torch.rand(x.size()) # noisy y data (tensor), shape=(100, 1)<br/><br/># The code below is deprecated in Pytorch 0.4. Now, autograd directly supports tensors<br/># x, y = Variable(x, requires_grad=False), Variable(y, requires_grad=False)<br/><br/>def save():<br/> # save net1<br/> net1 = torch.nn.Sequential(<br/> torch.nn.Linear(1, 10),<br/> torch.nn.ReLU(),<br/> torch.nn.Linear(10, 1)<br/> )<br/> optimizer = torch.optim.SGD(net1.parameters(), lr=0.5)<br/> loss_func = torch.nn.MSELoss()<br/><br/>for t in range(100):<br/> prediction = net1(x)<br/> loss = loss_func(prediction, y)<br/> optimizer.zero_grad()<br/> loss.backward()<br/> optimizer.step()<br/><br/># plot result<br/> plt.figure(1, figsize=(10, 3))<br/> plt.subplot(131)<br/> plt.title('Net1')<br/> plt.scatter(x.data.numpy(), y.data.numpy())<br/> plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Two ways to save the net:</p>
<pre> torch.save(net1, 'net.pkl') # save entire net<br/> torch.save(net1.state_dict(), 'net_params.pkl') # save only the parameters<br/><br/>def restore_net():<br/> # restore entire net1 to net2<br/> net2 = torch.load('net.pkl')<br/> prediction = net2(x)<br/><br/># plot result<br/> plt.subplot(132)<br/> plt.title('Net2')<br/> plt.scatter(x.data.numpy(), y.data.numpy())<br/> plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)<br/><br/><br/>def restore_params():<br/> # restore only the parameters in net1 to net3<br/> net3 = torch.nn.Sequential(<br/> torch.nn.Linear(1, 10),<br/> torch.nn.ReLU(),<br/> torch.nn.Linear(10, 1)<br/> )<br/><br/># copy net1's parameters into net3<br/> net3.load_state_dict(torch.load('net_params.pkl'))<br/> prediction = net3(x)</pre>
<p>Plotting the results:</p>
<pre># plot result<br/> plt.subplot(133)<br/> plt.title('Net3')<br/> plt.scatter(x.data.numpy(), y.data.numpy())<br/> plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)<br/> plt.show()<br/><br/># save net1<br/>save()<br/><br/># restore entire net (may slow)<br/>restore_net()<br/><br/># restore only the net parameters<br/>restore_params()</pre>
<p>The output of the code will look similar to the graphs that are shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-791 image-border" src="assets/221a7acc-e365-426a-ab76-8f37e4ffff1f.png" style="width:53.00em;height:19.08em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running with batches</h1>
                </header>
            
            <article>
                
<p>Torch helps you organize your data through <kbd>DataLoader</kbd>. We can use it to package the data through batch training. We can have our own data format <span>(NumPy array, for example, or any other) </span>loaded into Tensor, along with a wrapper.</p>
<p>The following is an example of a dataset where random numbers are taken into the dataset in batches and trained:</p>
<pre>import torch<br/>import torch.utils.data as Data<br/><br/>torch.manual_seed(1) # reproducible<br/><br/>BATCH_SIZE = 5<br/><br/>x = torch.linspace(1, 10, 10) # this is x data (torch tensor)<br/>y = torch.linspace(10, 1, 10) # this is y data (torch tensor)<br/><br/>torch_dataset = Data.TensorDataset(x, y)<br/>loader = Data.DataLoader(<br/> dataset=torch_dataset, # torch TensorDataset format<br/> batch_size=BATCH_SIZE, # mini batch size<br/> shuffle=True, # random shuffle for training<br/> num_workers=2, # subprocesses for loading data<br/>)<br/><br/>def show_batch():<br/> for epoch in range(3): # train entire dataset 3 times<br/> for step, (batch_x, batch_y) in enumerate(loader): # for each training step<br/> # train your data...<br/> print('Epoch: ', epoch, '| Step: ', step, '| batch x: ',<br/> batch_x.numpy(), '| batch y: ', batch_y.numpy())<br/><br/>if __name__ == '__main__':<br/> show_batch()</pre>
<p>The output of the code is as follows:</p>
<pre>Epoch:  0 | Step:  0 | batch x:  [ 5.  7. 10.  3.  4.] | batch y:  [6. 4. 1. 8. 7.]
Epoch:  0 | Step:  1 | batch x:  [2. 1. 8. 9. 6.] | batch y:  [ 9. 10.  3.  2.  5.]
Epoch:  1 | Step:  0 | batch x:  [ 4.  6.  7. 10.  8.] | batch y:  [7. 5. 4. 1. 3.]
Epoch:  1 | Step:  1 | batch x:  [5. 3. 2. 1. 9.] | batch y:  [ 6.  8.  9. 10.  2.]
Epoch:  2 | Step:  0 | batch x:  [ 4.  2.  5.  6. 10.] | batch y:  [7. 9. 6. 5. 1.]
Epoch:  2 | Step:  1 | batch x:  [3. 9. 1. 8. 7.] | batch y:  [ 8.  2. 10.  3.  4.]</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimization algorithms</h1>
                </header>
            
            <article>
                
<p>There is always doubt about which optimization algorithm should be used in our implementation of the neural network for a better output. This is done by modifying the key parameters, such as the <strong>weights </strong>and <strong>bias </strong>values.</p>
<p>These algorithms are used to minimize (or maximize) error (<em>E</em>(<em>x</em>)), which is dependent on the internal parameters. They are used for computing the target results (<em>Y</em>) from the set of predictors (<em>x</em>) that are used in the model.</p>
<p>Now, let's look at the different types of algorithms by using the following example:</p>
<pre>%matplotlib inline<br/><br/>import torch<br/>import torch.utils.data as Data<br/>import torch.nn.functional as F<br/>import matplotlib.pyplot as plt<br/><br/># torch.manual_seed(1) # reproducible<br/><br/>LR = 0.01<br/>BATCH_SIZE = 32<br/>EPOCH = 12<br/><br/># dummy dataset<br/>x = torch.unsqueeze(torch.linspace(-1, 1, 1000), dim=1)<br/>y = x.pow(2) + 0.1*torch.normal(torch.zeros(*x.size()))<br/><br/># plot dataset<br/>plt.scatter(x.numpy(), y.numpy())<br/>plt.show()</pre>
<p>Putting dateset into torch dataset:</p>
<pre>torch_dataset = Data.TensorDataset(x, y)<br/>loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)<br/><br/># default network<br/>class Net(torch.nn.Module):<br/> def __init__(self):<br/> super(Net, self).__init__()<br/> self.hidden = torch.nn.Linear(1, 20) # hidden layer<br/> self.predict = torch.nn.Linear(20, 1) # output layer<br/><br/>def forward(self, x):<br/> x = F.relu(self.hidden(x)) # activation function for hidden layer<br/> x = self.predict(x) # linear output<br/> return x<br/><br/>if __name__ == '__main__':<br/> # different nets<br/> net_SGD = Net()<br/> net_Momentum = Net()<br/> net_RMSprop = Net()<br/> net_Adam = Net()<br/> nets = [net_SGD, net_Momentum, net_RMSprop, net_Adam]<br/><br/># different optimizers<br/> opt_SGD = torch.optim.SGD(net_SGD.parameters(), lr=LR)<br/> opt_Momentum = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8)<br/> opt_RMSprop = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)<br/> opt_Adam = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))<br/> optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]<br/><br/>loss_func = torch.nn.MSELoss()<br/> losses_his = [[], [], [], []] # record loss</pre>
<p>Training the model for various epochs:</p>
<pre><br/> for epoch in range(EPOCH):<br/> print('Epoch: ', epoch)<br/> for step, (b_x, b_y) in enumerate(loader): # for each training step<br/> for net, opt, l_his in zip(nets, optimizers, losses_his):<br/> output = net(b_x) # get output for every net<br/> loss = loss_func(output, b_y) # compute loss for every net<br/> opt.zero_grad() # clear gradients for next train<br/> loss.backward() # backpropagation, compute gradients<br/> opt.step() # apply gradients<br/> l_his.append(loss.data.numpy()) # loss recoder<br/><br/>labels = ['SGD', 'Momentum', 'RMSprop', 'Adam']<br/> for i, l_his in enumerate(losses_his):<br/> plt.plot(l_his, label=labels[i])<br/> plt.legend(loc='best')<br/> plt.xlabel('Steps')<br/> plt.ylabel('Loss')<br/> plt.ylim((0, 0.2))<br/> plt.show()</pre>
<p>The output of executing the preceding code block is displayed in the following plot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-792 image-border" src="assets/94766868-218f-469f-bf52-4d762dae6b47.png" style="width:37.25em;height:25.42em;"/></p>
<p>The output of the Epoch count will look like this:</p>
<pre>Epoch: 0
Epoch:  1
Epoch:  2
Epoch:  3
Epoch:  4
Epoch:  5
Epoch:  6
Epoch:  7
Epoch:  8
Epoch:  9
Epoch:  10
Epoch:  11</pre>
<p>We will plot all the optimizers and represent them in the graph, as follows: </p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-793 image-border" src="assets/747f9f2a-2f1a-4180-bc46-f6f3d32b9554.png" style="width:38.42em;height:26.17em;"/></p>
<p>In the next section, we will look at RNNs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recurrent neural networks</h1>
                </header>
            
            <article>
                
<p>With RNNs, unlike feedforward neural networks, we can use the internal memory to process inputs in a sequential manner. In RNN, the connection between nodes forms a directed graph along a temporal sequence. This helps in tasking the RNN with largely unsegmented and connected speech or character recognition. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The MNIST database</h1>
                </header>
            
            <article>
                
<p>The <strong>MNIST</strong> database consists of 60,000 handwritten digits. It also consists of a test dataset that's made up of 10,000 digits. While it is a subset of the NIST dataset, all the digits in this dataset are size-normalized and have been centered on a 28 x 28 pixels-sized image. Here, every pixel contains a value of 0-255 with its grayscale value.</p>
<div class="packt_infobox">The MNIST dataset can be found at <a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>.<br/>
<span>The NIST dataset can be found a</span> <a href="https://www.nist.gov/srd/nist-special-database-19">https://www.nist.gov/srd/nist-special-database-19</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">RNN classification</h1>
                </header>
            
            <article>
                
<p><span>Here, we will look at an example of how to build an RNN to identify handwritten numbers from the MNIST database:</span></p>
<pre>import torch<br/>from torch import nn<br/>import torchvision.datasets as dsets<br/>import torchvision.transforms as transforms<br/>import matplotlib.pyplot as plt<br/><br/># torch.manual_seed(1) # reproducible<br/><br/># Hyper Parameters<br/>EPOCH = 1 # train the training data n times, to save time, we just train 1 epoch<br/>BATCH_SIZE = 64<br/>TIME_STEP = 28 # rnn time step / image height<br/>INPUT_SIZE = 28 # rnn input size / image width<br/>LR = 0.01 # learning rate<br/>DOWNLOAD_MNIST = True # set to True if haven't download the data<br/><br/># Mnist digital dataset<br/>train_data = dsets.MNIST(<br/> root='./mnist/',<br/> train=True, # this is training data<br/> transform=transforms.ToTensor(), # Converts a PIL.Image or numpy.ndarray to<br/> # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]<br/> download=DOWNLOAD_MNIST, # download it if you don't have it<br/>)<br/><br/></pre>
<p>Plotting one example:</p>
<pre><br/>print(train_data.train_data.size()) # (60000, 28, 28)<br/>print(train_data.train_labels.size()) # (60000)<br/>plt.imshow(train_data.train_data[0].numpy(), cmap='gray')<br/>plt.title('%i' % train_data.train_labels[0])<br/>plt.show()<br/><br/># Data Loader for easy mini-batch return in training<br/>train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)</pre>
<p>Converting test data into Variable, pick 2000 samples to speed up testing:</p>
<pre><br/>test_data = dsets.MNIST(root='./mnist/', train=False, transform=transforms.ToTensor())<br/>test_x = test_data.test_data.type(torch.FloatTensor)[:2000]/255. # shape (2000, 28, 28) value in range(0,1)<br/>test_y = test_data.test_labels.numpy()[:2000] # covert to numpy array<br/><br/>class RNN(nn.Module):<br/> def __init__(self):<br/> super(RNN, self).__init__()<br/><br/>self.rnn = nn.LSTM( # if use nn.RNN(), it hardly learns<br/> input_size=INPUT_SIZE,<br/> hidden_size=64, # rnn hidden unit<br/> num_layers=1, # number of rnn layer<br/> batch_first=True, # input &amp; output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)<br/> )<br/><br/>self.out = nn.Linear(64, 10)<br/><br/>def forward(self, x):<br/> # x shape (batch, time_step, input_size)<br/> # r_out shape (batch, time_step, output_size)<br/> # h_n shape (n_layers, batch, hidden_size)<br/> # h_c shape (n_layers, batch, hidden_size)<br/> r_out, (h_n, h_c) = self.rnn(x, None) # None represents zero initial hidden state<br/><br/># choose r_out at the last time step<br/> out = self.out(r_out[:, -1, :])<br/> return out<br/><br/>rnn = RNN()<br/>print(rnn)<br/><br/>optimizer = torch.optim.Adam(rnn.parameters(), lr=LR) # optimize all cnn parameters<br/>loss_func = nn.CrossEntropyLoss() # the target label is not one-hotted<br/><br/></pre>
<p>Training and testing the epochs:</p>
<pre>for epoch in range(EPOCH):<br/> for step, (b_x, b_y) in enumerate(train_loader): # gives batch data<br/> b_x = b_x.view(-1, 28, 28) # reshape x to (batch, time_step, input_size)<br/><br/>output = rnn(b_x) # rnn output<br/> loss = loss_func(output, b_y) # cross entropy loss<br/> optimizer.zero_grad() # clear gradients for this training step<br/> loss.backward() # backpropagation, compute gradients<br/> optimizer.step() # apply gradients<br/><br/>if step % 50 == 0:<br/> test_output = rnn(test_x) # (samples, time_step, input_size)<br/> pred_y = torch.max(test_output, 1)[1].data.numpy()<br/> accuracy = float((pred_y == test_y).astype(int).sum()) / float(test_y.size)<br/> print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.2f' % accuracy)<br/><br/># print 10 predictions from test data<br/>test_output = rnn(test_x[:10].view(-1, 28, 28))<br/>pred_y = torch.max(test_output, 1)[1].data.numpy()<br/>print(pred_y, 'prediction number')<br/>print(test_y[:10], 'real number')</pre>
<p>The following files need to be downloaded and extracted to train the images:</p>
<div class="output_area">
<div class="output_subarea output_text output_stream output_stdout">
<pre>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist/MNIST/raw/train-images-idx3-ubyte.gz<br/>100.1%<br/>Extracting ./mnist/MNIST/raw/train-images-idx3-ubyte.gz<br/>Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz<span> to ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz<br/></span>113.5%<br/>Extracting ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz<br/>Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz<span> to ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz<br/></span>100.4%<br/>Extracting ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz<br/>Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz<span> to ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz<br/></span>180.4%<br/>Extracting ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz<br/>Processing...<br/>Done!<br/>torch.Size([60000, 28, 28])<br/>torch.Size([60000])<br/>/usr/local/lib/python3.7/site-packages/torchvision/datasets/mnist.py:53: UserWarning: train_data has been renamed data<br/>  warnings.warn("train_data has been renamed data")<br/>/usr/local/lib/python3.7/site-packages/torchvision/datasets/mnist.py:43: UserWarning: train_labels has been renamed targets<br/> warnings.warn("train_labels has been renamed targets")</pre>
<p>The output of the preceding code is as follows:</p>
</div>
</div>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-794 image-border" src="assets/b9e4d3d4-eb13-4231-9257-661e0a37dcf2.png" style="width:21.25em;height:21.92em;"/></p>
<p>Let's take the processing further with this code:</p>
<div class="cell code_cell rendered selected">
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="output_subarea output_text output_stream output_stderr">
<pre>/usr/local/lib/python3.7/site-packages/torchvision/datasets/mnist.py:58: UserWarning: test_data has been renamed data
  warnings.warn("test_data has been renamed data")
/usr/local/lib/python3.7/site-packages/torchvision/datasets/mnist.py:48: UserWarning: test_labels has been renamed targets
  warnings.warn("test_labels has been renamed targets")<br/><br/>RNN(<br/>  (rnn): LSTM(28, 64, batch_first=True)<br/>  (out): Linear(in_features=64, out_features=10, bias=True)<br/>)</pre>
<p>The output of epochs is as follows:</p>
</div>
</div>
<div class="output_area">
<div class="output_subarea output_text output_stream output_stdout">
<pre>Epoch:  0 | train loss: 2.3156 | test accuracy: 0.12
Epoch:  0 | train loss: 1.1875 | test accuracy: 0.57
Epoch:  0 | train loss: 0.7739 | test accuracy: 0.68
Epoch:  0 | train loss: 0.8689 | test accuracy: 0.73
Epoch:  0 | train loss: 0.5322 | test accuracy: 0.83
Epoch:  0 | train loss: 0.3657 | test accuracy: 0.83
Epoch:  0 | train loss: 0.2960 | test accuracy: 0.88
Epoch:  0 | train loss: 0.3869 | test accuracy: 0.90
Epoch:  0 | train loss: 0.1694 | test accuracy: 0.92
Epoch:  0 | train loss: 0.0869 | test accuracy: 0.93
Epoch:  0 | train loss: 0.2825 | test accuracy: 0.91
Epoch:  0 | train loss: 0.2392 | test accuracy: 0.94
Epoch:  0 | train loss: 0.0994 | test accuracy: 0.91
Epoch:  0 | train loss: 0.3731 | test accuracy: 0.94
Epoch:  0 | train loss: 0.0959 | test accuracy: 0.94
Epoch:  0 | train loss: 0.1991 | test accuracy: 0.95
Epoch:  0 | train loss: 0.0711 | test accuracy: 0.94
Epoch:  0 | train loss: 0.2882 | test accuracy: 0.96
Epoch:  0 | train loss: 0.4420 | test accuracy: 0.95
[7 2 1 0 4 1 4 9 5 9] prediction number
[7 2 1 0 4 1 4 9 5 9] real number</pre></div>
</div>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">RNN cyclic neural network – regression</h1>
                </header>
            
            <article>
                
<p class="mce-root">Now, we will deal with a regression problem under RNN. <span>The cyclic neural network provides memory to the neural network. For the serial data, the cyclic neural network can achieve better results</span>. We will use RNN here in this example to predict time series data.</p>
<div class="packt_tip">To find out more about circular neural networks, go to <a href="https://iopscience.iop.org/article/10.1209/0295-5075/18/3/003/meta">https://iopscience.iop.org/article/10.1209/0295-5075/18/3/003/meta</a>.</div>
<p class="mce-root"/>
<p>The following code is for the logistic regression:</p>
<pre class="CodeMirror-cursor">%matplotlib inline<br/><br/>import torch<br/>from torch import nn<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/><br/># torch.manual_seed(1) # reproducible<br/><br/># Hyper Parameters<br/>TIME_STEP = 10 # rnn time step<br/>INPUT_SIZE = 1 # rnn input size<br/>LR = 0.02 # learning rate<br/><br/># show data<br/>steps = np.linspace(0, np.pi*2, 100, dtype=np.float32) # float32 for converting torch FloatTensor<br/>x_np = np.sin(steps)<br/>y_np = np.cos(steps)<br/>plt.plot(steps, y_np, 'r-', label='target (cos)')<br/>plt.plot(steps, x_np, 'b-', label='input (sin)')<br/>plt.legend(loc='best')<br/>plt.show()<br/><br/></pre>
<p>The <kbd>RNN</kbd> class is defined in the following code. We will use <kbd>r_out</kbd> in a linear way to calculated the predicted output. We can also use a <kbd>for</kbd> loop to calculate the predicted output with <kbd>torch.stack</kbd>:</p>
<pre class="CodeMirror-cursor"><br/>class RNN(nn.Module):<br/> def __init__(self):<br/>     super(RNN, self).__init__()<br/><br/> self.rnn = nn.RNN(<br/> input_size=INPUT_SIZE,<br/> hidden_size=32, # rnn hidden unit<br/> num_layers=1, # number of rnn layer<br/> batch_first=True, # input &amp; output will have batch size as 1s dimension. e.g. (batch, time_step, input_size)<br/> )<br/> self.out = nn.Linear(32, 1)<br/><br/> def forward(self, x, h_state):<br/>     # x (batch, time_step, input_size)<br/>     # h_state (n_layers, batch, hidden_size)<br/>     # r_out (batch, time_step, hidden_size)<br/>     r_out, h_state = self.rnn(x, h_state)<br/><br/>     outs = [] # save all predictions<br/>     for time_step in range(r_out.size(1)):                                              outs.append(self.out(r_out[:, time_step, :]))<br/>     return torch.stack(outs, dim=1), h_state<br/>    <br/>//instantiate RNN<br/>rnn = RNN()<br/>print(rnn)</pre>
<p class="CodeMirror-cursor">The output is as follows:</p>
<pre class="CodeMirror-cursor">"""<br/>RNN (<br/> (rnn): RNN(1, 32, batch_first=True)<br/> (out): Linear (32 -&gt; 1)<br/>)<br/>"""<br/><br/></pre>
<p class="CodeMirror-cursor">We need to optimize RNN parameters now, as shown in the following code, before running the <kbd>for</kbd> loop to give the prediction:</p>
<pre class="CodeMirror-cursor">optimizer = torch.optim.Adam(rnn.parameters(), lr=LR) <br/>loss_func = nn.MSELoss()<br/>h_state = None<br/>plt.figure(1, figsize=(12, 5))<br/>plt.ion() </pre>
<p>The following block of code will look like a motion picture effect when it's run, which we can't represent here in this book. We have added a few screenshots to help you visualize this. We are using <kbd>x</kbd> as an input <kbd>sin</kbd> value and <kbd>y</kbd> as an output fitting <kbd>cos</kbd> value. Because a relationship exists between the two curves, we will use <kbd>sin</kbd> to predict <kbd>cos</kbd>:</p>
<pre class="CodeMirror-cursor">for step in range(100):<br/> start, end = step * np.pi, (step+1)*np.pi # time range<br/> # use sin predicts cos<br/> steps = np.linspace(start, end, TIME_STEP, dtype=np.float32, endpoint=False) # float32 for converting torch FloatTensor<br/> x_np = np.sin(steps)<br/> y_np = np.cos(steps)<br/><br/> x = torch.from_numpy(x_np[np.newaxis, :, np.newaxis]) # shape (batch, time_step, input_size)<br/> y = torch.from_numpy(y_np[np.newaxis, :, np.newaxis])<br/><br/> prediction, h_state = rnn(x, h_state) # rnn output<br/><br/>h_state = h_state.data # repack the hidden state, break the connection from last iteration<br/><br/> loss = loss_func(prediction, y) # calculate loss<br/> optimizer.zero_grad() # clear gradients for this training step<br/> loss.backward() # backpropagation, compute gradients<br/> optimizer.step() # apply gradients</pre>
<p class="CodeMirror-cursor">Plotting the results:</p>
<pre class="CodeMirror-cursor"><br/> plt.plot(steps, y_np.flatten(), 'r-')<br/> plt.plot(steps, prediction.data.numpy().flatten(), 'b-')<br/> plt.draw(); plt.pause(0.05)<br/><br/>plt.ioff()<br/>plt.show()</pre>
<p class="CodeMirror-code">The output of the preceding code is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-795 image-border" src="assets/30e1a235-cfa4-4e32-8513-e9ac707ebcf2.png" style="width:40.25em;height:28.67em;"/></p>
<p>The following is a plot graph that will be generated after iteration 10:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-796 image-border" src="assets/1e9db5f3-5b02-4c09-8269-9eb57193c681.png" style="width:48.42em;height:20.33em;"/></p>
<p>The following is a plot graph that will be generated after iteration 25:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-797 image-border" src="assets/8aa151a8-b47a-41ba-9b09-faa0635f0a87.png" style="width:29.92em;height:20.17em;"/></p>
<p>We are not showing all 100 iteration output images here, but we will skip to the final output, iteration 100, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-798 image-border" src="assets/93c8dff1-5c2e-4c1d-8efb-e6f16c330095.png" style="width:37.08em;height:24.58em;"/></p>
<p>In the next section, we will look at NLP.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Natural language processing </h1>
                </header>
            
            <article>
                
<p>Now is the time to experiment with a few NLP techniques with the help of PyTorch. This will be more useful for those of you who haven't written code in any deep learning framework before, but who may have better understanding of NLP core problems and algorithms.</p>
<p>In this chapter, we will look into simple examples with small dimensions, so that we can see how the weight of the layers changes as the network is training. You can try out your own model once you understand the network and how it works.</p>
<p>Before working on any NLP-based problems, we need to understand the basic building blocks on deep learning, including affine maps, non-linearities, and objective functions. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Affine maps</h1>
                </header>
            
            <article>
                
<p><strong>Affine maps</strong> are one of the basic building components of deep learning, and are represented as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9ac3ea96-07ab-425b-9c2b-2487fec88f51.png" style="width:7.00em;height:1.33em;"/></p>
<p><span>In this case, the matrix is represented by</span> <span class="math notranslate nohighlight"><span class="MathJax"><span class="math"><span><span class="mrow"><span class="mi">A</span></span></span></span></span></span><span> and the vectors are represented </span>by <em>x</em> and <em>b</em>. <em>A</em> and <em>b</em> are the <span class="math notranslate nohighlight"><span class="MathJax"><span class="MJX_Assistive_MathML">parameters that need to be learned, while</span></span></span><span> </span><em><span class="math notranslate nohighlight"><span class="MathJax"><span class="math"><span><span class="mrow"><span class="mi">b</span></span></span></span></span></span></em><span> is the bias. </span></p>
<p>A simple example to explain this is as follows:</p>
<pre>import torch<br/>import torch.nn as nn<br/>import torch.nn.functional as F<br/>import torch.optim as optim<br/><br/>torch.manual_seed(1)<br/>lin = nn.Linear(6, 3) # maps from R^6 to R^3, parameters A, b<br/># data is 2x5. A maps from 6 to 3... can we map "data" under A?<br/>data = torch.randn(2, 6)<br/>print(lin(data)</pre>
<p>After this, run the program with the following command:</p>
<pre><strong>$ python3 torch_nlp.py</strong></pre>
<p>The output will be as follows:</p>
<pre class="p1"><span class="s1">tensor([[ 1.1105, -0.1102, -0.3235],<br/></span><span class="s1"><span class="Apple-converted-space">        </span>[ 0.4800,<span class="Apple-converted-space">  </span>0.1633, -0.2515]], grad_fn=&lt;AddmmBackward&gt;)</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Non-linearities</h1>
                </header>
            
            <article>
                
<p>First, we need to identify why we need non-linearities. Consider, we have two affine maps: <kbd>f(x)=Ax+b</kbd> and <kbd>g(x)=Cx+d</kbd>. <kbd>f(g(x))</kbd> is shown in the following equation:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3c3d54a3-b248-4007-8b84-6f120f64839a.png" style="width:22.83em;height:1.42em;"/></p>
<p class="mce-root CDPAlignLeft CDPAlign">Here, we can see that when affine maps are composed together, the resultant is an affine map, where <em>Ad+b</em><span> is a vector and <em>AC</em> is a matrix.</span></p>
<p class="mce-root">We can identify neural networks as long chains of affine compositions. Previously, it was possible that non-linearities were introduced in-between the affine layers. But thankfully, it isn't the case any longer, and hence that helps in building more powerful and efficient models.</p>
<p class="mce-root">While working with the most common functions such as tanh (x), σ(x) and ReLU (x), we see that there are a few core non-linearities, as shown in the following code block:</p>
<pre>#let's see more about non-linearities<br/>#Most of the non-linearities in PyTorch are present in torch.functional which we import as F)<br/># Please make a note that unlike affine maps, there are mostly no parameters in non-linearites <br/># That is, they don't have weights that are updated during training.<br/>#This means that during training the weights are not updated.<br/>data = torch.randn(2, 2)<br/>print(data)<br/>print(F.relu(data))</pre>
<p>The output of the preceding code is as follows:</p>
<pre class="p1"><span class="s1">tensor([[ 0.5848, 0.2149],<br/> [-0.4090, -0.1663]])<br/>tensor([[0.5848, 0.2149],<br/> [0.0000, 0.0000]])</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Objective functions</h1>
                </header>
            
            <article>
                
<p>The objective function (also called the loss function or cost function) will help your network to minimize. It works by selecting a training instance, running it through your neural network, then computing the loss of the output.</p>
<p>The derivative of the loss function is updated for finding the parameters of the model. Like, if your model predicts an answer confidently, and the answer turns out to be wrong, the the computed loss will be high. If the predicted answer is correct, then the loss is low.</p>
<p>How is the network minimized?</p>
<ol>
<li>First, the function will select a training instance</li>
<li>Then, it is passed through our neural network to get the output</li>
<li>Finally, the loss of the output is calculated</li>
</ol>
<p>In our training examples we need to minimize the loss function to minimize the probability of wrong results with the actual dataset. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building network components in PyTorch</h1>
                </header>
            
            <article>
                
<p>Before shifting our focus to NLP, in this section we will use non-linearities and affine maps to build a network in PyTorch. In this example, we will learn to compute a loss function using the built in negative log likelihood in PyTorch and using backpropagation for updating the parmeters.</p>
<p>Please note that all the components of the network need to be inherited from <kbd>nn.Module</kbd> and also override the <kbd>forward()</kbd> method. Considering boilerplate, these are the details we should remember. The network components are provided functionality when we inherit those components from <kbd>nn.Module</kbd></p>
<p>Now, as mentioned previously, we will look at an example, in which the network takes a scattered bag-of-words (BoW) representation and and the output is a probability distribution into two labels, that is, English and Spanish. Also, this model is an example of logistic regression.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">BoW classifier using logistic regression</h1>
                </header>
            
            <article>
                
<p>Probabilities will be logged onto our two labels English and Spanish on which our generated model will map a sparse BoW representation. In the vocabulary, we will assign each word as an index. Let's say for example, we have two words in our vocabulary, that is hello and world, which have indices as zero and one, respectively. For example, <span>for the sentence </span><em>hello hello hello hello hello,</em> the BoW vector is<span> </span><em>[5,0]</em><span>. Similarly the BoW vector for <em>hello world world hello world</em> is <em>[2,3]</em>, and so on.</span></p>
<p>Generally, it is <em>[Count(hello),Count(world)].</em></p>
<p>Let us denote is BOW vector as <em>x.</em></p>
<p class="mce-root">The network output is as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8905bb53-f8c9-4a9a-9d76-58c0c236c43e.png" style="width:9.58em;height:1.25em;"/></p>
<p class="mce-root CDPAlignLeft CDPAlign">Next, we need to pass the input through an affine map and then use log softmax:</p>
<pre class="mce-root">data = [("El que lee mucho y anda mucho, ve mucho y sabe mucho".split(), "SPANISH"),<br/> ("The one who reads a lot and walks a lot, sees a lot and knows a lot.".split(), "ENGLISH"),<br/> ("Nunca es tarde si la dicha es buena".split(), "SPANISH"),<br/> ("It is never late if the joy is good".split(), "ENGLISH")]<br/><br/>test_data = [("Que cada palo aguante su vela".split(), "SPANISH"),<br/> ("May every mast hold its own sail".split(), "ENGLISH")]<br/><br/>#each word in the vocabulary is mapped to an unique integer using word_to_ix, and that will be considered as that word's index in BOW<br/><br/>word_to_ix = {}<br/>for sent, _ in data + test_data:<br/> for word in sent:<br/> if word not in word_to_ix:<br/> word_to_ix[word] = len(word_to_ix)<br/>print(word_to_ix)<br/><br/>VOCAB_SIZE = len(word_to_ix)<br/>NUM_LABELS = 2<br/><br/>class BoWClassifier(nn.Module): # inheriting from nn.Module!<br/><br/>def __init__(self, num_labels, vocab_size):<br/><br/>#This calls the init function of nn.Module. The syntax might confuse you, but don't be confused. Remember to do it in nn.module <br/><br/> super(BoWClassifier, self).__init__()</pre>
<p>Next, we will define the parameters that are needed. Here, those parameters are <kbd>A</kbd> and <kbd>B</kbd>, and the following code block explains the further implementations are required:</p>
<pre class="mce-root"> # let's look at the prarmeters required for affine mapping<br/> # nn.Linear() is defined using Torch that gives us the affine maps.<br/>#We need to ensure that we understand why the input dimension is vocab_size<br/> # num_labels is the output<br/> self.linear = nn.Linear(vocab_size, num_labels)<br/><br/># Important thing to remember: parameters are not present in the non-linearity log softmax. So, let's now think about that.<br/><br/>def forward(self, bow_vec):<br/> #first, the input is passed through the linear layer<br/> #then it is passed through log_softmax<br/> #torch.nn.functional contains other non-linearities and many other fuctions<br/><br/> return F.log_softmax(self.linear(bow_vec), dim=1)<br/><br/>def make_bow_vector(sentence, word_to_ix):<br/> vec = torch.zeros(len(word_to_ix))<br/> for word in sentence:<br/> vec[word_to_ix[word]] += 1<br/> return vec.view(1, -1)<br/><br/>def make_target(label, label_to_ix):<br/> return torch.LongTensor([label_to_ix[label]])<br/><br/>model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)</pre>
<p class="mce-root">Now, the model knows its own parameters. The first output is <kbd>A</kbd>, while the second is <kbd>B</kbd>, as follows:</p>
<pre class="mce-root">#A component is assigned to a class variable in the __init__ function<br/># of a module, which was done with the line<br/># self.linear = nn.Linear(...)<br/><br/># Then from the PyTorch devs, knowledge of the nn.linear's parameters #is stored by the module (here-BoW Classifier)<br/><br/>for param in model.parameters():<br/> print(param)<br/><br/><br/>#Pass a BoW vector for running the model<br/># the code is wrapped since we don't need to train it<br/>torch.no_grad()<br/>with torch.no_grad():<br/> sample = data[0]<br/> bow_vector = make_bow_vector(sample[0], word_to_ix)<br/> log_probs = model(bow_vector)<br/> print(log_probs)</pre>
<p>The output of the preceding code is as follows:</p>
<pre><br/>{'El': 0, 'que': 1, 'lee': 2, 'mucho': 3, 'y': 4, 'anda': 5, 'mucho,': 6, 've': 7, 'sabe': 8, 'The': 9, 'one': 10, 'who': 11, 'reads': 12, 'a': 13, 'lot': 14, 'and': 15, 'walks': 16, 'lot,': 17, 'sees': 18, 'knows': 19, 'lot.': 20, 'Nunca': 21, 'es': 22, 'tarde': 23, 'si': 24, 'la': 25, 'dicha': 26, 'buena': 27, 'It': 28, 'is': 29, 'never': 30, 'late': 31, 'if': 32, 'the': 33, 'joy': 34, 'good': 35, 'Que': 36, 'cada': 37, 'palo': 38, 'aguante': 39, 'su': 40, 'vela': 41, 'May': 42, 'every': 43, 'mast': 44, 'hold': 45, 'its': 46, 'own': 47, 'sail': 48}<br/>Parameter containing:<br/>tensor([[-0.0347, 0.1423, 0.1145, -0.0067, -0.0954, 0.0870, 0.0443, -0.0923,<br/> 0.0928, 0.0867, 0.1267, -0.0801, -0.0235, -0.0028, 0.0209, -0.1084,<br/> -0.1014, 0.0777, -0.0335, 0.0698, 0.0081, 0.0469, 0.0314, 0.0519,<br/> 0.0708, -0.1323, 0.0719, -0.1004, -0.1078, 0.0087, -0.0243, 0.0839,<br/> -0.0827, -0.1270, 0.1040, -0.0212, 0.0804, 0.0459, -0.1071, 0.0287,<br/> 0.0343, -0.0957, -0.0678, 0.0487, 0.0256, -0.0608, -0.0432, 0.1308,<br/> -0.0264],<br/> [ 0.0805, 0.0619, -0.0923, -0.1215, 0.1371, 0.0075, 0.0979, 0.0296,<br/> 0.0459, 0.1067, 0.1355, -0.0948, 0.0179, 0.1066, 0.1035, 0.0887,<br/> -0.1034, -0.1029, -0.0864, 0.0179, 0.1424, -0.0902, 0.0761, -0.0791,<br/> -0.1343, -0.0304, 0.0823, 0.1326, -0.0887, 0.0310, 0.1233, 0.0947,<br/> 0.0890, 0.1015, 0.0904, 0.0369, -0.0977, -0.1200, -0.0655, -0.0166,<br/> -0.0876, 0.0523, 0.0442, -0.0323, 0.0549, 0.0462, 0.0872, 0.0962,<br/> -0.0484]], requires_grad=True)<br/>Parameter containing:<br/>tensor([ 0.1396, -0.0165], requires_grad=True)<br/>tensor([[-0.6171, -0.7755]])</pre>
<p>We got the tensor output values. But, as we can see from the preceding code, these values aren't in correspondence to the log probability whether which is <kbd>English</kbd> and which corresponds to word <kbd>Spanish</kbd>. We need to train the model, and for that it's important to define these values to the log probabilities. </p>
<pre>label_to_ix = {"SPANISH": 0, "ENGLISH": 1}</pre>
<p>Let's start training our model then. We start with passing instances through the model to the those log probabilities. Then, the loss function is computed, and once the loss function is computer we calculate the gradient of this loss function. Finally, the parameters are updated with a gradient step. The <kbd>nn</kbd> package in PyTorch provides the loss functions. We want nn.NLLLoss() as the negative log likelihood loss. Optimization functions are also defined is <kbd>torch.optim</kbd>.</p>
<p class="mce-root">Here, we will just use <span><strong>Stochastic Gradient Descent</strong> (</span><strong>SGD</strong>):</p>
<pre class="mce-root"># Pass the BoW vector for running the model<br/># the code is wrapped since we don't need to train it<br/>torch.no_grad()<br/><br/>with torch.no_grad():<br/> sample = data[0]<br/> bow_vector = make_bow_vector(sample[0], word_to_ix)<br/> log_probs = model(bow_vector)<br/> print(log_probs)<br/><br/># We will run this on data that can be tested temporarily, before training, just to check the before and after difference using touch.no_grad():<br/><br/>with torch.no_grad():<br/> for instance, label in test_data:<br/> bow_vec = make_bow_vector(instance, word_to_ix)<br/> log_probs = model(bow_vec)<br/> print(log_probs)<br/><br/><br/>#The matrix column corresponding to "creo" is printed<br/>print(next(model.parameters())[:, word_to_ix["mucho"]])<br/><br/>loss_function = nn.NLLLoss()<br/>optimizer = optim.SGD(model.parameters(), lr=0.1)</pre>
<p>We don't want to pass the training data again and again for no reason. Real datasets have multiple instances and not just 2. It is reasonable to train the model for epochs between 5 to 30.</p>
<p>The following code shows the range for our example:</p>
<pre class="mce-root">for epoch in range(100):<br/> for instance, label in data:<br/> # Firstly, remember that gradients are accumulated by PyTorch<br/> # It's important that we clear those gradients before each instance<br/> model.zero_grad()<br/><br/>#The next step is to prepare our BOW vector and the target should be #wrapped in also we must wrap the target in a tensor in the form of an #integer<br/> # For example, as considered above, if the target word is SPANISH, #then, the integer wrapped should be 0<br/>#The loss function is already trained to understand that when the 0th element among the log probabilities is the one that is in accordance to SPANISH label<br/><br/> bow_vec = make_bow_vector(instance, word_to_ix)<br/> target = make_target(label, label_to_ix)<br/><br/># Next step is to run the forward pass<br/> log_probs = model(bow_vec)<br/><br/></pre>
<p class="mce-root">Here, we will compute the various factors such as loss, gradient, and updating the parameters by calling the function optimizer.step():</p>
<pre class="mce-root"><br/> loss = loss_function(log_probs, target)<br/> loss.backward()<br/> optimizer.step()<br/><br/>with torch.no_grad():<br/> for instance, label in test_data:<br/> bow_vec = make_bow_vector(instance, word_to_ix)<br/> log_probs = model(bow_vec)<br/> print(log_probs)<br/><br/># After computing and the results, we see that the index that corresponds to Spanish has gone up, and for English is has gone down!<br/>print(next(model.parameters())[:, word_to_ix["mucho"]])</pre>
<p>The output is as follows:</p>
<pre><br/>tensor([[-0.7653, -0.6258]])<br/>tensor([[-1.0456, -0.4331]])<br/>tensor([-0.0071, -0.0462], grad_fn=&lt;SelectBackward&gt;)<br/>tensor([[-0.1546, -1.9433]])<br/>tensor([[-0.9623, -0.4813]])<br/>tensor([ 0.4421, -0.4954], grad_fn=&lt;SelectBackward&gt;)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Now, we have the basic understanding of performing text-based processing using PyTorch. We also have a better understanding on how RNN works and how can we approach NLP-related problems using PyTorch. </p>
<p>In the upcoming chapters, we will build applications using what we have learned about neural networks and NLP. Happy coding!</p>


            </article>

            
        </section>
    </body></html>