<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Object Detection and Instance Segmentation with CNN</h1>
                </header>
            
            <article>
                
<p>Until now, in this book, we have been mostly using <strong>convolutional neural networks</strong> (<strong>CNNs</strong>) for classification. Classification classifies the whole image into one of the classes with respect to the entity having the maximum probability of detection in the image. But what if there is not one, but multiple entities of interest and we want to have the image associated with all of them? One way to do this is to use tags instead of classes, where these tags are all classes of the penultimate Softmax classification layer with probability above a given threshold. However, the probability of detection here varies widely by size and placement of entity, and from the following image, we can actually say, <em>How confident is the model that the identified entity is the one that is claimed?</em> What if we are very confident that there is an entity, say a dog, in the image, but its scale and position in the image is not as prominent as that of its owner, a <em>Person</em> entity? So, a <em>Multi-Class Tag</em> is a valid way but not the best for this purpose:</p>
<div class="CDPAlignCenter CDPAlign"><img height="194" src="assets/6abbc29c-4951-427c-bc3e-f85cbefac003.jpeg" width="291"/></div>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li><span>The differences between object detection and image classification</span></li>
<li>Traditional, non-CNN approaches for object detection</li>
<li>Region-based CNN and its features</li>
<li>Fast R-CNN</li>
<li>Faster R-CNN</li>
<li>Mask R-CNN</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The differences between object detection and image classification</h1>
                </header>
            
            <article>
                
<p>Let's take another example. You are watching the movie <em>101 Dalmatians</em>, and you want to know how many Dalmatians you can actually count in a given movie scene from that movie. Image Classification could, at best, tell you that there is at least one dog or one <em>Dalmatian</em> (depending upon which level you have trained your classifier for), but not exactly how many of them there are.</p>
<p><span>Another issue with classification-based models is that they do not tell you where </span><span>the identified entity </span><span>in the image is. Many times, this is very important. Say, for example, you saw your neighbor's dog<em> </em>playing with him (<em>Person</em>) and his cat. You took a snap of them and wanted to extract the image of the dog from there to search on the web for its breed or similar dogs like it. The only problem here is that searching the whole image might not work, and without identifying individual objects from the image, you have to do the cut-extract-search job manually for this task, as shown in the following image:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="205" src="assets/8d289e70-4022-4ba7-b696-a5b2e1017574.jpeg" width="308"/></div>
<p>So, you essentially need a technique that not only identifies the entities in an image but also tells you their placement in the image. This is what is called <strong>object detection</strong>. Object detection gives you bounding boxes and class labels (along with the probability of detection) of all the entities identified in an image. The output of this system can be used to empower multiple advanced use cases that work on the specific class of the objects detected.</p>
<p>Take, for example, the Facial Recognition feature that you have in Facebook, Google Photos, and many other similar apps. In it, before you identify <em>who is</em> there in an image taken in at a party, you need to detect all the faces in that image; then you can pass these faces through your face recognition/classification module to get/classify their names. So, the Object nomenclature in object detection is not limited to linguistic entities but includes anything that has specific boundaries and enough data to train the system, as shown in the following image:</p>
<div class="CDPAlignCenter CDPAlign"><img height="203" src="assets/ea2c10b7-68bc-4eee-8901-c33a84738f3a.jpeg" width="305"/></div>
<p>Now, if you want to find out how many <span>of the guests present at your party </span>were actually <strong>enjoying</strong> it, you can even run an object detection for <strong>Smiling Faces</strong> or a <strong>Smile Detector</strong>. There are very powerful and efficient trained models of object detectors available for most of the detectable human body parts (eye, face, upper body, and so on), popular human expressions (such as a smile), and many other general objects as well. So, the next time you use the <strong>Smile Shutter</strong> on your smartphone (a feature made to automatically click the image when most of the faces in <span>the scene </span>are detected as smiling), you know what is powering this feature.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why is object detection much more challenging than image classification?</h1>
                </header>
            
            <article>
                
<p>From our understanding of CNN and image classification so far, let's try to understand how we can approach the object detection problem, and that should logically lead us to the discovery of the underlying complexity and challenges. Assume we are dealing with monochromatic images for simplicity.</p>
<p>Any object detection at a high level may be considered a combination of two tasks (we will refute this later):</p>
<ul>
<li>Getting the right bounding boxes (or as many of them to filter later)</li>
<li>Classifying the object in that bounding box (while returning the classification effectiveness for filtering)</li>
</ul>
<p>So, object detection not only has to cater to all the challenges of image classification (second objective), but also faces new challenges of finding the right, or as many as possible, bounding boxes. As we already know how to use CNNs for the purpose of image classification, and the associated challenges, we can now concentrate on our first task and explore how effective (classification accuracy) and efficient (computational complexity) our approach is—or rather how challenging this task is going to be.</p>
<p>So, we start with randomly generating bounding boxes from the image. Even if we do not worry about the computational load of generating so many candidate boxes, technically termed as <strong>Region Proposals</strong> (regions that we send as proposals for classifying objects), we still need to have some mechanism for finding the best values for the following parameters:</p>
<ul>
<li>Starting (or center) coordinates to extract/draw the candidate <span>bounding</span> box</li>
<li>Length of the candidate bounding box</li>
<li>Width of the <span>candidate bounding box</span></li>
<li>Stride across each axis (distance from one starting location to another in the <em>x</em>-horizontal axis and <span><em>y</em>-vertical</span> axis)</li>
</ul>
<p>Let's assume that we can generate such an algorithm that can give us the most optimal value of these parameters. Still, will one value for these parameters work in most of the cases, or in fact, in some general cases? From our experience, we know that each object will have a different scale, so we know that one fixed value for <em>L</em> and <em>W</em> for these boxes will not work. Also, we can understand that the same object, say Dog, may be present in varying proportions/scales and positions in different images, as in some of our earlier examples. So this confirms our belief that we need boxes of not only different scales but also different sizes. </p>
<p>Let's assume that, correcting from the previous analogy, we want to extract <em>N</em> number of candidate boxes per starting coordinate in the image, where <em>N</em> encompasses most of the sizes/scales that may fit our classification problem. Although that seems to be a rather challenging job in itself, let's assume we have that magic number and it is far from a combination of <em>L[1,l-image] x W[1,w-image]</em> (all combinations of <em>L</em> and <em>W</em> where length is a set of all integers between 1 and the length of the actual image and breadth is from 1 to the breadth of the image); that will lead us to <em>l*w</em> boxes per coordinate:</p>
<div class="CDPAlignCenter CDPAlign"><img height="170" src="assets/c76e332d-472c-46bd-8e0d-4f51af7b7126.jpeg" width="256"/></div>
<p>Then, another question is about how many starting coordinates we need to visit in our image from where we will extract these <em>N</em> boxes each, or the Stride. Using a very big stride will lead us to extract sub-images in themselves, instead of a single homogeneous object that can be classified effectively and used for the purpose of achieving some of the objectives in our earlier examples. Conversely, too short a stride (say, 1 pixel in each direction) may mean a lot of candidate boxes.</p>
<p>From the preceding illustration, we can understand that even after hypothetically relaxing most of the constraints, we are nowhere close to making a system that we can fit in our Smartphones to detect smiling selfies or even bright faces in real time (even after an hour in fact). Nor can it have our robots and self-driving cars identify <span>objects as they move </span>(and navigate their way by avoiding them). This intuition should help us appreciate the advancements in the field of object detection and why it is such an impactful area of work.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Traditional, nonCNN approaches to object detection</h1>
                </header>
            
            <article>
                
<p>Libraries such as OpenCV and some others saw rapid inclusion in the software bundles for Smartphones, Robotic projects, and many others, to provide detection capabilities of specific objects (face, smile, and so on), and Computer Vision like benefits, though with some constraints even before the prolific adoption of CNN.</p>
<p>CNN-based research in this area of object detection and Instance Segmentation provided many advancements and performance enhancements to this field, not only enabling large-scale deployment of these systems but also opening avenues for many new solutions. But before we plan to jump into CNN based advancements, it will be a good idea to understand how the challenges cited in the earlier section were answered to make object detection possible in the first place (even with all the constraints), and then we will logically start our discussion about the different researchers and the application of CNN to solve other problems that still persist with the use of traditional approaches.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Haar features, cascading classifiers, and the Viola-Jones algorithm</h1>
                </header>
            
            <article>
                
<p>Unlike CNN, or the deepest learning for that matter, which is known for its capability of generating higher conceptual features automatically, which in-turn gives a major boost to the classifier, in case of traditional machine learning applications, such features need to be hand crafted by SMEs.</p>
<p>As we may also understand from our experience working on CPU-based machine learning classifiers, their performance is affected by high dimensionality in data and the availability of too many features to apply to the model, especially with some of the very popular and sophisticated classifiers such as <strong>Support Vector Machines</strong> (<strong>SVM</strong>), which used to be considered state-of-the-art until some time ago. </p>
<p>In this section, we will understand some of the innovative ideas <span>drawing inspirations from different fields of science and mathematics </span>that led to the resolution of some of the cited challenges above, to fructify the concept of real-time object detection in non-CNN systems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Haar Features</h1>
                </header>
            
            <article>
                
<p class="mce-root">Haar or Haar-like features are formations of rectangles with varying pixel density. Haar features sum up the pixel intensity in the adjacent rectangular regions at specific locations in the detection region. Based on the difference between the sums of pixel intensities across regions, they categorize the different subsections of the image.</p>
<div class="packt_infobox">Haar-like features have their name attributed to the mathematics term of Haar wavelet, which<span> is a sequence of rescaled square-shaped functions that together form a wavelet </span><span>family or basis. </span></div>
<div class="packt_tip">Because Haar-like features work on the difference between pixel intensities across regions, they work best with monochrome images. This is also the reason the images used earlier and in also this section are monochrome for better intuition.</div>
<p class="mce-root">These categories can be grouped into three major groups, as follows:</p>
<ul>
<li class="mce-root">Two rectangle features</li>
<li class="mce-root">Three rectangle features</li>
<li class="mce-root">Four rectangle features</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="215" src="assets/b4eecaef-6d1f-4db9-b0f1-59ca9198ba09.png" width="215"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Haar-like Features</div>
<p>With some easy tricks, the computation of varying intensities across the image becomes very efficient and can be processed at a very high rate in real time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cascading classifiers</h1>
                </header>
            
            <article>
                
<p>Even if we can extract Haar features from a particular region very quickly, it does not solve the problem of extracting such features from a lot of different places in the image; this is where the concept of cascading features comes in to help. It was observed that only 1 in 10,000 sub-regions turns positive for faces in classification, but we have to extract all features and run the whole classifier across all regions. Further, it was observed that <span>by using </span>just a few of the features (two in the first layer of the cascade), the classifier could eliminate a very high proportion of the regions (50% in the first region of the cascade). Also, if the sample consists of <span>just </span>these reduced region samples, then only slightly more features (10 features in the second layer of the cascade) are required for a classifier that can weed out a lot more cases, and so on. So we do classification in layers, starting with a classifier that requires very low computational power to weed out most of the subregions, gradually increasing the computation load required for the remaining subset, and so on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Viola-Jones algorithm</h1>
                </header>
            
            <article>
                
<p>In 2001, <span>Paul Viola and Michael Jones proposed a solution that could work well to answer some of the preceding challenges, but with some constraints. Though it is an almost two decades old algorithm, some of the most popular computer vision software to date, or at least till recently, used to embed it in some form or another. This fact makes it very important to understand this very simple, yet powerful, algorithm before we move on to CNN-based approaches for Region Proposal.</span></p>
<div class="packt_infobox">OpenCV, one of the most popular software libraries for computer vision, uses cascading classifiers as the predominant mode for object detection, and Haar-featuring-like Cascade classifier is very popular with OpenCV. A lot of pretrained Haar classifiers are available for this for multiple types of general objects.</div>
<p><span>This algorithm is not only capable of delivering detections with high <strong>TPRs</strong> (<strong>True Positive Rates</strong>) and low <strong>FPRs</strong> (<strong>False Positive Rates</strong>), it can also work in real time (process at least two frames per second).</span></p>
<div class="packt_tip">High TPR combined with Low FPR is a very important criterion for determining the robustness of an algorithm.</div>
<p><span>The constraints of their proposed algorithm were the following:</span></p>
<ul>
<li>It could work only for detecting, not recognizing faces (they proposed the algorithm for faces, though the same could be used for many other objects).</li>
<li>The faces had to be present in the image as a frontal view. No other view could be detected.</li>
</ul>
<p>At the heart of this algorithm are the Haar (like) Features and Cascading Classifiers. Haar Features are described later in a subsection. The Viola-Jones algorithm uses a subset of Haar features to determine general features on a face such as:</p>
<ul>
<li>Eyes (determined by a two-rectangle feature (<span>horizontal)</span>, with a dark horizontal rectangle above the eye forming the brow, followed by a lighter rectangle below)</li>
<li>Nose (three-rectangle feature <span>(vertical)</span>, with the nose as the center light rectangle and one darker rectangle on either side on the nose, forming the temple), and so on</li>
</ul>
<p>These fast-to-extract features can then be used to make a classifier to detect (distinguish) faces (from non-faces). </p>
<div class="packt_tip"><span>Haar features, with some tricks, are very fast to compute.</span></div>
<div class="CDPAlignCenter CDPAlign"><img height="104" src="assets/8614f6b9-ba3d-4666-9c6a-d7a7540fcdd6.jpg" width="398"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Viola-Jones algorithm and Haar-like Features for detecting faces</div>
<p><span>These Haar-like features are then used in the cascading classifiers to expedite the detection problem without losing the robustness of detection.</span></p>
<p><span>The Haar Features and cascading classifiers thus led to some of the very robust, effective, and fast individual object detectors of the previous generation. But still, the training of these cascades for a new object was very time consuming, and they had a lot of constraints, as mentioned before. That is where the new generation CNN-based object detectors come to the rescue.</span></p>
<div class="packt_tip">In this chapter, we have covered only the basis of Haar-Cascades or Haar features (in the non-CNN category) as they remained predominant for a long time and were the basis of many new types. Readers are encouraged to also explore some of the later and much effective SIFT and HOG-based features/cascades (associated papers are given in the <em>References</em> section). </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">R-CNN – Regions with CNN features</h1>
                </header>
            
            <article>
                
<p>In the <span>'Why is object detection much more challenging than image classification?'</span> section, we used a non-CNN method to draw region proposals and CNN for classification, and we realized that this is not going to work well because the regions generated and fed into CNN were not optimal. R-CNN or regions with CNN features, as the name suggests, flips that example completely and use CNN to generate features that are classified using a (non-CNN) technique called <strong>SVM</strong> (<strong>Support Vector Machines</strong>)</p>
<p>R-CNN uses the sliding window method (much like we discussed earlier, taking some <em>L x W</em> and stride) to generate around 2,000 regions of interest, and then it converts them into features for classification using CNN. Remember what we discussed <span>in the transfer learning chapter—</span>the last flattened layer (<span>before the classification or softmax layer</span>) can be extracted to transfer learning from models trained on generalistic data, and further train them (often requiring much less data as compared to a model with similar performance that has been trained from scratch using domain-specific data) to model domain-specific models. R-CNNs also use a similar mechanism to improve their effectiveness on specific object detection:</p>
<div class="CDPAlignCenter CDPAlign"><img height="142" src="assets/6173be31-0eb7-4bce-9345-27e63b442d91.png" width="459"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">R-CNN – Working </div>
<div class="packt_infobox">The original paper on R-CNN claims that on a PASCAL VOC 2012 dataset, it has improved the <strong>mean average precision</strong> (<strong>mAP</strong>) by more than 30% relative to the previous best result on that data while achieving a mAP of 53.3%.</div>
<div class="packt_tip">We saw very high precision figures for the image classification exercise (using CNN) over the ImageNet data. Do not use that figure with the comparison statistics given here, as not only are <span>the datasets used </span>different (and hence not comparable), but also the tasks in hand (classification versus object detection) are quite different, and object detection is much more challenging a task than image classification.</div>
<div class="packt_infobox">PASCAL <strong>VOC</strong> (<strong>Visual Object Challenge</strong>): Every area of research requires some sort of standardized dataset and standard KPIs to compare results across different studies and algorithms. Imagenet, the dataset we used for image classification, cannot be used as a standardized dataset for object detection, as object-detection requires (train, test, and validation set) data labeled with not only the object class but also its position. ImageNet does not provide this. Therefore, in most object detection studies, we may see the use of a standardized object-detection dataset, such as PASCAL VOC. The PASCAL VOC dataset has 4 variants so far, VOC2007, VOC2009, VOC2010, and VOC2012. VOC2012 is the latest (and richest) of them all.</div>
<p>Another place we stumbled at was the differing scales (and location) of the regions of interest, <em>recognition using region</em>. This is what is called the <strong>localization</strong> challenge; it is solved in R-CNN by using a varying range of receptive fields, starting from as high a region with 195 x 195 pixels and 32 x 32 strides, to lesser downwards.</p>
<div class="packt_tip">This approach is called <span><strong>recognition using region</strong>.</span></div>
<p>Wait a minute! Does that ring a bell? We said that we will use CNN to generate features from this region, but CNN uses a constant-size input to produce a fixed-size flattened layer. We do require fixed-size features (flattened vector size) as input to our SVMs, but here the input region size is changing. So how does that work? R-CNN uses a popular technique called <strong>Affine Image Warping</strong> to compute a fixed-size CNN input from each region proposal, regardless of the region's shape.</p>
<div class="mce-root packt_tip">In geometry, an affine transformation is the name given to a transformation function between affine spaces that preserves points, straight lines, <span>and </span>planes. Affine spaces are structures<span> that generalize the properties of Euclidian spaces while preserving </span><span>only the properties related to parallelism</span><span> and respective scale</span><span>.</span></div>
<p>Besides the challenges that we have covered, there exists another challenge that is worth mentioning. The candidate regions that we generated in the first step (on which we performed classification in the second step) were not very accurate, or they were lacking tight boundaries around the object identified. So we include a third stage in this method, which improves the accuracy of the bounding boxes by running a regression function (called <strong>bounding-box regressors</strong>) to identify the boundaries of separation.</p>
<p>R-CNN proved to be very successful when compared to the earlier end-to-end non-CNN approaches. But it uses CNN only for converting regions to features. As we understand, CNNs are very powerful for image classifications as well, but because our CNN will work only on input region images and not on flattened region features, we cannot use it here directly. In the next section, we will see how to overcome this obstacle. </p>
<div class="packt_infobox">R-CNN is very important to cover from the perspective of understanding the background use of CNN in object detection as it has been a giant leap from <span>all </span>non-CNN-based approaches. But because of further improvements in CNN-based object detection, as we will discuss next, R-CNN is not actively worked upon now and the code is not maintained any longer. </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fast R-CNN – fast region-based CNN</h1>
                </header>
            
            <article>
                
<p>Fast R-CNN, or Fast Region-based CNN method, is an improvement over the previously covered R-CNN. To be precise about the improvement statistics, as compared to R-CNN, it is:</p>
<ul>
<li><span>9x faster in training</span></li>
<li><span>213x faster at scoring/servicing/testing (0.3s per image processing), ignoring the time spent on region proposals</span></li>
<li><span>Has higher mAP of 66% on the PASCAL VOC 2012 dataset</span></li>
</ul>
<p>Where R-CNN uses a smaller (five-layer) CNN, Fast R-CNN uses the deeper VGG16 network, which accounts for its improved accuracy. Also, R-CNN is slow because it performs a ConvNet forward pass for each object proposal without sharing computation:</p>
<div class="CDPAlignCenter CDPAlign"><img height="192" src="assets/a12c5041-0ca5-492b-8a7f-12be6edd2eb9.png" width="448"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Fast R-CNN: Working</div>
<p>In Fast R-CNN, the deep VGG16 CNN provides essential computations for all the stages, namely:</p>
<ul>
<li><strong>Region of Interest</strong> (<strong>RoI</strong>) computation</li>
<li>Classification Objects (or background) for the region contents</li>
<li>Regression for enhancing the bounding box</li>
</ul>
<p>The input to the CNN, in this case, is not raw (candidate) regions from the image, but the (complete) actual image itself; the output is not the last flattened layer but the convolution (map) layer before that. From the so-generated convolution map, a the RoI pooling layer (a variant of max-pooling) is used to generate the flattened fixed-length RoI corresponding to each object proposal are generated, which are then passed through some <strong>fully connected</strong> (<strong>FC</strong>) layers.</p>
<div class="packt_infobox"><span>The RoI </span>pooling<span> is a variant of max </span>pooling (that we used in our initial chapters in this book)<span>, in which output size is fixed and input rectangle is a parameter.</span></div>
<div class="packt_tip">The RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent.</div>
<p>The output from the penultimate FC layer is then used for both:</p>
<ul>
<li>Classification (SoftMax layer) with as many classes as object proposals, +1 additional class for the background (none of the classes found in the region)</li>
<li>Sets of regressors that produce the four numbers (two numbers denoting the x, y coordinates of the upper-left corner for the box for that object, and the next two numbers corresponding to the height and width of that object found in that region) for each object-proposal that is required to make bounding boxes precise for that particular object</li>
</ul>
<p class="mce-root">The result achieved with Fast R-CNN is great. What is even greater is the use of a powerful CNN network to provide very effective features for all three challenges that we need to overcome. But there are still some drawbacks, and there is scope for further improvements as we will understand in our next section on Faster R-CNN.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Faster R-CNN – faster region proposal network-based CNN</h1>
                </header>
            
            <article>
                
<p>We saw in the earlier section that Fast R-CNN brought down the time required for scoring (testing) images drastically, but the reduction ignored the time required for generating Region Proposals, which use a separate mechanism (though pulling from the convolution map from CNN) and continue proving a bottleneck. Also, we observed that though all three challenges were resolved using the common features from convolution-map in Fast R-CNN, they were using different mechanisms/models.</p>
<p>Faster R-CNN improves upon these drawbacks and proposes the concept of <strong>Region Proposal Networks</strong> (<strong>RPNs</strong>), bringing down the scoring (testing) time to 0.2 seconds per image, even including time for Region Proposals.</p>
<div class="packt_tip"><span>Fast R-CNN was doing the scoring (testing) in 0.3 seconds per image, that too excluding the time required for the process equivalent to Region Proposal.</span></div>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="271" src="assets/f947c085-62de-43b0-bde7-f7a27bb31127.png" width="265"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Faster R-CNN: Working - The Region Proposal Networking acting as Attention Mechanism</div>
<div>
<p>As shown in the earlier figure, a VGG16 (or another) CNN works directly on the image, producing a convolutional map (similar to what was done in Fast R-CNN). Things differ from here, where now there are two branches, one feeding into the RPN and the other into the detection Network. This is <span>again</span><span> </span><span>an extension of the same CNN for prediction, leading to a <strong>Fully Convolutional Network</strong> (<strong>FCN</strong>). The </span>RPN<span> acts as an Attention Mechanism and also shares full-image convolutional features with the detection network. </span><span>Also, now because all the parts in the network can use efficient GPU-based computation, it thus reduces the overall time required:</span></p>
</div>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="180" src="assets/1600a405-9d97-4b14-aeaa-b7b322fb5d71.jpg" width="303"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span>Faster R-CNN: Working - The Region Proposal Networking acting as Attention Mechanism</span></div>
<div class="packt_tip"><span>For a greater understanding of the Attention Mechanism, refer to the chapter on Attention Mechanisms for CNN in this book.</span></div>
<p>The RPN works in a sliding window mechanism, where a window slides (much like CNN filters) across the last convolution map from the shared convolutional layer. With each slide, the sliding window produces <em>k (k=N<sub>Scale</sub> × N<sub>Size</sub>)</em> number of Anchor Boxes (similar to Candidate Boxes), where <em>N<sub>Scale</sub></em> is the number of (pyramid like) scales per <em>size</em> of the <em>N<sub>Size </sub></em>sized (aspect ratio) box extracted from the center of the sliding window, much like the following figure.</p>
<p>The RPN leads into a flattened, FC layer. This, in turn, leads into two networks, one for predicting the four numbers for each of the <em>k</em> boxes (determining the coordinates, length and width of the box as in Fast R-CNN), and another into a binomial classification model that determines the objectness or probability of finding any of the given objects in that box. The output from the RPN leads into the detection network, which detects which particular class of object is in each of the k boxes given the position of the box and its objectness. </p>
<div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ce65a2bc-2168-4e83-be93-176926aef7e0.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Faster R-CNN: Working - extracting different scales and sizes</div>
</div>
<p>One problem in this architecture is the training of the two networks, namely the Region Proposal and detection network. We learned that CNN is trained using backpropagating across all layers while reducing the losses layers with every iteration. But because of the split into two different networks, we could at a time backpropagate across only one network. To resolve this issue, the training is done iteratively across each network, while keeping the weights of the other network constant. This helps in converging both the networks quickly.</p>
<p>An important feature of the RPN architecture is that it has <span>translation invariance with respect to both the functions, one that is producing the anchors, and another that is producing the attributes (its coordinate and objectness) for the anchors. Because of translation invariance, a reverse operation, or producing the portion of the image given a vector map of an anchor map is feasible.</span></p>
<div class="packt_tip"><span>Owing to Translational Invariance, we can move in either direction in a CNN, that is from image to (region) proposals, and from the proposals to the corresponding portion of the image.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mask R-CNN – Instance segmentation with CNN</h1>
                </header>
            
            <article>
                
<p>Faster R-CNN is state-of-the-art stuff in object detection today. But there are problems overlapping the area of object detection that Faster R-CNN cannot solve effectively, which is where Mask R-CNN, an evolution of Faster R-CNN can help.</p>
<p>This section introduces the concept of instance segmentation, which is a combination of the standard object detection problem as described in this chapter, and the challenge of s<span>emantic segmentation</span>.</p>
<div class="packt_tip"><span>In semantic segmentation, as applied to images, the goal is to classify each pixel into a fixed set of categories without differentiating object instances.</span></div>
<p>Remember our example of counting the number of dogs in the image in the intuition section? We were able to count the number of dogs easily, because they were very much apart, with no overlap, so essentially just counting the number of objects did the job. Now, take the following image, for instance, and count the number of tomatoes using object detection. It will be a daunting task because the Bounding Boxes will have so much of an overlap that it will be difficult to distinguish the Instances of tomatoes from the boxes.</p>
<p>So, essentially, we need to go further, beyond bounding boxes and into pixels to get that level separation and identification. Like we use to classify bounding boxes with object names in object detection, in Instance Segment, we segment/ classify, each pixel with not only the specific object name but also the object-instance.</p>
<p><span>The object detection and Instance Segmentation could be treated as two different tasks, one logically leading to another, much like we discovered the tasks of finding Region Proposals and Classification in the case of object detection. But as in the case of object detection, and especially with techniques like Fast/Faster R-CNN, we discovered that it would be much effective if we have a mechanism to do them simultaneously, while also utilizing much of the computation and network to do so, to make the tasks seamless. </span></p>
<div class="CDPAlignCenter CDPAlign"><img height="217" src="assets/d580f08c-98db-4d19-9f25-fcffec41326e.jpeg" width="316"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Instance Segmentation – Intuition</div>
<p>Mask R-CNN is an extension of Faster R-CNN covered in the earlier network, and uses all the techniques used in Faster R-CNN, with one addition—an additional path in the network to generate a Segmentation Mask (or Object Mask) for each detected Object Instance in parallel. Also, because of this approach of using most of the existing network, it adds only a minimal overhead to the entire processing and has a scoring (test) time almost equivalent to that of Faster R-CNN. It has one of the best accuracies across all single-model solutions as applied to the COCO2016 challenge (using the COCO2015 dataset).</p>
<div class="packt_infobox">Like, PASCAL VOC, COCO is another large-scale standard (<span>series of</span>) dataset (from Microsoft). Besides object detection, <span>COCO is also used for segmentation and captioning. COCO is more extensive than many other datasets and much of the recent comparison on object detection is done on this for comparison purposes. The COCO dataset comes in three variants, namely COCO 2014, COCO 2015, and COCO 2017.</span></div>
<p>In Mask R-CNN, besides having the two branches that generate the objectness and localization for each anchor box or RoI, there also exists a third FCN that takes in the RoI and predicts a segmentation mask in a pixel-to-pixel manner for the given anchor box.</p>
<p>But there still remain some challenges. Though Faster R-CNN does demonstrate transformational invariance (that is, we could trace from the convolutional map of the RPN to the pixel map of the actual image), the convolutional map has a different structure from that of the actual image pixels. So, there is no pixel-to-pixel alignment between network inputs and outputs, which is important for our purpose of providing pixel-to-pixel masking using this network. To solve this challenge, Mask R-CNN uses a quantization-free layer (named RoIAlign in the original paper) that helps align the exact spatial locations. This layer not only provides exact alignment but also helps in improving the accuracy to a great extent, because of which Mask R-CNN is able to outperform many other networks:</p>
<div class="CDPAlignCenter CDPAlign"><img height="207" src="assets/50bfd31a-acb9-4003-8b02-bcdf236793f3.jpg" style="text-align: center;color: #333333;font-size: 1em" width="382"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Mask R-CNN – Instance Segmentation Mask (illustrative output)</div>
<div>
<p>The concept of instance segmentation is very powerful and can lead to realizing a lot of very impactful use cases that were not possible with object detection alone.</p>
<div class="packt_tip"><span>We can even use instance segmentation to estimate human poses in the same framework and eliminate them.</span></div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Instance segmentation in code</h1>
                </header>
            
            <article>
                
<p>It's now time to put the things that we've learned into practice. We'll use the COCO dataset and its API for the data, and use Facebook Research's Detectron project (link in References), which provides the Python implementation of many of the previously discussed techniques under an Apache 2.0 license. The code works with Python2 and Caffe2, so we'll need a virtual environment with the given configuration.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the environment</h1>
                </header>
            
            <article>
                
<p>The virtual environment, with Caffe2 installation, can be created as per the <kbd>caffe2</kbd> installation instructions on the Caffe2 repository link in the <em>References</em> Section. Next, we will install the dependencies.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Python dependencies (Python2 environment)</h1>
                </header>
            
            <article>
                
<p>We can install the Python dependencies as shown in the following code block:</p>
<div class="packt_tip">Python 2X and Python 3X are two different flavors of Python (or more precisely CPython), and not a conventional upgrade of version, therefore the libraries for one variant might not be compatible with another. Use Python 2X for this section.</div>
<div class="packt_infobox"><em>When we refer to the (interpreted) programming language Python, we need to refer to it with the specific interpreter (since it is an interpreted language as opposed to a compiled one like Java). The interpreter that we implicitly refer to as the Python interpreter (like the one you download from Python.org or the one that comes bundled with Anaconda) is technically called CPython, on which is the default byte-code interpreter of Python, which is written in C. But there are other Python interpreters also like Jython (build on Java), PyPy (written in Python itself - not so intuitive, right?), IronPython (.NET implementation of Python). </em></div>
<pre>pip install numpy&gt;=1.13 pyyaml&gt;=3.12 matplotlib opencv-python&gt;=3.2 setuptools Cython mock scipy</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Downloading and installing the COCO API and detectron library (OS shell commands)</h1>
                </header>
            
            <article>
                
<p><span>We will then download and  install the Python dependencies as shown in the following code block:</span></p>
<pre># COCO API download and install<br/># COCOAPI=/path/to/clone/cocoapi<br/>git clone https://github.com/cocodataset/cocoapi.git $COCOAPI<br/>cd $COCOAPI/PythonAPI<br/>make install<br/><br/># Detectron library download and install<br/># DETECTRON=/path/to/clone/detectron
git clone https://github.com/facebookresearch/detectron $DETECTRON<br/><span>cd $DETECTRON/lib &amp;&amp; make</span></pre>
<p>Alternatively, we can download and use the Docker image of the environment (requires Nvidia GPU support):</p>
<pre><span># DOCKER image build</span><br/><span>cd $DETECTRON/docker docker build -t detectron:c2-cuda9-cudnn7.</span><br/><span>nvidia-docker run --rm -it detectron:c2-cuda9-cudnn7 python2 tests/test_batch_permutation_op.py</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the COCO dataset folder structure</h1>
                </header>
            
            <article>
                
<p>Now we will see the code to prepare the COCO dataset folder structure as follows:</p>
<pre># We need the following Folder structure: coco [coco_train2014, coco_val2014, annotations]<br/>mkdir -p $DETECTRON/lib/datasets/data/coco<br/>ln -s /path/to/coco_train2014 $DETECTRON/lib/datasets/data/coco/<br/>ln -s /path/to/coco_val2014 $DETECTRON/lib/datasets/data/coco/<br/>ln -s /path/to/json/annotations $DETECTRON/lib/datasets/data/coco/annotations</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running the pre-trained model on the COCO dataset</h1>
                </header>
            
            <article>
                
<p>We can now implement the pre-trained model on the COCO dataset as shown in the following code snippet:</p>
<pre>python2 tools/test_net.py \
    --cfg configs/12_2017_baselines/e2e_mask_rcnn_R-101-FPN_2x.yaml \
    TEST.WEIGHTS https://s3-us-west-2.amazonaws.com/detectron/35861858/12_2017_baselines/e2e_mask_rcnn_R-101-             FPN_2x.yaml.02_32_51.SgT4y1cO/output/train/coco_2014_train:coco_2014_valminusminival/generalized_rcnn/model_final.pkl \
    NUM_GPUS 1</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ol>
<li>
<p><span>Paul Viola and Michael Jones, </span><span>Rapid object detection using a boosted cascade of simple features, <em>Conference on Computer Vision and Pattern Recognition</em>, </span><span>2001.</span></p>
</li>
<li>
<p><span>Paul Viola and Michael Jones, </span><span>Robust Real-time object detection, </span><span><em>International Journal of Computer Vision</em>, </span><span>2001.</span></p>
</li>
<li>
<p>Itseez2015opencv, OpenCV,<span> </span><em>Open Source Computer Vision Library</em>, Itseez, 2015.</p>
</li>
<li>
<p>Ross B. Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik,<span> </span><em>Rich feature hierarchies for accurate object detection and semantic segmentation</em>, CoRR, arXiv:1311.2524, 2013.</p>
</li>
<li>
<p>Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik,<span> </span><em>Rich feature hierarchies for accurate object detection and semantic segmentation</em>, Computer Vision and Pattern Recognition, 2014.</p>
</li>
<li>
<p>M. Everingham, L. VanGool, C. K. I. Williams, J. Winn, A. Zisserman,<span> </span><em>The PASCAL Visual Object Classes Challenge 2012</em>, VOC2012, Results.</p>
</li>
<li>
<p>D. Lowe.<span> </span><em>Distinctive image features from scale-invariant keypoints</em>, IJCV, 2004.</p>
</li>
<li>
<p>N. Dalal and B. Triggs.<span> </span><em>Histograms of oriented gradients for human detection</em>. In CVPR, 2005.</p>
</li>
<li>
<p>Ross B. Girshick, Fast R-CNN, CoRR, arXiv:1504.08083, 2015.</p>
</li>
<li>
<p>Rbgirshick, fast-rcnn, GitHub,<span> </span><a href="https://github.com/rbgirshick/fast-rcnn">https://github.com/rbgirshick/fast-rcnn</a>, Feb-2018.</p>
</li>
<li>
<p>Shaoqing Ren, Kaiming He, Ross B. Girshick, Jian Sun, Faster R-CNN:<span> </span><em>Towards Real-Time Object Detection with Region Proposal Networks</em>, CoRR, arXiv:1506.01497, 2015.</p>
</li>
<li>
<p>Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun, Faster R-CNN:<span> </span><em>Towards Real-Time Object Detection with Region Proposal Networks</em>, Advances in<span> </span><strong>Neural Information Processing Systems</strong><span> </span>(<strong>NIPS</strong>), 2015.</p>
</li>
<li>
<p>Rbgirshick, py-faster-rcnn, GitHub,<span> </span><a href="https://github.com/rbgirshick/py-faster-rcnn">https://github.com/rbgirshick/py-faster-rcnn</a>, Feb-2018.</p>
</li>
<li>
<p>Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Dollar, Kaiming He,</p>
Detectron, GitHub,<span> </span><a href="https://github.com/facebookresearch/Detectron">https://github.com/facebookresearch/Detectron</a>, Feb-2018.</li>
<li>
<p>Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, C. Lawrence Zitnick,<span> </span><em>Microsoft COCO: Common Objects in Context</em>, CoRR, arXiv:1405.0312, 2014.</p>
</li>
<li>
<p>Kaiming He, Georgia Gkioxari, Piotr Dollar, Ross B. Girshick, Mask R-CNN, CoRR, arXiv:1703.06870, 2017.</p>
</li>
<li>
<p>Liang-Chieh Chen, Alexander Hermans, George Papandreou, Florian Schroff, Peng Wang, Hartwig Adam, MaskLab:<span> </span><em>Instance Segmentation by Refining Object Detection with Semantic and Direction Features</em>, CoRR, arXiv:1712.04837, 2017.</p>
</li>
<li>
<p>Anurag Arnab, Philip H. S. Torr,<span> </span><em>Pixelwise Instance Segmentation with a Dynamically Instantiated Network</em>, CoRR, arXiv:1704.02386, 2017.</p>
</li>
<li>
<p>Matterport, Mask_RCNN, GitHub,<span> </span><a href="https://github.com/matterport/Mask_RCNN">https://github.com/matterport/Mask_RCNN</a>, Feb-2018.</p>
</li>
<li>
<p>CharlesShang, FastMaskRCNN, GitHub,<span> </span><a href="https://github.com/CharlesShang/FastMaskRCNN">https://github.com/CharlesShang/FastMaskRCNN</a>, Feb-2018.</p>
</li>
<li>
<p>Caffe2, Caffe2, GitHub,<span> </span><a href="https://github.com/caffe2/caffe2">https://github.com/caffe2/caffe2</a>, Feb-2018.</p>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we started from the very simple intuition behind the task of object detection and then proceeded to very advanced concepts, such as Instance Segmentation, which is a contemporary research area. Object detection is at the heart of a lot of innovation in the field of Retail, Media, Social Media, Mobility, and Security; there is a lot of potential for using these technologies to create very impactful and profitable features for both enterprise and social consumption. </p>
<p>From the Algorithms perspective, this chapter started with the legendary Viola-Jones algorithm and its underlying mechanisms, such as Haar Features and Cascading Classifiers. Using that intuition, we started exploring the world of CNN for object detection with algorithms, such as R-CNN, Fast R-CNN, up to the very state-of-the-art Faster R-CNN.</p>
<p>In this chapter, we also laid the foundations and introduced a very recent and impactful field of research called <strong>instance segmentation</strong>. We also covered some state-of-the-art Deep CNNs based on methods, such as Mask R-CNN, for easy and performant implementation of instance segmentation.</p>
<p> </p>
<p class="mce-root"/>


            </article>

            
        </section>
    </body></html>