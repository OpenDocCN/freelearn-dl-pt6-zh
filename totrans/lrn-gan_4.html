<html><head></head><body><div id="book-columns"><div id="book-inner"><div class="chapter" title="Chapter 4. Building Realistic Images from Your Text"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/><span class="koboSpan" id="kobo.1.1">Chapter 4. Building Realistic Images from Your Text</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">For many real-life complex problems, a single Generative Adversarial Network may not be sufficient to solve it. </span><span class="koboSpan" id="kobo.2.2">Instead it's better to decompose the complex problem into multiple simpler sub-problems and use multiple GANs to work on each sub-problem separately. </span><span class="koboSpan" id="kobo.2.3">Finally, you can stack or couple the GANs together to find a solution.</span></p><p><span class="koboSpan" id="kobo.3.1">In this chapter, we will first learn the technique of stacking multiple generative networks to generate realistic images from textual information. </span><span class="koboSpan" id="kobo.3.2">Next, you will couple two generative networks, to automatically discover relationships among various domains (relationships between shoes and handbags or actors and actresses).</span></p><p><span class="koboSpan" id="kobo.4.1">We will cover the following topics in this chapter:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.5.1">What is StackGAN? </span><span class="koboSpan" id="kobo.5.2">Its concept and architecture</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.6.1">Synthesizing realistic images from text description using TensorFlow</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.7.1">Discovering cross-domain relationships with DiscoGAN</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.8.1">Generating handbag images from edges using PyTorch</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.9.1">Transforming gender (actor-to-actress or vice-versa) with facescrub data</span></li></ul></div><div class="section" title="Introduction to StackGAN"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec21"/><span class="koboSpan" id="kobo.10.1">Introduction to StackGAN</span></h1></div></div></div><p><span class="koboSpan" id="kobo.11.1">The idea of StackGAN </span><a id="id163" class="indexterm"/><span class="koboSpan" id="kobo.12.1">was originally proposed by </span><span class="emphasis"><em><span class="koboSpan" id="kobo.13.1">Han Zhang</span></em></span><span class="koboSpan" id="kobo.14.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.15.1">Tao Xu</span></em></span><span class="koboSpan" id="kobo.16.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.17.1">Hongsheng Li</span></em></span><span class="koboSpan" id="kobo.18.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.19.1">Shaoting Zhang</span></em></span><span class="koboSpan" id="kobo.20.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.21.1">Xiaolei Huang</span></em></span><span class="koboSpan" id="kobo.22.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.23.1">Xiaogang Wang</span></em></span><span class="koboSpan" id="kobo.24.1">, and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.25.1">Dimitris Metaxas</span></em></span><span class="koboSpan" id="kobo.26.1"> [</span><span class="emphasis"><em><span class="koboSpan" id="kobo.27.1">arXiv: 1612.03242,2017</span></em></span><span class="koboSpan" id="kobo.28.1">] in the paper </span><span class="emphasis"><em><span class="koboSpan" id="kobo.29.1">Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks</span></em></span><span class="koboSpan" id="kobo.30.1">, where GAN has been used to synthesize forged images starting from the text description.</span></p><p><span class="koboSpan" id="kobo.31.1">Synthesizing photo realistic images from text is a challenging problem in Computer Vision and has tremendous practical application. </span><span class="koboSpan" id="kobo.31.2">The problem of generating images from text can be decomposed into two manageable sub-problems using StackGAN. </span><span class="koboSpan" id="kobo.31.3">In this approach, we stack two stages of the generative network based on certain conditions (such as textual description and the output of the previous stage) to achieve this challenging task of realistic image generation from text input.</span></p><p><span class="koboSpan" id="kobo.32.1">Let us define some concepts and notation before diving into the model architecture and implementation details:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.33.1">Io</span></em></span><span class="koboSpan" id="kobo.34.1">: This is the</span><a id="id164" class="indexterm"/><span class="koboSpan" id="kobo.35.1"> original image</span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.36.1">t</span></em></span><span class="koboSpan" id="kobo.37.1">: Text description</span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.38.1">t</span></em></span><span class="koboSpan" id="kobo.39.1">: Text embedding</span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.40.1">µ(t)</span></em></span><span class="koboSpan" id="kobo.41.1">: Mean of text embedding</span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.42.1">∑(t)</span></em></span><span class="koboSpan" id="kobo.43.1">: Diagonal covariance matrix of text embedding</span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.44.1">pdata</span></em></span><span class="koboSpan" id="kobo.45.1">: Real data distribution</span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.46.1">pz</span></em></span><span class="koboSpan" id="kobo.47.1">: Gaussian distribution of noise</span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.48.1">z</span></em></span><span class="koboSpan" id="kobo.49.1">: Randomly sampled noise from Gaussian distribution</span></li></ul></div><div class="section" title="Conditional augmentation"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec46"/><span class="koboSpan" id="kobo.50.1">Conditional augmentation</span></h2></div></div></div><p><span class="koboSpan" id="kobo.51.1">As we already know from </span><a class="link" href="ch02.html" title="Chapter 2. Unsupervised Learning with GAN"><span class="koboSpan" id="kobo.52.1">Chapter 2</span></a><span class="koboSpan" id="kobo.53.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.54.1">Unsupervised Learning with GAN</span></em></span><span class="koboSpan" id="kobo.55.1">, in Conditional GAN both the generator and </span><a id="id165" class="indexterm"/><span class="koboSpan" id="kobo.56.1">discriminator network receive additional conditioning variables </span><span class="emphasis"><em><span class="koboSpan" id="kobo.57.1">c</span></em></span><span class="koboSpan" id="kobo.58.1"> to yield </span><span class="emphasis"><em><span class="koboSpan" id="kobo.59.1">G(z;c)</span></em></span><span class="koboSpan" id="kobo.60.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.61.1">D(x;c)</span></em></span><span class="koboSpan" id="kobo.62.1">. </span><span class="koboSpan" id="kobo.62.2">This formulation helps the generator to generate images conditioned on variable </span><span class="emphasis"><em><span class="koboSpan" id="kobo.63.1">c</span></em></span><span class="koboSpan" id="kobo.64.1">. </span><span class="koboSpan" id="kobo.64.2">The conditioning augmentation yields more training pairs given a small number of image-text pairs and is useful for modeling text to image translation as the same sentence usually maps to objects with various appearances. </span><span class="koboSpan" id="kobo.64.3">The textual description is first converted to text embedding </span><span class="emphasis"><em><span class="koboSpan" id="kobo.65.1">t</span></em></span><span class="koboSpan" id="kobo.66.1"> by encoding through an encoder and then transformed nonlinearly using a char-CNN-RNN model to create conditioning latent variables as the input of a stage-I generator network.</span></p><p><span class="koboSpan" id="kobo.67.1">Since the latent space for text </span><a id="id166" class="indexterm"/><span class="koboSpan" id="kobo.68.1">embedding is usually high dimensional, to mitigate the problem of discontinuity in latent data manifold with a limited amount of data, a conditioning augmentation technique is applied to produce additional conditioning variable </span><span class="emphasis"><em><span class="koboSpan" id="kobo.69.1">c^</span></em></span><span class="koboSpan" id="kobo.70.1"> sampled from a Gaussian distribution </span><span class="emphasis"><em><span class="koboSpan" id="kobo.71.1">N(µ(t), ∑(t))</span></em></span><span class="koboSpan" id="kobo.72.1">.</span></p><div class="section" title="Stage-I"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec04"/><span class="koboSpan" id="kobo.73.1">Stage-I</span></h3></div></div></div><p><span class="koboSpan" id="kobo.74.1">In this stage, the GAN network learns the following:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.75.1">Generating rough </span><a id="id167" class="indexterm"/><span class="koboSpan" id="kobo.76.1">shapes and basic colors for creating objects conditioned on textual description</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.77.1">Generating background regions from random noise sampled from prior distribution</span></li></ul></div><p><span class="koboSpan" id="kobo.78.1">The low resolution coarse images generated in this stage might not look real because they have some defects </span><a id="id168" class="indexterm"/><span class="koboSpan" id="kobo.79.1">such as object shape distortion, missing object parts, and so on.</span></p><p><span class="koboSpan" id="kobo.80.1">The stage-I GAN trains the discriminator </span><span class="emphasis"><em><span class="koboSpan" id="kobo.81.1">D0</span></em></span><span class="koboSpan" id="kobo.82.1"> (maximize the loss) and the generator </span><span class="emphasis"><em><span class="koboSpan" id="kobo.83.1">G0</span></em></span><span class="koboSpan" id="kobo.84.1"> (minimize the loss), alternatively as shown in the following equation:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.85.1"><img src="graphics/B08086_04_01.jpg" alt="Stage-I"/></span></div></div><div class="section" title="Stage-II"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec05"/><span class="koboSpan" id="kobo.86.1">Stage-II</span></h3></div></div></div><p><span class="koboSpan" id="kobo.87.1">In this stage, the</span><a id="id169" class="indexterm"/><span class="koboSpan" id="kobo.88.1"> GAN network only focuses on drawing details and rectifying defects in low resolution images generated from stage-I (such as a lack of vivid object parts, shape distortion, and some omitted details from the text) to generate high resolution realistic images conditioned on textual description.</span></p><p><span class="koboSpan" id="kobo.89.1">The stage-II GAN alternatively trains the discriminator </span><span class="emphasis"><em><span class="koboSpan" id="kobo.90.1">D</span></em></span><span class="koboSpan" id="kobo.91.1"> (maximize the loss) and generator </span><span class="emphasis"><em><span class="koboSpan" id="kobo.92.1">G</span></em></span><span class="koboSpan" id="kobo.93.1"> (minimize the loss), conditioned on the result of low resolution </span><span class="emphasis"><em><span class="koboSpan" id="kobo.94.1">G</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.95.1">0</span></em></span></sub><span class="emphasis"><em><span class="koboSpan" id="kobo.96.1">(z; c^0)</span></em></span><span class="koboSpan" id="kobo.97.1"> and the Gaussian latent variable </span><span class="emphasis"><em><span class="koboSpan" id="kobo.98.1">c^</span></em></span><span class="koboSpan" id="kobo.99.1">:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.100.1"><img src="graphics/B08086_04_02.jpg" alt="Stage-II"/></span></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note05"/><span class="koboSpan" id="kobo.101.1">Note</span></h3><p><span class="koboSpan" id="kobo.102.1">Random noise </span><span class="emphasis"><em><span class="koboSpan" id="kobo.103.1">z</span></em></span><span class="koboSpan" id="kobo.104.1"> is replaced with Gaussian conditioning variables </span><span class="emphasis"><em><span class="koboSpan" id="kobo.105.1">c^</span></em></span><span class="koboSpan" id="kobo.106.1"> in stage-II. </span><span class="koboSpan" id="kobo.106.2">Also, the</span><a id="id170" class="indexterm"/><span class="koboSpan" id="kobo.107.1"> conditioning augmentation in stage-II has different fully connected layers to generate different means and standard deviation of the text embedding.</span></p></div></div></div></div><div class="section" title="Architecture details of StackGAN"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec47"/><span class="koboSpan" id="kobo.108.1">Architecture details of StackGAN</span></h2></div></div></div><p><span class="koboSpan" id="kobo.109.1">As illustrated in the</span><a id="id171" class="indexterm"/><span class="koboSpan" id="kobo.110.1"> following figure, for the generator network </span><span class="emphasis"><em><span class="koboSpan" id="kobo.111.1">G</span></em></span><sub><span class="koboSpan" id="kobo.112.1">0</span></sub><span class="koboSpan" id="kobo.113.1"> of stage-I, the text embedding </span><span class="emphasis"><em><span class="koboSpan" id="kobo.114.1">t</span></em></span><span class="koboSpan" id="kobo.115.1"> is first fed into a fully connected layer to generate </span><span class="emphasis"><em><span class="koboSpan" id="kobo.116.1">µ0</span></em></span><span class="koboSpan" id="kobo.117.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.118.1">σ0</span></em></span><span class="koboSpan" id="kobo.119.1"> (</span><span class="emphasis"><em><span class="koboSpan" id="kobo.120.1">σ0</span></em></span><span class="koboSpan" id="kobo.121.1"> is the diagonal values of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.122.1">∑0</span></em></span><span class="koboSpan" id="kobo.123.1">) for the Gaussian distribution </span><span class="emphasis"><em><span class="koboSpan" id="kobo.124.1">N(µ0(t); ∑0(t))</span></em></span><span class="koboSpan" id="kobo.125.1"> and then the text conditioning variable </span><span class="emphasis"><em><span class="koboSpan" id="kobo.126.1">c^0</span></em></span><span class="koboSpan" id="kobo.127.1"> is then sampled from the Gaussian distribution.</span></p><p><span class="koboSpan" id="kobo.128.1">For the discriminator network </span><span class="emphasis"><em><span class="koboSpan" id="kobo.129.1">D0</span></em></span><span class="koboSpan" id="kobo.130.1"> of stage-I, the text embedding </span><span class="emphasis"><em><span class="koboSpan" id="kobo.131.1">t</span></em></span><span class="koboSpan" id="kobo.132.1"> is first compressed to </span><span class="emphasis"><em><span class="koboSpan" id="kobo.133.1">Nd</span></em></span><span class="koboSpan" id="kobo.134.1"> dimensions with a fully connected layer and then spatially replicated to </span><span class="emphasis"><em><span class="koboSpan" id="kobo.135.1">Md</span></em></span><span class="koboSpan" id="kobo.136.1"> x </span><span class="emphasis"><em><span class="koboSpan" id="kobo.137.1">Md</span></em></span><span class="koboSpan" id="kobo.138.1"> x </span><span class="emphasis"><em><span class="koboSpan" id="kobo.139.1">Nd</span></em></span><span class="koboSpan" id="kobo.140.1"> tensor. </span><span class="koboSpan" id="kobo.140.2">The image is passed through a series of down-sampling blocks to squeeze into </span><span class="emphasis"><em><span class="koboSpan" id="kobo.141.1">Md</span></em></span><span class="koboSpan" id="kobo.142.1"> x </span><span class="emphasis"><em><span class="koboSpan" id="kobo.143.1">Md</span></em></span><span class="koboSpan" id="kobo.144.1"> spatial dimension and then concatenated using a filter map along the channel dimension with the text tensor. </span><span class="koboSpan" id="kobo.144.2">The resulting tensor goes through a 1x1 convolutional layer to jointly learn features across the image and the text and finally output the decision score using one node fully connected layer.</span></p><p><span class="koboSpan" id="kobo.145.1">The generator of stage-II is designed as an encoder-decoder network with residual blocks and the text embedding </span><span class="emphasis"><em><span class="koboSpan" id="kobo.146.1">t</span></em></span><span class="koboSpan" id="kobo.147.1"> to generate the </span><span class="emphasis"><em><span class="koboSpan" id="kobo.148.1">Ng</span></em></span><span class="koboSpan" id="kobo.149.1"> dimensional text conditioning vector </span><span class="emphasis"><em><span class="koboSpan" id="kobo.150.1">c^</span></em></span><span class="koboSpan" id="kobo.151.1">, which is spatially replicated to </span><span class="emphasis"><em><span class="koboSpan" id="kobo.152.1">Md</span></em></span><span class="koboSpan" id="kobo.153.1"> x </span><span class="emphasis"><em><span class="koboSpan" id="kobo.154.1">Md</span></em></span><span class="koboSpan" id="kobo.155.1"> x </span><span class="emphasis"><em><span class="koboSpan" id="kobo.156.1">Nd</span></em></span><span class="koboSpan" id="kobo.157.1"> tensor. </span><span class="koboSpan" id="kobo.157.2">The stage-I result </span><span class="emphasis"><em><span class="koboSpan" id="kobo.158.1">s0</span></em></span><span class="koboSpan" id="kobo.159.1"> generated is then fed into several down-sampling blocks (that is, encoder) until it is squeezed to spatial size </span><span class="emphasis"><em><span class="koboSpan" id="kobo.160.1">Mg</span></em></span><span class="koboSpan" id="kobo.161.1"> x </span><span class="emphasis"><em><span class="koboSpan" id="kobo.162.1">Mg</span></em></span><span class="koboSpan" id="kobo.163.1">. </span><span class="koboSpan" id="kobo.163.2">The image features concatenated with text features along the channel dimension are passed through several residual blocks, to learn multi-modal representations across image and text features. </span><span class="koboSpan" id="kobo.163.3">Finally, the resulting tensors goes through a series of up-sampling layers (that is, decoder) to generate a </span><span class="emphasis"><em><span class="koboSpan" id="kobo.164.1">W</span></em></span><span class="koboSpan" id="kobo.165.1"> x </span><span class="emphasis"><em><span class="koboSpan" id="kobo.166.1">H</span></em></span><span class="koboSpan" id="kobo.167.1"> high resolution image.</span></p><p><span class="koboSpan" id="kobo.168.1">The discriminator of stage-II is similar to stage-1 with only extra down-sampling blocks to cater for the large image size in this stage. </span><span class="koboSpan" id="kobo.168.2">During training of the discriminator, the positive sample pairs is built from the real images and their corresponding text descriptions, whereas the negative</span><a id="id172" class="indexterm"/><span class="koboSpan" id="kobo.169.1"> sample consists of two groups: one having real images with mismatched text embedding and the other having synthetic images with their corresponding text embedding:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.170.1"><img src="graphics/B08086_04_03.jpg" alt="Architecture details of StackGAN"/></span><div class="caption"><p><span class="koboSpan" id="kobo.171.1">Figure 1. </span><span class="koboSpan" id="kobo.171.2">The architecture of the StackGAN.</span></p><p><span class="koboSpan" id="kobo.172.1">Source: </span><span class="emphasis"><em><span class="koboSpan" id="kobo.173.1">arXiv: 1612.03242,2017</span></em></span>
</p></div></div><p><span class="koboSpan" id="kobo.174.1">The stage-I generator first draws a low resolution image by sketching a rough shape and basic colors of the object from the given text and painting the background from a random noise vector. </span><span class="koboSpan" id="kobo.174.2">The stage-II generator corrects defects and adds compelling details into stage-I results, yielding a more realistic high resolution image conditioned on stage-I results.</span></p><p><span class="koboSpan" id="kobo.175.1">The up-sampling blocks consist of the nearest-neighbor up-sampling followed by the 33 convolutions, each of stride of 1. </span><span class="koboSpan" id="kobo.175.2">Batch normalization and </span><code class="literal"><span class="koboSpan" id="kobo.176.1">ReLU</span></code><span class="koboSpan" id="kobo.177.1"> activation functions are applied after every convolution except the last one. </span><span class="koboSpan" id="kobo.177.2">The residual blocks again consist of 33 convolutions, each of stride 1, followed by batch normalization and </span><code class="literal"><span class="koboSpan" id="kobo.178.1">ReLU</span></code><span class="koboSpan" id="kobo.179.1"> activation function. </span><span class="koboSpan" id="kobo.179.2">The down-sampling blocks consist of 44 convolutions each of stride 2, followed by batch normalization and Leaky-ReLU, except batch normalization is not present in the first convolution</span><a id="id173" class="indexterm"/><span class="koboSpan" id="kobo.180.1"> layer.</span></p></div><div class="section" title="Synthesizing images from text with TensorFlow"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec48"/><span class="koboSpan" id="kobo.181.1">Synthesizing images from text with TensorFlow</span></h2></div></div></div><p><span class="koboSpan" id="kobo.182.1">Let us implement the code to</span><a id="id174" class="indexterm"/><span class="koboSpan" id="kobo.183.1"> synthesize realistic images from text and produce mind blowing result:</span></p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="koboSpan" id="kobo.184.1">First clone the </span><code class="literal"><span class="koboSpan" id="kobo.185.1">git</span></code><span class="koboSpan" id="kobo.186.1"> repository: </span><a class="ulink" href="https://github.com/Kuntal-G/StackGAN.git"><span class="koboSpan" id="kobo.187.1">https://github.com/Kuntal-G/StackGAN.git</span></a><span class="koboSpan" id="kobo.188.1"> and change the directory to </span><code class="literal"><span class="koboSpan" id="kobo.189.1">StackGAN</span></code><span class="koboSpan" id="kobo.190.1">:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.191.1">git clone https://github.com/Kuntal-G/StackGAN.git</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.192.1">cd StackGAN</span></strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note06"/><span class="koboSpan" id="kobo.193.1">Note</span></h3><p><span class="koboSpan" id="kobo.194.1">Currently the code is compatible with an older version of TensorFlow (0.11), so you need to have TensorFlow version below 1.0 to successfully run this code. </span><span class="koboSpan" id="kobo.194.2">You can modify your TensorFlow version using: </span><code class="literal"><span class="koboSpan" id="kobo.195.1">sudo pip install tensorflow==0.12.0</span></code><span class="koboSpan" id="kobo.196.1">.</span></p><p><span class="koboSpan" id="kobo.197.1">Also make sure torch</span><a id="id175" class="indexterm"/><span class="koboSpan" id="kobo.198.1"> is installed in your system. </span><span class="koboSpan" id="kobo.198.2">More information can be found here: </span><a class="ulink" href="http://torch.ch/docs/getting-started.html"><span class="koboSpan" id="kobo.199.1">http://torch.ch/docs/getting-started.html</span></a><span class="koboSpan" id="kobo.200.1">.</span></p></div></div></li><li class="listitem"><span class="koboSpan" id="kobo.201.1">Then install the following packages using the </span><code class="literal"><span class="koboSpan" id="kobo.202.1">pip</span></code><span class="koboSpan" id="kobo.203.1"> command:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.204.1">sudo pip install prettytensor progressbar python-dateutil easydict pandas torchfile requests</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.205.1">Next download the </span><a id="id176" class="indexterm"/><span class="koboSpan" id="kobo.206.1">pre-processed char-CNN-RNN text embedding birds model from: </span><a class="ulink" href="https://drive.google.com/file/d/0B3y_msrWZaXLT1BZdVdycDY5TEE/view"><span class="koboSpan" id="kobo.207.1">https://drive.google.com/file/d/0B3y_msrWZaXLT1BZdVdycDY5TEE/view</span></a><span class="koboSpan" id="kobo.208.1"> using the following command:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.209.1">python google-drive-download.py 0B3y_msrWZaXLT1BZdVdycDY5TEE Data/ birds.zip</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.210.1">Now extract the downloaded file using the </span><code class="literal"><span class="koboSpan" id="kobo.211.1">unzip</span></code><span class="koboSpan" id="kobo.212.1"> command:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.213.1">unzip Data/birds.zip</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.214.1">Next download and extract the birds image data from Caltech-UCSD:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.215.1">wget http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/
CUB_200_2011.tgz -O Data/birds/CUB_200_2011.tgz</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.216.1">tar -xzf CUB_200_2011.tgz</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.217.1">Now we will do preprocessing on the images to split into training and test sets and save the images in pickle format:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.218.1">python misc/preprocess_birds.py</span></strong></span>
</pre></div><div class="mediaobject"><span class="koboSpan" id="kobo.219.1"><img src="graphics/B08086_04_04.jpg" alt="Synthesizing images from text with TensorFlow"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.220.1">Now we will </span><a id="id177" class="indexterm"/><span class="koboSpan" id="kobo.221.1">download the pre-trained char-CNN-RNN text embedding model from: </span><a class="ulink" href="https://drive.google.com/file/d/0B3y_msrWZaXLNUNKa3BaRjAyTzQ/view"><span class="koboSpan" id="kobo.222.1">https://drive.google.com/file/d/0B3y_msrWZaXLNUNKa3BaRjAyTzQ/view</span></a><span class="koboSpan" id="kobo.223.1"> and save it to the </span><code class="literal"><span class="koboSpan" id="kobo.224.1">models/</span></code><span class="koboSpan" id="kobo.225.1"> directory using:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.226.1">python google-drive-download.py 0B3y_msrWZaXLNUNKa3BaRjAyTzQ models/ birds_model_164000.ckpt</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.227.1">Also download the char-CNN-RNN text encoder for birds from </span><a class="ulink" href="https://drive.google.com/file/d/0B0ywwgffWnLLU0F3UHA3NzFTNEE/view"><span class="koboSpan" id="kobo.228.1">https://drive.google.com/file/d/0B0ywwgffWnLLU0F3UHA3NzFTNEE/view</span></a><span class="koboSpan" id="kobo.229.1"> and save it under the </span><code class="literal"><span class="koboSpan" id="kobo.230.1">models/text_encoder</span></code><span class="koboSpan" id="kobo.231.1"> directory:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.232.1">python google-drive-download.py 0B0ywwgffWnLLU0F3UHA3NzFTNEE models/text_encoder/  lm_sje_nc4_cub_hybrid_gru18_a1_c512_0.00070_1_10_trainvalids.txt_iter30000.t7</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.233.1">Next, we will add some sentences to the </span><code class="literal"><span class="koboSpan" id="kobo.234.1">example_captions.txt</span></code><span class="koboSpan" id="kobo.235.1"> file to generate some exciting images of birds:</span><p><code class="literal"><span class="koboSpan" id="kobo.236.1">A white bird with a black crown and red beak</span></code></p><p><code class="literal"><span class="koboSpan" id="kobo.237.1">this bird has red breast and yellow belly</span></code></p></li><li class="listitem"><span class="koboSpan" id="kobo.238.1">Finally, we will execute the </span><code class="literal"><span class="koboSpan" id="kobo.239.1">birds_demo.sh</span></code><span class="koboSpan" id="kobo.240.1"> file under the </span><code class="literal"><span class="koboSpan" id="kobo.241.1">demo</span></code><span class="koboSpan" id="kobo.242.1"> directory to generate realistic bird images from the text description given in the </span><code class="literal"><span class="koboSpan" id="kobo.243.1">example_captions.txt</span></code><span class="koboSpan" id="kobo.244.1"> file:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.245.1">sh demo/birds_demo.sh</span></strong></span>
</pre></div><div class="mediaobject"><span class="koboSpan" id="kobo.246.1"><img src="graphics/B08086_04_05.jpg" alt="Synthesizing images from text with TensorFlow"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.247.1">Now the </span><a id="id178" class="indexterm"/><span class="koboSpan" id="kobo.248.1">generated images will be saved under the </span><code class="literal"><span class="koboSpan" id="kobo.249.1">Data/birds/example_captions/</span></code><span class="koboSpan" id="kobo.250.1"> directory as shown in the following screenshot:</span><div class="mediaobject"><span class="koboSpan" id="kobo.251.1"><img src="graphics/B08086_04_06.jpg" alt="Synthesizing images from text with TensorFlow"/></span></div></li></ol></div><p><span class="koboSpan" id="kobo.252.1">Voila, you have now generated impressive bird images from the textual description. </span><span class="koboSpan" id="kobo.252.2">Play with your </span><a id="id179" class="indexterm"/><span class="koboSpan" id="kobo.253.1">own sentences to describe birds and visually verify the results with the description.</span></p></div></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Discovering cross-domain relationships with DiscoGAN"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec22"/><span class="koboSpan" id="kobo.1.1">Discovering cross-domain relationships with DiscoGAN</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">Cross-domain relationships are often </span><a id="id180" class="indexterm"/><span class="koboSpan" id="kobo.3.1">natural to humans and they can easily identify the relationship between data from various domains without supervision (for example, recognizing</span><a id="id181" class="indexterm"/><span class="koboSpan" id="kobo.4.1"> relationships between an English sentence and its translated sentence in Spanish or choosing a shoe to fit the style of a dress), but learning this relation automatically is very challenging and requires a lot of ground truth pairing information that illustrates the relations.</span></p><p>
<span class="strong"><strong><span class="koboSpan" id="kobo.5.1">Discovery Generative Adversarial Networks</span></strong></span><span class="koboSpan" id="kobo.6.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.7.1">DiscoGAN</span></strong></span><span class="koboSpan" id="kobo.8.1">) </span><span class="emphasis"><em><span class="koboSpan" id="kobo.9.1">arXiv: 1703.05192 ,2017</span></em></span><span class="koboSpan" id="kobo.10.1"> discovers the relationship between two visual domains and successfully transfers styles from one domain to another by generating new images of one domain given an image from the other domain without any pairing information. </span><span class="koboSpan" id="kobo.10.2">DiscoGAN seeks to have two GANs coupled together that can map each domain to its counterpart domain. </span><span class="koboSpan" id="kobo.10.3">The key idea behind DiscoGAN is to make sure that all images in domain 1 are representable by images in domain 2, and use the reconstruction loss to measure how well the original image is reconstructed after the two translations—that is, from domain 1 to domain 2 and back to domain 1.</span></p><div class="section" title="The architecture and model formulation of DiscoGAN"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec49"/><span class="koboSpan" id="kobo.11.1">The architecture and model formulation of DiscoGAN</span></h2></div></div></div><p><span class="koboSpan" id="kobo.12.1">Before diving into the model formulation and various </span><code class="literal"><span class="koboSpan" id="kobo.13.1">loss</span></code><span class="koboSpan" id="kobo.14.1"> functions associated with DiscoGAN, let us first</span><a id="id182" class="indexterm"/><span class="koboSpan" id="kobo.15.1"> define some</span><a id="id183" class="indexterm"/><span class="koboSpan" id="kobo.16.1"> related terminology and concepts:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.17.1">G</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.18.1">AB</span></em></span></sub><span class="koboSpan" id="kobo.19.1">: The </span><code class="literal"><span class="koboSpan" id="kobo.20.1">generator</span></code><span class="koboSpan" id="kobo.21.1"> function</span><a id="id184" class="indexterm"/><span class="koboSpan" id="kobo.22.1"> that translates input image </span><span class="emphasis"><em><span class="koboSpan" id="kobo.23.1">x</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.24.1">A</span></em></span></sub><span class="koboSpan" id="kobo.25.1"> from domain A into image </span><span class="emphasis"><em><span class="koboSpan" id="kobo.26.1">x</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.27.1">AB</span></em></span></sub><span class="koboSpan" id="kobo.28.1"> in domain B</span></li><li class="listitem" style="list-style-type: disc"> <span class="emphasis"><em><span class="koboSpan" id="kobo.29.1">G</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.30.1">BA</span></em></span></sub><span class="koboSpan" id="kobo.31.1">: The </span><code class="literal"><span class="koboSpan" id="kobo.32.1">generator</span></code><span class="koboSpan" id="kobo.33.1"> function that translates input image </span><span class="emphasis"><em><span class="koboSpan" id="kobo.34.1">x</span><sub><span class="koboSpan" id="kobo.35.1">B</span></sub></em></span><span class="koboSpan" id="kobo.36.1"> from domain B into image </span><span class="emphasis"><em><span class="koboSpan" id="kobo.37.1">x</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.38.1">BA</span></em></span></sub><span class="koboSpan" id="kobo.39.1"> in domain A</span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.40.1">G</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.41.1">AB</span></em></span></sub><span class="koboSpan" id="kobo.42.1">(</span><span class="emphasis"><em><span class="koboSpan" id="kobo.43.1">x</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.44.1">A</span></em></span></sub><span class="koboSpan" id="kobo.45.1">): This is the complete set of all possible resulting values for all </span><span class="emphasis"><em><span class="koboSpan" id="kobo.46.1">x</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.47.1">A</span></em></span></sub><span class="koboSpan" id="kobo.48.1">s in domain A that should be contained in domain B</span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.49.1">G</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.50.1">BA</span></em></span></sub><span class="koboSpan" id="kobo.51.1">(</span><span class="emphasis"><em><span class="koboSpan" id="kobo.52.1">x</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.53.1">B</span></em></span></sub><span class="koboSpan" id="kobo.54.1">): This is the complete set of all possible resulting values for all </span><span class="emphasis"><em><span class="koboSpan" id="kobo.55.1">x</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.56.1">B</span></em></span></sub><span class="koboSpan" id="kobo.57.1">s in domain B, that should be contained in domain A</span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.58.1">D</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.59.1">A</span></em></span></sub><span class="koboSpan" id="kobo.60.1">: The </span><code class="literal"><span class="koboSpan" id="kobo.61.1">discriminator</span></code><span class="koboSpan" id="kobo.62.1"> function in domain A</span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.63.1">D</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.64.1">B</span></em></span></sub><span class="koboSpan" id="kobo.65.1">: The </span><code class="literal"><span class="koboSpan" id="kobo.66.1">discriminator</span></code><span class="koboSpan" id="kobo.67.1"> function in domain B</span><div class="mediaobject"><span class="koboSpan" id="kobo.68.1"><img src="graphics/B08086_04_07.jpg" alt="The architecture and model formulation of DiscoGAN"/></span><div class="caption"><p><span class="koboSpan" id="kobo.69.1">Figure-2: DiscoGAN architecture with two coupled GAN models</span></p><p><span class="koboSpan" id="kobo.70.1">Source: </span><span class="emphasis"><em><span class="koboSpan" id="kobo.71.1">arXiv- 1703.05192, 2017</span></em></span>
</p></div></div></li></ul></div><p><span class="koboSpan" id="kobo.72.1">The generator </span><a id="id185" class="indexterm"/><span class="koboSpan" id="kobo.73.1">modules of DiscoGAN consist of an encoder-decoder pair to perform back to back image translation. </span><span class="koboSpan" id="kobo.73.2">A generator </span><span class="emphasis"><em><span class="koboSpan" id="kobo.74.1">G</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.75.1">AB</span></em></span></sub><span class="koboSpan" id="kobo.76.1"> first translates input image </span><span class="emphasis"><em><span class="koboSpan" id="kobo.77.1">x</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.78.1">A</span></em></span></sub><span class="koboSpan" id="kobo.79.1"> from domain A into the image </span><span class="emphasis"><em><span class="koboSpan" id="kobo.80.1">x</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.81.1">AB</span></em></span></sub><span class="koboSpan" id="kobo.82.1"> in</span><a id="id186" class="indexterm"/><span class="koboSpan" id="kobo.83.1"> domain B. </span><span class="koboSpan" id="kobo.83.2">Then the generated image is translated back to domain A image </span><span class="emphasis"><em><span class="koboSpan" id="kobo.84.1">x</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.85.1">ABA</span></em></span></sub><span class="koboSpan" id="kobo.86.1"> to match the original input image using reconstruction loss (equation-3) with some form of distance metrics, such as MSE, Cosine distance, and hinge-loss. </span><span class="koboSpan" id="kobo.86.2">Finally, the translated output image </span><span class="emphasis"><em><span class="koboSpan" id="kobo.87.1">x</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.88.1">AB</span></em></span></sub><span class="koboSpan" id="kobo.89.1"> of the generator is fed into the discriminator and gets scored by comparing it to the real image of domain B:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.90.1"><img src="graphics/B08086_04_08.jpg" alt="The architecture and model formulation of DiscoGAN"/></span></div><p><span class="koboSpan" id="kobo.91.1">The generator </span><span class="emphasis"><em><span class="koboSpan" id="kobo.92.1">GAB</span></em></span><span class="koboSpan" id="kobo.93.1"> receives</span><a id="id187" class="indexterm"/><span class="koboSpan" id="kobo.94.1"> two types of losses as shown (equation-5):</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.95.1">L</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.96.1">CONSTA</span></em></span></sub><span class="koboSpan" id="kobo.97.1">: A reconstruction loss that measures how well the original image is reconstructed after the</span><a id="id188" class="indexterm"/><span class="koboSpan" id="kobo.98.1"> two translations domain A-&gt; domain B-&gt; domain A</span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em><span class="koboSpan" id="kobo.99.1">L</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.100.1">GANB</span></em></span></sub><span class="koboSpan" id="kobo.101.1">: Standard GAN loss that measures how realistic the generated image is in domain B</span></li></ul></div><p><span class="koboSpan" id="kobo.102.1">Whereas the discriminator </span><span class="emphasis"><em><span class="koboSpan" id="kobo.103.1">D</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.104.1">B</span></em></span></sub><span class="koboSpan" id="kobo.105.1"> receives the standard GAN discriminator loss as shown (equation-6):</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.106.1"><img src="graphics/B08086_04_09.jpg" alt="The architecture and model formulation of DiscoGAN"/></span></div><p><span class="koboSpan" id="kobo.107.1">The two coupled GANs are trained simultaneously and both the GANs learn mapping from one domain to another along with reverse mapping for reconstruction of the input images from both domains using two reconstruction losses: </span><span class="emphasis"><em><span class="koboSpan" id="kobo.108.1">L</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.109.1">CONSTA</span></em></span></sub><span class="koboSpan" id="kobo.110.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.111.1">L</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.112.1">CONSTB</span></em></span></sub><span class="koboSpan" id="kobo.113.1">.</span></p><p><span class="koboSpan" id="kobo.114.1">The parameters are shared between generators </span><span class="emphasis"><em><span class="koboSpan" id="kobo.115.1">G</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.116.1">AB</span></em></span></sub><span class="koboSpan" id="kobo.117.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.118.1">G</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.119.1">BA</span></em></span></sub><span class="koboSpan" id="kobo.120.1"> of two GANs and the generated images </span><span class="emphasis"><em><span class="koboSpan" id="kobo.121.1">x</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.122.1">BA</span></em></span></sub><span class="koboSpan" id="kobo.123.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.124.1">x</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.125.1">AB</span></em></span></sub><span class="koboSpan" id="kobo.126.1"> are then fed into the separate discriminators </span><span class="emphasis"><em><span class="koboSpan" id="kobo.127.1">L</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.128.1">DA</span></em></span></sub><span class="koboSpan" id="kobo.129.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.130.1">L</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.131.1">DB</span></em></span></sub><span class="koboSpan" id="kobo.132.1"> respectively:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.133.1"><img src="graphics/B08086_04_10.jpg" alt="The architecture and model formulation of DiscoGAN"/></span></div><p><span class="koboSpan" id="kobo.134.1">The total generator loss, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.135.1">L</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.136.1">G</span></em></span></sub><span class="koboSpan" id="kobo.137.1">, is the sum of two GAN losses of the coupled model and reconstruction</span><a id="id189" class="indexterm"/><span class="koboSpan" id="kobo.138.1"> loss of each partial model as shown (equation-7). </span><span class="koboSpan" id="kobo.138.2">And the total discriminator loss, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.139.1">L</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.140.1">D</span></em></span></sub><span class="koboSpan" id="kobo.141.1">, is the sum of the two discriminators losses </span><span class="emphasis"><em><span class="koboSpan" id="kobo.142.1">L</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.143.1">DA</span></em></span></sub><span class="koboSpan" id="kobo.144.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.145.1">L</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.146.1">DB</span></em></span></sub><span class="koboSpan" id="kobo.147.1">, which discriminate real and </span><a id="id190" class="indexterm"/><span class="koboSpan" id="kobo.148.1">fake images in domain A and domain B, respectively (equation- 8). </span><span class="koboSpan" id="kobo.148.2">In order to achieve bijective mapping having one-to-one correspondence, the DiscoGAN model is constrained by two </span><span class="emphasis"><em><span class="koboSpan" id="kobo.149.1">L</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.150.1">GAN</span></em></span></sub><span class="koboSpan" id="kobo.151.1"> losses and two </span><span class="emphasis"><em><span class="koboSpan" id="kobo.152.1">L</span></em></span><sub><span class="emphasis"><em><span class="koboSpan" id="kobo.153.1">CONST</span></em></span></sub><span class="koboSpan" id="kobo.154.1"> reconstruction losses.</span></p><p><span class="koboSpan" id="kobo.155.1">Injective mapping means that every member of </span><span class="strong"><strong><span class="koboSpan" id="kobo.156.1">A</span></strong></span><span class="koboSpan" id="kobo.157.1"> has its own unique matching member in </span><span class="strong"><strong><span class="koboSpan" id="kobo.158.1">B</span></strong></span><span class="koboSpan" id="kobo.159.1"> and surjective mapping means that every </span><span class="strong"><strong><span class="koboSpan" id="kobo.160.1">B</span></strong></span><span class="koboSpan" id="kobo.161.1"> has at least one matching </span><span class="strong"><strong><span class="koboSpan" id="kobo.162.1">A</span></strong></span><span class="koboSpan" id="kobo.163.1">.</span></p><p><span class="koboSpan" id="kobo.164.1">Bijective mapping means both injective and surjective are together and there is a perfect one-to-one correspondence between the members of the sets:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.165.1"><img src="graphics/B08086_04_11.jpg" alt="The architecture and model formulation of DiscoGAN"/></span></div></div><div class="section" title="Implementation of DiscoGAN"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec50"/><span class="koboSpan" id="kobo.166.1">Implementation of DiscoGAN</span></h2></div></div></div><p><span class="koboSpan" id="kobo.167.1">Let's now dig into the</span><a id="id191" class="indexterm"/><span class="koboSpan" id="kobo.168.1"> code to understand the concept (loss and measuring criteria) along with the architecture of DiscoGAN.</span></p><p><span class="koboSpan" id="kobo.169.1">The generator takes an input image of size 64x64x3 and feeds it through an encoder-decoder pair. </span><span class="koboSpan" id="kobo.169.2">The encoder part of the generator consists of five convolution layers with 4x4 filters, each followed by batch normalization and Leaky ReLU. </span><span class="koboSpan" id="kobo.169.3">The decoder part consists of five deconvolution layers with 4x4 filters, followed by a batch normalization and </span><code class="literal"><span class="koboSpan" id="kobo.170.1">ReLU</span></code><span class="koboSpan" id="kobo.171.1"> activation function, and outputs a target domain image of size 64x64x3. </span><span class="koboSpan" id="kobo.171.2">The following is the generator code snippet:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.172.1">class Generator(nn.Module):
 
            self.main = nn.Sequential(
            # Encoder
                nn.Conv2d(3, 64, 4, 2, 1, bias=False),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),
                nn.BatchNorm2d(64 * 2),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),
                nn.BatchNorm2d(64 * 4),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),
                nn.BatchNorm2d(64 * 8),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Conv2d(64 * 8, 100, 4, 1, 0, bias=False),
                nn.BatchNorm2d(100),
                nn.LeakyReLU(0.2, inplace=True),
             
             # Decoder
                nn.ConvTranspose2d(100, 64 * 8, 4, 1, 0, bias=False),
                nn.BatchNorm2d(64 * 8),
                nn.ReLU(True),
                nn.ConvTranspose2d(64 * 8, 64 * 4, 4, 2, 1, bias=False),
                nn.BatchNorm2d(64 * 4),
                nn.ReLU(True),
                nn.ConvTranspose2d(64 * 4, 64 * 2, 4, 2, 1, bias=False),
                nn.BatchNorm2d(64 * 2),
                nn.ReLU(True),
                nn.ConvTranspose2d(64 * 2, 64, 4, 2, 1, bias=False),
                nn.BatchNorm2d(64),
                nn.ReLU(True),
                nn.ConvTranspose2d(64,3, 4, 2, 1, bias=False),
                nn.Sigmoid()
                
                . </span><span class="koboSpan" id="kobo.172.2">. </span><span class="koboSpan" id="kobo.172.3">. 
                
            </span><span class="koboSpan" id="kobo.172.4">)</span></pre></div><p><span class="koboSpan" id="kobo.173.1">The discriminator is similar to the encoder part of the generator and consists of five convolution layers</span><a id="id192" class="indexterm"/><span class="koboSpan" id="kobo.174.1"> with 4x4 filters, each followed by a batch normalization and </span><code class="literal"><span class="koboSpan" id="kobo.175.1">LeakyReLU</span></code><span class="koboSpan" id="kobo.176.1"> activation function. </span><span class="koboSpan" id="kobo.176.2">Finally, we apply the </span><code class="literal"><span class="koboSpan" id="kobo.177.1">sigmoid</span></code><span class="koboSpan" id="kobo.178.1"> function on the final convolution layer (</span><code class="literal"><span class="koboSpan" id="kobo.179.1">conv-5</span></code><span class="koboSpan" id="kobo.180.1">) to generate a scalar probability score between [0,1] to judge real/fake data. </span><span class="koboSpan" id="kobo.180.2">The following is the discriminator code snippet:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.181.1">class Discriminator(nn.Module):

        self.conv1 = nn.Conv2d(3, 64, 4, 2, 1, bias=False)
        self.relu1 = nn.LeakyReLU(0.2, inplace=True)

        self.conv2 = nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False)
        self.bn2 = nn.BatchNorm2d(64 * 2)
        self.relu2 = nn.LeakyReLU(0.2, inplace=True)

        self.conv3 = nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False)
        self.bn3 = nn.BatchNorm2d(64 * 4)
        self.relu3 = nn.LeakyReLU(0.2, inplace=True)

        self.conv4 = nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False)
        self.bn4 = nn.BatchNorm2d(64 * 8)
        self.relu4 = nn.LeakyReLU(0.2, inplace=True)

        self.conv5 = nn.Conv2d(64 * 8, 1, 4, 1, 0, bias=False)
        
        . </span><span class="koboSpan" id="kobo.181.2">. </span><span class="koboSpan" id="kobo.181.3">. </span><span class="koboSpan" id="kobo.181.4">.
        
   </span><span class="koboSpan" id="kobo.181.5">return torch.sigmoid( conv5 ), [relu2, relu3, relu4]</span></pre></div><p><span class="koboSpan" id="kobo.182.1">Then we define the loss criteria for the generator and reconstruction using mean square error and binary cross entropy measures:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.183.1">recon_criterion = nn.MSELoss()
gan_criterion = nn.BCELoss()


optim_gen = optim.Adam( gen_params, lr=args.learning_rate, betas=(0.5,0.999), weight_decay=0.00001)
optim_dis = optim.Adam( dis_params, lr=args.learning_rate, betas=(0.5,0.999), weight_decay=0.00001)</span></pre></div><p><span class="koboSpan" id="kobo.184.1">Now we start </span><a id="id193" class="indexterm"/><span class="koboSpan" id="kobo.185.1">generating images from one domain to other and calculate the reconstruction loss to understand how well the original image is reconstructed after two translations (</span><code class="literal"><span class="koboSpan" id="kobo.186.1">ABA</span></code><span class="koboSpan" id="kobo.187.1"> or </span><code class="literal"><span class="koboSpan" id="kobo.188.1">BAB</span></code><span class="koboSpan" id="kobo.189.1">):</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.190.1">AB = generator_B(A)
BA = generator_A(B)

ABA = generator_A(AB)
BAB = generator_B(BA)

# Reconstruction Loss
recon_loss_A = recon_criterion( ABA, A )
recon_loss_B = recon_criterion( BAB, B )</span></pre></div><p><span class="koboSpan" id="kobo.191.1">Next, we calculate the generator loss and discriminator loss across each domain:</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.192.1"># Real/Fake GAN Loss (A)
A_dis_real, A_feats_real = discriminator_A( A )
A_dis_fake, A_feats_fake = discriminator_A( BA )

dis_loss_A, gen_loss_A = get_gan_loss( A_dis_real, A_dis_fake, gan_criterion, cuda )
fm_loss_A = get_fm_loss(A_feats_real, A_feats_fake, feat_criterion)

# Real/Fake GAN Loss (B)
B_dis_real, B_feats_real = discriminator_B( B )
B_dis_fake, B_feats_fake = discriminator_B( AB )

dis_loss_B, gen_loss_B = get_gan_loss( B_dis_real, B_dis_fake, gan_criterion, cuda )
fm_loss_B = get_fm_loss( B_feats_real, B_feats_fake, feat_criterion )

gen_loss_A_total = (gen_loss_B*0.1 + fm_loss_B*0.9) * (1.-rate) + recon_loss_A * rate
gen_loss_B_total = (gen_loss_A*0.1 + fm_loss_A*0.9) * (1.-rate) + recon_loss_B * rate</span></pre></div><p><span class="koboSpan" id="kobo.193.1">Finally, we </span><a id="id194" class="indexterm"/><span class="koboSpan" id="kobo.194.1">calculate the total loss of the </span><code class="literal"><span class="koboSpan" id="kobo.195.1">discogan</span></code><span class="koboSpan" id="kobo.196.1"> model by summing up the losses from two cross domains (</span><code class="literal"><span class="koboSpan" id="kobo.197.1">A</span></code><span class="koboSpan" id="kobo.198.1"> and </span><code class="literal"><span class="koboSpan" id="kobo.199.1">B</span></code><span class="koboSpan" id="kobo.200.1">):</span></p><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.201.1">if args.model_arch == 'discogan':
    gen_loss = gen_loss_A_total + gen_loss_B_total
    dis_loss = dis_loss_A + dis_loss_B</span></pre></div></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Generating handbags from edges with PyTorch"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec23"/><span class="koboSpan" id="kobo.1.1">Generating handbags from edges with PyTorch</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">In this example, we will generate</span><a id="id195" class="indexterm"/><span class="koboSpan" id="kobo.3.1"> realistic handbag images from corresponding edges using the </span><code class="literal"><span class="koboSpan" id="kobo.4.1">pix2pix</span></code><span class="koboSpan" id="kobo.5.1"> dataset from Berkley. </span><span class="koboSpan" id="kobo.5.2">Make sure</span><a id="id196" class="indexterm"/><span class="koboSpan" id="kobo.6.1"> you have PyTorch (</span><a class="ulink" href="http://pytorch.org/"><span class="koboSpan" id="kobo.7.1">http://pytorch.org/</span></a><span class="koboSpan" id="kobo.8.1">) and OpenCV (</span><a class="ulink" href="http://docs.opencv.org/trunk/d7/d9f/tutorial_linux_install.html"><span class="koboSpan" id="kobo.9.1">http://docs.opencv.org/trunk/d7/d9f/tutorial_linux_install.html</span></a><span class="koboSpan" id="kobo.10.1">) installed on your machine before going through the following steps:</span></p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="koboSpan" id="kobo.11.1">First clone</span><a id="id197" class="indexterm"/><span class="koboSpan" id="kobo.12.1"> the </span><code class="literal"><span class="koboSpan" id="kobo.13.1">git</span></code><span class="koboSpan" id="kobo.14.1"> repository and change the directory to </span><code class="literal"><span class="koboSpan" id="kobo.15.1">DiscoGAN</span></code><span class="koboSpan" id="kobo.16.1">:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.17.1">git clone https://github.com/SKTBrain/DiscoGAN.git
cd DiscoGAN</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.18.1">Next download the </span><code class="literal"><span class="koboSpan" id="kobo.19.1">edges2handbags</span></code><span class="koboSpan" id="kobo.20.1"> dataset using the following command:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.21.1">python ./datasets/download.py edges2handbags</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.22.1">And then apply image translation between two domains: edges and handbags with the downloaded dataset:</span><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.23.1">python ./discogan/image_translation.py --task_name='edges2handbags'</span></pre></div><div class="mediaobject"><span class="koboSpan" id="kobo.24.1"><img src="graphics/B08086_04_12.jpg" alt="Generating handbags from edges with PyTorch"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.25.1">Now, the images will be saved after every 1,000 iterations (as per the </span><code class="literal"><span class="koboSpan" id="kobo.26.1">image_save_interval</span></code><span class="koboSpan" id="kobo.27.1"> argument) per epoch, under the </span><code class="literal"><span class="koboSpan" id="kobo.28.1">results</span></code><span class="koboSpan" id="kobo.29.1"> directory with the respective task name used previously during the image translation step:</span><div class="mediaobject"><span class="koboSpan" id="kobo.30.1"><img src="graphics/B08086_04_13.jpg" alt="Generating handbags from edges with PyTorch"/></span></div></li></ol></div><p><span class="koboSpan" id="kobo.31.1">The following is a</span><a id="id198" class="indexterm"/><span class="koboSpan" id="kobo.32.1"> sample output of the</span><a id="id199" class="indexterm"/><span class="koboSpan" id="kobo.33.1"> cross-domain images generated from domain A to domain B:</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.34.1"><img src="graphics/B08086_04_14.jpg" alt="Generating handbags from edges with PyTorch"/></span><div class="caption"><p><span class="koboSpan" id="kobo.35.1">Figure-3: On the left-hand side are cross-domain (A -&gt; B -&gt; A) generated images (edges -&gt; handbags -&gt; edges), while on the right-hand side are cross-domain (B -&gt; A -&gt; B) generated images of (handbags -&gt; edges -&gt; handbags)</span></p></div></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Gender transformation using PyTorch"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec24"/><span class="koboSpan" id="kobo.1.1">Gender transformation using PyTorch</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">In this example, we will transform</span><a id="id200" class="indexterm"/><span class="koboSpan" id="kobo.3.1"> the gender of actor-to-actress or vice versa using facial images of celebrities from the </span><code class="literal"><span class="koboSpan" id="kobo.4.1">facescrub</span></code><span class="koboSpan" id="kobo.5.1"> dataset. </span><span class="koboSpan" id="kobo.5.2">Just like the previous example, please make sure you have PyTorch and OpenCV installed on your machine before executing the following steps:</span></p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="koboSpan" id="kobo.6.1">First clone the </span><code class="literal"><span class="koboSpan" id="kobo.7.1">git</span></code><span class="koboSpan" id="kobo.8.1"> repository and change directory to </span><code class="literal"><span class="koboSpan" id="kobo.9.1">DiscoGAN</span></code><span class="koboSpan" id="kobo.10.1"> (you can skip this step if you executed the previous example of generating handbags from edges):</span><div class="informalexample"><pre class="programlisting"><span class="koboSpan" id="kobo.11.1">git clone </span><span class="strong"><strong><span class="koboSpan" id="kobo.12.1">https://github.com/SKTBrain/DiscoGAN.git</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.13.1">cd DiscoGAN</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.14.1">Next download the </span><code class="literal"><span class="koboSpan" id="kobo.15.1">facescrub</span></code><span class="koboSpan" id="kobo.16.1"> dataset using the following command:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.17.1">python ./datasets/download.py facescrub</span></strong></span>
</pre></div></li><li class="listitem"><span class="koboSpan" id="kobo.18.1">And then apply image translation between two domains, male and female, with the</span><a id="id201" class="indexterm"/><span class="koboSpan" id="kobo.19.1"> downloaded dataset:</span><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong><span class="koboSpan" id="kobo.20.1">python ./discogan/image_translation.py --task_name= facescrub</span></strong></span>
</pre></div><div class="mediaobject"><span class="koboSpan" id="kobo.21.1"><img src="graphics/B08086_04_15.jpg" alt="Gender transformation using PyTorch"/></span></div></li><li class="listitem"><span class="koboSpan" id="kobo.22.1">Now, the images will be saved after every 1,000 iterations (as per the </span><code class="literal"><span class="koboSpan" id="kobo.23.1">image_save_interval</span></code><span class="koboSpan" id="kobo.24.1"> argument) per epoch, under the </span><code class="literal"><span class="koboSpan" id="kobo.25.1">results</span></code><span class="koboSpan" id="kobo.26.1"> directory with the respective task name (</span><code class="literal"><span class="koboSpan" id="kobo.27.1">facescrub</span></code><span class="koboSpan" id="kobo.28.1">) and epoch interval:</span><div class="mediaobject"><span class="koboSpan" id="kobo.29.1"><img src="graphics/B08086_04_16.jpg" alt="Gender transformation using PyTorch"/></span></div></li></ol></div><p><span class="koboSpan" id="kobo.30.1">The following is the sample output of the cross-domain images generated from domain A (male) to domain B (female):</span></p><div class="mediaobject"><span class="koboSpan" id="kobo.31.1"><img src="graphics/B08086_04_17.jpg" alt="Gender transformation using PyTorch"/></span><div class="caption"><p><span class="koboSpan" id="kobo.32.1">Figure-4: On the left-hand side are cross-domain (A -&gt; B -&gt; A) generated images (Male -&gt; Female -&gt; Male), while on the right-hand side are cross-domain (B -&gt; A -&gt; B) generated images of (Female -&gt; Male -&gt; Female)</span></p></div></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="DiscoGAN versus CycleGAN"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec25"/><span class="koboSpan" id="kobo.1.1">DiscoGAN versus CycleGAN</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">The main objective of both DiscoGAN (discussed previously) and CycleGAN (discussed in </span><a class="link" href="ch02.html" title="Chapter 2. Unsupervised Learning with GAN"><span class="koboSpan" id="kobo.3.1">Chapter 2</span></a><span class="koboSpan" id="kobo.4.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.5.1">Unsupervised Learning with GAN</span></em></span><span class="koboSpan" id="kobo.6.1">) introduces a new approach to the problem of image-to-image translation by </span><a id="id202" class="indexterm"/><span class="koboSpan" id="kobo.7.1">finding a mapping between a source domain X and a target domain Y for a given image, without pairing information.</span></p><p><span class="koboSpan" id="kobo.8.1">From an architecture standpoint both the models consist of two GANs that map one domain to its counterpart domain and compose their losses as functions of the traditional generator loss (normally seen in GANs) and the reconstruction loss/cycle consistency loss.</span></p><p><span class="koboSpan" id="kobo.9.1">There isn't a huge dissimilarity between the two models, except from the fact that DiscoGAN uses two reconstruction losses (a measure of how well the original image is reconstructed after the two translations X-&gt;Y-&gt;X), whereas CycleGAN uses a single cycle consistency loss with two translators F and G (F translates the image from domain X to domain Y and G performs the reverse) to make sure the two equilibriums (</span><span class="emphasis"><em><span class="koboSpan" id="kobo.10.1">F(G(b)) = b and G(F(a)) = a</span></em></span><span class="koboSpan" id="kobo.11.1">, given </span><span class="emphasis"><em><span class="koboSpan" id="kobo.12.1">a</span></em></span><span class="koboSpan" id="kobo.13.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.14.1">b</span></em></span><span class="koboSpan" id="kobo.15.1"> are images in domain X , Y respectively) are maintained.</span></p></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec26"/><span class="koboSpan" id="kobo.1.1">Summary</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">So far you have learned the approach of solving complex real-life problems (such as synthesizing images from text and discovering cross-domain relationships) by combining multiple GAN models together using StackGAN and DiscoGAN. </span><span class="koboSpan" id="kobo.2.2">In the next chapter, you will learn an important technique for dealing with small datasets in deep learning using pre-trained models and feature transfer and how to run your deep models at a large scale on a distributed system.</span></p></div></div></div></body></html>