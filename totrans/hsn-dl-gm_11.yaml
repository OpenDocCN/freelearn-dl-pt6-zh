- en: Rewards and Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rewards are a fundamental aspect of reinforcement learning, and the concept
    is easy to grasp. After all, we partly teach and train others—dogs and children,
    for instance—with reinforcement through rewards. The concept of implementing rewards
    or a `reward` function in a simulation can be somewhat difficult, and prone to
    a lot of trial and error. This is the reason for waiting until a later and more
    advanced chapter to talk about rewards, building `reward` functions, and reward
    assistance methods such as Curriculum Learning, Backplay, Curiosity Learning,
    and Imitation Learning / Behavioral Cloning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a quick summary of the concepts we will cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Rewards and `reward` functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sparsity of rewards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Curriculum Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Backplay
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Curiosity Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While this is an advanced chapter, it is also an essential one and not something
    you want to skip over. Likewise, many of the top-performing RL demos, such as
    AlphaStar from DeepMind, use the advanced algorithms in this chapter to teach
    agents to do tasks that were previously not thought possible.
  prefs: []
  type: TYPE_NORMAL
- en: Rewards and reward functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We often face this preconceived notion of rewards-based learning or training
    as comprising of an action being completed, followed by a reward, be it good or
    bad. While this notion of RL works completely fine for a single action-based task,
    such as the old multi-arm bandit problem we looked at earlier, or teaching a dog
    a trick, recall that reinforcement learning is really about an agent learning
    the value of actions by anticipating future rewards through a series of actions.
    At each action step, when the agent is not exploring, the agent will determine
    its next course of action based on what it perceives as having the best reward.
    What is not always so clear is what those rewards should represent numerically,
    and to what extent that matters. Therefore, it is often helpful to map out a simple
    set of `reward` functions that describe the learning behavior we want our agent
    to train on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s open up the Unity editor to the GridWorld example and learn how to create
    a set of `reward` functions and mappings that describe that training, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Open up the `GridWorld` example from the Assets | ML-Agents | Examples | GridWorld
    | Scenes folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the trueAgent object in the Hierarchy and then switch the agent's brain,
    at Grid Agent | Brain, to GridWorldLearning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the GridAcademy and set the Grid Academy | Brains | Control option to
    enabled.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select and disable the Main Camera in the scene. This will make the agent's
    camera the primary camera, and the one we can view the scene with.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open up and prepare a Python or Anaconda window for training. Check previous
    chapters or the Unity documentation if you need to remember how to do this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the scene and project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Launch the sample into training using the following command at the Python/Anaconda
    window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the first things you will appreciate about this sample is how quickly
    it trains. Remember that the primary reason the agent trains so quickly is because
    the state space is so small; 5x5 in this example. An example of the simulation
    running is shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9c4ababe-df7d-4469-8711-394029344d77.png)'
  prefs: []
  type: TYPE_IMG
- en: GridWorld example running on 5x5 grid
  prefs: []
  type: TYPE_NORMAL
- en: Run the sample until completion. It does not take long to run, even on older
    systems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Notice how the agent quickly goes from a negative reward to a positive reward
    as it learns to place the cube over the green +. However, did you notice that
    the agent starts training from a negative mean reward? The agent starts with a
    zero reward value, so let's examine where the negative reward is coming from.
    In the next section, we look at how to build the `reward` functions by looking
    at the code.
  prefs: []
  type: TYPE_NORMAL
- en: Building reward functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building `reward` functions can be quite simple, as this one will be, or extremely
    complex, as you may well imagine. While this step is optional for training these
    examples, it is almost mandatory when you go to build your own environments. It
    can also identify problems in your training, and ways of enhancing or easing training
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up the Unity editor and follow this exercise to build these sample `reward`
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: Select the trueAgent object in the Hierarchy window and then click the target
    icon beside the Grid Agent component.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select Edit Script from the Contact menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After the script opens in your editor, scroll down to the `AgentAction` method
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We want to focus on the highlighted lines, `AddReward` and `SetReward`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`AddReward(-.1f)`: This first line denotes a step reward. Every step the agent
    takes will cost the agent a negative reward. This is the reason we see the agent
    show negative rewards until it finds the positive reward.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SetReward(1f)`: This the final positive reward the agent receives, and it
    is set to the maximum value of `1`. In these types of training scenarios, we prefer
    to use a range of rewards from -1 to +1.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SetReward(-1f)`: This is the pit of death reward, and a final negative reward.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using each of the previous statements, we can map these to `reward` functions
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`AddReward(-.1f)` = ![](img/7a5780bb-6d40-4c81-9281-60fa27a826b0.png)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SetReward(1f)` = ![](img/46cf177b-acec-48c4-895b-52e4affbe006.png)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SetReward(-1f)` = ![](img/93703d4c-bc45-428f-8af7-cabf822b5d37.png)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: One thing to notice here is that `AddReward` is an incremental reward, while
    `SetReward` sets the final value. So, the agent only ever sees a positive reward
    by reaching the final goal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By mapping these `reward` functions, we can see that the only way an agent can
    learn a positive reward is by finding its way to a goal. This is the reason the
    agent begins with a negative reward, it essentially only first learns to avoid
    wasting time or moves until it randomly encounters the goal. From there, the agent
    can quickly assign value to states based on previous positive rewards received.
    The issue is that the agent first needs to encounter a positive reward before
    we begin with the actual training. We discuss this particular problem in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Sparsity of rewards
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We call the situation where an agent does not get enough, or any, positive
    rewards, a sparsity of rewards. The simplest way to show how a sparsity of rewards
    can happen is by example, and fortunately, the GridWorld example can easily demonstrate
    this for us. Open the editor to the GridWorld example and follow this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the GridWorld sample scene from where we left it in the last exercise.
    For the purposes of this exercise, it is also helpful to have trained the original
    sample to completion. GridWorld is one of those nice compact examples that train
    quickly and is an excellent place to test basic concepts, or even hyperparameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select the GridAcademy and change the Grid Academy | Reset Parameters | gridSize
    to `25`, as shown in the following screen excerpt:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/2fe193fd-2c8a-4336-ac0b-1d21aa78cf42.png)'
  prefs: []
  type: TYPE_IMG
- en: Setting the GridAcademy gridSize parameter
  prefs: []
  type: TYPE_NORMAL
- en: Save the scene and the project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Launch the sample into training with the following command from your Python/Anaconda
    window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This will launch the sample and, assuming you still have the agentCam as the
    main camera, you should see the following in the Game window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a0dfe420-200c-4c5c-bd83-ccb2c2ce722f.png)'
  prefs: []
  type: TYPE_IMG
- en: The GridWorld with a grid size of 25x25
  prefs: []
  type: TYPE_NORMAL
- en: We have extended the game play space from a 5x5 grid to a 25x25 grid, making
    the goal (+) symbol much more difficult for the agent to randomly find.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What you will quickly notice after a few reported iterations is how poorly the
    agent is performing in some cases even, reporting less than a -1 mean reward.
    What's more, the agent could continue training like this for a long time. In fact,
    it is possible the agent could never discover a reward within 100, 200, 1,000,
    or more iterations. Now, this may appear to be a problem of state, and, in some
    ways, you may think of it that way. However, remember that the input state into
    our agent is the same camera view, a state of 84x84 pixels image, and we have
    not changed that. So, for the purposes of this example, think of state in the
    policy RL algorithm as remaining fixed. Therefore, our best course of action in
    order to fix the problem is to increase the rewards.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stop the training example from the Python/Anaconda window by typing *Ctrl* +
    *C*. In order to be fair, we will increase the number of rewards for goals and
    deaths equally.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Back in the editor, select the GridAcademy and increase the numObstacles and
    numGoals on the Grid Academy | Reset Parameters component properties, as shown
    in the following excerpt:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a0980bb1-9142-4054-8d0d-873996c266bb.png)'
  prefs: []
  type: TYPE_IMG
- en: Updating the number of Obstacles and Goals
  prefs: []
  type: TYPE_NORMAL
- en: Save the scene and the project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Launch the training session with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is to denote that we are running the sample with five times the number
    of obstacles and goals.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let the agent train for 25,000 iterations and notice the performance increase.
    Let the agent train to completion and compare the results to our first run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The problem of sparsity of rewards is generally encountered more frequently
    in discrete action tasks, such as GridWorld/Hallway and so on. because the `reward`
    function is often absolute. In continuous learning tasks, the `reward` function
    is often more gradual and is typically measured by some progress to a goal, and
    not just the goal itself.
  prefs: []
  type: TYPE_NORMAL
- en: By increasing the number of obstacles and goals—the negative and positive rewards—we
    are able to train the agent much more quickly, although it is likely you will
    see very erratic cycles of training, and the agent never truly gets as good as
    the original. In fact, the training actually may diverge at some point later on.
    The reason for this is partly because of its limited vision, and we have only
    partially corrected the sparse rewards problem. We can, of course, fix the issue
    of sparse rewards in this example by simply increasing the number of goals and
    obstacles. You can go back and try a value of 25 for the number of obstacles and
    rewards and see much more stable, long-term results.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, in many RL problems, an increasing number of rewards is not an option,
    and we need to look at cleverer methods, as we will see in the next section. Fortunately,
    a number of methods have arisen, in very brief time, looking to address the problem
    of sparse or difficult rewards. Unity, being at the top, quickly jumped on and
    implemented a number of methods, the first of which we will look at is called
    Curriculum Learning, which we will discuss in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Curriculum Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Curriculum Learning allows for an agent to progressively learn a difficult task
    by stepping up the `reward` function. While the reward remains absolute, the agent
    finds or achieves the goal in a simpler manner, and so learns the purpose of the
    reward. Then, as the training progresses and as the agent learns, the difficulty
    of receiving a reward increases, which, in turn, forces the agent to learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unity, of course, has a few samples of this, and we will look at the `WallJump`
    example of how a Curriculum Learning sample is set up in the following exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the WallJump scene from the Assets | ML-Agents | Examples | WallJump |
    Scenes folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the Academy object in the Hierarchy window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click both Control options on Wall Jump Academy | Brains | Control parameter
    as shown in the following excerpt:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/561b1d1e-5aca-4ddf-8e80-565983988cad.png)'
  prefs: []
  type: TYPE_IMG
- en: Setting the multiple brains to learning
  prefs: []
  type: TYPE_NORMAL
- en: This sample uses multiple brains in order to better separate the learning by
    task. In fact, all the brains will be trained in tandem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Curriculum Learning uses a second configuration file to describe the curriculum
    or steps of learning the agent will undergo.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the `ML-Agents/ml-agents/config/curricul/wall-jump` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the `SmallWallJumpLearning.json` file in a text editor. The file is shown
    for reference as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This JSON file defines the configuration the SmallWallJumpLearning brain will
    take as part of its curriculum or steps to learning. The definition for all these
    parameters are well documented in the Unity documentation, but we will take a
    look at parameters from the documentation as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`measure` *–* What to measure learning progress, and advancement in lessons
    by:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: reward – Uses a measure received reward.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: progress – Uses ratio of steps/max_steps.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`thresholds` (float array) –Points in value of measure where the lesson should
    be increased.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_lesson_length` (int) *–* The minimum number of episodes that should be
    completed before the lesson can change. If a measure is set to reward, the average
    cumulative reward of the last `min_lesson_length` episodes will be used to determine
    if the lesson should change. Must be non-negative.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What we can see by reading this file is that there are three lessons set by
    a `measure` of `progress` defined by the number of episodes. The episode boundaries
    are defined at `.1` or 10%, `.3` or 30%, and `.5` or 50% of the total episodes.
    With each lesson, we set parameters defined by boundaries, and in this example
    the parameter is `small_wall_height` with a first lesson boundary of `1.5` to
    `2.0`, a second lesson boundary of `2.0` to `2.5`, and a third lesson at `2.5`
    to `4.0`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open up a Python/Anaconda window and prepare it for training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Launch the training session with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The extra bit that is highlighted adds the folder to the secondary curriculum
    configuration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will need to wait for at least half of the full training steps to run in
    order to see all three levels of training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This example introduced one technique we can use to solve the problem of sparse
    or difficult to achieve rewards. In the next section, we look at a specialized
    form of Curriculum Training called Backplay.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Backplay
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In late 2018, Cinjon Resnick released an innovative paper, titled *Backplay:* *Man
    muss immer umkehren*, ([https://arxiv.org/abs/1807.06919](https://arxiv.org/abs/1807.06919))
    that introduced a refined form of Curriculum Learning called Backplay. The basic
    premise is that you start the agent more or less at the goal, and then progressively
    move the agent back during training. This method may not work for all situations,
    but we will use this method with Curriculum Training to see how we can improve
    the VisualHallway example in the following exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the VisualHallway scene from the Assets |ML-Agents | Examples | Hallway
    | Scenes folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure the scene is reset to the default starting point. If you need to,
    pull down the source from ML-Agents again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the scene for learning using the VisualHallwayLearning brain, and make sure
    that the agent is just using the default visual observations of 84x84.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select the Academy object and in the Inspector window add a new Hallway Academy | Reset
    Parameter called `distance`, as shown in the following excerpt:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/cf28b7dd-01eb-4697-bab2-15e476686232.png)'
  prefs: []
  type: TYPE_IMG
- en: Setting a new Reset Parameter on the Academy
  prefs: []
  type: TYPE_NORMAL
- en: You can use Reset Parameters for more than just Curriculum Learning, as they
    can help you easily configure training parameters within the editor. The parameter
    we are defining here is going to set the distance, the agent is away from the
    back goal region. This sample is intended to show the concept of Backplay, and
    in order to properly implement it we would need to move the agent right in front
    of the proper goal—we will defer from doing this for now.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the VisualHallwayArea | Agent and open the Hallway Academy script in
    your code editor of choice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scroll down to the `AgentReset` method and adjust the top line to that shown
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This single line of code will adjust the starting offset of the agent to the
    now preset Reset Parameters of the Academy. Likewise, as the Academy updates those
    parameters during training, the agent will also see updated values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the file and return to the editor. The editor will recompile your code
    changes and let you know if everything is okay. A red error in the console will
    typically mean you have a compiler error, likely caused by incorrect syntax.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open a prepared Python/Anaconda window and run the training session with the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This will run the session in regular mode, without Curriculum Learning, but
    it will adjust the starting position of the agent to be closer to the goals. Let
    this sample run and see how well the agent performs now that it starts so close
    to the goals.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let the training run for a while and observe the difference in training from
    the original. One thing you will notice is that the agent can't help but run into
    the reward now, which is what we are after. The next piece we need to implement
    is the Curriculum Learning part, where we will move the agent back as it learns
    to find the reward in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Backplay through Curriculum Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last section, we implemented the first part of Backplay, which is having
    the agent start next to, or very close to the goal. The next part we need to accomplish
    is progressively moving the agent back to its intended starting point using Curriculum
    Learning. Open up the Unity editor to the VisualHallway scene again and follow
    these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the `ML-Agents/ml-agents/config` folder with a file explorer or command
    shell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new folder called `hallway` and navigate to the new folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open a text editor or create a new JSON text file called `VisualHallwayLearning.json`
    in the new directory. **JavaScript Object Notation** (**JSON**)is intended to
    describe objects in JavaScript, it has become a standard for configuration settings
    as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enter the following JSON text in the new file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This configuration file defines a curriculum that we will use to train an agent
    on Backplay. The file defines a `measure` of `rewards` and `thresholds` that define
    when the agent will advance to the next level of training. When a reward threshold
    is hit for a minimum episode length of `100` steps, than the training will advance
    to the next `distance` parameter. Notice how we define the distance parameter
    with `12`, representing a distance close to the goals, and then decreasing. You
    could, of course, create a function that maps different range values, but we will
    leave that up to you.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the file after you are done editing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Launch a training session from a Python/Anaconda window with the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'After the training starts, notice how the curriculum is getting set in the
    Python/Anaconda window, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6fe16960-1238-46f1-ba9e-737ee9c2831b.png)'
  prefs: []
  type: TYPE_IMG
- en: Watching the curriculum parameters getting set in training
  prefs: []
  type: TYPE_NORMAL
- en: Wait for the agent to train, and see how many levels of training it can accomplish
    before the end of the session.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, one thing we need to come clean about is that this sample is more an innovative
    example than a true example of Backplay. Actual Backplay is described as putting
    the agent at the goal and working backward. In this example, we are putting the
    agent almost at the goal and working backward. The difference is subtle, but,
    by now, hopefully you can appreciate that, in terms of training, it could be significant.
  prefs: []
  type: TYPE_NORMAL
- en: Curiosity Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, we have considered just the extrinsic or external rewards an agent
    may receive in an environment. The Hallway example, for instance, gives a +1 external
    reward when the agent reaches the goal, and a -1 external reward if it gets the
    wrong goal. However, real animals like us can actually learn based on internal
    motivations, or by using an internal `reward` function. A great example of this
    is a baby (a cat, a human, or whatever) that has an obvious natural motivation
    to be curious through play. The curiosity of playing provides the baby with an
    internal or intrinsic reward, but the actual act itself gives it a negative external
    or extrinsic reward. After all, the baby is expending energy, a negative external
    reward, yet it plays on and on in order to learn more general information about
    its environment. This, in turn, allows it to explore more of the environment and
    ultimately attain some very difficult goal, such as hunting, or going to work.
  prefs: []
  type: TYPE_NORMAL
- en: This form of internal or intrinsic reward modeling falls into a subclass of
    RL, called Motivated Reinforcement Learning. As you may well imagine, this whole
    arc of learning could have huge applications in gaming, from creating NPCs to
    more believable opponents that actually get motivated by some personality trait
    or emotion. Imagine having a computer opponent that can get angry, or even, compassionate?
    Of course, we are a long way from getting there, but in the interim, Unity has
    added an intrinsic reward system in order to model agent curiosity, and this is
    called Curiosity Learning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Curiosity Learning** (**CL**) was first developed by researchers at the University
    of California, Berkley, in a paper called *Curiosity-Driven Exploration by **Self-Supervised
    Prediction,* which you can find at [https://pathak22.github.io/noreward-rl/.](https://pathak22.github.io/noreward-rl/) The
    paper goes on to describe a system of solving sparse rewards problems using forward
    and inverse neural networks. They called the system an **Intrinsic Curiosity Module**
    (**ICM**), with the intent for it to be used as a layer or module on top of other
    RL systems. This is exactly what Unity did, and they have added this as a module
    to ML-Agents.'
  prefs: []
  type: TYPE_NORMAL
- en: The Lead Researcher at Unity, Dr. Arthur Juliani, has an excellent blog post
    on their implementation that can be found at [https://blogs.unity3d.com/2018/06/26/solving-sparse-reward-tasks-with-curiosity/](https://blogs.unity3d.com/2018/06/26/solving-sparse-reward-tasks-with-curiosity/).
  prefs: []
  type: TYPE_NORMAL
- en: 'ICM works by using an inverse neural network that is trained using the current
    and next observation of the agent. It uses an encoder to encode a prediction on
    what the action was between the two states, current and next. Then, the forward
    network is trained on the current observation and action in which it encodes to
    the next observation. The difference is then taken between the real and predicted
    encodings from the inverse and forward models. In this case, the bigger the difference,
    the bigger the surprise, and the more intrinsic the rewards. A diagram extracted
    from Dr. Juliani''s blog is shown as follows, describing how this works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c0c6c7f-8ad5-4fa4-a258-c0327d8e0ddf.png)'
  prefs: []
  type: TYPE_IMG
- en: Inner workings of the Curiosity Learning Module
  prefs: []
  type: TYPE_NORMAL
- en: The diagram shows the depiction of the two models and layers in blue, forward
    and inverse, with the blue lines depicting network flow, the green box representing
    the intrinsic model calculation, and the reward output in the form of the green
    dotted lines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, that''s enough theory, its time to see how this CL works in practice.
    Fortunately, Unity has a very well developed environment that features this new
    module that is called Pyramids. Let''s open Unity and follow the next exercise
    to see this environment in action:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the Pyramid scene from the Assets | ML-Agents | Examples | Pyramids | Scenes
    folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the AreaPB(1) to AreaPB(15) in the Hierarchy window and then deactivate
    these objects in the Inspector window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Leave the scene in player mode. For the first time, we want you to play the
    scene on your own and figure out the goal. Even if you read the blog or played
    the scene, try again, but this time, think what reward functions would need to
    be in place.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press Play in the editor and start playing the game in Player mode. If you have
    not played the game before or understand the premise, don't be surprised if it
    takes you a while to solve the puzzle.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, for those of you that didn't read or play ahead, here is the premise. The
    scene starts where the agent is randomly placed into an area of rooms with pyramids
    of stone in which one has a switch. The goal of the agent is to activate the switch
    that then spawns a pyramid of sand boxes with a large gold box on top. The switch
    turns from red to green after it is activated. After the pyramid appears, the
    agent then needs to knock the pyramid over and retrieve the gold box. It certainly
    is not the most complex of puzzles, but one that does require a bit of exploration
    and curiosity.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine if we tried to model this form of curiosity, or need to explore, with
    a set of `reward` functions. We would need a `reward` function for activating
    the button, moving to rooms, knocking over blocks, and, of course, getting the
    gold box. Then we would have to determine the value of each of those objectives,
    perhaps using some form of **Inverse Reinforcement Learning** (**IRL**). However,
    with Curiosity Learning, we can create the reward function for just the end goal
    of getting the box (+1), and perhaps a small negative step goal (.0001), then
    use intrinsic curiosity rewards to let the agent learn the remaining steps. Quite
    a clever trick, and we will see how this works in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The Curiosity Intrinsic module in action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With our appreciation of the difficulty of the Pyramids task, we can move on
    to training the agent with curiosity in the following exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the Pyramids scene in the editor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the AreaRB | Agent object in the Hierarchy window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Switch the Pyramid Agent | Brain for the PyramidsLearning brain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the Academy object in the Hierarchy window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enable the Control option on the Academy | Pyramid Academy | Brains | Control
    property, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/31a355e2-ae0d-4f6f-8635-5f620ace0450.png)'
  prefs: []
  type: TYPE_IMG
- en: Setting the Academy to Control
  prefs: []
  type: TYPE_NORMAL
- en: Open a Python or Anaconda console and prepare it for training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the `trainer_config.yaml` file located in the `ML-Agents/ml-agents/config`
    folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scroll down to the `PyramidsLearning` configuration section, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'There are three new configuration parameters highlighted in bold:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`use_curiosity`: Set this to `true` to use the module, but it is generally
    `false` by default.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`curiosity_strength`: This is how strongly the agent values the intrinsic reward
    of curiosity over the extrinsic ones.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`curiosity_enc_size`: This is the size of the encoded layer we compress the
    network to. If you think back to autoencoders, you can see the size of 256 is
    quite large, but also consider the size of the state space or observation space
    you may be encoding.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Leave the parameters at the values they are set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Launch the training session with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: While this training session may take a while, it can be entertaining to watch
    how the agent explores. Even with the current settings, using only one training
    area, you may be able to see the agent solve the puzzle on a few iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Since ICM is a module, it can quickly be activated for any other example we
    want to see the effects on, which is what we will do in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Trying ICM on Hallway/VisualHallway
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Not unlike the agents we train, we learn quite well from trial and error. This
    is the reason we practice, practice, and practice more of those very difficult
    tasks such as dancing, singing, or playing an instrument. RL is no different and
    requires the practitioner to learn the ins and outs training through the rigors
    of trial, error, and further exploration. Therefore, in this next exercise, we
    are going to combine Backplay (Curriculum Learning) and Curiosity Learning together
    into our old friend, the Hallway, and see what effect it has, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the Hallway or VisualHallway scene (your preference) as we last left it,
    with Curriculum Learning enabled and set to simulate Backplay.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the `trainer_config.yaml` configuration file location in the `ML-Agents/ml-agents/config`
    folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scroll down to the `HallwayLearning `or `VisualHallwayLearning` brain configuration
    parameters and add the following additional configuration lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This will enable the curiosity module for this example. We use the same settings
    for curiosity as we used for the last Pyrmarids example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make sure this sample is prepared for curriculum Backplay as we configured it
    in that section. If you need to, go back and review that section and add the capability
    to this example before continuing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This may require you to create a new curricula file that uses the same parameters
    as we did previously. Remember that the curricula file needs to have the same
    name as the brain it is being used against.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Python/Anaconda window prepared for training and start training with
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Let the training run until completion, as the results can be interesting and
    show some of the powerful possibilities of layering learning enhancements for
    extrinsic and intrinsic rewards.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This exercise showed how to run an agent with both Curriculum Learning simulating
    Backplay, and Curiosity Learning adding an aspect of agent motivation to the learning.
    As you may well imagine, intrinsic reward learning and the whole field of Motivated
    Reinforcement Learning may lead to some interesting advances and enhancements
    to our DRL.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will review a number of helpful exercises that should
    help you learn more about these concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While your motivation may vary as to why you are reading this book, hopefully
    by now you can appreciate the value of just doing things on your own. As always,
    we present these exercises for your enjoyment and learning, and hope you have
    fun completing them:'
  prefs: []
  type: TYPE_NORMAL
- en: Select another sample scene that uses discrete actions and write the reward
    functions that go with it. Yes, that means you will need to open up and look at
    the code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a continuous action scene and try writing the reward functions for it.
    While this one may be difficult, it is essential if you want to build your own
    control training agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add Curriculum Learning to one of the other discrete action samples we have
    explored. Decide on how you can break the training into levels of difficulty and
    create parameters for controlling the evolution of the training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add Curriculum Learning to a continuous action sample. This is more difficult,
    and you likely want to perform exercise number two first.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement actual Backplay on the Hallway environment by placing the agent starting
    at the goal and then, as the agent trains, move it back to the desired start with
    Curriculum Learning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement Backplay on another discrete action example you have run and see the
    effect it has on training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement Curiosity Learning on the VisualPyramids example and notice the difference
    in training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement Curiosity Learning on a continuous action example and notice the effect
    it has on training. Is it what you expected?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Disable Curiosity Learning on the Pyramids example and see what effect this
    has on agent training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Think of a way in which you could add Backplay to the VisualPyramids example.
    You'll get bonus points if you actually build it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As you can see, the exercises are getting more demanding as we progress through
    the book. Remember, even completing one or two of these exercises will make a
    difference in your take-away knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at a fundamental component of RL, and that is rewards.
    We learned that, when building training environments, it was best that we defined
    a set of `reward` functions our agent will live by. By understanding these equations,
    we get a better sense of how frequent or sparse rewards can negatively affect
    training. We then looked at a few methods, the first of which is called Curriculum
    Learning, that could be used to ease or step the agent's extrinsic rewards. After
    that, we explored another technique, called Backplay, that used a reverse play
    technique and Curriculum Training to enhance an agent's training. Finally, we
    looked at internal or intrinsic rewards, and the concept of Motivated Reinforcement
    Learning. We then learned that the first intrinsic reward system developed into
    ML-Agents was to give an agent a motivation for curiosity. We looked at how to
    use Curiosity Learning on a few examples, and even incorporated it with Backplay
    via Curriculum Learning.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we look to more reward helper solutions in the form of
    Imitation and Transfer Learning, where we will learn how a human's gameplay experience
    can be mapped to a form of learning called Imitation Learning or Behavioral Cloning.
  prefs: []
  type: TYPE_NORMAL
