- en: Deep Learning for NLU and NLG Problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen the rule-based approach and various machine learning techniques
    to solve NLP tasks in the previous chapters. In this chapter, we will see the
    bleeding edge subset of machine learning technique called **deep learning** (**DL**).
    In the past four to five years, neural networks and deep learning techniques have
    been creating a lot of buzz in the artificial intelligence area because many tech
    giants use these cutting-edge techniques to solve real-life problems, and the
    results from these techniques are extremely impressive. Tech giants such as Google,
    Apple, Amazon, OpenAI, and so on spend a lot of time and effort to create innovative
    solutions for real-life problems. These efforts are mostly to develop artificial
    general intelligence and make the world a better place for human beings.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first understand the overall AI, in general, to give you a fair idea
    of why deep learning is creating a lot of buzz nowadays. We will cover the following
    topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: How NLU and NLG are different from each other
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The basics of neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building NLP and NLG applications using various deep learning techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After understanding the basics of DL, we will touch on some of the most recent
    innovations happening in the deep learning field. So let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: An overview of artificial intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will see the various aspects of AI and how deep learning
    is related to AI. We will see the AI components, various stages of AI, and different
    types of AI; at the end of this section, we will discuss why deep learning is
    one of the most promising techniques in order to achieve AI.
  prefs: []
  type: TYPE_NORMAL
- en: The basics of AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we talk about AI, we think about an intelligent machine, and this is the
    basic concept of AI. AI is an area of science that is constantly progressing in
    the direction of enabling human-level intelligence in machines. The basic idea
    behind AI is to enable intelligence in machines so that they can also perform
    some of the tasks performed only by humans. We are trying to enable human-level
    intelligence in machines using some cool algorithmic techniques; in this process,
    whatever kind of intelligence is acquired by the machines is artificially generated.
    Various algorithmic techniques that are used to generate AI for machines are mostly
    part of machine learning techniques. Before getting into the core machine learning
    and deep learning part, we will understand other facts related to AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'AI is influenced by many branches; in *Figure 9.1*, we will see those branches
    that heavily influence artificial intelligence as a single branch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3eb3f19e-7bdd-44de-b847-6e7ee61f62f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: AI influence by other branches'
  prefs: []
  type: TYPE_NORMAL
- en: Components of AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First of all, we will see the key components of AI. These components will be
    quite useful for us to understand that direction the world is going.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to me, there are two components, that you can see in *Figure 9.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f6deded-5916-4866-97a9-5155f078c649.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: Components of AI'
  prefs: []
  type: TYPE_NORMAL
- en: Let's see the AI components in detail. We will also see some examples.
  prefs: []
  type: TYPE_NORMAL
- en: Automation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automation is a well-known component of AI. People around the world work heavily
    on automation and we have achieved tremendous success in the area of automatic
    tasks performed by machines. We will look at some examples that are intuitive
    enough for you to understand the automation concept in AI.
  prefs: []
  type: TYPE_NORMAL
- en: In the automobile sector, we are using automatic robots to manufacture vehicles.
    These robots follow a set of instructions and perform the definite tasks. Here,
    these robots are not intelligent robots that can interact with humans and ask
    questions or respond to human questions, but these robots are just following a
    set of instructions to achieve great accuracy and efficiency in manufacturing
    with high speed. So these kinds of robots are examples of automation in the AI
    area.
  prefs: []
  type: TYPE_NORMAL
- en: The other example is in the area of DevOps. Nowadays, DevOps is using machine
    learning to automate many human-intensive processes such as, in order to maintain
    in-house servers, the DevOps team gets a bunch of recommendations after analyzing
    various logs of servers, and after getting the recommendations, another machine
    learning model prioritizes the alerts and recommendations. This kind of application
    really saves time for the DevOps team to deliver great work on time. These kinds
    of applications really help us understand that automation is a very important
    component of AI.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's see how intelligence is going to impact the world as part of AI.
  prefs: []
  type: TYPE_NORMAL
- en: Intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we say intelligence, as humans, our expectations are really high. Our goal
    is that we really want machines to understand our behavior and emotions. We also
    want machines to react intelligently based on human actions and all the reactions
    generated by machines should be in such a way that it mimics human intelligence.
    We want to achieve this goal since the mid 1900s. Around the globe, many researchers,
    groups of scientists, and communities are doing a lot of cool research to make
    machines as intelligent as humans.
  prefs: []
  type: TYPE_NORMAL
- en: We want that after acquiring intelligence, machines will perform majority of
    the tasks for humans with better accuracy and this is the single broad expectation.
    During the last four-five years, we have started to successfully achieve this
    broad goal and, as a result of so many years of efforts, Google recently announced
    that Google Assistant can hear natural language from humans and interpret the
    speech signal as accurately as a human. The other example is that the Facebook
    research group did a very powerful research in order to build a system that is
    good at applying reasoning for questions and answers. Tesla and Google self-driving
    cars are a complex AI system but very useful and intelligent. Self-driving cars
    and chatbots are part of narrow AI. You can also find many other examples on the
    web, that are coming out now and then.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are certain subcomponents that can be included as part of intelligence.
    Refer to *Figure 9.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/554c0bd2-6c22-48cd-83d8-298ffc080763.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3: Subcomponents of intelligence'
  prefs: []
  type: TYPE_NORMAL
- en: Intelligence is a combination of all the components described in the preceding
    figure. All these components--reasoning, learning, learning from experience, problem
    solving, perception, and linguistics intelligence--come very naturally to humans
    but not to machines. So we need the techniques that enable intelligence for machines.
  prefs: []
  type: TYPE_NORMAL
- en: Before learning the name of the techniques that we will use later in this chapter,
    let's understand the various stages of AI.
  prefs: []
  type: TYPE_NORMAL
- en: Stages of AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are three main stages for the AI system. We will see the following stages
    in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine intelligence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine consciousness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before getting into the details of each stage of AI, refer to *Figure 9.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2e3f1cb-a353-48e5-9db6-a6d10b86c164.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.4: Stages of AI (Image credit: https://cdn-images-1.medium.com/max/1600/0*aefkt8m-V66Wf5-j.png)'
  prefs: []
  type: TYPE_NORMAL
- en: We will begin from the bottom to the top, so we will understand the machine
    learning stage first, then machine intelligence, and finally machine consciousness.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have learned a lot of things about machine learning in the previous chapters,
    but I want to give you an AI perspective of it in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: ML techniques are a set of algorithms that explain how to generate or reach
    the defined output. This kind of algorithm is used by intelligent systems that
    try to learn from experience. The system that uses MLL algorithms is keen to learn
    from the historical data or real-time data. So, at this stage of AI, we focus
    on algorithms that learn patterns or specific structures from the data using features
    that we have provided to the ML system. To make it clear, let's take an example.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you want to build a sentiment analysis application. We can use historical
    tagged data, hand-crafted features, and the Naive Bayes ML algorithm. As a result,
    we can have an intelligent system that has learned from its learning example--how
    to provide a sentiment tag for an unseen new data instance.
  prefs: []
  type: TYPE_NORMAL
- en: Machine intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine intelligence is again a set of algorithms, but most of the algorithms
    are heavily influenced by how the human brain learns and thinks. Using neuroscience,
    biology, and mathematics, the AI researcher came up with a set of advance-level
    algorithms that help machines to learn from the data without providing hand-crafted
    features. In this stage, algorithms use unlabeled or labeled data. Here, you just
    define the end goal, and the advanced algorithms figure out their own way to achieve
    the expected result.
  prefs: []
  type: TYPE_NORMAL
- en: If you compare the algorithms that we are using at this stage with the traditional
    ML algorithms, then the major difference is that, in this machine intelligence
    stage, we are not giving hand-crafted features as input to any algorithm. As these
    algorithms are inspired by human brains, the algorithm itself learns the features
    and patterns and generates the output. Currently, the world of AI is in this stage.
    People across the world use these advanced algorithms that seem very promising
    to achieve human-like intelligence for machines.
  prefs: []
  type: TYPE_NORMAL
- en: '**Artificial neural networks** (**ANNs**) and deep learning techniques are
    used to achieve machine intelligence.'
  prefs: []
  type: TYPE_NORMAL
- en: Machine consciousness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine consciousness is one of the most discussed topics in AI as our end goal
    is to reach here.
  prefs: []
  type: TYPE_NORMAL
- en: We want machines to learn the way humans learn. As humans, we don't need a lot
    of data; we don't take so much time to understand the abstract concepts. We learn
    from small amounts of data or without data. Most of the time, we learn from our
    experiences. If we want to build a system that is as conscious as a human, then
    we should know how to generate consciousness for machines. However, are we fully
    aware how our brain works and reacts in order to transfer this knowledge to machines
    and make them as conscious as we are? Unfortunately, right now we aren't aware
    of this. We expect that in this stage, machines learn without data or with very
    small amounts of data and use their self-experience to achieve the defined output.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a lot of interesting researches going on, and I encourage you to
    check out this YouTube video of researcher John Searle, who talks about machine
    consciousness: [https://www.youtube.com/watch?v=rHKwIYsPXLg](https://www.youtube.com/watch?v=rHKwIYsPXLg).
    This video may give you a fresh perspective about consciousness in AI.'
  prefs: []
  type: TYPE_NORMAL
- en: We have seen the various stages of AI. ANNs and deep learning are a part of
    the machine intelligence stage. At the end of the *A brief overview of deep learning*
    section, you have the necessary details that can help you understand why deep
    learning is the new buzzword in AI.
  prefs: []
  type: TYPE_NORMAL
- en: Now we are going to see the various types of AI.
  prefs: []
  type: TYPE_NORMAL
- en: Types of artificial intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are three types of AI, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Artificial narrow intelligence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artificial general intelligence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artificial superintelligence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artificial narrow intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Artificial narrow intelligence** (**ANI**) is a type of AI that covers some
    of the basic tasks such as template-based chatbot, basic personal assistant application
    like the initial versions of Siri by Apple.'
  prefs: []
  type: TYPE_NORMAL
- en: This type of intelligence is majorly focused on basic prototyping of applications.
    This type of intelligence is the starting point for any application and then you
    can improve the basic prototype. You can add the next layer of intelligence by
    adding artificial general intelligence, but only if your end users really need
    that kind of functionality. We have also seen this kind of basic chatbot in [Chapter
    7](0dc5bd44-3b7d-47ac-8b0d-51134007b483.xhtml), *Rule-Based System for NLP*.
  prefs: []
  type: TYPE_NORMAL
- en: Artificial general intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Artificial general intelligence** (**AGI**) is a type of AI that is used
    to build systems that are capable of performing human-level tasks. What do I mean
    by human-level tasks? Tasks such as building self-driving cars. Google self-driving
    cars and Tesla autopilot are the most famous examples. Humanoid robots also try
    to use this type of AI.'
  prefs: []
  type: TYPE_NORMAL
- en: NLP-level examples are sophisticated chatbots that ignore spelling mistakes
    and grammatical mistakes and understand your query or questions. The deep learning
    techniques seem very promising for the understanding of the natural language by
    humans.
  prefs: []
  type: TYPE_NORMAL
- en: We are now at a stage where people and communities around the world use basic
    concepts, and by referring to each other's research works, try to build systems
    that have AGI.
  prefs: []
  type: TYPE_NORMAL
- en: Artificial superintelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The way to achieve **artificial superintelligence** (**ASI**) is a bit difficult
    for us because, in this type of AI, we expect that machines are smarter than humans
    in order to learn specific tasks and capable enough to perform multiple tasks
    as humans do in their life. This kind of superintelligence is right now a dream
    for us, but we are trying to achieve this in such a way that machines and systems
    always complement human skills and will not create a threat for humans.
  prefs: []
  type: TYPE_NORMAL
- en: Goals and applications of AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the time and section where we need to understand the goals and applications
    for AI in general for various areas. These goals and applications are just to
    give you an idea about the current state of AI-enabled applications, but if you
    can think of some crazy but useful application in any area, then you should try
    to include it in this list. You should try to implement various types and stages
    of AI in that application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s see the areas where we want to integrate various stages of AI and
    make those applications AI-enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: Reasoning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Natural language processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robotics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing general intelligence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computer vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated learning and scheduling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speech analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can refer to *Figure 9.5**,* which shows many different areas and related
    applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bb5e865d-edf1-43d8-8e2d-b739a33dfbed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5 : Various areas of AI and applications (Image credit: http://vincejeffs.com/)'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's see the applications from some of the areas in the preceding list.
  prefs: []
  type: TYPE_NORMAL
- en: AI-enabled applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, I will give you a brief idea about AI-enabled applications. Some of the
    applications are related to the NLP domain as well:'
  prefs: []
  type: TYPE_NORMAL
- en: Enabling reasoning for any system will be very exciting stuff. Under this area,
    we can build a Q/A system that can use the reasoning to derive answers for the
    asked questions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we can enable the reasoning for an AI-based system, then those systems will
    be very good at decision making and will improvise the existing decision making
    system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In machine learning, we want the perfect architecture of an ML-based application
    that can be decided by machines themselves. This is, according to me, an AI-enabled
    application in ML.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we are talking in terms of AI-enabled NLP applications, then we really
    need NLP systems that can understand the context of human natural language and
    react and behave more like humans.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Humanoid robots are the best application to describe an AI-enabled system. Robots
    should acquire perception, which is a long term AI goal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we are talking about general intelligence, according to me, systems should
    react more like humans do. Particularly machine reactions should match with real
    human behavior. After analyzing certain situations, machines should react the
    same or better than humans.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nowadays, computer vision has many applications that give us solid proof that
    AI will be achieved very soon in this area. The applications are object identification,
    image recognition, skin cancer detection using image recognition techniques, generating
    face images from machines, generating text for images and vice versa, and others.
    All these applications give us concrete proof about AI-driven computer vision.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated learning and scheduling is a kind of building system that works for
    your personal assistance and manages your schedule. Regarding the AI part, we
    really expect that every user of the system will get a personalized experience
    so automating the learning of a person's personal choice is very important for
    AI-driven scheduling. To achieve this goal, automated learning systems should
    also learn how to choose the best suited model for a particular user.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speech analysis is a different form of NL, but unfortunately, we are not talking
    about this concept in this book. Here, we are talking about a speech recognition
    system in terms of a potential AI-enabled area. By enabling AI with this speech
    recognition area, we can understand the human environment and thinking process
    that is generated under the effect of a person's sociology, psychology, and philosophy.
    We can also predict their personality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After seeing all these fascinating applications, there are three really interesting
    questions that come to our mind: what are the reasons that lead us to produce
    AI-driven systems, why the time is so perfect for us to build an AI-driven system,
    and how can we build an AI-enabled system?'
  prefs: []
  type: TYPE_NORMAL
- en: Since the mid 1900s, we are trying to bring intelligence in machines. During
    this phase, researchers and scientists have given a lot of cool concepts. For
    example, an artificial neuron, also known as **McCulloch-Pitts model** (**MCP**),
    is inspired by the human brain and the purpose of this concept is to understand
    the human brain biological working process and represent this process in terms
    of mathematics and physics. So it will be helpful to implement AI for machines.
  prefs: []
  type: TYPE_NORMAL
- en: 'They successfully gave the mathematical representation of how a single neuron
    works, but there is one outcome of this model that wasn''t good for training purposes.
    So researcher Frank Rosenblatt, in his 1958 paper, came up with the *perceptron*,
    introducing *dynamic weight* and *threshold* concepts. After this, many researchers
    have developed concepts such as backpropagation and multilayer neural networks
    based on the earlier concepts. The research community wanted to implement the
    developed concepts in practical applications, and the first researcher, Geoffrey
    Hinton, demonstrated the use of the generalized backpropagation algorithm to train
    multilayer neural networks. From that point, researchers and communities started
    to use this generalized model, but in the late 1900s, the amount of data was less
    compared to now and computational devices were slow as well as costly. So we didn''t
    get the expected results. However, with the result that was achieved at the time,
    the researcher had faith that these are the concepts that will be used to enable
    an AI-driven world. Now we have a lot of data as well as computation devices that
    are fast, cheap, and capable enough to process large amounts of data. When we
    apply these old concepts of ANNs in the current era to develop applications such
    as a universal machine translation system, speech recognition system, image recognition
    system, and so on, we get very promising results. Let''s take an example. Google
    is using ANNs to develop a universal machine translation system and this system
    will translate multiple languages. This is because we have large datasets available
    and also fast computation capabilities that can help us process the datasets using
    ANNs. We have used neural networks that are not one or two, but many, layers deep.
    The result achieved is so impressive that every big tech giant is using deep learning
    models to develop an AI-enabled system. According to me, data, computational capabilities,
    and solid old concepts are the key components that are perfect to develop an AI-driven
    system. You can refer to *Figure 9.6* for a brief history of the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22310548-27cd-42aa-81c0-7e85d666f896.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.6: History of ANN (Image credit: https://image.slidesharecdn.com/deeplearning-170124234229/95/deep-learning-9-638.jpg?cb=1485303074)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.7* will give you an idea about the long term history of neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6e94d72-7ea6-4939-8352-9451f1524f9c.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.7: History of ANN (Image credit: http://qingkaikong.blogspot.in/2016/11/machine-learning-3-artificial-neural.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s move toward the next question: how can we enable AI? The answer
    is deep learning. This is one of the most favorite techniques to enable AI for
    non-AI systems. There are a few cases where deep learning is not used to enable
    AI, but in the NLP domain, deep learning is majorly used to enable AI. To develop
    general intelligence, we can use deep learning. We get very promising results
    from this technique. Experiments such as machines generating human faces understands
    the speech of humans more accurately in a noisy environment, self-driving cars,
    reasoning for question answer systems are just a few of the experiments. The deep
    learning techniques are using lots and lots of data and high computational capability
    to train systems on the given data. When we apply the right deep learning model
    on a large amount of data, we will get a magical, impressive, and promising result.
    These are the reasons deep learning is creating lot of buzz nowadays. So I guess
    now you know why deep learning is the buzzword in the AI world.'
  prefs: []
  type: TYPE_NORMAL
- en: Further in this chapter, we will see deep learning techniques in detail and
    develop NLP applications using deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing NLU and NLG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already seen the NLU and NLG definitions, details, and differences in
    [Chapter 3](f65e61fc-1d20-434f-b606-f36cf401fc41.xhtml), *Understanding Structure
    of Sentences*. In this section, we are comparing these two subareas of NLP in
    terms of an AI-enabled application.
  prefs: []
  type: TYPE_NORMAL
- en: Natural language understanding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier, we have seen that NLU is more about dealing with an understanding of
    the structure of the language, whether it is words, phrases, or sentences. NLU
    is more about applying various ML techniques on already generated NL. In NLU,
    we focus on syntax as well as semantics. We also try to solve the various types
    of ambiguities related to syntax and semantics. We have seen the lexical ambiguity,
    syntactic ambiguity, semantic ambiguity, and pragmatics ambiguity.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's see where we can use AI that helps machines understand the language
    structure and meaning more accurately and efficiently. AI and ML techniques are
    not much behind to address these aspects of NL. To give an example, deep learning
    gives us an impressive result in machine translation. Now when we talk about solving
    syntactic ambiguity and semantic ambiguity, we can use deep learning. Suppose
    you have a NER tool that will use deep learning and Word2vec, then we can solve
    the syntactic ambiguity. This is just one application, but you can also improve
    parser results and POS taggers.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's talk about pragmatics ambiguity, where we really need AGI as well
    as ASI. This ambiguity occurs when you try to understand the long distance context
    of a sentence with other previously written or spoken sentences, and it also depends
    on the speaker's intent of speaking or writing.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see an example of pragmatics ambiguity. You and your friend are having
    a conversation, and your friend told you long ago that she had joined an NGO and
    would do some social activity for poor students. Now you ask her how was the social
    activity. In this case, you and your friend know about what social activities
    you are talking about. This is because as humans, our brain stores the information
    as well as knows when to fetch that information, how to interpret it, and what
    is the relevance of the fetched information to your current conversation that
    you are having with your friend. Both you and your friend can understand the context
    and relevance of each other's questions and answers, but machines don't have this
    kind of capability of understanding the context and speaker's intent.
  prefs: []
  type: TYPE_NORMAL
- en: This is what we expect from an intelligent machine. We want the machine to understand
    this kind of complex situation as well. Enabling this kind of capability of resolving
    pragmatics ambiguity is included in MSI. This will definitely be possible in future,
    but right now, we are at a stage where machines are trying to adopt AGI and using
    statistical techniques to understand semantics.
  prefs: []
  type: TYPE_NORMAL
- en: Natural language generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NLG is an area where we are trying to teach machines how to generate NL in
    a sensible manner. This, in itself, is a challenging AI task. Deep learning has
    really helped us perform this kind of challenging task. Let me give you an example.
    If you are using Google''s new inbox, then you may notice that when you reply
    to any mail, you will get three most relevant replies in the form of sentences
    for the given mail. Google used millions of e-mails and made an NLG model that
    was trained using deep learning to generate or predict the most relevant reply
    for any given mail. You can refer to *Figure 9.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d3f8aff-2f1b-416f-bbb4-014607e475c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.8: Google''s new inbox smart reply'
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from this application, there is another application: after seeing the
    images, the machine will provide a caption of a particular image. This is also
    an NLG application that uses deep learning. The task of generating language is
    less complex than the generation of NL, that is, coherence, and this is where
    we need AGI.'
  prefs: []
  type: TYPE_NORMAL
- en: We have talked a lot about the term, deep learning, but how does it actually
    work and why is it so promising? This we will see in an upcoming section of this
    chapter. We will explain the coding part for NLU and NLG applications. We are
    also going to develop NLU and NLG applications from scratch. Before that, you
    must understand the concepts of ANN and deep learning. I will include mathematics
    in upcoming sections and try my best to keep it simple. Let's dive deep into the
    world of ANN and deep learning!
  prefs: []
  type: TYPE_NORMAL
- en: A brief overview of deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Machine learning is a sub-branch of AI and deep learning is a sub-branch of
    ML. Refer to *Figure 9.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/335cc3a3-947b-4a28-b9b8-5fab7d792e9b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.9: Deep learning as a sub-branch of ML'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning uses ANN that is not just one or two layers, but many layers deep,
    called **deep neural network** (**DNN**). When we use DNN to solve a given problem
    by predicting a possible result for the same problem, it is called **deep learning**.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning can use labeled data or unlabeled data, so we can say that deep
    learning can be used in supervised techniques as well as unsupervised techniques.
    The main idea of using deep learning is that using DNN and a humongous amount
    of data, we want the machines to generalize the particular tasks and provide us
    with a result that we think only humans can generate. Deep learning includes a
    bunch of techniques and algorithms that can help us solve various problems in
    NLP such as machine translation, question answering system, summarization, and
    so on. Apart from NLP, you can find other areas of applications such as image
    recognition, speech recognition, object identification, handwritten digit recognition,
    face detection, and artificial face generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning seems promising to us in order to build AGI and ASI. You can
    see some of the applications where deep learning has been used, in *Figure 9.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40f17204-35b6-4839-a335-e0562411b318.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.10: Applications using deep learning (Image credit: http://www.fullai.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: This section gives you a brief overview about deep learning. We will see many
    aspects of deep learning in this chapter, but before that, I want to explain concepts
    that are related to deep learning and ANN. These concepts will help you understand
    the technicality of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Basics of neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of neural networks is one of the oldest techniques in ML. Neural
    network is derived from the human brain. In this section, we will see the human
    brain's components and then derive the ANN.
  prefs: []
  type: TYPE_NORMAL
- en: In order to understand ANN, we first need to understand the basic workflow of
    the human brain. You can refer to F*igure 9.11:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2cf65ec-ebf8-470e-af8a-3d7c730d07c1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.11: Neurons of human brain (Image credit: https://en.wikipedia.org/wiki/File:Blausen_0657_MultipolarNeuron.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The human brain consists of an estimated hundreds of billion nerve cells called
    **neurons**. Each neuron performs three jobs that are mentioned as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Receiving a signal: It receives a set of signals from its **dendrites**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deciding to pass the signal to the cell body: It integrates those signals together
    to decide whether or not the information should be passed on to the cell body'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sending the signal: If some of the signals pass a certain threshold, it sends
    these signals, called **action potentials**, onward via its axon to the next set
    of neurons'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can refer to *Figure 9.12*, which demonstrate components that are used
    to perform these three jobs in the biological neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e260ec9-4f13-4237-afbc-fb74a9120076.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.12: Demonstrates the components that performs the three jobs'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a very brief overview on how our brain learns and processes some decision.
    Now the question is: can we build an ANN that uses a non-biological substrate
    like silicon or other metal? We can build it, and then by providing a lot of computer
    power and data, we can solve the problems much faster as compared to humans.'
  prefs: []
  type: TYPE_NORMAL
- en: ANN is a biologically inspired algorithm that learns to identify the pattern
    in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen a brief history of ANN earlier in this chapter, but now it's time
    to see ANN and its history in detail.
  prefs: []
  type: TYPE_NORMAL
- en: The first computation model of the neuron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In mid-1943, researchers McCulloch-Pitts invented the first computation model
    of a neuron. Their model is fairly simple. The model has a neuron that receives
    binary inputs, sums them, and, if the sum exceeds a certain threshold value, then
    output is one, if not then output is zero. You can see the pictorial representation
    in *Figure 9.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fca196dd-1bd8-44ae-959a-c85e4ae2733c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.13: McCulloch-Pitts computation model of neuron (Image credit for
    NN: http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html)'
  prefs: []
  type: TYPE_NORMAL
- en: It looks very simple, but as it was invented in the early days of AI, an invention
    of this kind of model was a really big deal.
  prefs: []
  type: TYPE_NORMAL
- en: Perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After a few years of the invention of the first computational model of neuron,
    a psychologist named Frank Rosenblatt found out that the McCulloch-Pitts model
    did not have the mechanism to learn from the input data. So he invented the neural
    network that was built on the idea of the first computational model of neuron.
    Frank Rosenblatt called this model the **perceptron**. It is also called a **single-layer
    feedforward neural network**. We call this model a feed forward neural network
    because, in this neural network, data flows in only one direction--the forward
    direction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s understand the working of the perceptron that has incorporated the
    idea of having weights on the given inputs. If you provide some training set of
    input output examples, it should learn a function from it by increasing and decreasing
    the weights continuously for each of the training examples, depending on what
    was the output of the given input example. These weight values are mathematically
    applied to the input such that after each iteration, the output prediction gets
    more accurate. This whole process is called **training**. Refer to *Figure 9.14*
    to understand the schematic of Rosenblatt''s perceptron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db870c10-9eaf-4c45-8c20-98c94b354709.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.14: Schematic of Rosenblatt''s perceptron (Image credit for NN: http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html)'
  prefs: []
  type: TYPE_NORMAL
- en: We will see the ANN-related mathematical concepts such as gradient descent,
    activation function, and loss function in the next section. So get ready for some
    mathematics!
  prefs: []
  type: TYPE_NORMAL
- en: Understanding mathematical concepts for ANN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section is very important because ML, ANN, and DL use a bunch of mathematical
    concepts and we are going to see some of the most important ones. These concepts
    will really help you optimize your ML, ANN, and DL models. We will also see different
    types of activation functions and some tips about which activation function you
    should select. We are going to see the following mathematical concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gradient descent is a very important optimization technique that has been used
    by almost any neural network. In order to explain these techniques, I want to
    give an example. I have a dataset of students' scores and hours of study for each
    of the students. We want to predict the test scores of a student just by his amount
    of hours of study. You would say that this looks like an ML linear regression
    example. You are right; we are using linear regression to make a prediction. Why
    linear regression and what is the connection with gradient descent? Let me answer
    this and then we will see the code and some cool visualization.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression is the ML technique that uses statistical methods and allows
    us to study relationships between two continuous quantitative variables. Here,
    those variables are students' scores and students' hours of study. Usually in
    linear regression, we try to get a line that is the best fit for our dataset,
    which means that whatever calculation we are doing is just to get a line of the
    best fit for the given dataset. Getting this line of the best fit is the goal
    of linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Let's talk about the connection of linear regression with gradient descent.
    Gradient descent is the most popular optimization technique that we are using
    to optimize the accuracy of the linear regression and minimize the loss or error
    function. Gradient descent is the technique that helps us minimize our error function
    and maximize our prediction accuracy. The mathematical definition for gradient
    descent is that it is a first-order iterative optimization algorithm. This algorithm
    is used to find a local minimum of a function using gradient descent. Each taken
    step is proportional to the negative of the gradient of the function at the current
    point. You can think of gradient descent using this real-life example. Suppose
    you are at the top of a mountain and now you want to reach the bottom that has
    a beautiful lake, so you need to start descending it. Now you don't know in what
    direction you should start walking. In this case, you observe the land near you
    and try to find the way where the land tends to descend. This will give you an
    idea in what direction you should go. If you take your first steps in the descending
    direction and each time you follow the same logic, then it is very likely that
    you would reach the lake. This is exactly what we are doing with the mathematical
    formula for gradient descent. In ML and DL, we think about everything in terms
    of optimization so gradient descent is the technique that is used to minimize
    the loss function over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other example is you have a deep bowl in which you put a small ball from
    its one end. You can observe that after some time, the ball reduces its speed
    and tries to reach the base of the bowl. Refer to *Figure 9.15*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ccd7026-a2d1-4e25-8359-f6f415e00701.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.15: Intuition of gradient descent (Image credit: https://iamtrask.github.io/img/sgd_optimal.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You must also check out the image given at this GitHub link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch9/gradientdescentexample/gradient_descent_example.gif](https://github.com/jalajthanaki/NLPython/blob/master/ch9/gradientdescentexample/gradient_descent_example.gif)'
  prefs: []
  type: TYPE_NORMAL
- en: This figure shows the process or steps of getting the line of best fit using
    gradient descent. It's just the visualization that gives you an overall idea of
    what we are going to do in the code. By the way, loss function, error function,
    and cost function are synonyms of each other. Gradient descent is also known as
    **steepest descent**.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's get to the code and I will explain things as we go. Here, we are not
    developing a predictive model; we will implement and understand gradient descent.
    The dataset and code is at [https://github.com/jalajthanaki/NLPython/tree/master/ch9/gradientdescentexample](https://github.com/jalajthanaki/NLPython/tree/master/ch9/gradientdescentexample)
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, let''s understand the dataset. It is the dataset of the students''
    test scores and the amount of hours they studied. We know that between these two
    attributes, there should be a relationship--the less amount you study, poorer
    is the score of the student, and the more you study, better the score will be.
    We will prove the relationship using linear regression. The **X** value means
    the first column of the dataset, that is, the amount of hours the student studies,
    and **Y** value means the second column, which is the test score. Refer to *Figure
    9.16*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e4f467a3-28fe-4951-a6cb-8afb47dd4033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.16: Sample data from the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Let's define the main function that is going to read our dataset and some basic
    hyperparameters. We have also called a function that we will use to compute the
    errors and actual gradient descent. You can see the code snippet in *Figure 9.17:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d364e86d-0991-4537-879a-a3f8c1030013.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.17: Code snippet of gradient descent'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the output in *Figure 9.18*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9edc5f4-b226-47c6-b0cb-c4405e178a48.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.18: Output of gradient descent'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in *Figure 9.18*, we have called two functions: `compute_error_for_line_points()`,
    which will compute the error between the actual value and predicted value, and
    `gradient_descent_runner()`, which will calculate the gradient for us. We need
    to understand first how we are going to calculate errors and then gradient descent.'
  prefs: []
  type: TYPE_NORMAL
- en: Calculating error or loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many ways to calculate the error for ML algorithms, but in this chapter
    we will be using one of the most popular techniques: sum of squared distance error.
    Now we are going straight into details.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What does this error function do for us? Recall our goal: we want to get the
    line of best fit for our dataset. Refer to *Figure 9.19*, which is the equation
    of line slope. Here, *m* is the slope of line, *b* is the *y* intercept, *x* and
    *y* are the data points--in our case, *x* is the numbers of hours the student
    studies and *y* is the test score. Refer to *Figure 9.19:*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2cb6dfd-29ae-4090-9ad4-5e03a397f7b7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.19: Line slope equation (Image credit: https://www.tes.com/lessons/Xn3MVjd8CqjH-Q/y-mx-b)'
  prefs: []
  type: TYPE_NORMAL
- en: Using the preceding equation, we are drawing the line and starting with random
    values of slope *m* and *y* intercept *b* and using our first column data points
    as value of x so that we get a value of *y*. In the training data, we already
    have *y* values, which means that we know the test score of each student. So for
    each student, we need to calculate the error. Let's take a very intuitive example;
    note that we are working with dummy values for explanation. Suppose you get *y*
    value of 41.0 by putting a random value of *m* and *b*. Now you have the actual
    value of *y*, that is, 52.5, then the difference between the predicted value and
    real value is 11.5\. This is just for a single data point but we need to calculate
    for every data point. So to do this kind of error calculation, we are using sum
    of squared distance error.
  prefs: []
  type: TYPE_NORMAL
- en: Now how are we calculating the sum of squared distance error and why are we
    using sum of squared distance error?
  prefs: []
  type: TYPE_NORMAL
- en: 'So let''s begin with the first question, the equation to calculate sum of squared
    distance error is given in *Figure 9.20*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8931db5e-3324-424f-85ed-a696f34d7cb6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.20: Equation for calculating sum of squared distance error (Image
    credit: https://spin.atomicobject.com/wp-content/uploads/linear_regression_error1.png)'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the last part *mx[i]+b* is the line that we have drawn by choosing
    a random value of *m* and *b* and we can actually put *y* in place of *mx[i]+b*.
    So here, we are calculating the difference between the original *y* value with
    the generated *y* value. We are subtracting the original *y* value and generated
    *y* value and squaring this value for each data point. We are squaring the value
    because we don't want to deal with negative values as we are performing sum after
    calculating square and we want to measure the overall magnitude. We don't want
    the actual value as we are trying to minimize this overall magnitude. Now back
    to the equation; we have calculated the square of the difference of the original
    *y* value and the generated *y* value. Now we are performing summation for all
    these points; we will use the sigma notation to indicate the summation operation
    for all the data points in the dataset. By this time, we have the sum value that
    indicates the error magnitude and we will divide these values by the total number
    of data points. After this, we will get the actual error value that we want.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the animated image at this GitHub link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch9/gradientdescentexample/gradient_descent_example.gif](https://github.com/jalajthanaki/NLPython/blob/master/ch9/gradientdescentexample/gradient_descent_example.gif)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that the line is moving for each iteration in order to generate
    the line with the best fit for our dataset. We are updating the value of *m* and
    *b* according to the value of our error. Now, for each timestamp, the line is
    static and we need to calculate the error. Refer to *Figure 9.21*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55438cce-8392-482b-89c1-518e5710f473.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.21: Calculating distance between line and data points at given timestamp
    (Image credit: http://statutor.org/c3/glmregression/IMAG004.JPG)'
  prefs: []
  type: TYPE_NORMAL
- en: Now we need to express our intuitive example and equation in a technical manner
    as per the given equation. Here, we are calculating the distance from each data
    point to the line that we have drawn, squaring them, summing them all together,
    and then dividing by the total number of points. So, after every iteration or
    timestamp, we can calculate our error value and get to know how bad our line is
    or how good our line is. If our line is bad, then to get the line of best fit,
    we update the values of *m* and *b*. So the error value provides us with an indication
    of whether there is a possibility of improvement or not in order to generate the
    line of best fit. So eventually we want to minimize the value of error that we
    are getting here in order to generate the line of best fit. How will we minimize
    this error and generate the line of best fit? The next step is called **gradient
    descent**.
  prefs: []
  type: TYPE_NORMAL
- en: A couple of reasons for sum of squared error is that, for linear regression,
    this is the most popular technique to calculate the error. You can also use this
    if you have a large dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the coding part and then we will jump to our core part of calculating
    gradient descent. See the code snippet in *Figure 9.22*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a21e70f-f0b9-470a-8ade-14ad3528328d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.22: Code snippet for calculating sum of squared error'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's see the next step--calculating gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using the error function, we know whether we should update our line in order
    to generate the line of best fit or not, but how to update the line is what we
    are going to see in this section. How will we minimize this error and generate
    the line of best fit? So to answer this question, first of all, let''s get some
    basic understanding about gradient descent and the coding part, where we are just
    left with our one last function, `gradient_descent_runner()`. Refer to *Figure
    9.23*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/28309b03-4d15-4525-97e6-8b57c4eb05fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.23: A 3D graph for understanding gradient descent (Image credit: https://spin.atomicobject.com/wp-content/uploads/gradient_descent_error_surface.png)'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Figure 9.23*, this is a three dimensional graph. These two
    graphs are the same; their viewing angle is different. So these graphs show all
    the possible values of slope *m*, *y*-intercept *b,* and *error*. These are pairs
    of three values including *m*, *b,* and *error*. Here, the *x* axis is a slope
    value, the *y* axis is a *y*-intercept, and the *z* axis is the error value. We
    try to get the point where the error is the least. If you see the graph carefully,
    then you can observe that at the bottom of the curve, the error value is the least.
    The point where the value is the least is called **local minima** in ML. In complex
    datasets, you may find multiple local minima; here our dataset is simple so we
    have a single local minima. If you have a complex and high-dimensional dataset
    where you have multiple local minima, then you need to do a second-order optimization
    to decide which local minima you should choose for better accuracy. We are not
    going to see second-order optimization in this book. Now let's look back to our
    graph where we can visually identify the point that gives us the smallest error
    value and the same point also gives us the ideal value of *y*-intercept that is
    *b* and slope value that is *m*. When we get the ideal value for *b* and *m*,
    we will put these values in our *y=mx+c* equation and then magic will happen and
    we will get the line of best fit. This is not the only way to get the line of
    best fit, but my motive is to give you an in-depth idea about gradient descent
    so that we can later use this concept in DL.
  prefs: []
  type: TYPE_NORMAL
- en: Now visually, you can see the smallest point where the error is the smallest,
    but how to reach this point? The answer is by calculating the gradient. Gradient
    is also called **slope** but this is not the slope value *m* so don't get confused.
    We are talking about slope in the direction of getting us to that smallest error
    point. So we have some *b* value and some *m* value and after every iteration,
    we update these *b* and *m* values so that we can reach that smallest error value
    point. So in perspective of the three dimensional image, if you are at the top
    of the curve, for every iteration we calculate the gradient and error and then
    update the values of *m* and *b* to reach the bottom of that curve. We need to
    reach to the bottom of the curve and by calculating the gradient value, we get
    an idea about the direction in which we should take our next step. So gradient
    is the tangent line that keeps telling us the direction we need to move in, whether
    it's upward or downward, to reach the smallest error point and obtain ideal *b*
    and *m* values to generate the line of best fit. Refer to *Figure 9.24**:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/878ff758-2c70-4dd2-a661-c83f951ffabd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.24: Gradient values and direction (Image credit: https://sebastianraschka.com/images/faq/closed-form-vs-gd/ball.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s see the last but not least equation of calculating gradient descent.
    In *Figure 9.25*, you can see the equations of gradient descent that are nothing
    but a partial derivative of our error function. We have taken the equation of
    sum of squared error and performed partial derivatives with respect to *m* and
    *b* to calculate gradient descent. The outcome is in *Figure 9.25*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29fe39a3-9a7f-4cfd-9db2-1a62096c4983.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.25: Equations for calculating gradient descent (Image credit: https://spin.atomicobject.com/wp-content/uploads/linear_regression_gradient1.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The left-hand side ** symbol is the symbol of partial derivative. Here, we
    have two equations because we take our error function and generate the partial
    derivative with respect to variable *m*, and in the second equation, we generate
    the partial derivative with respect to variable *b*. With these two equations,
    we will get the updated values of *b* and *m*.
  prefs: []
  type: TYPE_NORMAL
- en: To calculate the gradient, we need to derive the partial derivative of the error
    function. For some problems in ML and DL, we don't know the partial derivative
    of the error function, which implies that we can't find the gradient. So we don't
    know how to deal with this kind of function. Your error function should be differentiable,
    which means that your error function should have partial derivatives. Another
    thing here is that we are using the linear equation, but if you have high-dimensional
    data, then you can use the non-linear function if you know the error function.
    Gradient descent doesn't give us the minima when we start for the first time.
    Gradient just tells us how to update our *m* and *b* values, whether we should
    update with a positive value or negative value. So gradient gives us an idea how
    to update values of *m* and *b*, which means that by calculating the gradient,
    we are getting the direction and trying to reach the point where we get the smallest
    error value and best values for *m* and *b*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it is time to jump to the code again and finish gradient descent. Refer
    to *Figure 9.26*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98e08ba5-5746-4e21-9bbf-d77310291c34.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.26: Code snippet of the actual gradient descent runner function'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code, we have multiplied `m_gradient` and `b_gradient` with the learning
    rate, so learning rate is an important hyperparameter. Be careful while selecting
    its value. If you select a very high value, your model may not train at all. If
    you select a very low value, then it would take a lot of time to train and there
    is a chance of overfitting as well. Refer to *Figure 9.27*, which provides you
    with an intuition about a good learning rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3356443c-239a-4cae-b929-f1b2c07852b4.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.27: Learning rate intuition (Image credit: http://cs231n.github.io/assets/nn3/learningrates.jpeg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is it for the coding part of linear regression and gradient descent. Let''s
    run the code and *Figure 9.28* will give you an idea about the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50062f5a-ebb4-432e-93eb-b6d0f1e96ebb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.28: Code snippet of output'
  prefs: []
  type: TYPE_NORMAL
- en: There are types of gradient descent, so let's name a few of them, but we are
    not going into detail. You can explore gradient descent with momentum, Adagrad,
    Adam, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'I will provide you with a link that will be helpful to you if you really want
    to explore more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/](https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/)'
  prefs: []
  type: TYPE_NORMAL
- en: Now it's time to understand the activation function, so let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's see the activation function first. I want to give you an idea at what
    stage of ANN we will use this activation function. In our discussion of the perceptron,
    we said that neural networks will generate an output of one if it exceeds a certain
    threshold; otherwise, the output will be zero. This whole mechanism to calculate
    the threshold and generate the output based on this threshold is taken care by
    the activation function.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions are able to provide us with values that lie between 0 and
    1\. After this, using our threshold value, we can generate output value 1 or output
    value 0\. Suppose our threshold value is 0.777 and our activation function output
    is 0.457, then our resultant output will be 0; if our activation function output
    is 0.852, then our resultant output will be 1\. So, here is how the activation
    function works in ANN.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, in neural networks, we have a certain weight and input value for each
    neuron. We are summing them and generating the weighted sum value. When we pass
    these values through a non-linear function, this non-linear function activates
    certain numbers of neurons to get the output for a complex task; this activation
    process of neurons using certain non-linear mathematical functions is known as
    an **activation function** or **transfer function**. Activation functions map
    input nodes to the output nodes in a certain fashion using certain mathematical
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of having an activation function in ANN is to introduce non-linearity
    in the network. Let's understand this step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s concentrate on the structure of ANN. This ANN structure can be further
    divided into three sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Architecture:** Architecture is all about deciding the arrangement of neurons
    and layers in the ANN'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Activities:** In order to generate the output of complex tasks, we need to
    see the activities of the neurons--how one neuron responds to another to generate
    complex behavior'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning rule:** When ANN generates the output, we need to update our ANN
    weight at each timestamp to optimize the output using the error function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation function is part of the activities section. As we mentioned, we will
    introduce non-linearity into the ANN. The reason behind it is that without non-linearity,
    the ANN can't produce complex behavior to solve complex tasks. Most of the time
    in DL, we use non-linear activation functions to get the complex behavior. Apart
    from that, we also want to map our input to the output in a non-linear manner.
  prefs: []
  type: TYPE_NORMAL
- en: If you are not using a non-linear activation function, then the ANN won't give
    you significant amount of useful output for complex tasks because you are passing
    out the matrices, and if you are using more than one layer in your ANN with a
    linear activation function, you get an output that is the summation of the input
    value, weights, and bias from all the layers. This output gives you another linear
    function and that means that this linear function converts the behavior of a multi-layer
    ANN to single-layer ANN. This kind of behavior is not at all useful to solve complex
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'I want to highlight the idea of connectionism. The connectionism in ANN is
    to use neurons that are interconnected with each other and produce complex behavior,
    just like human brains, and we cannot achieve this kind of behavior without introducing
    non-linearity in the ANN. Refer to *Figure 9.29* to understand activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6a090d1-e8d9-4265-8cb7-e23ef2f2ae4d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.29: ANN with activation function (Image credit: https://cdn-images-1.medium.com/max/800/0*bWX2_ecf3l6lKyVA.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we are going to cover these functions mentioned in the preceding image:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transfer potential:** This is the function that aggregates inputs and weights.
    More specifically, this function performs the summation of inputs and weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Activation function:** This function takes the output of the transfer potential
    function as input and applies a non-linear mathematical transformation using an
    activation function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Threshold function:** Based on the activation function, the threshold function
    either activates the neuron or does not activate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transfer potential is a simple summation function that will perform the summing
    of inner dot product of the input to the weights of the connection. You can see
    the equation in *Figure 9.30*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b062cfeb-aadd-4c24-a36b-07c72963f899.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.30: Summation equation for transfer potential (Image credit: https://cdn-images-1.medium.com/max/800/0*005k9F1JxQ0oKEeM.png)'
  prefs: []
  type: TYPE_NORMAL
- en: This transfer potential is generally a dot product, but it can use any mathematical
    equation such as a multi-quadratic function.
  prefs: []
  type: TYPE_NORMAL
- en: The activation function, on the other hand, should be any differentiable and
    non-linear function. It needs to be differentiable so that we can calculate the
    error gradient, and this function has to have a non-linear property to gain complex
    behavior from the neural network. Typically, we are using the sigmoid function
    as activation function, which takes the transfer potential output value as input
    to calculate the final output and then calculates the error between our actual
    output and the generated one. Then, we will use the concept of calculating the
    gradient of error as well as applying a backpropagation optimization strategy
    in order to update the weight of the connection of the ANN.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.31* expresses the transfer potential function in terms of theta,
    also called the **logit**, which we will be using in the equation of the logistic
    sigmoid activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/005346b4-39e1-49c8-af03-0251dfe912ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.31: Transfer potential output in the form of logit value (Image credit:
    https://cdn-images-1.medium.com/max/800/0*mPYW0-FKPTOSACPP.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the equation of the logistic sigmoid function in *Figure 9.32*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e50a73ae-faee-4fe8-a072-5ea00aa17d77.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.32: Logistic sigmoid activation function (Image credit: https://cdn-images-1.medium.com/max/800/0*SwSxznoodb2762_9.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The whole idea behind an activation function is roughly modeled the way neurons
    communicate in the brain with each other. Each one is activated through its action
    potential if it reaches a certain threshold; then we know whether to activate
    a neuron or not. The activation function simulates the spike of the brain's action
    potential. **Deep neural nets** (**DNN**) are called **universal approximator**
    functions because they can compute any function at any instance. They can calculate
    any differentiable linear as well as non-linear function. Now you might ask me
    when to use this activation function. We will see this in the next paragraph.
  prefs: []
  type: TYPE_NORMAL
- en: There is a variety of activation functions available. Be careful while using
    them. We should not use any of them just because it sounds new and cool. Here,
    we will talk about how you know which one you should use. We will see three main
    activation functions because of their wide usage in DL, although there are other
    activation functions that you can use.
  prefs: []
  type: TYPE_NORMAL
- en: 'These three activation functions are mentioned as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tanh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReLU and its variants
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sigmoid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The sigmoid function is very easy to understand in terms of its mathematical
    concepts. Its mathematical formula is shown in *Figure 9.33*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2bccfbf5-54c4-44c4-8594-b8e7d88684b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.33: Sigmoid function equation (Image credit: https://cdn-images-1.medium.com/max/800/1*QHPXkxGmIyxn7mH4BtRJXQ.png)'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Figure 9.33*, the sigmoid function will take the given equation,
    take a number, and squash this number in the range of zero and one. It produces
    an s-shaped curve.
  prefs: []
  type: TYPE_NORMAL
- en: This function is the first one to be used in ANN as an activation function because
    it could be interpreted as the firing rate of neuron--zero means no firing and
    one is fully saturated firing. When we use this activation function for DNN, we
    get to know some limitations of this activation function that makes it less popular
    nowadays.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some basic problems with these functions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It suffers from the gradient vanishing problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has a slow convergence rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is not a zero-centric function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s understand each of the problems in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vanishing gradient problem**: You can find this problem when you are training
    certain ANNs with gradient-based methods and mostly in ANNs with backpropagation.
    This problem makes it really hard to learn and tune the parameters of the earlier
    layers in the ANN. This becomes more problematic when you add more layers to your
    ANN. If we choose the activation function wisely, then this problem can be solved.
    I want to give you details about the problem first and then we will discuss the
    cause behind it.'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient-based methods learn the values of parameters by understanding how a
    small change in the input parameters and weights will affect the output of the
    NN. If this gradient is too small, then changes in the parameters will be such
    that they cause very small changes in the output of ANN. In this case, after some
    iterations the ANN can't learn the parameters effectively and will not converge
    in the way we want. This is exactly what happens in a gradient vanishing problem.
    The gradient of the output of the network with respect to the parameters in the
    early layers becomes very small. You can say that even if there is a large change
    in the value of the parameters for input layers and weights, this does not provide
    a big effect in the output.
  prefs: []
  type: TYPE_NORMAL
- en: I'm giving you all these details because you can face this same problem with
    the sigmoid function as well. The most basic thing is that this vanishing gradient
    problem depends on the choice of your activation function. Sigmoid squashes the
    input into a small range of output in a non-linear manner. If you give a real
    number to the sigmoid function, it will squash that number in the range of [0,1].
    So there are large regions of input space that are mapped to a very small range.
    Even a large change in input parameters will produce a very small change in output
    because the gradient of this region is small. For the sigmoid function, when a
    neuron saturates close to either zero or one, the gradient at this region is very
    close to zero. During backpropagation, this local gradient will be multiplied
    by the gradient of each layer's output gate. So if the first layer maps to a large
    input region, we get a very small gradient as well as a very small change in the
    output of the first layer. This small change passes to the next layer and makes
    even smaller changes in the second layer's output. If we have a DNN, there is
    no change in the output after some layers. This is the problem with the sigmoid
    activation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the vanishing gradient problem in detail at this link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b](https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Low convergence rate**: Due to this vanishing gradient problem, sometimes
    the ANN with the sigmoid activation function converges very slowly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you really want to dig deep into the vanishing gradient problem, then you
    can check out this link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://cs224d.stanford.edu/notebooks/vanishing_grad_example.html](https://cs224d.stanford.edu/notebooks/vanishing_grad_example.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Non-zero-centric function**: The sigmoid function is not a zero-centric activation
    function. What this means is that the sigmoid function''s output range is [0,1],
    which means the value of the function''s output will be always positive so that
    makes the gradient of the weights become either all positive or all negative.
    This makes the gradient update go too far in different directions and this makes
    optimization harder.'
  prefs: []
  type: TYPE_NORMAL
- en: Due to these limitations, the sigmoid function is not used recently in DNNs.
    Although you can solve these problems using other functions, you can also use
    the sigmoid activation function only at the last layer of your ANN.
  prefs: []
  type: TYPE_NORMAL
- en: TanH
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To overcome the problems of the sigmoid function, we will introduce an activation
    function named **hyperbolic tangent function** (**TanH**). The equation of TanH
    is given in *Figure 9.34*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/854e270c-fe31-4bf5-9c0a-116d5813e8c4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.34: Tanh activation function equation (Image credit: https://cdn-images-1.medium.com/max/800/1*HJhu8BO7KxkjqRRMSaz0Gw.png)'
  prefs: []
  type: TYPE_NORMAL
- en: This function squashes the input region in the range of [-1 to 1] so its output
    is zero-centric, which makes optimization easier for us. This function also suffers
    from the vanishing gradient problem, so we need to see other activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: ReLu and its variants
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Rectified Linear Unit** (**ReLu**) is the most popular function in the industry.
    See its equation in *Figure 9.35*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02c4f3a4-8c9b-405a-88bd-47b79e3981dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.35: ReLu activation function equation (Image credit: https://cdn-images-1.medium.com/max/800/1*JtJaS_wPTCshSvAFlCu_Wg.png)'
  prefs: []
  type: TYPE_NORMAL
- en: If you will see the ReLu mathematical equation, then you will know that it is
    just *max(0,x)*, which means that the value is zero when *x* is less than zero
    and linear with the slope of *1* when *x* is greater than or equal to zero. A
    researcher named Krizhevsky published a paper on image classification and said
    that they get six times faster convergence using ReLu as an activation function.
    You can read this research paper by clicking on [http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf).
    This function is simple and doesn't have any complex computation and is less expensive
    compared to sigmoid and TanH. This is the reason that this function learns faster.
    Apart from this, it also doesn't have the vanishing gradient problem.
  prefs: []
  type: TYPE_NORMAL
- en: We used to apply the activation function in each layer present in the DNN. Nowadays,
    ReLu is used for most of the DNN, but it is applied to the hidden layers of DNN.
    The output layer should use softmax if you are solving a classification problem
    because the softmax function gives us the probability for each class. We have
    used the softmax activation function in the word2vec algorithm. In case of a regression
    problem, the output layer should use a linear function because the signal goes
    through unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from all these wonderful advantages of ReLu, it has one problem: some
    units of the neural network can be fragile and die during training, which means
    that a big gradient flowing through a ReLu neuron could cause a weight update
    that makes it never activate on any data point again. So the gradient flowing
    through it will always be zero from that point on. To overcome this limitation
    of ReLu, a variant of ReLu has been introduced--Leaky ReLu. Instead of the function
    being zero when *x* is less than zero (*x<0*), Leaky ReLu has a small negative
    slope. Refer to *Figure 9.36*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f25df7dd-d259-467e-a794-793304489049.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.36: Leaky ReLu (Image credit: http://wangxinliu.com/images/machine_learning/leakyrelu.png)'
  prefs: []
  type: TYPE_NORMAL
- en: There is another variant called **maxout** that is a generalized form of both
    ReLu and Leaky ReLu, but it doubles the parameters of each neuron, which is a
    disadvantage.
  prefs: []
  type: TYPE_NORMAL
- en: Now you know enough about activation functions, so which one should you use?
    The answer is ReLu, but if too many neurons die, then use Leaky ReLu or maxout.
    This activation function is applied to the hidden layer. For the output layer,
    use the softmax function if you are solving a classification problem or linear
    activation function if you are solving a regression problem. The sigmoid and TanH
    shouldn't be used in DNNs. This is quite an interesting research area and there
    is much room to come up with great activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other activation functions out there that you can check out: identity
    function, binary step function, ArcTan, and so on. Here, we will check the third
    important concept--loss functions.'
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes, loss functions are also referred to as **cost functions** or **error
    functions**. A loss function gives us an idea of how good the ANN performs with
    respect to the given training examples. So first, we define the error function
    and when we start to train our ANN, we will get the output. We compare the generated
    output with the expected output given as part of the training data and calculate
    the gradient value of this error function. We backpropagate the error gradient
    in the network so that we can update the existing weights and bias values to optimize
    our generated output. The error function is the main part of the training. There
    are various error functions available. If you ask me which error function to choose,
    then there is no specific answer because all ANN training and optimization is
    based on this loss function. So it depends on your data and problem statement.
    If you ask somebody which error function you have used in your ANN, then indirectly
    you are asking them the whole logic of the training algorithm. Whatever error
    function you will use, make sure that the function must be differentiable. I have
    listed down some of the most popular error functions:'
  prefs: []
  type: TYPE_NORMAL
- en: Quadratic cost function also known as **mean squared error** or **sum squared
    error**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-entropy cost function also known as **Bernoulli negative log likelihood**
    or **binary cross-entropy**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kullback-Leibler divergence also known as **information divergence**, **information
    gain**, **relative entropy**, or **KLIC**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from these three, there are many other loss functions such as exponential
    cost, Hellinger distance, Generalized Kullback-Leibler divergence, and Itakura-Saito
    distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, we are using sum of square error for regression and cross-entropy
    for categorical data and classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen the most important mathematical and theoretical concepts to develop
    ANN. In the next section, we will see the implementation of our first ANN. Let's
    jump to the implementation part.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of ANN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will implement our first ANN in Python using `numpy` as
    our dependency. During this implementation, you can relate how gradient descent,
    activation function, and loss function have been integrated into our code. Apart
    from this, we will see the concept of backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: We will see the implementation of a single-layer NN with backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: Single-layer NN with backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we will see the concept of backpropagation first, then we will start coding
    and I will explain things as we code.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a single-layer neural network, we have input that we feed to the first layer.
    These layer connections have some weights. We use the input, weight, and bias
    and sum them. This sum passes through the activation function and generates the
    output. This is an important step; whatever output has been generated should be
    compared with the actual expected output. As per the error function, calculate
    the error. Now use the gradient of the error function and calculate the error
    gradient. The process is the same as we have seen in the gradient descent section.
    This error gradient gives you an indication of how you can optimize the generated
    output. Error gradient flows back in the ANN and starts updating the weight so
    that we get a better output in the next iteration. The process of flowing back
    the error gradient in ANN to update weight in order to generate more accurate
    output is called **backpropagation**. In short, backpropagation is a popular training
    technique to train a neural network by updating the weight via gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: All other aspects of calculation and math will be shown in the coding part.
    So let's code our own single-layer feedforward neural network with backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will define the main function and our abstract steps. Here, we will
    give the input and output values. As our data is labeled, it is a supervised learning
    example. The second step will be the training, and we will repeat the training
    to iterate for 10,000 times. We will first start with a random weight and adjust
    the weight as per the activation function and error function. Refer to *Figure
    9.37**:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03a11c0f-d2bc-4e98-a6c2-b9f0e873be54.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.37: Code snippet of the main function for a single-layer ANN'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we are using sigmoid as the activation function. We will use the sigmoid
    derivative to calculate gradient of the sigmoid curve. Our error function is a
    simple subtraction of the actual output from the generated output. We multiply
    this error value with the gradient to get the error gradient that helps us adjust
    the weight of NN. The new updated weight and input again passes through the ANN,
    calculates gradient descent of the sigmoid curve and error gradient, and adjusts
    the weight until we get minimum error. Refer to *Figure 9.38* and *Figure 9.39*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2fb5c460-e1c5-47a2-ad6a-f3ee8882a085.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.38: Code snippet of a single-layer ANN'
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f446caec-fd22-43e4-8771-e158be9be204.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.39: Code snippet of ANN'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you run the code, you will get the following result. Refer to *Figure
    9.40*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69ce0eab-3e60-4411-8c06-d3e494f48e10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.40: Output snippet of single layer ANN'
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Build a three-layer deep ANN using `numpy` as a dependency. (Hint: In a single-layer
    ANN, we used single layer, but here, you will use three layers. Backpropagation
    usually uses recursively taken derivatives, but in our one layer demo, there was
    no recursion. So you need to apply recursive derivatives.)'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning and deep neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, just shift from ANN to DNN. In the upcoming section, we will see deep learning,
    architecture of DNN, and compare the approaches of DL for NLP and ML for NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting DL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen some basic details about DL. Here, the purpose is just to recall
    things as a little refresher. ANN that is not two or three layers but many layers
    deep is called DNN. When we use many layers deep neural networks on lots of data
    using lots of computing power, we call this process deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see the architecture of a deep neural network.
  prefs: []
  type: TYPE_NORMAL
- en: The basic architecture of DNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will see the architecture of a DNN. The pictorial representation
    looks very simple and is defined with some cool mathematical formulas in the form
    of activation function, activation function for hidden layer, loss function, and
    so on. In *Figure 9.41*, you can see the basic architecture of a DNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/70fd1f89-7609-4bdc-86e2-a8786869b8be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.41: Architecture of DNN (Image credit: https://cdn-images-1.medium.com/max/800/1*5egrX--WuyrLA7gBEXdg5A.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Now why are we using a multi-layer deep neural network, are there any certain
    reasons for this, and what is the significance of having many layers?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let me explain why we are using multi-layer DNNs. Suppose, as a coder, you
    want to develop a system that identifies the images of fruits. Now you have some
    images of oranges and apples and you develop a logic such as I can identify images
    using the color of the fruits and you have also added shape as an identification
    parameter. You do some coding and are ready with the result. Now if someone tells
    you that we also have images that are black and white. Now you need to redo your
    coding work. Some varieties of images are too complex for you, as a human, to
    code, although your brain is very good at identifying the actual fruit name. So
    if you have such a complex problem and you don''t know how to code or you know
    less details about the features or parameters that will be helpful for the machine
    to solve the problem, then you use a deep neural network. There are several reasons
    and those are mentioned as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: DNN has been derived using the abstract concept of how a human brain works.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using DNN, we flip the approach of our coding. Initially, we provided features
    like color, shape, and so on to the machine to identify the fruit name in the
    given images, but with DNN and DL, we provide many examples to the machine and
    the machine will learn about the features by itself. After this, when we provide
    a new image of a fruit to the machine, it will predict the name of the fruit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now you really want to know how DNN can learn features by itself, so let''s
    highlight some points as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: DNN uses a cascade of many layers of non-linear processing units that are used
    for feature extraction and transformation. Each successive layer of DNN uses the
    output from the previous layer as input, and this process is very similar to how
    the human brain transmits information from one neuron to the other. So we try
    to implement the same structure with the help of DNN.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In DL, features have been learned using multiple levels of representation with
    the help of DNNs. Higher levels of features or representation are derived from
    the lower level of features. So we can say that the concept of deriving features
    or representation in DNN is hierarchical. We learn something new using this lower
    level of ideas and we try to learn something extra. Our brain also uses and derives
    concepts in a hierarchical manner. This different level of features or representation
    is related to different levels of abstraction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-layers of DNN helps the machine to derive the hierarchical representation
    and this is the significance of having many layers as part of the architecture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the help of DNN and mathematical concepts, machines are capable to mimic
    some of the processes of the human brain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DL can be applied to a supervised as well as unsupervised dataset to develop
    NLP applications such as machine translation, summarization, question answering
    system, essay generation, image caption tagging, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we will move to the next section where we will discuss the need of deep
    learning in NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning in NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The early era of NLP is based on the rule-based system, and for many applications,
    an early prototype is based on the rule-based system because we did not have huge
    amounts of data. Now, we are applying ML techniques to process natural language,
    using statistical and probability-based approaches where we are representing words
    in form of one-hot encoded format or co-occurrence matrix.
  prefs: []
  type: TYPE_NORMAL
- en: In this approach, we are getting mostly syntactic representations instead of
    semantic representations. When we are trying out lexical-based approaches such
    as bag of words, ngrams, and so on, we cannot differentiate certain context.
  prefs: []
  type: TYPE_NORMAL
- en: 'We hope that all these issues will be solved by DNN and DL because nowadays,
    we have huge amounts of data that we can use. We have developed good algorithms
    such as word2vec, GloVe, and so on in order to capture the semantic aspect of
    natural language. Apart from this, DNN and DL provide some cool capabilities that
    are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Expressibility:** This capability expresses how well the machine can do approximation
    for a universal function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trainability:** This capability is very important for NLP applications and
    indicates how well and fast a DL system can learn about the given problem and
    start generating significant output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generalizability:** This indicates how well the machine can generalize the
    given task so that it can predict or generate an accurate result for unseen data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from the preceding three capabilities, there are other capabilities that
    DL provides us with, such as interpretability, modularity, transferability, latency,
    adversarial stability, and security.
  prefs: []
  type: TYPE_NORMAL
- en: We know languages are complex things to deal with and sometimes we also don't
    know how to solve certain NLP problems. The reason behind this is that there are
    so many languages in the world that have their own syntactic structure and word
    usages and meanings that you can't express in other languages in the same manner.
    So we need some techniques that help us generalize the problem and give us good
    results. All these reasons and factors lead us in the direction of the usage of
    DNN and DL for NLP applications.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's see the difference between classical NLP techniques and DL NLP techniques
    because that will connect our dots in terms of how DL can be more useful for us
    to solve NLP domain-related problems.
  prefs: []
  type: TYPE_NORMAL
- en: Difference between classical NLP and deep learning NLP techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will compare the classical NLP techniques and DL techniques
    for NLP. So let''s begin! Refer to *Figure 9.42*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32b50a1a-d3b9-4b84-be2e-f38cf4f102b9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.42: Classical NLP approach (Image credit: https://s3.amazonaws.com/aylien-main/misc/blog/images/nlp-language-dependence-small.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to *Figure 9.43* for DL techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae3bd2b6-0018-4e98-9396-9120c9f2d9b1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.43: Deep learning approach for NLP (Image credit: https://s3.amazonaws.com/aylien-main/misc/blog/images/nlp-language-dependence-small.png)'
  prefs: []
  type: TYPE_NORMAL
- en: In classical NLP techniques, we preprocessed the data in the early stages before
    generating features out of the data. In the next phase, we use hand-crafted features
    that are generated using NER tools, POS taggers, and parsers. We feed these features
    as input to the ML algorithm and train the model. We will check the accuracy,
    and if the accuracy is not good, we will optimize some of the parameters of the
    algorithm and try to generate a more accurate result. Depending on the NLP application,
    you can include the module that detects the language and then generates features.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's see the deep learning techniques for an NLP application. In this approach,
    we do some basic preprocessing on the data that we have. Then we convert our text
    input data to a form of dense vectors. To generate the dense vectors, we will
    use word-embedding techniques such as word2vec, GloVe, doc2vec, and so on, and
    feed these dense vector embedding to the DNN. Here, we are not using hand-crafted
    features but different types of DNN as per the NLP application, such as for machine
    translation, we are using a variant of DNN called **sequence-to-sequence model**.
    For summarization, we are using another variant, that is, **Long short-term memory
    units**.(**LSTMs**). The multiple layers of DNNs generalize the goal and learn
    the steps to achieve the defined goal. In this process, the machine learns the
    hierarchical representation and gives us the result that we validate and tune
    the model as per the necessity.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you really want to see the coding of different variants of DNNs, then use
    this GitHub link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/wagamamaz/tensorflow-tutorial](https://github.com/wagamamaz/tensorflow-tutorial)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next section is the most interesting part of this chapter. We are going
    to build two major applications: one is for NLU and one is for NLG. We are using
    TensorFlow and Keras as our main dependencies to code the example. We will understand
    a variant of DNN such as sequence-to-sequence and LSTM as we code them for better
    understanding.'
  prefs: []
  type: TYPE_NORMAL
- en: Guess what we are going to build? We are going to build a machine translator
    as part of an NLP application and we will generate a summary from recipes. So
    let's jump to the coding part! I will give you some interesting exercises!
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning techniques and NLU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section is coding-based and I will explain concepts as we go. The application
    that we are building here is one of the main applications in NLU.
  prefs: []
  type: TYPE_NORMAL
- en: There are so many languages spoken, written, or read by humans. Have you ever
    tried to learn a new language? If yes, then you know how difficult it is to acquire
    the skill of speaking a new language or writing a new language. Have you ever
    thought how Google translator is used in order to translate languages? If you
    are curious, then let's begin developing a machine translation application using
    a deep learning technique. Don't worry about questions like what type of DNN we
    will use because I'm explaining things to you in detail. So let's do some translation!
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that DL takes a lot of computing power so we are not going to actually
    train the model, although I will give you details about the training code, we
    will use the trained model to replicate the results at our end. Just to give you
    an idea: Google uses 100 GPU for one week continuously to train the language translation
    model. So we get through the code, understand the concept, use an already trained
    model, and see the result.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to use any specific version of TensorFlow, you can follow this
    command. If you want to install TensorFlow 0.12 version, you can install it with
    the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to use the version of TensorFlow then when you run the code please
    update you import statements. You can use the simple following command to install
    TensorFlow for CPU. I''m using GPU version only:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to run on GPU, you can use a cloud platform such as Google Cloud,
    AWS, or any other cloud platform or you need a GPU-enabled computer. To install
    TensorFlow for GPU, you can follow this link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.tensorflow.org/install/](https://www.tensorflow.org/install/)'
  prefs: []
  type: TYPE_NORMAL
- en: Machine translation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Machine translation** (**MT**) is a widely known application in the NLU domain.
    Researchers and tech giants are experimenting a lot in order to make a single
    MT system that can translate any language. This MT system is called a universal
    machine translation system. So the long-term goal is that we want to build a single
    MT system that can translate English to German and the same MT system should also
    translate English to French. We are trying to make one system that can help us
    translate any language. Let''s talk about the efforts and experiments done by
    researchers till date to build a universal machine translation system.'
  prefs: []
  type: TYPE_NORMAL
- en: In 1954, the first machine translation demo had been given, which translated
    250 words between Russian and English. This was a dictionary-based approach, and
    this approach used the mapping of words for source and target languages. Here,
    translation was done word by word and it wasn't able to capture syntactic information,
    which means that the accuracy was not good.
  prefs: []
  type: TYPE_NORMAL
- en: The next version was interlingual; it took the source language and generated
    an intermediary language to encode and represent a certain rule about the source
    language syntax, grammar, and so on and then generated a target language from
    the intermediary language. This approach was good compared to the first one but
    soon this approach was replaced by **statistical machine translation** (**SMT**)
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: IBM used this SMT approach; they broke the text into segments and then compared
    it to an aligned bilingual corpus. After this, using statistical techniques and
    probabilities, the most likely translation was chosen.
  prefs: []
  type: TYPE_NORMAL
- en: The most used SMT in the world is Google translation, and recently, Google published
    a paper stating that their machine translation system uses deep learning to generate
    the great result. We are using the TensorFlow library, which is an open source
    library for deep learning provided by Google. We will code to know how to do machine
    translation using deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: We are using movies subtitles as our dataset. This dataset includes both German
    and English languages. We are building a model that will translate the German
    language into English and vice versa. You can download the data from [http://opus.lingfil.uu.se/OpenSubtitles.php](http://opus.lingfil.uu.se/OpenSubtitles.php).
    Here, I'm using the pickle format of data. Using `pickle`, which is a Python dependency,
    we can serialize our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin with, we are using LSTMs network that is used to remember long term
    and short term dependencies. We are using TensorFlow''s built-in `data_utils`
    class to preprocess the data. Then we need to define the vocabulary size on which
    we need to train the model. Here, our dataset has a small size of vocabulary so
    we are considering all the words in the dataset, but we define vocab (vocabulary)
    size such as 30,000 words, that is, a small set of training dataset. We will use
    the `data_utils` class to read the data from the data directory. This class gives
    us tokenized and formatted words from both languages. Then we define TensorFlow''s
    placeholder that are encoders and decoders for inputs. These both will be integer
    tensors that represent the discrete values. They are embedded into dense representation.
    We will feed our vocabulary words to the encoder and the encoded representation
    that is learned to the decoder. You can see the code at this Github link: [https://github.com/jalajthanaki/NLPython/tree/master/ch9/MT/Machine_Translation_GR_EN](https://github.com/jalajthanaki/NLPython/tree/master/ch9/MT/Machine_Translation_GR_EN)[.](https://github.com/jalajthanaki/NLPython/tree/master/ch9/MT/Machine_Translation_GR_EN)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can build our model. You can see the code snippets in *Figure 9.44*,
    *Figure 9.45*, and *Figure 9.46*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4ef819d-cc0a-4548-8ff7-6133c93cc300.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.44: Code snippet for MT'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ddc743f-d3f6-41be-a541-b9daef007fce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.45: Code snippet for MT'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9f73c8c-821a-4a91-a700-78a88fa3a8b9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.46: Code snippet for MT'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s understand this encoder- and decoder-based system. Google recently
    published a paper where they discuss the system that they integrated into their
    translation system, that is, **neural machine translation** (**NMT**). It is an
    encoder decoder-based model with the new NMT architecture. Earlier, Google translated
    from language A to language English and then to language B. Now, Google translator
    can translate directly from one language to the other. Refer to *Figure 9.47*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5fc7c9c-052a-41d1-862c-597580b4dce2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.47: LSTM-based encoder and decoder architecture (Image credit: https://camo.githubusercontent.com/242210d7d0151cae91107ee63bff364a860db5dd/687474703a2f2f6936342e74696e797069632e636f6d2f333031333674652e706e67
    )'
  prefs: []
  type: TYPE_NORMAL
- en: Now with the existence of NMT, there is no need to memorize phrase-to-phrase
    translation. With the help of NMT, a translation system can encode semantics of
    the sentences. This encoding is generalized so that it can translate from Chinese
    to English, French to English as well as translate language pairs like Korean
    to Japanese, which has not been seen before.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now can we use this simple LSTM-based encoder-decoder architecture? We will
    see some of the fundamental details of the architecture. Refer to *Figure 9.48*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1bfafce8-bdeb-437a-b0d3-988c3a855dcc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.48: LSTM recurrent NN for translation (Image credit: https://smerity.com/articles/2016/google_nmt_arch.html)'
  prefs: []
  type: TYPE_NORMAL
- en: We can use LSTM recurrent NN to encode a sentence of language A. The RNN splits
    a hidden state **S**, as shown in *Figure 9.48*. This **S** represents the vectorized
    content of the sentence. After that, we pass this vectorized form to the decoder
    that generates the translated sentence in language B, word by word. It's easy
    to understand this architecture, isn't it? However, this architecture has some
    drawbacks. This architecture has limited memory. The hidden state **S** of the
    LSTM is where we are trying to cram the whole sentence that we want to translate,
    but here **S** is usually a few hundred floating point numbers long. We need to
    fit our sentence into this fixed dimensionality and if we force our sentence to
    fit into this fixed dimensionality, then our network becomes more lossy, which
    means that we lose some information if we are forcefully fitting our sentence
    into a fixed size of dimensionality. We could increase the hidden size of LSTMs
    because their main purpose is to remember long-term dependencies, but if we increase
    the hidden size, then the training time increases exponentially. So, we should
    not use an architecture that takes a lot of time to converge.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will introduce another architecture--attention-based encoder-decoder model.
    As humans, when we see a long sentence and we need to translate it, then we probably
    glance back at the source sentence a couple of times to make sure that we are
    capturing all the details. The human mind iteratively pays attention to the relevant
    parts of the source sentence. We want the neural network do the same thing for
    us by letting it store and refer to the previous output of the LST. This increases
    the storage of our model without changing the functionality of LSTMs. Refer to
    *Figure 9.49*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f9e91ecb-06ff-442c-83f8-49d575088208.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.49: Architecture of attention-based NMT (Image credit: https://heuritech.files.wordpress.com/2016/01/trad_attention1.png?w=470)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the LSTM output from the encoders stored, we can query each output
    asking how relevant it is to the current computation happening in the decoder.
    Each encoder output gets a relevancy score that we can convert to a probability
    score using the softmax activation function. Refer to *Figure 9.50*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c29a7b47-d0f5-4a0e-9bc4-95ece769d7b1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.50: SoftMax function to generate the relevance score (Image credit:
    https://smerity.com/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we extract a context vector that is the weighted summation of the encoder''s
    output depending on how relevant they are. Now let''s get back to the code. To
    implement this attention-based functionality, we will use TensorFlow''s built-in
    embedding attention sequence-to-sequence function. This function will take encoder
    and decoder inputs as arguments as well as some additional hyperparameters. This
    function is the same architecture that we have discussed. TensorFlow has some
    really great built-in models that we can use easily. Refer to *Figure* *9.51*
    for the code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54e5ca7f-7f32-49fc-a610-f911509a9e7f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.51: Code snippet for attention-based sequence-to-sequence model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to *Figure 9.52* for the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2288a615-d3dc-4e07-9581-93bfbee1bbd5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.52: Output for MT'
  prefs: []
  type: TYPE_NORMAL
- en: You can also follow this link, [https://www.tensorflow.org/tutorials/seq2seq](https://www.tensorflow.org/tutorials/seq2seq),
    to run the MT example without making your customized code. This tutorial is an
    example of French to English and English to French translation system. This is
    a really easy way to run this example. I would recommend you to use this way because
    customized code is much complicated to begin with.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need to download 2.4GB training `giga-fren.tar` dataset from this
    link: `http://www.statmt.org/wmt10/training-giga-fren.tar`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now you need to store this data in the `data_dir` directory and save your trained
    model time to time. For this, we need to create a checkpoint directory inside
    `train_dir`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After that, you can execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If the preceding command takes a lot of memory of the GPU, then execute this
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Once your epoch reaches 340 K with batch size 64, you can use the model for
    translation before that also you can use it but accuracy will as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Our German to English (GR_EN) translation model gives us a fairly good result
    and we are doing only one round of training, but if we really want to get the
    same accuracy that Google has in their translation system, then we need to train
    this model for several weeks using high computational capability such as 100 GPUs
    for several weeks continuously running. Here, we are definitely not going to implement
    that model, but I will explain its working. So let's dive conceptually.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the output doesn''t have sufficient context for the encoded source sentence,
    then the model won''t be able to give us a good translation result. In this case,
    we need to give the information about the future words so that the encoder output
    is determined by the words on the left and right. As humans, we use this kind
    of full context to understand the meaning of the sentence. This will happen on
    the machine level by including bidirectional encoders so that it contains two
    **recurrent neural nets** (**RNN**). One goes forward over the sentence and the
    other goes backward. So for each word, it concatenates the vector outputs that
    produce the vector with context of both sides. Refer to *Figure 9.53*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ffcd4d3b-eac4-456c-a130-7548bcfc79d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.53: Bidirectional RNN architecture for MT (Image credit: http://img.yantuwan.com/weiyuehao.php?http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW81aVM2Gdgrxfsa0vR4YVib2wCIHNabCic1Hr144r4PAuSDMLNMHGgWz12GtibYdgF1jTvHtuniauHYSw/0?wx_fmt=png)'
  prefs: []
  type: TYPE_NORMAL
- en: Google has included lots of layers to the models as well as encoders that have
    one bidirectional RNN layer and seven unidirection layers. The decoder has eight
    unidirectional RNN layers. If you add more layers, then the training time increases.
    Here, we are using only one bidirectional layer. If all layers are bidirectional,
    then the whole layer would have to finish their computation before other layer
    dependencies could start their computation. With the help of a unidirection layer,
    we can perform computation in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: This is all about machine translation. We have generated the machine translation
    output but still there is more room for improvement. Using DL, we are going to
    build a single universal machine translation system.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's begin our next part of coding that is based on NLG.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning techniques and NLG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going build a very simple but intuitive application
    for NLG. We are going to generate a one-line summary from shot articles. We will
    see all the details about summarization in this section.
  prefs: []
  type: TYPE_NORMAL
- en: This application took a lot of training time so you can put your model to train
    on CPU and meanwhile, you can do some other task. If you don't have any other
    task, then let me give you one.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Try to figure out how you can generate a Wikipedia article by just providing
    some starting character sequences. Don''t take me wrong! I''m serious! You seriously
    need to think on this. This is the dataset that you can use: [https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset](https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset).
    Jump to the download section and download this dataset named Download WikiText-103
    word level (181 MB).'
  prefs: []
  type: TYPE_NORMAL
- en: '(Hint: See this link, [https://github.com/kumikokashii/lstm-text-generator](https://github.com/kumikokashii/lstm-text-generator).)'
  prefs: []
  type: TYPE_NORMAL
- en: Don't worry ;after understanding the concepts of summarization, you can attempt
    this. So let's begin the summarization journey!
  prefs: []
  type: TYPE_NORMAL
- en: Recipe summarizer and title generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before jumping into the code, I want to give you some brief background about
    summarization. Architecture and other technical parts will be understood as we
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Semantics is a really big deal in NLP. As data increases in the density of the
    text, information also increases. Nowadays, people around you really expect that
    you say the most important thing effectively in a short amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: Text summarization started in the 90s. The Canadian government built a system
    named **forecast generator** (**FoG**) that uses weather forecast data and generates
    a summary. That was the template-based approach where the machine just needed
    to fill in certain values. Let me give you an example, **Saturday will be sunny
    with 10% chances of rain**. The word *sunny* and *10%* are actually generated
    by FoG.
  prefs: []
  type: TYPE_NORMAL
- en: The other areas are finance, medical, and so on. In the recent world, doctors
    find the summarization of a patient's medical history very useful and they can
    diagnose people efficiently and effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of summary that are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Extractive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abstractive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most summarization tools in the past were of the extractive type; they selected
    an existing set of words from the article to create a summary for the article.
    As humans, we do something more; that is, when we summarize, we build an internal
    semantic representation of what we have read. Using this internal semantic representation,
    we can summarize text. This kind of summarization is called **abstractive summarization**.
  prefs: []
  type: TYPE_NORMAL
- en: So let's build an abstractive summarization tool using Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Keras is a high-level wrapper for TensorFlow and Theano. This example needs
    multiple GPUs for more than 12 hours. If you want to reproduce the result at your
    end, then it shall take a lot of computation power.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the steps for the coding part. Here, for the first time, we are using
    Python 3:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clone the GitHub Repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://github.com/jalajthanaki/recipe-summarization](https://github.com/jalajthanaki/recipe-summarization)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Initialized submodules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Go inside the folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Install dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up directories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Scrape recipes from the web or use the existing one at this link:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Tokenize the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize word embeddings with `GloVe vectors`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Get the GloVe vectors trained model:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize embeddings:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Make predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, for vectorization, we are using GloVe because we want a global-level
    representation of the words for summarization, and we are using the sequence-to-sequence
    model (Seq2Seq model) to train our data. Seq2Seq is the same model that we discussed
    in the *Machine translation* section. See the code snippets in *Figure 9.54*,
    *Figure 9.55*, and *Figure 9.56*, and after training, you can see the output in
    *Figure 9.57*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/774350b6-b0c1-4852-8511-ce23d7b33e67.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.54: Tokenization code snippet'
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following figure for vocab building using GloVe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e784e55-84ac-4929-8f20-b64e775d0ada.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.55: Vocab building using GloVe'
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following figure to train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/964a4407-12e0-4da8-9f5f-fe3571ea7c66.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.56: Training of the model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples are given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84094480-18c5-45b1-8d25-32a62a772ce9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.57: Prediction result of the model'
  prefs: []
  type: TYPE_NORMAL
- en: I know that the summarization example will take a lot of computational power
    and maybe there will be a situation where your local machine does not have enough
    memory (RAM) to run this code. In that case, don't worry; there are various cloud
    options available that you can use. You can use Google Cloud, Amazon Web Services
    (AWS), or any other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you have enough idea about the NLU and NLG applications. I have also put
    one more application related to the NLG domain at this GitHub Link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/tensorflow/models/tree/master/im2txt](https://github.com/tensorflow/models/tree/master/im2txt)'
  prefs: []
  type: TYPE_NORMAL
- en: This application generates captions for images; this is a kind of combined application
    of computer vision and NLG. Necessary details are on GitHub so check out this
    example as well.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see the gradient descent-based optimization strategy.
    TensorFlow provides us with some variants of the gradient descent algorithm. Once
    we have an idea of how all these variants work and what are the drawbacks and
    advantages of each of them, then it will be easy for us to choose the best option
    for the optimization of our DL algorithm. So let's understand the gradient descent-based
    optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent-based optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss gradient descent-based optimization options
    that are provided by TensorFlow. Initially, it will not be clear which optimization
    option you should use, but as and when you know the actual logic of the DL algorithm,
    it will became much clearer to you.
  prefs: []
  type: TYPE_NORMAL
- en: We use a gradient descent-based approach to develop an intelligent system. Using
    this algorithm, the machine can learn how to identify patterns from the data.
    Here, our end goal is to obtain the local minimum and the objective function is
    the final prediction that the machine will make or result that is generated by
    the machine. In the gradient descent-based algorithm, we are not concentrating
    on how to achieve the best final goal for our objective function in the first
    step, but we will iteratively or repeatedly take small steps and select the intermediate
    best option that leads us to achieve the final best option, that is, our local
    minima. This kind of educated guess and check method works well to obtain local
    minima. When the DL algorithm obtains local minima, the algorithm can generate
    the best result. We have already seen the basic gradient descent algorithm. If
    you face overfitting and underfitting situations, you can optimize the algorithm
    using different types of gradient descent. There are various flavors of gradient
    descent that can help us in order to generate the ideal local minima, control
    the variance of the algorithm, update our parameters, and lead us to converge
    our ML or DL algorithm. Let's take an example. If you have function Y = X, then
    the partial derivative of the given function is 2X. When we randomly guess the
    stating value and we start with value X = 3, then Y = 2(3) =6 and to obtain local
    minima, we need to take a step in the negative direction--so Y = -6\. After the
    first iteration, if you guess the value X = 2.3, then Y = 2(2.3) = 4.6 and we
    need to move in the negative direction again--Y = -4.6--because we get a positive
    value. If we get a negative value, then we move in the positive direction. After
    certain iterations, the value of Y is very near zero and that is our local minima.
    Now let's start with basic gradient descent. Let's start exploring varieties of
    gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: '**Basic gradient descent**'
  prefs: []
  type: TYPE_NORMAL
- en: In basic gradient descent, we calculate the gradient of loss function with regards
    to the parameters present in the entire training dataset, and we need to calculate
    gradient for the entire dataset to perform a single update. For a single update,
    we need to consider the whole training dataset as well as all parameters so it
    is very slow. You can see the equation in *Figure 9.58:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b8ef16a-da38-4822-862e-66d42a50b2a2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.58: Equation for gradient descent (Image credit: http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges)'
  prefs: []
  type: TYPE_NORMAL
- en: You can find the sample logic code for understanding purposes in *Figure 9.59:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4224bd92-3687-45ac-b1be-64c67d519676.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.59: Sample code for gradient descent (Image credit: http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges)'
  prefs: []
  type: TYPE_NORMAL
- en: As this technique is slow, we will introduce a new technique called Stochastic
    Gradient Descent.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stochastic gradient descent**'
  prefs: []
  type: TYPE_NORMAL
- en: In this technique, we update the parameters for each training example and label
    so we just need to add a loop for our training dataset and this method updates
    the parameters faster compared to basic gradient descent. You can see the equation
    in *Figure 9.60:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ea66aa6-aeaf-46f8-aa38-cd42d767e815.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.60: Equation for stochastic gradient descent (Image credit: http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges)'
  prefs: []
  type: TYPE_NORMAL
- en: You can find the sample logic code for understanding purposes in *Figure 9.61:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4983e3bb-2285-4c0b-880b-f9e100da46a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.61: Sample code for stochastic gradient descent (Image credit: http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges)'
  prefs: []
  type: TYPE_NORMAL
- en: This method also has some issues. This method makes convergence complicated
    and sometimes updating the parameters is too fast. The algorithm can overshoot
    the local minima and keep running. To avoid this problem, another method is introduced
    called Mini-Batch Gradient Descent.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mini-batch gradient descent**'
  prefs: []
  type: TYPE_NORMAL
- en: In this method, we will take the best part from both basic gradient descent
    and stochastic gradient descent. We will take a subset of the training dataset
    as a batch and update the parameters from them. This type of gradient descent
    is used for basic types of ANNs.
  prefs: []
  type: TYPE_NORMAL
- en: You can see the equation in *Figure 9.62:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea637bf4-9d39-42be-b88c-db54c3eb1513.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.62: Equation for mini-batch gradient descent (Image credit: http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges)'
  prefs: []
  type: TYPE_NORMAL
- en: You can find the sample logic code for understanding purposes in *Figure 9.63:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/23b21169-fb95-4586-88f0-4e59625d1649.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.63: Sample code for mini-batch gradient descent (Image credit: http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges)'
  prefs: []
  type: TYPE_NORMAL
- en: If we have a high-dimensional dataset, then we can use some other gradient descent
    method; let's begin with momentum.
  prefs: []
  type: TYPE_NORMAL
- en: '**Momentum**'
  prefs: []
  type: TYPE_NORMAL
- en: If all the possible parameters' values surface curves much more steeply in one
    dimension than in another, then in this kind of case, this are very common around
    local optima. In these scenarios, SGD oscillates across the slopes. So to solve
    this oscillation issue, we will use the momentum method. You can see the equation
    in *Figure 9.64:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0de2757-8da0-4dd8-9622-31f965891b99.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.64: Equation for momentum (Image credit: http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges)'
  prefs: []
  type: TYPE_NORMAL
- en: If you see the equation, we are adding a fraction of the direction of the gradient
    from the previous time step to the current step, and we amplify the parameter
    update in the right direction that speeds up our convergence and reduces the oscillation.
    So here, the concept of momentum is similar to the concept of momentum in physics.
    This variant doesn't slow down when local minima is obtained because at that time,
    the momentum is high. In this situation, our algorithm can miss the local minima
    entirely and this problem can be solved by Nesterov accelerated gradient.
  prefs: []
  type: TYPE_NORMAL
- en: '**Nesterov accelerated gradient**'
  prefs: []
  type: TYPE_NORMAL
- en: 'This method was invented by Yurii Nesterov. He was trying to solve the issue
    that occurred in the momentum technique. He has published a paper that you can
    see at this link:'
  prefs: []
  type: TYPE_NORMAL
- en: You can see the equation in *Figure 9.65:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/301534eb-f693-43e5-bf8f-3b1607dd3f3d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.65: Equation for Nesterov accelerated gradient (Image credit: http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges)'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we are doing the same calculation that we have done for momentum
    but we have changed the order of calculation. In momentum, we compute the gradient
    make jump in that direction amplified by the momentum, whereas in the Nesterov
    accelerated gradient method, we first make a jump based on the previous momentum
    then calculate gradient and after that we add a correction and generate the final
    update for our parameter. This helps us provide parameter values more dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: '**Adagrad**'
  prefs: []
  type: TYPE_NORMAL
- en: Adagrad is stands for adaptive gradient. This method allows the learning rate
    to adapt based on the parameters. This algorithm provides a big update for infrequent
    parameters and a small update for frequent parameters. You can see the equation
    in *Figure 9.66:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/666299f3-4f5a-46af-86f7-2deef8e1188f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.66: Equation for Adagrad (Image credit: http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges)'
  prefs: []
  type: TYPE_NORMAL
- en: This method provides a different learning rate for every parameter at the given
    timestamp based on the past gradient computed for that parameter. Here, we don't
    need to manually tune our learning rate although it has a limitation. As per the
    equation, the learning rate is always decreasing as the accumulation of the squared
    gradients placed in the denominator is always positive, and as the denominator
    grows, the whole term will decrease. Sometimes, the learning rate becomes so small
    that the ML-model stops learning. To solve this problem. the method called Adadelta
    has come into the picture.
  prefs: []
  type: TYPE_NORMAL
- en: '**Adadelta**'
  prefs: []
  type: TYPE_NORMAL
- en: Adadelta is an extension of Adagrad. In Adagrad, we constantly add the square
    root to the sum causing the learning rate to decrease. Instead of summing all
    the past square roots, we restrict the window to the accumulated past gradient
    to a fixed size.
  prefs: []
  type: TYPE_NORMAL
- en: You can see the equation in *Figure 9.67:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/72c1a17c-1bd5-4af0-93d7-c52406dbcd26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.67: Equation for Adadelta (Image credit: http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges)'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the equation, we will use the sum of gradient as a decaying
    average of all past squared gradients. Here, the running average *E[g][t]* at
    a given timestamp is dependent on the previous average and the current gradient.
  prefs: []
  type: TYPE_NORMAL
- en: After seeing all the optimization techniques, you know how we can calculate
    the individual learning rate for each parameter, how we can calculate the momentum,
    and how we can prevent the decaying learning rate. Still, there is room for improvement
    by applying some adaptive momentum and that leads us to our final optimization
    method called **Adam**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Adam**'
  prefs: []
  type: TYPE_NORMAL
- en: Adam stands for adaptive momentum estimation. As we are calculating the learning
    rate for each parameter, we can also store the momentum changes for each of them
    separately.
  prefs: []
  type: TYPE_NORMAL
- en: You can see the equation in *Figure 9.68:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ffc6ace5-3525-4373-95cf-9de59ab2ca74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.68: Mean and variance for Adam (Image credit: http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges)'
  prefs: []
  type: TYPE_NORMAL
- en: First, we will be calculating the mean of the gradient, then we are going to
    calculate the uncentered variance of the gradient, and use these values to update
    the parameters. Just like an Adadelta. You can see the equation of Adam in *Figure
    9.69:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2fe35988-9e9c-466d-9bd0-4bb0368661d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.69: Equation for Adam (Image credit: http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges)'
  prefs: []
  type: TYPE_NORMAL
- en: So now you want to know which method we should use; according to me, Adam is
    the best overall choice because it outperforms the other methods. You can also
    use Adadelta and Adagrad. If your data is sparse, then you should not use SGD,
    momentum, or Nesterov.
  prefs: []
  type: TYPE_NORMAL
- en: Artificial intelligence versus human intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the last one year, you may have heard this kind of question. In the AI
    world, these kinds of questions have become common. People have created a hype
    that AI will make humanity vanish and machines will take away all the powers from
    us. Now let me tell you, this is not the truth. These kinds of threats sound like
    science-fiction stories. According to me, AI is in its high pace development phase
    but its purpose is to complement humanity and make human life easier. We are still
    figuring out some of the complex and unknown truths of this universe that can
    help us provide more insight on how we can build AI-enabled systems. So AI is
    purely going to help us. AI will amaze our lives for sure but it is not going
    to be saturated with its inventions soon. So enjoy this AI phase and contribute
    to the AI ecosystem in a positive manner.
  prefs: []
  type: TYPE_NORMAL
- en: People have concerns that AI will take away our jobs. It will not take away
    your job. It will make your job easier. If you are a doctor and want to give your
    final words on some cancer report, AI will help you. In the Information Technology
    (IT) industry, there's a concern that AI will replace the coders. If you believe
    that very soon researchers and tech companies will be able to build machines that
    are more powerful than humans and that the AI shift will happen soon and machines
    will take away our jobs, then it is better for you to acquire ML, DL, and AI related
    skill sets to have jobs, and perhaps you are the last person on this planet who
    has some job to do! We assume that AI would take away some jobs, but this AI ecosystem
    will also create so many new jobs. So don't worry! This discussion can be ongoing
    but I want to really give you guys some time window to think on this.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations guys! We have made it to the last chapter! I really appreciate
    your efforts. In this chapter, you have learned a lot of things such as artificial
    intelligence aspects that help you understand why deep learning is the buzzword
    nowadays. We have seen the concept of ANNs. We have seen concepts such as gradient
    descent, various activation functions, and loss functions. We have seen the architecture
    of DNN and the DL life cycle. We have also touched on the basics of the sequence-to-sequence
    model and developed applications such as machine translation, title generation,
    and summarization. We have also seen the gradient descent-based optimization techniques.
  prefs: []
  type: TYPE_NORMAL
- en: The next sections are Appendices A to C, that will provide you with an overview
    about frameworks such as hadoop, spark, and so on. You can also see the installation
    guide for these frameworks as well as other tools and libraries. Apart from this,
    you can find cheatsheets for many Python libraries that are very handy if you
    are new to Python. There are some tips from my side if you really want to improve
    your data science as well as NLP skills. I have also provided Gitter links in
    the appendices that you can use to connect with me in case you have any questions.
  prefs: []
  type: TYPE_NORMAL
