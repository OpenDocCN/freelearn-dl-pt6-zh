<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Fraud Analytics Using Autoencoders and Anomaly Detection</h1>
                </header>
            
            <article>
                
<p>Detecting and preventing fraud in financial companies, such as banks, insurance companies, and credit unions, is an important task in order to see a business grow. So far, in the previous chapter, we have seen how to use classical supervised machine learning models; now it's time to use other, unsupervised learning algorithms, such as autoencoders.</p>
<p>In this chapter, we will use a dataset having more than 284,807 instances of credit card use and for each transaction, where only 0.172% transactions are fraudulent. So, this is highly imbalanced data. And hence it would make sense to use autoencoders to pre-train a classification model and apply an anomaly detection technique to predict possible fraudulent transactions; that is, we expect our fraud cases to be anomalies within the whole dataset.</p>
<p>In summary, we will learn the following topics through this end-to-end project:</p>
<ul>
<li>Outlier and anomaly detection using outliers</li>
<li>Using autoencoders in unsupervised learning</li>
<li>Developing a fraud analytics predictive model</li>
<li>Hyperparameters tuning, and most importantly, feature selection</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Outlier and anomaly detection</h1>
                </header>
            
            <article>
                
<p>Anomalies are the unusual and unexpected patterns in an observed world. Thus analyzing, identifying, understanding, and predicting anomalies from seen and unseen data is one of the most important task in data mining. Therefore, detecting anomalies allows extracting critical information from data which then can be used for numerous applications.</p>
<p>While anomaly is a generally accepted term, other synonyms, such as outliers, discordant observations, exceptions, aberrations, surprises, peculiarities or contaminants, are often used in different application domains. In particular, anomalies and outliers are often used interchangeably. Anomaly detection finds extensive use in fraud detection for credit cards, insurance or health care, intrusion detection for cyber-security, fault detection in safety critical systems, and military surveillance for enemy activities.</p>
<p>The importance of anomaly detection stems from the fact that for a variety of application domains anomalies in data often translate to significant actionable insights. When we start exploring a highly unbalanced dataset, there are three possible interpretation of your dataset using kurtosis.  Consequently, the following questions need to be answered and known by means of data exploration before applying the feature engineering:</p>
<ul>
<li>What is the percentage of the total data being present or not having null or missing values for all the available fields? Then try to handle those missing values and interpret them well without losing the data semantics.</li>
<li>What is the correlation between the fields? What is the correlation of each field with the predicted variable? What values do they take (that is, categorical or on categorical, numerical or alpha-numerical, and so on)?</li>
</ul>
<p>Then find out if the data distribution is skewed or not. You can identify the skewness by seeing the outliers or long tail (slightly skewed to the right or positively skewed, slightly skewed to the left or negatively skewed, as shown in Figure 1). Now identify if the outliers contribute towards making the prediction or not. More statistically, your data has one of the 3 possible kurtosis as follows:</p>
<ul>
<li>Mesokurtic if the measure of kurtosis is less than but almost equal to 3</li>
<li>Leptokurtic if the measure of kurtosis is more than 3</li>
<li>Platykurtic if the measure of kurtosis is less than 3</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="318" width="507" class="alignnone size-full wp-image-541 image-border" src="assets/1fa1b01a-9ef4-40c5-9b15-4fc6f24f7ff0.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 1: Different kind of skewness in imbalance dataset</div>
<p>Let’s give an example. Suppose you are interested in fitness walking and you walked on a sports ground or countryside in the last four weeks (excluding the weekends). You spent the following time (in minutes to finish a 4 KM walking track):15, 16, 18, 17.16, 16.5, 18.6, 19.0, 20.4, 20.6, 25.15, 27.27, 25.24, 21.05, 21.65, 20.92, 22.61, 23.71, 35, 39, and 50. Compute and interpret the skewness and kurtosis of these values using R would produce a density plot as follows.</p>
<p>The interpretation presented in <em>Figure 2</em> of the distribution of data (workout times) shows the density plot is skewed to the right so is leptokurtic. So the data points to the right-most position can be thought as the unusual or suspicious for our use case. So we can potentially identify or remove them to make our dataset balanced. However, this is not the purpose of this project but only the identification is.</p>
<div class="CDPAlignCenter CDPAlign"><img height="327" width="504" class="alignnone size-full wp-image-542 image-border" src="assets/7b9c075a-e64b-4e9d-8ed8-591699664df1.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 2: Histogram of the workout time (right-skewed)</div>
<p>Nevertheless, by removing the long tail, we cannot remove the imbalance completely. There is another workaround called outlier detection and removing those data points would be useful. </p>
<p>Moreover, we can also look at the box-plots for each individual feature. Where the box plot displays the data distribution based on five-number summaries: <strong>minimum</strong>, <strong>first quartile</strong>, median, <strong>third quartile</strong>, and <strong>maximum</strong>, as shown in <em>Figure 3</em>, where we can look for outliers beyond three (3) <strong>Inter-Quartile Range</strong> (<strong>IQR</strong>):</p>
<div class="CDPAlignCenter CDPAlign"><img height="289" width="253" class="alignnone size-full wp-image-543 image-border" src="assets/e5b1e2a8-6ec6-4f45-af32-39e594a0f7d0.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 3: Outliers beyond three (3) Inter-Quartile Range (IQR)</div>
<p>Therefore, it would be useful to explore if removing the long tail could provide better predictions for supervised or unsupervised learning. But there is no concrete recommendation for this highly unbalanced dataset. In short, the skewness analysis does not help us in this regard. </p>
<p>Finally, if you observe your model cannot provide you the perfect classification but the <strong>mean square error</strong> (<strong>MSE</strong>) can provide some clue on finding the outlier or anomaly. For example, in our case,  even if our projected model cannot classify your dataset into fraud and non-fraud cases but the mean MSE is definitely higher for fraudulent transactions than for regular ones. So even it would sound naïve, still we can identify outlier instances by applying an MSE threshold for what we can consider outliers. For example, we can think of an instance with an MSE &gt; 0.02 to be an anomaly/outlier.</p>
<p>Now question would be how we can do so? Well, through this end-to-end project, we will see that how to use autoencoders and anomaly detection. We will also see how to use autoencoders to pre-train a classification model. Finally, we’ll see how we can measure model performance on unbalanced data. Let's get started with some knowing about autoencoders.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Autoencoders and unsupervised learning</h1>
                </header>
            
            <article>
                
<p>Autoencoders are artificial neural networks capable of learning efficient representations of the input data without any supervision (that is, the training set is unlabeled). This coding, typically, has a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction. More importantly, autoencoders act as powerful feature detectors, and they can be used for unsupervised pre-training of deep neural networks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working principles of an autoencoder</h1>
                </header>
            
            <article>
                
<p>An autoencoder is a network with three or more layers, where the input layer and the output layer have the same number of neurons, and the intermediate (hidden) layers have a lower number of neurons. The network is trained to simply reproduce in output, for each input data, the same pattern of activity in the input. The remarkable aspect of the problem is that, due to the lower number of neurons in the hidden layer, if the network can learn from examples, and generalize to an acceptable extent, it performs data compression: the status of the hidden neurons provides, for each example, a compressed version of the input and output common states.</p>
<p>The remarkable aspect of the problem is that, due to the lower number of neurons in the hidden layer, if the network can learn from examples, and generalize in an acceptable extent, it performs <em>data compression</em>: the status of the hidden neurons provides, for each example, a <em>compressed version</em> of the <em>input</em> and <em>output common states</em>. Useful applications of autoencoders are <strong>data denoising</strong> and <strong>dimensionality</strong> <strong>reduction</strong> for data visualization.</p>
<p>The following schema shows how an autoencoder typically works. It reconstructs the received input through two phases: an encoding phase that corresponds to a dimensional reduction for the original input<em>,</em> and a decoding phase, capable of reconstructing the original input from the encoded (compressed) representation:</p>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref"><img height="260" width="704" class="alignnone size-full wp-image-545 image-border" src="assets/844e530e-9cd2-4222-95d4-a3e29a4cd44a.png"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref"><span>Figure 4: Encoder and decoder phases in autoencoder</span></div>
<p>As an unsupervised neural network, an autoencoders main characteristic is in its symmetrical structure. An autoencoder has two components: an encoder that converts the inputs to an internal representation, followed by a decoder that converts back the internal representation to the outputs. In other words, an autoencoder can be seen as a combination of an encoder, where we encode some input into a code, and a decoder, where we decode/reconstruct the code back to its original input as the output. Thus, a <strong>Multi-Layer Perceptron</strong> (<strong>MLP</strong>) typically has the same architecture as an autoencoder, except that the number of neurons in the output layer must be equal to the number of inputs.</p>
<p>As mentioned earlier, there is more than one way to train an autoencoder. The first one is by training the whole layer at once, similar to MLP. Although, instead of using some labeled output when calculating the cost function (as in supervised learning), we use the input itself. So, the <kbd>cost</kbd> function shows the difference between the actual input and the reconstructed input.</p>
<p>The second way is by greedy-training one layer at a time. This training implementation comes from the problem that was created by the backpropagation method in supervised learning (for example, classification). In a network with a large number of layers, the backpropagation method became very slow and inaccurate in gradient calculation. To solve this problem, Geoffrey Hinton applied some pretraining methods to initialize the classification weight, and this pretraining method was done to two neighboring layers at a time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Efficient data representation with autoencoders</h1>
                </header>
            
            <article>
                
<p>A big problem that plagues all supervised learning systems is the so-called <strong>curse of dimensionality</strong>: a progressive decline in performance while increasing the input space dimension. This occurs because the number of necessary samples to obtain a sufficient sampling of the input space increases exponentially with the number of dimensions. To overcome these problems, some optimizing networks have been developed.</p>
<p>The first are autoencoders networks: these are designed and trained for transforming an input pattern in itself, so that, in the presence of a degraded or incomplete version of an input pattern, it is possible to obtain the original pattern. The network is trained to create output data such as that presented in the entrance, and the hidden layer stores the data compressed, that is, a compact representation that captures the fundamental characteristics of the input data.</p>
<p>The second optimizing networks are <strong>Boltzmann machines</strong>: these types of networks consist of an input/output visible layer and one hidden layer. The connections between the visible layer and the hidden one are non-directional: data can travel in both directions, visible-hidden and hidden-visible, and the different neuronal units can be fully connected or partially connected.</p>
<p>Let's see an example. Decide which of the following series you think would be easier to memorize:</p>
<ul>
<li>45, 13, 37, 11, 23, 90, 79, 24, 87, 47</li>
<li>50, 25, 76, 38, 19, 58, 29, 88, 44, 22, 11, 34, 17, 52, 26, 13, 40, 20</li>
</ul>
<p>Seeing the preceding two series, it seems the first series would be easier for a human, because it is shorter, containing only a few numbers compared to the second one. However, if you take a careful look at the second series, you would find that even numbers are exactly two times the following numbers. Whereas the odd numbers are followed by a number times three plus one. This is a famous number sequence called the <strong>hailstone sequence</strong>.</p>
<p>However, if you can easily memorize long series, you can also recognize patterns in the data easily and quickly. During the 1970s, researchers observed that expert chess players were able to memorize the positions of all the pieces in a game by looking at the board for just five seconds. This might sound controversial, but chess experts don't have a more powerful memory than you and I do. The thing is that they can realize the chess patterns more easily than a non-chess player does. An autoencoder works such that it first observes the inputs, converts them to a better and internal representation, and can swallow similar to what it has already learned:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img height="269" width="465" class="alignnone size-full wp-image-546 image-border" src="assets/6b8afe47-97cc-4bc8-8cd9-12e01f78432d.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 5: Autoencoder in chess game perspective</span></div>
<p>Take a look at a more realistic figure concerning the chess example we just discussed: the hidden layer has two neurons (that is, the encoder itself), whereas the output layer has three neurons (in other words, the decoder). Because the internal representation has a lower dimensionality than the input data (it is 2D instead of 3D), the autoencoder is said to be under complete. An under complete autoencoder cannot trivially copy its inputs to the coding, yet it must find a way to output a copy of its inputs.</p>
<p>It is forced to learn the most important features in the input data and drop the unimportant ones. This way, an autoencoder can be compared with <strong>Principal Component Analysis</strong> (<strong>PCA</strong>), which is used to represent a given input using a lower number of dimensions than originally present.</p>
<p>Up to this point, we know how an autoencoder works. Now, it would be worth knowing anomaly detection using outlier identification.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Developing a fraud analytics model</h1>
                </header>
            
            <article>
                
<p>Before we fully start, we need to do two things: know the dataset, and then prepare our programming environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Description of the dataset and using linear models</h1>
                </header>
            
            <article>
                
<p>For this project, we will be using the credit card fraud detection dataset from Kaggle. The dataset can be downloaded from <a href="https://www.kaggle.com/dalpozz/creditcardfraud">https://www.kaggle.com/dalpozz/creditcardfraud</a>. Since I am using the dataset, it would be a good idea to be transparent by citing the following publication:</p>
<ul>
<li>Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson, and Gianluca Bontempi, <em>Calibrating Probability with Undersampling for Unbalanced Classification</em>. In Symposium on <strong>Computational Intelligence and Data Mining</strong> (<strong>CIDM</strong>), IEEE, 2015.</li>
</ul>
<p>The datasets contain transactions made by credit cards by European cardholders in September 2013 over the span of only two days. There is a total of 285,299 transactions, with only 492 frauds out of 284,807 transactions, meaning the dataset is highly imbalanced and the positive class (fraud) accounts for 0.172% of all transactions.</p>
<p>It contains only numerical input variables, which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. There are 28 features, namely <kbd>V1</kbd>, <kbd>V2</kbd>, ..., <kbd>V28</kbd>, that are principal components obtained with PCA, except for the <kbd>Time</kbd> and <kbd>Amount</kbd>. The feature <kbd>Class</kbd> is the response variable, and it takes value 1 in the case of fraud and 0 otherwise. We will see details later on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problem description</h1>
                </header>
            
            <article>
                
<p>Given the class imbalance ratio, we recommend measuring the accuracy using the <strong>Area Under the Precision-Recall Curve</strong> (<strong>AUPRC</strong>). Confusion matrix accuracy is not meaningful for imbalanced classification. Regarding this, use linear machine learning models, such as random forests, logistic regression, or support vector machines, by applying over-or under-sampling techniques. Alternatively, we can try to find anomalies in the data, since an assumption like only a few fraud cases being anomalies within the whole dataset.</p>
<p>When dealing with such a severe imbalance of response labels, we also need to be careful when measuring model performance. Because there are only a handful of fraudulent instances, a model that predicts everything as non-fraud will already achieve more than the accuracy of 99%. But despite its high accuracy, linear machine learning models won't necessarily help us find fraudulent cases.</p>
<p>Therefore, it would be worth exploring deep learning models, such as autoencoders. Additionally, we need to use anomaly detection for finding anomalies. In particular, we will see how to use autoencoders to pre-train a classification model and measure model performance on unbalanced data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing programming environment</h1>
                </header>
            
            <article>
                
<p>In particular, I am going to use several tools and technologies for this project. The following is the list explaining each technology:</p>
<ul>
<li><strong>H2O/Sparking water</strong>: For deep learning platform (see more in the previous chapter)</li>
<li><strong>Apache Spark</strong>: For data processing environment</li>
<li><strong>Vegas</strong>: An alternative to Matplotlib, similar to Python, for plotting. It can be integrated with Spark for plotting purposes</li>
<li><strong>Scala</strong>: The programming language for our project</li>
</ul>
<p>Well, I am going to create a Maven project, where all the dependencies will be injected into the <kbd>pom.xml</kbd> file. The full content of the <kbd>pom.xml</kbd> can be downloaded from the Packt repository. So let's do it:</p>
<pre>&lt;dependencies&gt;<br/>   &lt;dependency&gt;<br/>      &lt;groupId&gt;ai.h2o&lt;/groupId&gt;<br/>      &lt;artifactId&gt;sparkling-water-core_2.11&lt;/artifactId&gt;<br/>      &lt;version&gt;2.2.2&lt;/version&gt;<br/>   &lt;/dependency&gt;<br/>   &lt;dependency&gt;<br/>      &lt;groupId&gt;org.vegas-viz&lt;/groupId&gt;<br/>      &lt;artifactId&gt;vegas_2.11&lt;/artifactId&gt;<br/>      &lt;version&gt;0.3.11&lt;/version&gt;<br/>   &lt;/dependency&gt;<br/>   &lt;dependency&gt;<br/>     &lt;groupId&gt;org.vegas-viz&lt;/groupId&gt;<br/>     &lt;artifactId&gt;vegas-spark_2.11&lt;/artifactId&gt;<br/>     &lt;version&gt;0.3.11&lt;/version&gt;<br/>     &lt;/dependency&gt;<br/>&lt;/dependencies&gt;</pre>
<p>Now, Eclipse or your favorite IDE will pull all the dependencies. The first dependency will also pull all the Spark related dependencies compatible with this H2O version. Then, create a Scala file and provide a suitable name. Then we are ready to go.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 1 - Loading required packages and libraries</h1>
                </header>
            
            <article>
                
<p>So let's start by importing required libraries and packages:</p>
<pre><strong>package</strong> com.packt.ScalaML.FraudDetection<br/><br/><strong>import</strong> org.apache.spark.sql.SparkSession<br/><strong>import</strong> org.apache.spark.sql.functions._<br/><strong>import</strong> org.apache.spark.sql._<br/><strong>import</strong> org.apache.spark.h2o._<br/><strong>import</strong> _root_.hex.FrameSplitter<br/><strong>import</strong> water.Key<br/><strong>import</strong> water.fvec.Frame<br/><strong>import</strong> _root_.hex.deeplearning.DeepLearning<br/><strong>import</strong> _root_.hex.deeplearning.DeepLearningModel.DeepLearningParameters<br/><strong>import</strong> _root_.hex.deeplearning.DeepLearningModel.DeepLearningParameters.Activation<br/><strong>import</strong> java.io.File<br/><strong>import</strong> water.support.ModelSerializationSupport<br/><strong>import</strong> _root_.hex.{ ModelMetricsBinomial, ModelMetrics }<br/><strong>import</strong> org.apache.spark.h2o._<br/><strong>import</strong> scala.reflect.api.materializeTypeTag<br/><strong>import</strong> water.support.ModelSerializationSupport<br/><strong>import</strong> water.support.ModelMetricsSupport<br/><strong>import</strong> _root_.hex.deeplearning.DeepLearningModel<br/><strong>import</strong> vegas._<br/><strong>import</strong> vegas.sparkExt._<br/><strong>import</strong> org.apache.spark.sql.types._</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 2 - Creating a Spark session and importing implicits</h1>
                </header>
            
            <article>
                
<p>We then need to create a Spark session as the gateway of our program:</p>
<pre><strong>val</strong> spark = SparkSession<br/>        .builder<br/>        .master("local[*]")<br/>        .config("spark.sql.warehouse.dir", "tmp/")<br/>        .appName("Fraud Detection")<br/>        .getOrCreate()</pre>
<p class="mce-root">Additionally, we need to import implicits for spark.sql and h2o:</p>
<pre><strong>implicit </strong><strong>val</strong> sqlContext = spark.sqlContext<br/><strong>import</strong> sqlContext.implicits._<br/><strong>val</strong> h2oContext = H2OContext.getOrCreate(spark)<br/><strong>import</strong> h2oContext._<br/><strong>import</strong> h2oContext.implicits._</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 3 - Loading and parsing input data</h1>
                </header>
            
            <article>
                
<p>We load and get the transaction. Then we get the distribution:</p>
<pre><strong>val</strong> inputCSV = "data/creditcard.csv"<br/><br/><strong>val</strong> transactions = spark.read.format("com.databricks.spark.csv")<br/>        .option("header", "true")<br/>        .option("inferSchema", <strong>true</strong>)<br/>        .load(inputCSV)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 4 - Exploratory analysis of the input data</h1>
                </header>
            
            <article>
                
<p>As described earlier, the dataset contains numerical input variables <kbd>V1</kbd> to <kbd>V28</kbd>, which are the result of a PCA transformation of the original features. The response variable <kbd>Class</kbd> tells us whether a transaction was fraudulent (value = 1) or not (value = 0).</p>
<p>There are two additional features, <kbd>Time</kbd> and <kbd>Amount</kbd>. The <kbd>Time</kbd> column signifies the time in seconds between the current transaction and the first transaction. Whereas the <kbd>Amount</kbd> column signifies how much money was transferred in this transaction. So let's see a glimpse of the input data (only <kbd>V1</kbd>, <kbd>V2</kbd>, <kbd>V26</kbd>, and <kbd>V27</kbd> are shown, though) in <em>Figure 6</em>:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img height="263" width="681" class="alignnone size-full wp-image-162 image-border" src="assets/29ef9fd6-5183-47f2-8d32-6715d749d7cf.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 6: A snapshot of the credit card fraud detection dataset</div>
<p>We have been able to load the transaction, but the preceding DataFrame does not tell us about the class distribution. So, let's compute the class distribution and think about plotting them:</p>
<pre><strong>val</strong> distribution = transactions.groupBy("Class").count.collect<br/>Vegas("Class Distribution").withData(distribution.map(r =&gt; Map("class" -&gt; r(0), "count" -&gt; r(1)))).encodeX("class", Nom).encodeY("count", Quant).mark(Bar).show<br/>&gt;&gt;&gt;</pre>
<div class="CDPAlignCenter CDPAlign packt_figref"><img height="242" width="124" src="assets/df46daf3-9596-4e75-9ade-2d93f781f36e.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 7: Class distribution in the credit card fraud detection dataset</div>
<p>Now, let's see if the time has any important contribution to suspicious transactions. The <kbd>Time</kbd> column tells us the order in which transactions were done, but doesn't tell us anything about the actual times (that is, time of day) of the transactions. Therefore, normalizing them by day and binning those into four groups according to time of day to build a <kbd>Day</kbd> column from <kbd>Time</kbd> would be useful. I have written a UDF for this:</p>
<pre><strong>val</strong> daysUDf = udf((s: Double) =&gt; <br/><strong>if</strong> (s &gt; 3600 * 24) "day2" <br/><strong>else</strong> "day1")<br/><br/><strong>val</strong> t1 = transactions.withColumn("day", daysUDf(col("Time")))<br/><strong>val</strong> dayDist = t1.groupBy("day").count.collect</pre>
<p class="mce-root">Now let's plot it:</p>
<pre>Vegas("Day Distribution").withData(dayDist.map(r =&gt; Map("day" -&gt; r(0), "count" -&gt; r(1)))).encodeX("day", Nom).encodeY("count", Quant).mark(Bar).show<br/>&gt;&gt;&gt;</pre>
<div class="CDPAlignCenter CDPAlign packt_figref"><img height="251" width="121" src="assets/e01f7590-6be0-4115-acac-8410a5dbf6bf.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 8: Day distribution in the credit card fraud detection dataset</div>
<p>The preceding graph shows that the same number of transactions was made on these two days, but to be more specific, slightly more transactions were made in <kbd>day1</kbd>. Now let's build the <kbd>dayTime</kbd> column. Again, I have written a UDF for it:</p>
<pre><strong>val</strong> dayTimeUDf = udf((day: String, t: Double) =&gt; <strong>if</strong> (day == "day2") t - 86400 <strong>else</strong> t)<br/><strong>val</strong> t2 = t1.withColumn("dayTime", dayTimeUDf(col("day"), col("Time")))<br/><br/>t2.describe("dayTime").show()<br/>&gt;&gt;&gt;<br/>+-------+------------------+<br/>|summary| dayTime |<br/>+-------+------------------+<br/>| count| 284807|<br/>| mean| 52336.926072744|<br/>| stddev|21049.288810608432|<br/>| min| 0.0|<br/>| max| 86400.0|<br/>+-------+------------------+</pre>
<p>Now that we need to get the quantiles (<kbd>q1</kbd>, median, <kbd>q2</kbd>) and building time bins (<kbd>gr1</kbd>, <kbd>gr2</kbd>, <kbd>gr3</kbd>, and <kbd>gr4</kbd>):</p>
<pre><br/><strong>val</strong> d1 = t2.filter($"day" === "day1")<br/><strong>val</strong> d2 = t2.filter($"day" === "day2")<br/><strong>val</strong> quantiles1 = d1.stat.approxQuantile("dayTime", Array(0.25, 0.5, 0.75), 0)<br/><br/><strong>val</strong> quantiles2 = d2.stat.approxQuantile("dayTime", Array(0.25, 0.5, 0.75), 0)<br/><br/><strong>val</strong> bagsUDf = udf((t: Double) =&gt; <br/><strong>    if</strong> (t &lt;= (quantiles1(0) + quantiles2(0)) / 2) "gr1" <br/><strong>    else</strong><strong>if</strong> (t &lt;= (quantiles1(1) + quantiles2(1)) / 2) "gr2" <br/><strong>    else</strong><strong>if</strong> (t &lt;= (quantiles1(2) + quantiles2(2)) / 2) "gr3" <br/><strong>    else</strong> "gr4")<br/><br/><strong>val</strong> t3 = t2.drop(col("Time")).withColumn("Time", bagsUDf(col("dayTime")))</pre>
<p>Then let's get the distribution for class <kbd>0</kbd> and <kbd>1</kbd>:</p>
<pre><strong>val</strong> grDist = t3.groupBy("Time", "class").count.collect<br/><strong>val</strong> grDistByClass = grDist.groupBy(_(1))</pre>
<p>Now let's plot the group distribution for class <kbd>0</kbd>:</p>
<pre>Vegas("gr Distribution").withData(grDistByClass.get(0).get.map(r =&gt; Map("Time" -&gt; r(0), "count" -&gt; r(2)))).encodeX("Time", Nom).encodeY("count", Quant).mark(Bar).show<br/>&gt;&gt;&gt;</pre>
<div class="CDPAlignCenter CDPAlign packt_figref"><img height="215" width="135" src="assets/29070829-7d9b-40e4-918c-7d4de5fe8913.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9: Group distribution for class 0 in the credit card fraud detection dataset</div>
<p>From the preceding graph, it is clear that most of them are normal transactions. Now let's see the group distribution for <kbd>class 1</kbd>:</p>
<pre>Vegas("gr Distribution").withData(grDistByClass.get(1).get.map(r =&gt; Map("Time" -&gt; r(0), "count" -&gt; r(2)))).encodeX("Time", Nom).encodeY("count", Quant).mark(Bar).show<br/>&gt;&gt;&gt;</pre>
<div class="CDPAlignCenter CDPAlign packt_figref"><img height="215" width="136" src="assets/077d35cb-67f7-4002-82d8-9b53fb78d8ad.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 10: Group distribution for class 1 in the credit card fraud detection dataset</div>
<p>So, the distribution of transactions over the four <strong>Time</strong> bins shows that the majority of fraud cases happened in group 1. We can of course look at the distribution of the amounts of money that were transferred:</p>
<pre><strong>val</strong> c0Amount = t3.filter($"Class" === "0").select("Amount")<br/><strong>val</strong> c1Amount = t3.filter($"Class" === "1").select("Amount")<br/><br/>println(c0Amount.stat.approxQuantile("Amount", Array(0.25, 0.5, 0.75), 0).mkString(","))<br/><br/>Vegas("Amounts for class 0").withDataFrame(c0Amount).mark(Bar).encodeX("Amount", Quantitative, bin = Bin(50.0)).encodeY(field = "*", Quantitative, aggregate = AggOps.Count).show<br/>&gt;&gt;&gt;</pre>
<div class="CDPAlignCenter CDPAlign packt_figref"><img height="244" width="243" src="assets/491ab0c9-6738-411e-a8a2-fc6e00edba83.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11: Distribution of the amounts of money that were transferred for class 0</div>
<p>Now let's plot the same for <kbd>class 1</kbd>:</p>
<pre>Vegas("Amounts for class 1").withDataFrame(c1Amount).mark(Bar).encodeX("Amount", Quantitative, bin = Bin(50.0)).encodeY(field = "*", Quantitative, aggregate = AggOps.Count).show<br/>&gt;&gt;&gt;</pre>
<div class="CDPAlignCenter CDPAlign packt_figref"><img height="251" width="243" src="assets/3dfd4828-7a0d-44e5-902e-4ab6e1c6dc8b.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 12: Distribution of the amounts of money that were transferred for class 1</div>
<p>So, from the preceding two graphs, it can be observed that fraudulent credit card transactions had a higher mean amount of money that was transferred, but the maximum amount was much lower compared to regular transactions. As we have seen in the <kbd>dayTime</kbd> column that we manually constructed, it is not that significant, so we can simply drop it. Let's do it:</p>
<pre><strong>val</strong> t4 = t3.drop("day").drop("dayTime")</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 5 - Preparing the H2O DataFrame</h1>
                </header>
            
            <article>
                
<p>Up to this point, our DataFrame (that is, <kbd>t4</kbd>) is in Spark DataFrame. But it cannot be consumed by the H2O model. So, we have to convert it to an H2O frame. So let's do it:</p>
<pre><strong>val</strong> creditcard_hf: H2OFrame = h2oContext.asH2OFrame(t4.orderBy(rand()))</pre>
<p>We split the dataset to, say, 40% supervised training, 40% unsupervised training, and 20% test using H2O built-in splitter called FrameSplitter:</p>
<pre><strong>val</strong> sf = <strong>new</strong> FrameSplitter(creditcard_hf, Array(.4, .4), <br/>                Array("train_unsupervised", "train_supervised", "test")<br/>                .map(Key.make[Frame](_)), <strong>null</strong>)<br/><br/>water.H2O.submitTask(sf)<br/><strong>val</strong> splits = sf.getResult<br/><strong>val</strong> (train_unsupervised, train_supervised, test) = (splits(0), splits(1), splits(2))</pre>
<p>In the above code segment, <kbd>Key.make[Frame](_)</kbd> is used as a low-level task to split the frame based on the split ratio that also help attain distributed Key/Value pairs.</p>
<div class="packt_infobox">
<p>Keys are very crucial in H2O computing. H2O supports a distributed Key/Value store, with exact Java memory model consistency. The thing is that Keys are a means to find a link value somewhere in the Cloud, to cache it locally, to allow globally consistent updates to a link Value.   </p>
</div>
<p>Finally, we need to convert the <kbd>Time</kbd> column from String to Categorical (that is, <strong>enum</strong>) explicitly:</p>
<pre>toCategorical(train_unsupervised, 30)<br/>toCategorical(train_supervised, 30)<br/>toCategorical(test, 30)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 6 - Unsupervised pre-training using autoencoder</h1>
                </header>
            
            <article>
                
<p>As described earlier, we will be using Scala with the <kbd>h2o</kbd> encoder. Now it's time to start the unsupervised autoencoder training. Since the training is unsupervised, it means we need to exclude the <kbd>response</kbd> column from the unsupervised training set:</p>
<pre><strong>val</strong> response = "Class"<br/><strong>val</strong> features = train_unsupervised.names.filterNot(_ == response)</pre>
<p>The next task is to define the hyperparameters, such as the number of hidden layers with neurons, seeds for the reproducibility, the number of training epochs and the activation function for the deep learning model. For the unsupervised pre-training, just set the autoencoder parameter to <kbd>true</kbd>:</p>
<pre><strong>var</strong> dlParams = <strong>new</strong> DeepLearningParameters()<br/>    dlParams._ignored_columns = Array(response))// since unsupervised, we ignore the label<br/>    dlParams._train = train_unsupervised._key // use the train_unsupervised frame for training<br/>    dlParams._autoencoder = <strong>true </strong>// use H2O built-in autoencoder<strong><br/></strong>    dlParams._reproducible = <strong>true </strong>// ensure reproducibility<strong><br/></strong>    dlParams._seed = 42 // random seed for reproducibility<br/>    dlParams._hidden = Array[Int](10, 2, 10)<br/>    dlParams._epochs = 100 // number of training epochs<br/>    dlParams._activation = Activation.Tanh // Tanh as an activation function<br/>    dlParams._force_load_balance = <strong>false<br/><br/></strong><strong>var</strong> dl = <strong>new</strong> DeepLearning(dlParams)<br/><strong>val</strong> model_nn = dl.trainModel.get</pre>
<p>In the preceding code, we are applying a technique called <strong>bottleneck</strong> training, where the hidden layer in the middle is very small. This means that my model will have to reduce the dimensionality of the input data (in this case, down to two nodes/dimensions).</p>
<p>The autoencoder model will then learn the patterns of the input data, irrespective of given class labels. Here, it will learn which credit card transactions are similar and which transactions are outliers or anomalies. We need to keep in mind, though, that autoencoder models will be sensitive to outliers in our data, which might throw off otherwise typical patterns.</p>
<p>Once the pre-training is completed, we should save the model in the <kbd>.csv</kbd> directory:</p>
<pre><strong>val</strong> uri = <strong>new</strong> File(<strong>new</strong> File(inputCSV).getParentFile, "model_nn.bin").toURI ModelSerializationSupport.exportH2OModel(model_nn, uri)</pre>
<p>Reload the model and restore it for further use:</p>
<pre><strong>val</strong> model: DeepLearningModel = ModelSerializationSupport.loadH2OModel(uri)</pre>
<p>Now let's print the model's metrics to see how the training went:</p>
<pre>println(model)<br/>&gt;&gt;&gt;</pre>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/5dc693c5-f3ed-46b4-8012-b902b7470df6.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 13: Autoencoder model's metrics</div>
<p>Fantastic! The pre-training went very well, because we can see the RMSE and MSE are pretty low. We can also see that some features are pretty unimportant, such as <kbd>v16</kbd>, <kbd>v1</kbd>, <kbd>v25</kbd>, and so on. We will try to analyze it later on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 7 - Dimensionality reduction with hidden layers</h1>
                </header>
            
            <article>
                
<p>Since we used a shallow autoencoder with two nodes in the hidden layer in the middle, it would be worth using the dimensionality reduction to explore our feature space. We can extract this hidden feature with the <kbd>scoreDeepFeatures()</kbd> method and plot it to show the reduced representation of the input data.</p>
<div class="packt_infobox">The <kbd>scoreDeepFeatures()</kbd> method scores an auto-encoded reconstruction on-the-fly, and materialize the deep features of given layer. It takes the following parameters, frame Original data (can contain response, will be ignored) and layer index of the hidden layer for which to extract the features. Finally, a frame containing the deep features is returned. Where number of columns is the hidden [layer]</div>
<p>Now, for the supervised training, we need to extract the Deep Features. Let's do it from layer 2:</p>
<pre><strong>var</strong> train_features = model_nn.scoreDeepFeatures(train_unsupervised, 1) <br/>train_features.add("Class", train_unsupervised.vec("Class"))</pre>
<p>The plotting for eventual cluster identification is as follows:</p>
<pre>train_features.setNames(train_features.names.map(_.replaceAll("[.]", "-")))<br/>train_features._key = Key.make()<br/>water.DKV.put(train_features)<br/><br/><strong>val</strong> tfDataFrame = asDataFrame(train_features) Vegas("Compressed").withDataFrame(tfDataFrame).mark(Point).encodeX("DF-L2-C1", Quantitative).encodeY("DF-L2-C2", Quantitative).encodeColor(field = "Class", dataType = Nominal).show<br/>&gt;&gt;&gt;</pre>
<div class="CDPAlignCenter CDPAlign packt_figref"><img height="247" width="310" src="assets/7408e0ac-c725-476f-b0d4-52d0a2c442a3.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 14: Eventual cluster for classes 0 and 1</div>
<p>From the preceding figure, we cannot see any cluster of fraudulent transactions that is distinct from non-fraudulent instances, so dimensionality reduction with our autoencoder model alone is not sufficient to identify fraud in this dataset. But we could use the reduced dimensionality representation of one of the hidden layers as features for model training. An example would be to use the 10 features from the first or third hidden layer. Now, let's extract the Deep Features from layer 3:</p>
<pre>train_features = model_nn.scoreDeepFeatures(train_unsupervised, 2)<br/>train_features._key = Key.make()<br/>train_features.add("Class", train_unsupervised.vec("Class"))<br/>water.DKV.put(train_features)<br/><br/><strong>val</strong> features_dim = train_features.names.filterNot(_ == response)<br/><strong>val</strong> train_features_H2O = asH2OFrame(train_features)</pre>
<p class="mce-root">Now let's do unsupervised DL using the dataset of the new dimension again:</p>
<pre class="mce-root">dlParams = <strong>new</strong> DeepLearningParameters()<br/>        dlParams._ignored_columns = Array(response)<br/>        dlParams._train = train_features_H2O<br/>        dlParams._autoencoder = <strong>true<br/></strong>        dlParams._reproducible = <strong>true<br/></strong>        dlParams._ignore_const_cols = <strong>false<br/></strong>        dlParams._seed = 42<br/>        dlParams._hidden = Array[Int](10, 2, 10)<br/>        dlParams._epochs = 100<br/>        dlParams._activation = Activation.Tanh<br/>        dlParams._force_load_balance = <strong>false<br/><br/></strong>dl = <strong>new</strong> DeepLearning(dlParams)<br/><strong>val</strong> model_nn_dim = dl.trainModel.get</pre>
<p>We then save the model:</p>
<pre>ModelSerializationSupport.exportH2OModel(model_nn_dim, new File(new File(inputCSV).getParentFile, "model_nn_dim.bin").toURI)</pre>
<p>For measuring model performance on test data, we need to convert the test data to the same reduced dimensions as the training data:</p>
<pre><strong>val</strong> test_dim = model_nn.scoreDeepFeatures(test, 2)<br/><strong>val</strong> test_dim_score = model_nn_dim.scoreAutoEncoder(test_dim, Key.make(), <strong>false</strong>)<br/><br/><strong>val</strong> result = confusionMat(test_dim_score, test, test_dim_score.anyVec.mean)<br/>println(result.deep.mkString("n"))<br/>&gt;&gt;&gt;<br/>Array(38767, 29)<br/>Array(18103, 64)</pre>
<p>Now, this actually looks quite good in terms of identifying fraud cases: 93% of fraud cases were identified!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 8 - Anomaly detection</h1>
                </header>
            
            <article>
                
<p>We can also ask which instances were considered outliers or anomalies within our test data. Based on the autoencoder model that was trained before, the input data will be reconstructed, and for each instance, the MSE between actual value and reconstruction is calculated. I am also calculating the mean MSE for both class labels:</p>
<pre>test_dim_score.add("Class", test.vec("Class"))<br/><strong>val</strong> testDF = asDataFrame(test_dim_score).rdd.zipWithIndex.map(r =&gt; Row.fromSeq(r._1.toSeq :+ r._2))<br/><br/><strong>val</strong> schema = StructType(Array(StructField("Reconstruction-MSE", DoubleType, nullable = <strong>false</strong>), StructField("Class", ByteType, nullable = <strong>false</strong>), StructField("idRow", LongType, nullable = <strong>false</strong>)))<br/><br/><strong>val</strong> dffd = spark.createDataFrame(testDF, schema)<br/>dffd.show()<br/>&gt;&gt;&gt;</pre>
<div class="CDPAlignCenter CDPAlign packt_figref"><img height="237" width="159" src="assets/01c06441-36c9-4976-a9ea-c669408e55b3.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 15: DataFrame showing MSE, class, and row ID</div>
<p>Seeing this DataFrame, it's really difficult to identify outliers. But plotting them would provide some more insights:</p>
<pre>Vegas("Reduced Test", width = 800, height = 600).withDataFrame(dffd).mark(Point).encodeX("idRow", Quantitative).encodeY("Reconstruction-MSE", Quantitative).encodeColor(field = "Class", dataType = Nominal).show<br/>&gt;&gt;&gt;</pre>
<div class="CDPAlignCenter CDPAlign packt_figref"><img height="510" width="716" src="assets/add43d6c-82c0-472e-b39f-5ebc1dfc40e4.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 16: Distribution of the reconstructed MSE, across different row IDs</div>
<p>As we can see in the plot, there is no perfect classification into fraudulent and non-fraudulent cases, but the mean MSE is definitely higher for fraudulent transactions than for regular ones. But a minimum interpretation is necessary.</p>
<p>From the preceding figure, we can at least see that most of the <strong>idRows</strong> have an MSE of <strong>5µ</strong>. Or, if we extend the MSE threshold up to <strong>10µ</strong>, then the data points exceeding this threshold can be considered as outliers or anomalies, that is, fraudulent transactions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 9 - Pre-trained supervised model</h1>
                </header>
            
            <article>
                
<p>We can now try using the autoencoder model as a pre-training input for a supervised model. Here, I am again using a neural network. This model will now use the weights from the autoencoder for model fitting. However, transforming the classes from Int to Categorical in order to train for classification is necessary. Otherwise, the H2O training algorithm will treat it as a regression:</p>
<pre>toCategorical(train_supervised, 29)</pre>
<p>Now that the training set (that is, <kbd>train_supervised</kbd>) is ready for supervised learning, let's jump into it:</p>
<pre><strong>val</strong> train_supervised_H2O = asH2OFrame(train_supervised)<br/>        dlParams = <strong>new</strong> DeepLearningParameters()<br/>        dlParams._pretrained_autoencoder = model_nn._key<br/>        dlParams._train = train_supervised_H2O<br/>        dlParams._reproducible = <strong>true<br/></strong>        dlParams._ignore_const_cols = <strong>false<br/></strong>        dlParams._seed = 42<br/>        dlParams._hidden = Array[Int](10, 2, 10)<br/>        dlParams._epochs = 100<br/>        dlParams._activation = Activation.Tanh<br/>        dlParams._response_column = "Class"<br/>        dlParams._balance_classes = <strong>true<br/><br/></strong>dl = <strong>new</strong> DeepLearning(dlParams)<br/><strong>val</strong> model_nn_2 = dl.trainModel.get</pre>
<p>Well done! We have now completed the supervised training. Now, to see the predicted versus actual classes:</p>
<pre><strong>val</strong> predictions = model_nn_2.score(test, "predict")<br/>test.add("predict", predictions.vec("predict"))<br/>asDataFrame(test).groupBy("Class", "predict").count.show //print<br/>&gt;&gt;&gt;<br/>+-----+-------+-----+<br/>|Class|predict|count|<br/>+-----+-------+-----+<br/>| 1| 0| 19|<br/>| 0| 1| 57|<br/>| 0| 0|56804|<br/>| 1| 1| 83|<br/>+-----+-------+-----+</pre>
<p>Now, this looks much better! We did miss 17% of the fraud cases, but we also did not misclassify too many of the non-fraudulent cases. In real life, we would spend some more time trying to improve the model by example, performing grid searches for hyperparameter tuning, going back to the original features and trying different engineered features and/or trying different algorithms. Now, what about visualizing the preceding result? Let's do it using the <kbd>Vegas</kbd> package:</p>
<pre>Vegas().withDataFrame(asDataFrame(test)).mark(Bar).encodeY(field = "*", dataType = Quantitative, AggOps.Count, axis = Axis(title = "", format = ".2f"), hideAxis = <strong>true</strong>).encodeX("Class", Ord).encodeColor("predict", Nominal, scale = Scale(rangeNominals = List("#EA98D2", "#659CCA"))).configMark(stacked = StackOffset.Normalize).show<br/>&gt;&gt;&gt;</pre>
<div class="CDPAlignCenter CDPAlign packt_figref"><img height="252" width="180" src="assets/f46c6d3a-8edc-489b-9250-cba98b3b7f58.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 17: Predicted versus actual classes using the supervised trained model</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 10 - Model evaluation on the highly-imbalanced data</h1>
                </header>
            
            <article>
                
<p>Since the dataset is highly imbalanced towards non-fraudulent cases, using model evaluation metrics, such as accuracy or <strong>area under the curve</strong> (<strong>AUC</strong>), does not make sense. The reason is that these metrics would give overly optimistic results based on the high percentage of correct classifications of the majority class.</p>
<p>An alternative to AUC is to use the precision-recall curve, or the sensitivity (recall) -specificity curve. First, let's compute the ROC using the <kbd>modelMetrics()</kbd> method from the <kbd>ModelMetricsSupport</kbd> class:</p>
<pre><strong>val</strong> trainMetrics = ModelMetricsSupport.modelMetrics[ModelMetricsBinomial](model_nn_2, test)<br/><strong>val</strong> auc = trainMetrics._auc<br/><strong>val</strong> metrics = auc._tps.zip(auc._fps).zipWithIndex.map(x =&gt; x <strong>match</strong> { <strong>case</strong> ((a, b), c) =&gt; (a, b, c) })<br/><br/><strong>val</strong> fullmetrics = metrics.map(_ <strong>match</strong> { <strong>case</strong> (a, b, c) =&gt; (a, b, auc.tn(c), auc.fn(c)) })<br/><strong>val</strong> precisions = fullmetrics.map(_ <strong>match</strong> { <strong>case</strong> (tp, fp, tn, fn) =&gt; tp / (tp + fp) })<br/><br/><strong>val</strong> recalls = fullmetrics.map(_ <strong>match</strong> { <strong>case</strong> (tp, fp, tn, fn) =&gt; tp / (tp + fn) })<br/><strong>val</strong> rows = <strong>for</strong> (i &lt;- 0 until recalls.length) <strong>yield</strong> r(precisions(i), recalls(i))<br/><strong>val</strong> precision_recall = rows.toDF()</pre>
<p>Now that we have the <kbd>precision_recall</kbd> DataFrame, it would be exciting to plot it. So let's do it:</p>
<pre>Vegas("ROC", width = 800, height = 600).withDataFrame(precision_recall).mark(Line).encodeX("recall", Quantitative).encodeY("precision", Quantitative).show<br/>&gt;&gt;&gt;</pre>
<div class="CDPAlignCenter CDPAlign packt_figref"><img height="478" width="632" src="assets/f66e5dc0-0b53-4b60-9ba2-9186bfa32671.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 18: Precision-recall curve</div>
<div class="packt_infobox">Precision is the proportion of test cases predicted to be fraudulent that were truly fraudulent, also called <strong>true positive</strong> predictions. On the other hand, recall, or sensitivity, is the proportion of fraudulent cases that were identified as fraudulent. And specificity is the proportion of non-fraudulent cases that are identified as non-fraudulent.</div>
<p>The preceding precision-recall curve tells us the relationship between actual fraudulent predictions and the proportion of fraudulent cases that were predicted. Now, the question is how to compute the sensitivity and specificity. Well, we can do it using standard Scala syntax and plot it using the <kbd>Vegas</kbd> package:</p>
<pre><strong>val</strong> sensitivity = fullmetrics.map(_ <br/><strong>    match</strong> { <br/><strong>        case</strong> (tp, fp, tn, fn) =&gt; tp / (tp + fn) })<br/><strong>        val</strong> specificity = fullmetrics.map(_ <br/><strong>        match</strong> { <br/><strong>            case</strong> (tp, fp, tn, fn) =&gt; tn / (tn + fp) })<br/><strong>            val</strong> rows2 = <br/><strong>            for</strong> (i &lt;- 0 until specificity.length) <br/><strong>     yield</strong> r2(sensitivity(i), specificity(i))<br/><br/><strong>val</strong> sensitivity_specificity = rows2.toDF<br/>Vegas("sensitivity_specificity", width = 800, height = 600).withDataFrame(sensitivity_specificity).mark(Line).encodeX("specificity", Quantitative).encodeY("sensitivity", Quantitative).show<br/>&gt;&gt;&gt;</pre>
<div class="CDPAlignCenter CDPAlign packt_figref"><img height="364" width="482" src="assets/d09bc5bc-db14-4d43-8cd8-69615e21e749.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 19: Sensitivity versus specificity curve</div>
<p>Now the preceding sensitivity-specificity curve tells us the relationship between correctly predicted classes from both labels—for example, if we have 100% correctly predicted fraudulent cases, there will be no correctly classified non-fraudulent cases, and vice versa).</p>
<p>Finally, it would be great to take a closer look at this a little bit differently, by manually going through different prediction thresholds and calculating how many cases were correctly classified in the two classes. More specifically, we can visually inspect true positive, false positive, true negative, and false negative over different prediction thresholds—for example, 0.0 to 1.0:</p>
<pre><strong>val</strong> withTh = auc._tps.zip(auc._fps)<br/>            .zipWithIndex<br/>            .map(x =&gt; x <strong>match</strong> { <strong>case</strong> ((a, b), c) <br/>            =&gt; (a, b, auc.tn(c), auc.fn(c), auc._ths(c)) })<br/><strong>val</strong> rows3 = <strong>for</strong> (i &lt;- 0 until withTh.length) <strong>yield</strong> r3(withTh(i)._1, withTh(i)._2, withTh(i)._3, withTh(i)._4, withTh(i)._5)</pre>
<p>First, let's draw the true positive one:</p>
<pre>Vegas("tp", width = 800, height = 600).withDataFrame(rows3.toDF).mark(Line).encodeX("th", Quantitative).encodeY("tp", Quantitative).show<br/>&gt;&gt;&gt;</pre>
<div class="CDPAlignCenter CDPAlign packt_figref"><img height="330" width="438" src="assets/8744aaab-cff4-4f47-91a5-c79c408bd47e.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 20: True positives across different prediction thresholds in [0.0, 1.0]</div>
<p>Secondly, let's draw the false positive one:</p>
<pre>Vegas("fp", width = 800, height = 600).withDataFrame(rows3.toDF).mark(Line).encodeX("th", Quantitative).encodeY("fp", Quantitative).show<br/>&gt;&gt;&gt;</pre>
<div class="CDPAlignCenter CDPAlign packt_figref"><img height="485" width="643" src="assets/3a0053e8-ba09-438e-ba3a-95f59da4e307.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 21: False positives across different prediction thresholds in [0.0, 1.0]</div>
<p>However, the preceding figure is not easily interpretable. So let's provide a threshold of 0.01 for the <kbd>datum.th</kbd> and then draw it again:</p>
<pre>Vegas("fp", width = 800, height = 600).withDataFrame(rows3.toDF).mark(Line).filter("datum.th &gt; 0.01").encodeX("th", Quantitative).encodeY("fp", Quantitative).show<br/>&gt;&gt;&gt;</pre>
<div class="CDPAlignCenter CDPAlign packt_figref"><img height="549" width="728" src="assets/f4190313-1a84-41f5-8193-164149958df3.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 22: False positives across different prediction thresholds in [0.0, 1.0]</div>
<p>Then, it's the turn for the true negative one:</p>
<pre>Vegas("tn", width = 800, height = 600).withDataFrame(rows3.toDF).mark(Line).encodeX("th", Quantitative).encodeY("tn", Quantitative).show<br/>&gt;&gt;&gt;</pre>
<div class="CDPAlignCenter CDPAlign packt_figref"><img height="544" width="722" src="assets/4720dbe2-95f4-4b8c-a5ee-df4b4b9d61c9.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 23: False positives across different prediction thresholds in [0.0, 1.0]</div>
<p>Finally, let's draw the false negative one, as follows:</p>
<pre>Vegas("fn", width = 800, height = 600).withDataFrame(rows3.toDF).mark(Line).encodeX("th", Quantitative).encodeY("fn", Quantitative).show<br/>&gt;&gt;&gt;</pre>
<div class="CDPAlignCenter CDPAlign packt_figref"><img height="524" width="695" src="assets/bab24a7b-dd1a-4857-96dc-c82ae1521854.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 24: False positives across different prediction thresholds in [0.0, 1.0]</div>
<p>Therefore, the preceding plots tell us that we can increase the number of correctly classified non-fraudulent cases without losing correctly classified fraudulent cases when we increase the prediction threshold from the default 0.5 to 0.6.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Step 11 - Stopping the Spark session and H2O context</h1>
                </header>
            
            <article>
                
<p>Finally, stop the Spark session and H2O context. The following <kbd>stop()</kbd> method invocation will shut down the H2O context and Spark cluster, respectively:</p>
<pre>h2oContext.stop(stopSparkContext = <strong>true</strong>)<br/>spark.stop()</pre>
<p>The first one, especially, is more important, otherwise it sometimes does not stop the H2O flow but still holds the computing resources.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Auxiliary classes and methods</h1>
                </header>
            
            <article>
                
<p>In the preceding steps, we have seen some classes or methods that we should describe here, too. The first method, named <kbd>toCategorical()</kbd>, converts the Frame column from String/Int to enum; this is used to convert <kbd>dayTime</kbd> bags (that is, <kbd>gr1</kbd>, <kbd>gr2</kbd>, <kbd>gr3</kbd>, <kbd>gr4</kbd>) to a factor-like type. This function is also used to convert the <kbd>Class</kbd> column to a factor type in order to perform classification:</p>
<pre><strong>def</strong> toCategorical(f: Frame, i: Int): Unit = {<br/>    f.replace(i, f.vec(i).toCategoricalVec)<br/>    f.update()<br/>    }</pre>
<p>This builds a confusion matrix for anomaly detection according to a threshold if an instance is considered anomalous (if its MSE exceeds the given threshold):</p>
<pre><strong>def</strong> confusionMat(mSEs:water.fvec.Frame,actualFrame:water.fvec.Frame,thresh: Double):Array[Array[Int]] = {<br/><strong>    val</strong> actualColumn = actualFrame.vec("Class");<br/><strong>    val</strong> l2_test = mSEs.anyVec();<br/><strong>    val</strong> result = Array.ofDim[Int](2, 2)<br/><strong>    var</strong> i = 0<br/><strong>    var</strong> ii, jj = 0<br/><br/><strong>    for</strong> (i &lt;- 0 until l2_test.length().toInt) {<br/>        ii = <strong>if</strong> (l2_test.at(i) &gt; thresh) 1 <strong>else</strong> 0;<br/>        jj = actualColumn.at(i).toInt<br/>        result(ii)(jj) = result(ii)(jj) + 1<br/>        }<br/>    result<br/>    }</pre>
<p>Apart from these two auxiliary methods, I have defined three Scala case classes for computing precision, recall; sensitivity, specificity; true positive, true negative, false positive and false negative and so on. The signature is as follows:</p>
<pre><strong>case</strong><strong>class</strong> r(precision: Double, recall: Double)<br/><strong>case</strong><strong>class</strong> r2(sensitivity: Double, specificity: Double)<br/><strong>case</strong><strong>class</strong> r3(tp: Double, fp: Double, tn: Double, fn: Double, th: Double)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hyperparameter tuning and feature selection</h1>
                </header>
            
            <article>
                
<p>Here are some ways of improving the accuracy by tuning hyperparameters, such as the number of hidden layers, the neurons in each hidden layer, the number of epochs, and the activation function. The current implementation of the H2O-based deep learning model supports the following activation functions:</p>
<ul>
<li><kbd>ExpRectifier</kbd></li>
<li><kbd>ExpRectifierWithDropout</kbd></li>
<li><kbd>Maxout</kbd></li>
<li><kbd>MaxoutWithDropout</kbd></li>
<li><kbd>Rectifier</kbd></li>
<li><kbd>RectifierWthDropout</kbd></li>
<li><kbd>Tanh</kbd></li>
<li><kbd>TanhWithDropout</kbd></li>
</ul>
<p>Apart from the <kbd>Tanh</kbd> one, I have not tried other activation functions for this project. However, you should definitely try.</p>
<p>One of the biggest advantages of using H2O-based deep learning algorithms is that we can take the relative variable/feature importance. In previous chapters, we have seen that, using the random forest algorithm in Spark, it is also possible to compute the variable importance. So, the idea is that if your model does not perform well, it would be worth dropping less important features and doing the training again.</p>
<p>Let's see an example; in <em>Figure 13</em>, we have seen the most important features in unsupervised training in autoencoder. Now, it is also possible to find the feature importance during supervised training. I have observed feature importance here:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img height="595" width="693" src="assets/e5331bba-dac7-43af-a0b9-428c83482cad.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 25: False positives across different prediction thresholds in [0.0, 1.0]</div>
<p>Therefore, from <em>Figure 25</em>, it can be observed that the features Time, <kbd>V21</kbd>, <kbd>V17</kbd>, and <kbd>V6</kbd> are less important ones. So why don't you drop them and try training again and observe whether the accuracy has increased or not?</p>
<p>Nevertheless, grid searching or cross-validation techniques could still provide higher accuracy. However, I'll leave it up to you.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have used a dataset having more than 284,807 instances of credit card use and for each transaction where only 0.172% transactions are fraudulent. We have seen how we can use autoencoders to pre-train a classification model and how to apply anomaly detection techniques to predict possible fraudulent transactions from highly imbalanced data—that is, we expected our fraudulent cases to be anomalies within the whole dataset.</p>
<p>Our final model now correctly identified 83% of fraudulent cases and almost 100% of non-fraudulent cases. Nevertheless, we have seen how to use anomaly detection using outliers, some ways of hyperparameter tuning, and, most importantly, feature selection.</p>
<p>A <strong>recurrent neural network</strong> (<strong>RNN</strong>) is a class of artificial neural network where connections between units form a directed cycle. RNNs make use of information from the past. That way, they can make predictions in data with high temporal dependencies. This creates an internal state of the network that allows it to exhibit dynamic temporal behavior.</p>
<p>An RNN takes many input vectors to process them and output other vectors. Compared to a classical approach, using an RNN with <strong>Long Short-Term Memory cells</strong> (<strong>LSTMs</strong>) requires almost no feature engineering. Data can be fed directly into the neural network, which acts like a black box, modeling the problem correctly. The approach here is rather simple in terms of how much of the data was preprocessed.</p>
<p>In the next chapter, we will see how to develop an machine learning project using an RNN implementation called <strong>LSTM</strong> for <strong>human activity recognition</strong> (<strong>HAR</strong>), using a smartphones dataset. In short, our machine learning model will be able to classify the type of movement from six categories: walking, walking upstairs, walking downstairs, sitting, standing, and laying.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </body></html>