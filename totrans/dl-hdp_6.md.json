["```py\nfinal int numRows = 28;\n```", "```py\nfinal int numColumns = 28; \nint seed = 123; \nint numSamples = MnistDataFetcher.NUM_EXAMPLES; \nint batchSize = 1024; \nint iterations = 1; \nint listenerFreq = iterations/5; \n\n```", "```py\nlog.info(\"Load data....\"); \nDataSetIterator iter = new  MnistDataSetIterator(batchSize,numSamples,true); \n\n```", "```py\nlog.info(\"Build model....\"); \nMultiLayerConfiguration conf = new NeuralNetConfiguration.Builder() \n  .seed(seed) \n  .iterations(iterations) \n  .optimizationAlgo(OptimizationAlgorithm.LINE_GRADIENT_DESCENT) \n\n```", "```py\n .list(8) \n\n```", "```py\n  .layer(0, new RBM.Builder().nIn(numRows *    \n  numColumns).nOut(2000).lossFunction(LossFunctions.LossFunction\n  .RMSE_XENT).build()) \n  .layer(1, new RBM.Builder().nIn(2000).nOut(1000)\n  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build()) \n  .layer(2, new RBM.Builder().nIn(1000).nOut(500)\n  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build()) \n  .layer(3, new RBM.Builder().nIn(500).nOut(30)\n  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build()) \n\n```", "```py\n  .layer(4, new RBM.Builder().nIn(30).nOut(500)\n  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build())  \n  .layer(5, new RBM.Builder().nIn(500).nOut(1000)\n  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build()) \n  .layer(6, new RBM.Builder().nIn(1000).nOut(2000)\n  .lossFunction(LossFunctions.LossFunction.RMSE_XENT).build()) \n  .layer(7, new OutputLayer.Builder(LossFunctions.LossFunction.MSE)\n  .activation(\"sigmoid\").nIn(2000).nOut(numRows*numColumns).build()) \n\n```", "```py\n .pretrain(true).backprop(true) \n  .build(); \n\n```", "```py\nMultiLayerNetwork model = new MultiLayerNetwork(conf); \nmodel.init(); \n\nmodel.setListeners(new ScoreIterationListener(listenerFreq)); \n\nlog.info(\"Train model....\"); \nwhile(iter.hasNext())\n  { \n   DataSet next = iter.next(); \n   model.fit(new DataSet(next.getFeatureMatrix(),next\n   .getFeatureMatrix())); \n  } \n\n```", "```py\nint outputNum = 2;\nint inputNum = 1000;\nint iterations = 10;\nint seed = 123;\nint batchSize = 1024;\n\n```", "```py\nlog.info (\"Build model....\");\nMultiLayerConfiguration conf = new NeuralNetConfiguration.Builder ()\n.seed(seed)\n.gradientNormalization(GradientNormalization\n  .ClipElementWiseAbsoluteValue)\n.gradientNormalizationThreshold (1.0)\n.iterations(iterations)\n.updater(Updater.NESTEROVS)\n.momentum(0.5)\n.momentumAfter(Collections.singletonMap(3, 0.9))\n.optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT)\n.list()\n.layer(0, new AutoEncoder.Builder()\n.nIn(inputNum)\n.nOut(500)\n.weightInit(WeightInit.XAVIER)\n.lossFunction(LossFunction.RMSE_XENT)\n\n```", "```py\n.corruptionLevel (0.3)\n    .build())\n  .layer(1, new AutoEncoder.Builder()\n    .nIn(500)\n    .nOut(250)\n    .weightInit(WeightInit.XAVIER).lossFunction\n    (LossFunction.RMSE_XENT)\n    .corruptionLevel(0.3)\n    .build())\n  .layer(2, new AutoEncoder.Builder()\n    .nIn(250)\n    .nOut(125)\n    .weightInit(WeightInit.XAVIER).lossFunction         \n    (LossFunction.RMSE_XENT)\n    .corruptionLevel(0.3)\n    .build())\n  .layer(3, new AutoEncoder.Builder()\n     .nIn(125)\n     .nOut(50)\n     .weightInit(WeightInit.XAVIER).lossFunction\n     (LossFunction.RMSE_XENT)\n     .corruptionLevel(0.3)\n     .build())\n   .layer(4, new OutputLayer.Builder   \n   (LossFunction.NEGATIVELOGLIKELIHOOD)\n     .activation(\"softmax\")\n     .nIn(75)\n     .nOut(outputNum)\n     .build())\n   .pretrain(true)\n.backprop(false)\n.build();\n\n```", "```py\ntry {\n     model.fit(iter);\n    } \ncatch(Exception ex)\n   {\n     ex.printStackTrace();\n   }\n\n```"]