<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Implementing Deep Net Architectures to Recognize Handwritten Digits</h1>
                </header>
            
            <article>
                
<p>In the previous chapters, we have been through the essential concepts and have set up tools that are required for us to get our journey into <strong>Artificial Intelligence</strong> (<strong>AI</strong>) started. We also built a small prediction app to get our feet wet with the tools we will be using.</p>
<p>In this chapter, we are going to cover a more interesting and popular application of AI – Computer Vision, or Machine Vision. We will start by continuing from the previous chapter and ease into building <strong>convolutional neural networks</strong> (<strong>CNN</strong>), the most popular neural network type for Computer Vision. This chapter will also cover the essential concepts that were promised in <a href="1bfa8853-a79e-4b4a-aa9f-254392b158bb.xhtml" target="_blank">Chapter 1</a>, <em>Artificial Intelligence Concepts and Fundamentals</em>, but, in contrast, this chapter will have a very hands-on approach.</p>
<p>We will be covering the following topics in the chapter:</p>
<ul>
<li>Building a feedforward neural network to recognize handwritten digits</li>
<li>Remaining concepts of neural networks</li>
<li>Building a deeper neural network</li>
<li>Introduction to computer vision</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a feedforward neural network to recognize handwritten digits, version one</h1>
                </header>
            
            <article>
                
<p>In this section, we will use the knowledge that we gained from the last two chapters to tackle a problem that has unstructured data – image classification. The idea is to take a dive into solving a Computer Vision task with the current setup and the basics of neural networks that we are familiar with. We have seen that feedforward neural networks can be used for prediction using structured data; let's try that on images to classify handwritten digits.</p>
<p>To solve this task, we are going to leverage the <strong>MNSIT</strong> database and use the handwritten digits dataset. MNSIT stands for <strong>Modified National Institute of Standards and Technology</strong>. It is a large database that's commonly used for training, testing, and benchmarking image-related tasks in Computer Vision.</p>
<p>The MNSIT digits dataset contains 60,000 images of handwritten digits, which are used for training the model, and 10,000 images of handwritten digits, which are used for testing the model.</p>
<p>From here out, we will be using Jupyter Notebook to understand and execute this task. So, please start your Jupyter Notebook and create a new Python Notebook if you have not already done so.</p>
<p>Once you have your notebook ready, the first thing to do, as always, is to import all the necessary modules for the task at hand:</p>
<ol>
<li>Import <kbd>numpy</kbd> and set the <kbd>seed</kbd> for reproducibility:</li>
</ol>
<pre style="padding-left: 60px"><strong>import </strong><strong>numpy </strong><strong>as </strong><strong>np<br/></strong>np.random.seed(42)</pre>
<ol start="2">
<li>Load the Keras dependencies and the built-in MNSIT digits dataset:</li>
</ol>
<pre style="padding-left: 60px"><span class="kn"><strong>import</strong> </span><span class="nn"><strong>keras<br/></strong></span><strong>from</strong> <span class="nn"><strong>keras.datasets</strong> </span><span class="k"><strong>import</strong></span><span> </span><span class="n">mnist</span><span>  <br/></span><span class="kn"><strong>from</strong></span><span class="nn"> <strong>keras.models</strong></span><span class="k"> <strong>import </strong></span><span class="n">Sequential</span>
<span class="kn"><strong>from</strong></span><span class="nn"> <strong>keras.layers</strong> </span><span class="k"><strong>import </strong></span><span class="n">Dense</span>
<span class="kn"><strong>from</strong></span><span class="nn"> <strong>keras.optimizers</strong></span><span class="k"> <strong>import </strong></span><span class="n">SGD</span></pre>
<ol start="3">
<li>Load the data into the training and test sets, respectively:</li>
</ol>
<pre style="padding-left: 60px"><span class="p">(</span><span class="n">X_train</span><span class="p">, </span><span class="n">y_train</span><span class="p">), </span><span class="p">(</span><span class="n">X_test, y_test)= mnist.load_data()</span></pre>
<ol start="4">
<li>Check the number of training images, along with the size of each image. In this case, the size of each image is 28 x 28 pixels:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">X_train</span><span class="o">.</span><span class="n">shape<br/></span>(60000, 28, 28)</pre>
<ol start="5">
<li>Check the dependent variable, in this case, 60,000 cases with the right label:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">y_train</span><span class="o">.</span><span class="n">shape<br/></span>(60000,)</pre>
<ol start="6">
<li>Check the labels for the first 100 training samples:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">y_train</span> <span class="p">[</span><span class="mi">0</span> <span class="p">:</span><span class="mi">99</span><span class="p">]</span>  <br/>array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9,<br/>       1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9,<br/>       8, 5, 9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5, 6, 1, 0,<br/>       0, 1, 7, 1, 6, 3, 0, 2, 1, 1, 7, 9, 0, 2, 6, 7, 8, 3, 9, 0, 4, 6, 7,<br/>       4, 6, 8, 0, 7, 8, 3], dtype=uint8)</pre>
<ol start="7">
<li>Check the number of test images, along with the size of each image. In this case, the size of each image is 28 x 28 pixels:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">X_test</span><span class="o">.</span><span class="n">shape<br/></span>(10000, 28, 28)</pre>
<ol start="8">
<li>Check the samples in the test data, which are basically 2D arrays of size 28 x 28:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">X_test</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,<br/>          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,      <br/>          .<br/>          .<br/>,<br/>          0,   0],<br/>       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 121, 254, 207,<br/>         18,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,<br/>          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,<br/>          0,   0]], dtype=uint8)</pre>
<ol start="9">
<li>Check the dependent variable, in this case, 10,000 cases with the right label:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span>
(10000,)</pre>
<ol start="10">
<li>The right label for the previous first sample in the test set is as follows:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">y_test</span><span class="p">[</span><span class="mi">0</span><span class="p">]<br/></span>7</pre>
<ol start="11">
<li>Now, we need to pre-process the data by converting it from a 28 x 28 2D array into a normalized 1D array of 784 elements:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">X_train</span> <span class="o">= </span><span class="n">X_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">60000</span><span class="p">, </span><span class="mi">784</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">'float32'</span><span class="p">)</span>
<span class="n">X_test </span><span class="o">= </span><span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">10000</span><span class="p">, </span><span class="mi">784</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">'float32'</span><span class="p">)<br/></span><span class="n">X_train</span><span class="o">/=</span><span class="mi">255</span>
<span class="n">X_test</span> <span class="o">/=</span><span class="mi">255</span></pre>
<ol start="12">
<li>Check the first sample of the pre-processed dataset:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">X_test</span><span class="p">[</span><span class="mi">0</span><span class="p">]<br/></span>array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,<br/>        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,<br/>        .<br/>        .<br/>        .<br/>        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,<br/>        0.47450981,  0.99607843,  0.99607843,  0.85882354,  0.15686275,<br/>        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,<br/>        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,<br/>        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,<br/>        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,<br/>        0.        ,  0.        ,  0.        ,  0.47450981,  0.99607843,<br/>        0.81176472,  0.07058824,  0.        ,  0.        ,  0.        ,<br/>        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,<br/>        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,<br/>        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,<br/>        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,<br/>        0.        ,  0.        ,  0.        ,  0.        ], dtype=float32)</pre>
<ol start="13">
<li>The next step is to one-hot code the labels; in other words, we need to convert the data type of the labels (zero to nine) from numeric into categorical:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">n_classes</span><span class="o">=</span><span class="mi">10</span>
<span class="n">y_train</span><span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">y_train</span> <span class="p">,</span><span class="n">n_classes</span><span class="p">)</span>
<span class="n">y_test</span><span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">n_classes</span><span class="p">)</span></pre>
<ol start="14">
<li>View the first sample of the label that has been one-hot coded. In this case, the number was seven:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">y_test</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.])</pre>
<ol start="15">
<li>Now, we need to design our simple feedforward neural network with an input layer using the <kbd>sigmoid</kbd> activation function and 64 neurons. We will add a <kbd>softmax</kbd> function to the output layer, which does the classification by giving probabilities of the classified label:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">model</span><span class="o">=</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">, </span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">784</span><span class="p">,)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">))</span>  </pre>
<ol start="16">
<li>We can look at the structure of the neural network we just designed using the <kbd>summary()</kbd> function, which is a simple network with an input layer of 64 neurons and an output layer with 10 neurons. The output layer has 10 neurons we have 10 class labels to predict/classify (zero to nine):</li>
</ol>
<pre style="padding-left: 60px"><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
_______________________________________________________________<br/>Layer (type)                 Output Shape              Param #<br/>=================================================================<br/>dense_1 (Dense)              (None, 64)                50240     <br/>_______________________________________________________________<br/>dense_2 (Dense)              (None, 10)                650       <br/>=================================================================<br/>Total params: 50,890<br/>Trainable params: 50,890<br/>Non-trainable params: 0<br/>_________________________________________________________________</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="17">
<li>Next, we need to configure the model to use an optimizer, a cost function, and a metric to determine accuracy. Here, the optimizer that's being used is <strong>Scalar Gradient Descent (SGD)</strong> with a learning rate of 0.01. The loss function that's being used is the algebraic <strong>Mean Squared Error</strong> (<strong>MSE</strong>), and the metric to measure the correctness of the model is <kbd>accuracy</kbd>, which is the probability score:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'mean_squared_error'</span><span class="p">, </span><span class="n">optimizer=</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span><span class="n">metrics</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>  </pre>
<ol start="18">
<li>Now, we are ready to train the model. We want it to use 128 samples for every iteration of learning through the network, indicated by <kbd>batch_size</kbd>. We want each sample to iterate at least 200 times throughout the network, which is indicated by <kbd>epochs</kbd>. Also, we indicate the training and validation sets to be used. <kbd>Verbose</kbd> controls the output prints on the console:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,<br/></span><span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">validation_data</span> <span class="o">=</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>  </pre>
<ol start="19">
<li>Train on 60,000 samples, and then validate on 10,000 samples:</li>
</ol>
<pre style="padding-left: 60px">Epoch 1/200<br/>60000/60000 [==============================] - 1s - loss: 0.0915 - acc: 0.0895 - val_loss: 0.0911 - val_acc: 0.0955<br/>Epoch 2/200<br/>.<br/>.<br/>.<br/>60000/60000 [==============================] - 1s - loss: 0.0908 - acc: <br/>0.8579 - val_loss: 0.0274 - val_acc: 0.8649<br/>Epoch 199/200<br/>60000/60000 [==============================] - 1s - loss: 0.0283 - acc: 0.8585 - val_loss: 0.0273 - val_acc: 0.8656<br/>Epoch 200/200<br/>60000/60000 [==============================] - 1s - loss: 0.0282 - acc: 0.8587 - val_loss: 0.0272 - val_acc: 0.8658<br/>&lt;keras.callbacks.History at 0x7f308e68be48&gt;</pre>
<ol start="20">
<li class="mce-root">Finally, we can evaluate the model and how well the model predicts on the test dataset:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)<br/></span>9472/10000 [===========================&gt;..] - ETA: 0s<br/>[0.027176343995332718, 0.86580000000000001]</pre>
<p>This can be interpreted as having an error rate (MSE) of 0.027 and an accuracy of 0.865, which means it predicted the right label 86% of the time on the test dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a feedforward neural network to recognize handwritten digits, version two</h1>
                </header>
            
            <article>
                
<p>In the previous section, we built a very simple neural network with just an input and output layer. This simple neural network gave us an accuracy of 86%. Let's see if we can improve this accuracy further by building a neural network that is a little deeper than the previous version:</p>
<ol>
<li>Let's do this on a new notebook. Loading the dataset and data pre-processing will be the same as in the previous section:</li>
</ol>
<pre style="padding-left: 60px"><span class="kn"><strong>import </strong></span><span class="nn"><strong>numpy</strong> </span><span class="k"><strong>as</strong> </span><span class="nn"><strong>np<br/></strong></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)<br/></span><strong>import </strong><span class="nn"><strong>keras<br/></strong></span><span class="kn"><strong>from</strong> </span><span class="nn"><strong>keras.datasets</strong> </span><span class="k"><strong>import </strong></span><span class="n">mnist <br/></span><span class="kn"><strong>from</strong> </span><span class="nn"><strong>keras.models</strong> </span><span class="k"><strong>import </strong></span><span class="n">Sequential <br/></span><span class="kn"><strong>from</strong> </span><span class="nn"><strong>keras.layers</strong> </span><span class="k"><strong>import </strong></span><span class="n">Dense<br/></span><strong>from</strong> <span class="nn"><strong>keras.optimizers </strong></span><span class="k"><strong>import </strong></span><span class="n">SG<br/>#loading and pre-processing data<br/></span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">), </span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span><span class="o">= </span><span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">X_train</span><span class="o">= </span><span class="n">X_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span> <span class="mi">60000</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span><span class="o">.</span> <span class="n">astype</span><span class="p">(</span><span class="s1">'float32'</span><span class="p">)<br/></span>X_test <span class="o">=</span><span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span><span class="mi">784</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">'float32'</span><span class="p">)</span>
<span class="n">X_train</span><span class="o">/=</span><span class="mi">255</span>
<span class="n">X_test</span><span class="o">/=</span><span class="mi">255</span></pre>
<ol start="2">
<li>The design of the neural network is slightly different from the previous version. We will add a hidden layer with 64 neurons to the network, along with the input and output layers:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">model</span><span class="o">=</span><span class="n">Sequential</span><span class="p">()<br/></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">784</span><span class="p">,)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">))</span>
  </pre>
<ol start="3">
<li>Also, we will use the <kbd>relu</kbd> activation function for the input and hidden layer instead of the <kbd>sigmoid</kbd> function we used previously.</li>
</ol>
<ol start="4">
<li>We can inspect the model design and architecture as follows:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()<br/></span>_______________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>dense_1 (Dense)              (None, 64)                50240     <br/>_______________________________________________________________<br/>dense_2 (Dense)              (None, 64)                4160      <br/>_______________________________________________________________<br/>dense_3 (Dense)              (None, 10)                650       <br/>=================================================================<br/>Total params: 55,050<br/>Trainable params: 55,050<br/>Non-trainable params: 0<br/>_________________________________________________________________</pre>
<ol start="5">
<li>Next, we will configure the model to use the derivative <kbd>categorical_crossentropy</kbd> cost function rather than MSE. Also, the learning rate is increased from 0.01 to 0.1:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span><span class="n">optimizer</span><span class="o">=</span><span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span> <br/><span class="n">metrics</span> <span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span> </pre>
<ol start="6">
<li>Now, we will train the model, like we did in the previous examples:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">validation_data</span> <span class="o">=</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span></pre>
<ol start="7">
<li>Train on 60,000 samples and validate on 10,000 samples:</li>
</ol>
<pre style="padding-left: 60px">Epoch 1/200<br/>60000/60000 [==============================] - 1s - loss: 0.4785 - acc: 0.8642 - val_loss: 0.2507 - val_acc: 0.9255<br/>Epoch 2/200<br/>60000/60000 [==============================] - 1s - loss: 0.2245 - acc: 0.9354 - val_loss: 0.1930 - val_acc: 0.9436<br/>.<br/>.<br/>.<br/>60000/60000 [==============================] - 1s - loss: 4.8932e-04 - acc: 1.0000 - val_loss: 0.1241 - val_acc: 0.9774<br/>&lt;keras.callbacks.History at 0x7f3096adadd8&gt;</pre>
<p>As you can see, there is an increase in accuracy compared to the model we built in the first version.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a deeper neural network</h1>
                </header>
            
            <article>
                
<p>In this section, we will use the concepts we learned about in this chapter to build a deeper neural network to classify handwritten digits:</p>
<ol>
<li>We will start with a new notebook and then load the required dependencies:</li>
</ol>
<pre style="padding-left: 60px"><span class="kn"><strong>import</strong></span> <span class="nn"><strong>numpy</strong></span><span class="k"> <strong>as</strong> </span><span class="nn"><strong>np<br/></strong></span>np<span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="kn"><strong>import</strong></span><span class="nn"> <strong>keras</strong>
</span><span class="kn"><strong>from</strong></span><span class="nn"> <strong>keras.datasets</strong> </span><span class="k"><strong>import </strong></span><span class="n">mnist</span>
<span class="kn"><strong>from</strong> </span><span class="nn"><strong>keras.models</strong> </span><span class="k"><strong>import </strong></span><span class="n">Sequential<br/></span><strong>from </strong><span class="nn"><strong>keras.layers</strong> </span><span class="k"><strong>import </strong></span><span class="n">Dense</span>
<span class="kn"><strong>from</strong> </span><span class="nn"><strong>keras.layers </strong></span><strong>import </strong><span class="n">Dropout</span>
<span class="c1"><em># new!</em>
</span><span class="kn"><strong>from</strong>
</span><span class="nn"><strong>keras.layers.normalization<br/></strong></span><span class="c1"><em># new!</em>
</span><span class="k"><strong>import</strong>
</span><span class="n">BatchNormalization</span>
<span class="c1"><em># new!</em>
</span><span class="kn"><strong>from</strong> </span><span class="nn"><strong>keras</strong> </span><span class="k"><strong>import </strong></span><span class="n">regularizers</span>
<span class="c1"><em># new! </em>
</span><span class="kn"><strong>from</strong> </span><span class="nn"><strong>keras.optimizers</strong>
</span><span class="k"><strong>import </strong></span><span class="n">SGD</span></pre>
<ol start="2">
<li>We will now load and pre-process the data:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">(X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">),</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span><span class="o">= </span><span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">X_train</span><span class="o">= </span><span class="n">X_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">60000</span><span class="p">,</span><span class="mi">784</span><span class="p">)</span><span class="o">.</span>
<span class="n">astype</span><span class="p">(</span><span class="s1">'float32'</span><span class="p">)<br/></span><span class="n">X_test</span><span class="o">= </span><span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span><span class="mi">784</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">'float32'</span><span class="p">)<br/></span>X_train<span class="o">/=</span><span class="mi">255</span>
<span class="n">X_test</span><span class="o">/=</span><span class="mi">255</span>
<span class="n">n_classes</span><span class="o">=</span><span class="mi">10</span>
<span class="n">y_train</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span><span class="n">n_classes</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">n_classes</span><span class="p">)</span>
  </pre>
<ol start="3">
<li>Now, we will design a deeper neural architecture with measures to take care of overfitting and to provide better generalization:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">model</span><span class="o">=</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">,</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">784</span><span class="p">,)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()<br/></span>_______________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>dense_1 (Dense)              (None, 64)                50240     <br/>_______________________________________________________________<br/>batch_normalization_1 (Batch (None, 64)                256       <br/>_______________________________________________________________<br/>dropout_1 (Dropout)          (None, 64)                0         <br/>_______________________________________________________________<br/>dense_2 (Dense)              (None, 64)                4160      <br/>_______________________________________________________________<br/>batch_normalization_2 (Batch (None, 64)                256       <br/>_______________________________________________________________<br/>dropout_2 (Dropout)          (None, 64)                0         <br/>_______________________________________________________________<br/>dense_3 (Dense)              (None, 10)                650       <br/>=================================================================<br/>Total params: 55,562<br/>Trainable params: 55,306<br/>Non-trainable params: 256_______________________________________________________________</pre>
<ol start="4">
<li>This time, we will configure the model using an <kbd>adam</kbd> optimizer:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'categorical_crossentropy'</span><span class="p">,</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span><span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span></pre>
<ol start="5">
<li>Now, we will post that we will train the model for <kbd>200</kbd> epochs at a batch size of <kbd>128</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">, </span><span class="n">y_train</span><span class="p">, </span><span class="n">batch_size</span><span class="o">= </span><span class="mi">128</span><span class="p">, </span><span class="n">epochs</span><span class="o">= </span><span class="mi">200</span><span class="p">, </span><span class="n">verbose</span><span class="o">= </span><span class="mi">1</span><span class="p">, </span><span class="n">validation_data</span><span class="o">= </span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span></pre>
<ol start="6">
<li>Train on 60,000 samples and validate on 10,000 samples:</li>
</ol>
<pre style="padding-left: 60px">Epoch 1/200<br/>60000/60000 [==============================] - 3s - loss: 0.8586 - acc: 0.7308 - val_loss: 0.2594 - val_acc: 0.9230<br/>Epoch 2/200<br/>60000/60000 [==============================] - 2s - loss: 0.4370 - acc: 0.8721 - val_loss: 0.2086 - val_acc: 0.9363<br/>.<br/>.<br/>.<br/>Epoch 200/200<br/>60000/60000 [==============================] - 2s - loss: 0.1323 - acc: 0.9589 - val_loss: 0.1136 - val_acc: 0.9690<br/>&lt;keras.callbacks.History at 0x7f321175a748&gt;</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to Computer Vision</h1>
                </header>
            
            <article>
                
<p>Computer Vision can be defined as the subset of AI where we can teach a computer to <em>see</em>. We cannot just add a camera to a machine in order for it to <em>see</em>. For a machine to actually view the world like people or animals do, it relies on Computer Vision and image recognition techniques. Reading barcodes and face recognition are examples of Computer Vision. Computer Vision can be described as that part of the human brain that processes the information that's perceived by the eyes, nothing else.</p>
<p>Image recognition is one of the interesting uses of Computer Vision from an AI standpoint. The input that is received through Computer Vision on the machine is interpreted by the image recognition system, and based on what it sees, the output is classified.</p>
<p>In other words, we use our eyes to capture the objects around us, and those objects/images are processed in our brain, which allows us to visualize the world around us. This capability is given by Computer Vision to machines. Computer Vision is responsible for automatically extracting, analyzing, and understanding the required information from the videos or images that are fed in as input.</p>
<p>There are various Computer Vision application, and they are used in the following scenerios:</p>
<ul>
<li>Augmented reality</li>
<li>Robotics</li>
<li>Biometrics</li>
<li>Pollution monitoring</li>
<li>Agriculture</li>
<li>Medical image analysis</li>
<li>Forensics</li>
<li>Geoscience</li>
<li>Autonomous vehicles</li>
<li>Image restoration</li>
<li>Process control</li>
<li>Character recognition</li>
<li>Remote sensing</li>
<li>Gesture analysis</li>
<li>Security and surveillance</li>
<li><span>Face recognition</span></li>
<li>Transport</li>
<li>Retail</li>
<li>Industrial quality inspection</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Machine learning for Computer Vision</h1>
                </header>
            
            <article>
                
<p>It's important to use the appropriate ML theories and tools, which will be very helpful when we need to develop various applications that involve classifying images, detecting objects, and so on. Utilizing  <span>these theories to create computer vision applications requires an understanding of some basic machine learning concepts.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Conferences help on Computer Vision</h1>
                </header>
            
            <article>
                
<p>Some of the conferences to look for latest research and applications are as follows:</p>
<ul>
<li class="mce-root"><strong>Conference on Computer Vision and Pattern Recognition</strong> (<strong>CVPR</strong>)<span> </span>is held every year and is one of the popular conferences with research papers ranging from both theory and application across a wide domain</li>
<li class="mce-root"><strong>International Conference on Computer Vision </strong><span>(<strong>ICCV</strong>)</span>is another major conference held every other year attracting one of the best research papers</li>
<li class="mce-root"><strong>Special Interest Group on Computer Graphics</strong> (<strong>SIGGRAPH</strong>)<span> </span>and interactive techniques though more on computer graphics domain has several applications papers that utilizes computer vision techniques.</li>
</ul>
<p class="mce-root">Other notable conferences include<span> </span><span><strong>Neural Information Processing Systems</strong> (</span><strong>NIPS</strong>), <strong>International Conference on Machine Learning</strong><span> </span>(<strong>ICML</strong>), <strong>Asian Conference on Computer Vision</strong> (<strong>ACCV</strong>), <strong>European Conference on Computer Vision</strong> (<strong>ECCV</strong>), and so on.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we built a feedforward neural network to recognize handwritten digits in two versions. Then, we built <span>a neural network to classify handwritten digits, and, finally we gave a short introduction to Computer Vision.</span></p>
<p>In the next chapter, we will build a Machine Vision mobile app to classify flower species and retrieve the necessary information.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>For in-depth knowledge on computer vision, do refer the following Packt books:</p>
<ul>
<li><em>Deep Learning for Computer Vision</em> by Rajalingappaa Shanmugamani</li>
<li><em>Practical Computer Vision</em> by Abhinav Dadhich</li>
</ul>


            </article>

            
        </section>
    </body></html>