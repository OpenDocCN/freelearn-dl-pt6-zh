<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Learning Process in Neural Networks</h1>
                
            
            <article>
                
<p class="calibre2">Just as there are many different types of learning and approaches to human learning, so we can say about the machines as well. To ensure that a machine will be able to learn from experience, it is important to define the best available methodologies depending on the specific job requirements. This often means choosing techniques that work for the present case and evaluating them from time to time, to determine if we need to try something new.</p>
<p class="calibre2"> We have seen the basics of neural networks in <a href="part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4" class="calibre4">Chapter 1</a>, <em class="calibre14">Neural Network and Artificial Intelligence Concepts</em>, and also two simple implementations using R. In this chapter, we will deal with the learning process, that is how to train, test, and deploy a neural network machine learning model. The training phase is used for learning, to fit the parameters of the neural networks. The testing phase is used to assess the performance of fully-trained neural networks. Finally, in <span>the deployment phase, actual data is passed through the model to get the prediction.</span></p>
<p class="calibre2">The following is the list of concepts covered in this chapter:</p>
<ul class="calibre16">
<li class="calibre17">Learning process</li>
<li class="calibre17">Supervised learning</li>
<li class="calibre17">Unsupervised learning</li>
<li class="calibre17">Training, testing, and deploying a model</li>
<li class="calibre17">Evaluation metrics-error measurement and fine tuning; measuring accuracy of a model</li>
<li class="calibre17">Supervised learning model using neural networks</li>
<li class="calibre17">Unsupervised learning model using neural networks</li>
<li class="calibre17">Backpropagation</li>
</ul>
<p class="calibre2"><span>By</span> the end of the chapter, we will understand the basic concepts of the learning process and how to implement it in the R environment. We will discover different types of algorithms to implement a neural network. We will learn how to train, test, and deploy a model. We will know how to perform a correct valuation procedure.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">What is machine learning?</h1>
                
            
            <article>
                
<p class="calibre2">What do we mean by the term machine learning? The definition is quite difficult, to do so, we are asking large field of scientists to help. We can mention an artificial intelligence pioneer's quote:</p>
<div class="packt_quote"><em class="calibre14">"Field of study that gives computers the ability to learn without being explicitly programmed." <br class="title-page-name"/></em></div>
<p class="calibre42"><span> </span><span>– </span><span>Arthur Samuel</span></p>
<p class="calibre2">Machine learning is about training a model or an algorithm with data and then using the model to predict any new data. For example, a toddler is taught how to walk from his crawling phase. Initially, the toddler's parents hold the toddler's hand to help him up, and he is taught through the data that is given. On the basis of these procedures, if an obstacle presents itself in the toddler's path or if there is a turn somewhere, the toddler is able to navigate on his own after the training. The data used for training is the training data and the recipient continues to learn even after the formal training.</p>
<p class="calibre2">Machines too can be taught like toddlers to do a task based on training. First, we feed enough data to tell the machine what needs to be done on what circumstances. After the training, the machine can perform automatically and can also learn to fine-tune itself. This type of training the machine is called <strong class="calibre1">machine learning</strong>.</p>
<p class="calibre2">The main difference between machine learning and programming is that there is no coding/programming involved in machine learning, while programming is about giving the machine a set of instructions to perform. In machine learning, the data is the only input provided and the model is based on the algorithm we have decided to use.</p>
<p class="calibre2">The algorithm to be used is based on various factors of the data: the features (or independent variables), the type of dependent variable(s), the accuracy of the model, and the speed of training and prediction of the model.</p>
<p class="calibre2">Based on the independent variable(s) of the machine learning data, there are three different ways to train a model:</p>
<ul class="calibre16">
<li class="calibre17">Supervised learning</li>
<li class="calibre17">Unsupervised learning</li>
<li class="calibre17">Reinforcement learning</li>
</ul>
<p class="calibre2">The following figure shows the <span>different algorithms to train a machine learning model:</span></p>
<div class="cdpaligncenter"><img class="alignnone3" src="../images/00035.gif"/></div>
<p class="calibre2">In the following sections, we will go through them on by one.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Supervised learning</h1>
                
            
            <article>
                
<p class="calibre2"><strong class="calibre1">Supervised learning</strong> is a learning method where there is a part of the training data which acts as a teacher to the algorithm to determine the model. The machine is taught what to learn from the target data. The target data, or dependent or response variables, are the outcome of the collective action of the independent variables. The network training is done with the target data and its behavior with patterns of input data. The target labels are known in advance and the data is fed to the algorithm to derive the model.</p>
<p class="calibre2">Most of neural network usage is done using supervised learning. The weights and biases are adjusted based on the output values. The output can be categorical (like true/false or 0/1/2) or continuous (like 1,2,3, and so on). The model is dependent on the type of output variables, and in the case of neural networks, the output layer is built on the type of target variable.</p>
<div class="packt_tip">For neural networks, all the independent and dependent variables need to be numeric, as a neural network is based on mathematical models. It is up to the data scientist to convert the data to numbers to be fed into the model.</div>
<p class="calibre2">Supervised learning is depicted by the following diagram:</p>
<div class="cdpaligncenter"><img class="alignnone4" src="../images/00036.gif"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Unsupervised learning</h1>
                
            
            <article>
                
<p class="calibre2">In unsupervised learning (or self organization), the output layer is trained to organize the input data into another set of data without the need of a target variable. The input data is analyzed and patterns are found in it to derive the output, as shown in the following figure. Since there is no teacher (or target variable), this type of learning is called <strong class="calibre1">unsupervised learning</strong>.</p>
<div class="cdpaligncenter"><img class="alignnone5" src="../images/00037.gif"/></div>
<p class="calibre2">The different techniques available for unsupervised learning are as follows:</p>
<ul class="calibre16">
<li class="calibre17">Clustering (K-means, hierarchical)</li>
<li class="calibre17">Association techniques</li>
<li class="calibre17">Dimensionality reduction</li>
<li class="calibre17"><strong class="calibre1">Self Organizing Map </strong>(<strong class="calibre1">SOM</strong>)/ Kohonen networks</li>
</ul>
<p class="calibre2">To summarize, the two main types of machine learning are depicted in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border10" src="../images/00038.jpeg"/></div>
<p class="calibre2">For neural networks, we have both the types available, using different ways available in R.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Reinforcement learning</h1>
                
            
            <article>
                
<p class="calibre2">Reinforcement learning is a type of machine learning where there is constant feedback given to the model to adapt to the environment. There is a performance evaluation at each step to improve the model. For neural networks, there is a special type called <strong class="calibre1">Q-learning</strong>, combined with neuron to implement reinforcement learning in the backpropagation feedback mechanism. The details are out of scope of this book.</p>
<p class="calibre2">The following are the three types of learnings we have covered so far:</p>
<div class="cdpaligncenter"><img class="alignnone6" src="../images/00039.gif"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Training and testing the model</h1>
                
            
            <article>
                
<p class="calibre2">Training and testing the model forms the basis for further usage of the model for prediction in predictive analytics. Given a dataset of <em class="calibre14">100</em> rows of data, which includes the predictor and response variables, we split the dataset into a convenient ratio (say <em class="calibre14">70:30</em>) and allocate <em class="calibre14">70</em> rows for training and <em class="calibre14">30</em> rows for testing. The rows are selected in random to reduce bias.</p>
<p class="calibre2">Once the training data is available, the data is fed to the neural network to get the massive universal function in place. The training data determines the weights, biases, and activation functions to be used to get to output from input. Until recently, we could not say that a weight has a positive or a negative influence on the target variable. But now we've been able to shed some light inside the black box. For example, by plotting a trained neural network, we can discover trained synaptic weights and basic information about the training process.</p>
<p class="calibre2">Once the sufficient convergence is achieved, the model is stored in memory and the next step is testing the model. We pass the <em class="calibre14">30</em> rows of data to check if the actual output matches with the predicted output from the model. The evaluation is used to get various metrics which can validate the model. If the accuracy is too wary, the model has to be re-built with change in the training data and other parameters passed to the neural net function. We will cover more about the evaluation metrics later in this chapter.</p>
<p class="calibre2">After training and testing, the model is said to be deployed, where actual data is passed through the model to get the prediction. For example, the use case may be determining a fraud transaction or a home loan eligibility check based on various input parameters.</p>
<p class="calibre2">The training, testing, and deployment is represented in the following figure:</p>
<div class="cdpaligncenter"><img class="alignnone7" src="../images/00040.gif"/></div>
<p class="calibre2">So far, we have focused on the various algorithms available; it is now time to dedicate ourselves to the data that represents the essential element of each analysis.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">The data cycle</h1>
                
            
            <article>
                
<p class="mce-root">The data forms a key component for model building and the learning process. The data needs to be collected, cleaned, converted, and then fed to the model for learning. The overall data life cycle is shown as follows:</p>
<div class="cdpaligncenter"><img class="alignnone8" src="../images/00041.gif"/></div>
<p class="calibre2">One of the critical requirements for modeling is having good and balanced data. This helps in higher accuracy models and better usage of the available algorithms. A data scientist's time is mostly spent on cleansing the data before building the model.</p>
<p class="calibre2">We have seen the training and testing before deployment of the model. For testing, the results are captured as evaluation metrics, which helps us decide if we should use a particular model or change it instead.</p>
<p class="calibre2">We will see the evaluation metrics next.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Evaluation metrics</h1>
                
            
            <article>
                
<p class="calibre2">Evaluating a model involves checking if the predicted value is equal to the actual value during the testing phase. There are various metrics available to check the model, and they depend on the state of the target variable.</p>
<p class="calibre2">For a binary classification problem, the predicted target variable and the actual target variable can be in any of the following four states:</p>
<table class="calibre43">
<tbody class="calibre6">
<tr class="calibre7">
<td class="calibre8"><strong class="calibre1">Predicted</strong></td>
<td class="calibre8"><strong class="calibre1">Actual</strong></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">Predicted = TRUE</em></td>
<td class="calibre8"><em class="calibre14">Actual = TRUE</em></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">Predicted = TRUE</em></td>
<td class="calibre8"><em class="calibre14">Actual = FALSE</em></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">Predicted = FALSE</em></td>
<td class="calibre8"><em class="calibre14">Actual = TRUE</em></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">Predicted = FALSE</em></td>
<td class="calibre8"><em class="calibre14">Actual = FALSE</em></td>
</tr>
</tbody>
</table>
<p class="calibre2"> </p>
<p class="calibre2">When we have the predicted and actual values as same values, we are said to be accurate. If all predicted and actual values are same (either all <em class="calibre14">TRUE</em> or all <em class="calibre14">FALSE</em>), the model is <em class="calibre14">100</em> percent accurate. But, this is never the case.</p>
<p class="calibre2">Since neural networks are approximation models, there is always a bit of error possible. All the four states mentioned in the previous table are possible.</p>
<p class="calibre2">We define the following terminology and metrics for a model:</p>
<ul class="calibre16">
<li class="calibre17"><strong class="calibre1">True Positives</strong> (<strong class="calibre1">TP</strong>):<strong class="calibre1"> </strong>All cases where the predicted and actual are both <em class="calibre14">TRUE</em> (good accuracy).</li>
<li class="calibre17"><strong class="calibre1">True Negative</strong> (<strong class="calibre1">TN</strong>): All cases when predicted is <em class="calibre14">FALSE</em> and the actual is also <em class="calibre14">FALSE</em> (good accuracy).</li>
<li class="calibre17"><strong class="calibre1">False Positive </strong>(<strong class="calibre1">FP</strong>):<strong class="calibre1"> </strong>This is a case when we predict something as positive (<em class="calibre14">TRUE</em>), but it is actually negative. It is like a false alarm or an FP error. An example is when a male is predicted to be pregnant by a pregnancy test kit. All cases when predicted is <em class="calibre14">TRUE</em>, while the actual is <em class="calibre14">FALSE</em>. This is also called <strong class="calibre1">type 1 error</strong>.</li>
<li class="calibre17"><strong class="calibre1">False Negative</strong> (<strong class="calibre1">FN</strong>):<strong class="calibre1"> </strong>When we predict something as <em class="calibre14">FALSE</em>, but in actuality it is <em class="calibre14">TRUE</em>, then the case is called FN. For example, when a pregnant female is predicted as not being pregnant by a pregnancy test kit, it is an FN case. All cases when predicted is <em class="calibre14">FALSE</em> and actual <em class="calibre14">TRUE</em>. This is also called <strong class="calibre1">type 2 error</strong>.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Confusion matrix</h1>
                
            
            <article>
                
<p class="calibre2">When the values of the classification are plotted in a <em class="calibre14">nxn</em> matrix (<em class="calibre14">2x2</em> in case of binary classification), the matrix is called the <strong class="calibre1">confusion matrix</strong>. All the evaluation metrics can be derived from the confusion matrix itself:</p>
<table class="calibre44">
<tbody class="calibre6">
<tr class="calibre7">
<td class="calibre8"/>
<td class="calibre8"><strong class="calibre1">Predicted value</strong></td>
<td class="calibre8"><strong class="calibre1">Predicted value</strong></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">Actual values</em></td>
<td class="calibre8"><em class="calibre14">TRUE</em></td>
<td class="calibre8"><em class="calibre14">FALSE</em></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">TRUE</em></td>
<td class="calibre8"><em class="calibre14">TP</em></td>
<td class="calibre8"><em class="calibre14">FN</em></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><em class="calibre14">FALSE</em></td>
<td class="calibre8"><em class="calibre14">FP</em></td>
<td class="calibre8"><em class="calibre14">TN</em></td>
</tr>
</tbody>
</table>
<p class="calibre2"> </p>
<p class="calibre2">Now, let's look at some evaluation metrics in detail.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">True Positive Rate</h1>
                
            
            <article>
                
<p class="calibre2"><strong class="calibre1">True Positive Rate</strong> (<strong class="calibre1">TPR</strong>) or sensitivity or recall or hit rate is a measure of how many true positives were identified out of all the positives identified:</p>
<div class="calibre26"><img src="../images/00042.jpeg" class="calibre45"/></div>
<p class="calibre2"> </p>
<p class="calibre2">Ideally, the model is better if we have this closer to one.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">True Negative Rate</h1>
                
            
            <article>
                
<p class="calibre2"><strong class="calibre1">True Negative Rate</strong> (<strong class="calibre1">TNR</strong>) or specificity is the ratio of true negatives and total number of negatives we have predicted:</p>
<div class="calibre26"><img src="../images/00043.jpeg" class="calibre46"/></div>
<p class="calibre2"> </p>
<p class="calibre2">If this ratio is closer to zero, the model is more accurate.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Accuracy</h1>
                
            
            <article>
                
<p class="calibre2">Accuracy is the measure of how good our model is. It is expected to be closer to 1, if our model is performing well.</p>
<p class="calibre2">Accuracy is the ratio of correct predictions and all the total predictions:</p>
<div class="calibre47"><img src="../images/00044.jpeg" class="calibre48"/></div>
<p class="calibre2"> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Precision and recall</h1>
                
            
            <article>
                
<p class="calibre2">Precision and recall are again ratios between the <em class="calibre14">TP</em> with (<em class="calibre14">TP+FP</em>) and <em class="calibre14">TP</em> with (<em class="calibre14">TP+FN</em>) respectively. These ratios determine how relevant our predictions are compared to the actual.</p>
<p class="calibre2">Precision is defined as how many selected items are relevant. That is, how many of the predicted ones are actually correctly predicted.</p>
<p class="calibre2">The equation is:</p>
<div class="calibre26"><img src="../images/00045.gif" class="calibre49"/></div>
<p class="calibre2"> </p>
<p class="calibre2">If precision is closer to one, we are more accurate in our predictions.</p>
<p class="calibre2">Recall, on the other hand, tells how many relevant items we selected. Mathematically, it is:</p>
<div class="mce-root1"> <img src="../images/00046.gif" class="calibre50"/>
<p class="calibre2"> </p>
</div>
<p class="calibre2">The following diagram depicts clearly the discussion we have done so far:</p>
<div class="cdpaligncenter"><img class="alignnone9" src="../images/00047.jpeg"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">F-score</h1>
                
            
            <article>
                
<p class="calibre2">F-score, or F1-score, is another measure of accuracy. Technically, it is the harmonic mean of precision and recall:</p>
<p class="calibre51"><img src="../images/00048.jpeg" class="calibre52"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Receiver Operating Characteristic curve</h1>
                
            
            <article>
                
<p class="calibre2">A <strong class="calibre1">Receiver Operating Characteristic </strong>(<strong class="calibre1">ROC</strong>) curve is a graphical visual that illustrates the predictive ability of a binary classifier system. The ROC curve is created by plotting a graph of the TPR against the <strong class="calibre1">False Positive Rate</strong> (<strong class="calibre1">FPR</strong>) at various threshold settings. This gives us <strong class="calibre1">Sensitivity</strong> versus (<strong class="calibre1">1 - Specificity</strong>). A ROC curve typically looks like this: </p>
<div class="cdpaligncenter"><img class="alignnone10" src="../images/00049.gif"/></div>
<p class="calibre2">After acquiring the necessary skills, we are ready to analyze in detail the algorithms used for building the neural networks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Learning in neural networks</h1>
                
            
            <article>
                
<p class="calibre2">As we saw in <a target="_blank" href="part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4" class="calibre4">Chapter 1</a>, <em class="calibre14">Neural Network and Artificial Intelligence Concepts</em>, neural networks is a machine learning algorithm that has the ability to learn from data and give us predictions using the model built. It is a universal function approximation, that is, any input, output data can be approximated to a mathematical function. </p>
<p class="calibre2">The forward propagation gives us an initial mathematical function to arrive at output(s) based on inputs by choosing random weights. The difference between the actual and predicted is called the error term. The learning process in a feed-forward neural network actually happens during the backpropagation stage. The model is fine tuned with the weights by reducing the error term in each iteration. Gradient descent is used in the backpropagation process.</p>
<p class="calibre2">Let us cover the backpropagation in detail in this chapter, as it is an important machine learning aspect for neural networks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Back to backpropagation</h1>
                
            
            <article>
                
<p class="calibre2">We have covered the forward propagation in detail in <a target="_blank" href="part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4" class="calibre4">Chapter 1</a>, <em class="calibre14">Neural Network and Artificial Intelligence Concepts</em>, and a little about backpropagation using gradient descent. Backpropagation is one of the important concepts for understanding neural networks and it relies on calculus to update the weights and biases in each layer. Backpropagation of errors is similar to <em class="calibre14">learning from mistakes</em>. We correct ourselves in our mistakes (errors) in every iteration, until we reach a point called <strong class="calibre1">convergence</strong><em class="calibre14">. </em>The goal of backpropagation is to correct the weights in each layer and minimize the overall error at the output layer.</p>
<p class="calibre2">Neural network learning heavily relies on backpropagation in feed-forward networks. The usual steps of forward propagation and error correction are explained as follows:</p>
<ol class="calibre19">
<li value="1" class="calibre17"><span>Start the neural network forward propagation by assigning random weights and biases to each of the neurons in the hidden layer.</span></li>
<li value="2" class="calibre17">Get the sum of <em class="calibre14">sum(weight*input) + bias</em> at each neuron.</li>
<li value="3" class="calibre17">Apply the activation function (<em class="calibre14">sigmoid</em>) at each neuron.</li>
<li value="4" class="calibre17">Take this output and pass it onto the next layer neuron.</li>
<li value="5" class="calibre17">If the layer is the output layer, apply the weights and get the <span>sum of <em class="calibre14">sum(weight*input) + bias</em> at each output layer neuron.</span></li>
<li value="6" class="calibre17">Again, apply the activation function at the output layer neuron.</li>
<li value="7" class="calibre17">This forms the output of the neural network at the output layer for one forward pass.</li>
<li value="8" class="calibre17">Now, with the training data, we can identify the error term at each output neuron, by subtracting the actual output and the activation function output value.</li>
</ol>
<ol start="9" class="calibre19">
<li value="9" class="calibre17">The total of the errors is arrived at by using the following formula:</li>
</ol>
<div class="calibre28"><img src="../images/00050.jpeg" class="calibre53"/></div>
<p class="calibre2"> </p>
<p class="calibre54">A factor of <em class="calibre14">1/2</em> is used to cancel the exponent when the error function <em class="calibre14">E</em> is subsequently differentiated.</p>
<ol start="10" class="calibre19">
<li value="10" class="calibre17">The gradient descent technique requires calculation of the partial derivative of the error term (<em class="calibre14">E</em>) with respect to the weights of the network. Calculating the partial derivative of the full error with respect to the weight <em class="calibre14">w<sub class="calibre25">ij</sub> </em>is done using the <strong class="calibre1">chain rule</strong><em class="calibre14"> </em>of differentiation:</li>
</ol>
<div class="calibre26"><img src="../images/00051.jpeg" class="calibre55"/></div>
<p class="calibre2"> </p>
<p class="cdpalignleft">The derivative is defined as the rate of change of a value, the gradient descent uses the derivative (or slope) to minimize the error term and arrive at a correct set of weights.</p>
<ol start="11" class="calibre19">
<li value="11" class="calibre17">The first factor is partial derivative of the error term with respect to the output at that particular neuron <em class="calibre14">j</em> and <em class="calibre14">o<sub class="calibre25">j</sub></em> is equal to <em class="calibre14">y</em>:</li>
</ol>
<div class="mce-root1"><img src="../images/00052.jpeg" class="calibre56"/></div>
<ol start="12" class="calibre19">
<li value="12" class="calibre17"> The second factor in the chain rule is t<span>he partial derivative of the output of neuron <em class="calibre14">o<sub class="calibre25">j </sub></em></span> <span>with respect to its input, and is the partial derivative of the activation function (the <em class="calibre14">sigmoid</em> function):</span></li>
</ol>
<div class="calibre47"><img src="../images/00053.jpeg" class="calibre57"/></div>
<p class="calibre2"> </p>
<p class="calibre54">Here <em class="calibre14">net<sub class="calibre25">j </sub></em>is the input to the neuron.</p>
<ol start="13" class="calibre19">
<li value="13" class="calibre17">The third term in the chain rule is simply <em class="calibre14">o<sub class="calibre25">i</sub></em>.</li>
</ol>
<div class="calibre47"><img src="../images/00054.jpeg" class="calibre58"/></div>
<p class="calibre2"> </p>
<ol start="14" class="calibre19">
<li value="14" class="calibre17">Combining steps 11, 12, and 13, we get:</li>
</ol>
<div class="mce-root3"><img src="../images/00055.jpeg" class="calibre59"/></div>
<p class="calibre2"/>
<ol start="15" class="calibre19">
<li value="15" class="calibre17">The weight <em class="calibre14">w<sub class="calibre25">ij</sub></em> at each neuron (any layer) is updated with this partial derivative, combined with the learning rate. </li>
</ol>
<p class="calibre2">These steps are repeated until we have convergence of very low error term or a specified number of times.</p>
<p class="calibre2">All the steps are taken care of internally in the R packages available. We can supply the learning rate along with various other parameters. </p>
<p class="calibre2">The backpropagation is illustrated as follows: </p>
<div class="cdpaligncenter"><img src="../images/00056.jpeg" class="calibre60"/></div>
<p class="calibre2">As with all things in life, even an algorithm has further improvement margins. In the next section, we'll see how to do it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Neural network learning algorithm optimization</h1>
                
            
            <article>
                
<p class="calibre2"><span>The procedure used to carry out the learning process in a neural network is called the training algorithm. </span>The learning algorithm is what the machine learning algorithm chooses as model with the best optimization. The aim is to minimize the loss function and provide more accuracy. Here we illustrate some of the optimization techniques, other than gradient descent.</p>
<p class="calibre2">The <strong class="calibre1">Particle Swarm Optimization</strong> (<strong class="calibre1">PSO</strong>) method is inspired by observations of social and collective behavior on the movements of bird flocks in search of food or survival. It is similar to a fish school trying to move together. We know the position and velocity of the particles, and PSO aims at searching a solution set in a large space controlled by mathematical equations on position and velocity. It is bio-inspired from biological organism behavior for collective intelligence.</p>
<p class="calibre2"><strong class="calibre1">Simulated annealing</strong> is a method that works on a probabilistic approach to approximate the global optimum for the cost function. The method searches for a solution in large space with simulation.</p>
<p class="calibre2">Evolutionary methods are derived from the evolutionary process in biology, and<br class="title-page-name"/>
evolution can be in terms of reproduction, mutation, selection, and recombination.<br class="title-page-name"/>
A fitness function is used to determine the performance of a model, and based on this<br class="title-page-name"/>
function, we select our final model.</p>
<p class="calibre2">The <strong class="calibre1">Expectation Maximization </strong>(<strong class="calibre1">EM</strong>) method<strong class="calibre1"> </strong>is a statistical learning method that uses an iterative method to find maximum likelihood or maximum posterior estimate, thus minimizing the error.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Supervised learning in neural networks</h1>
                
            
            <article>
                
<p class="calibre2">As previously mentioned, supervised learning is a learning method where there is a part of training data which acts as a teacher to the algorithm to determine the model. In the following section, an example of a regression predictive modeling problem is proposed to understand how to solve it with neural networks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Boston dataset</h1>
                
            
            <article>
                
<p class="calibre2"><span>The dataset describes 13 numerical properties of houses in Boston suburbs, and is concerned with modeling the price of houses in those suburbs in thousands of dollars. As such, this is a regression predictive modeling problem. Input attributes include things like crime rate, proportion of non-retail business acres, chemical concentrations, and more. In the following list are shown all the variables followed by a brief description: </span></p>
<ul class="calibre16">
<li class="calibre17">Number of instances: <em class="calibre14">506</em></li>
<li class="calibre17">Number of attributes: <em class="calibre14">13</em> continuous attributes (including <kbd class="calibre13">class</kbd><br class="title-page-name"/>
attribute <kbd class="calibre13">MEDV</kbd>), and one binary-valued attribute</li>
</ul>
<p class="calibre2">Each of the attributes is detailed as follows:</p>
<ol class="calibre19">
<li class="calibre17" value="1"><kbd class="calibre13">crim</kbd> per capita crime rate by town.</li>
<li class="calibre17" value="2"><kbd class="calibre13"><span><span>zn</span></span></kbd> proportion of residential land zoned for lots over <em class="calibre14">25,000</em> square feet.</li>
<li class="calibre17" value="3"><kbd class="calibre13">indus</kbd> proportion of non-retail business acres per town.</li>
<li class="calibre17" value="4"><kbd class="calibre13">chas</kbd> Charles River dummy variable (<em class="calibre14">= 1</em> if tract bounds river; <em class="calibre14">0</em> otherwise).</li>
</ol>
<ol start="5" class="calibre19">
<li class="calibre17" value="5"><kbd class="calibre13">nox</kbd> nitric oxides concentration (parts per <em class="calibre14">10</em> million).</li>
<li class="calibre17" value="6"><kbd class="calibre13">rm</kbd> average number of rooms per dwelling.</li>
<li class="calibre17" value="7"><kbd class="calibre13">age</kbd> proportion of owner-occupied units built prior to <em class="calibre14">1940</em>.</li>
<li class="calibre17" value="8"><kbd class="calibre13">dis</kbd> weighted distances to five Boston employment centres</li>
<li class="calibre17" value="9"><kbd class="calibre13">rad</kbd> index of accessibility to radial highways.</li>
<li class="calibre17" value="10"><kbd class="calibre13">tax</kbd> full-value property-tax rate per <em class="calibre14">$10,000</em>.</li>
<li class="calibre17" value="11"><kbd class="calibre13">ptratio</kbd> pupil-teacher ratio by town.</li>
<li class="calibre17" value="12"><kbd class="calibre13">black</kbd> <em class="calibre14">1000(Bk - 0.63)^2</em> where <em class="calibre14">Bk</em> is the proportion of blacks by town.</li>
<li class="calibre17" value="13"><kbd class="calibre13">lstat</kbd> percent lower status of the population.</li>
<li class="calibre17" value="14"><kbd class="calibre13">medv</kbd> median value of owner-occupied homes in <em class="calibre14">$1000's</em>.</li>
</ol>
<p class="calibre2">Of these, <kbd class="calibre13">medv</kbd> is the response variable, while the other thirteen variables are possible predictors. The goal of this analysis is to fit a regression model that best explains the variation in <kbd class="calibre13">medv</kbd>.</p>
<p class="calibre2">There is a relation between the first thirteen columns and the <kbd class="calibre13">medv</kbd> response variable. We can predict the <kbd class="calibre13">medv</kbd> value based on the input thirteen columns.</p>
<div class="packt_infobox">This dataset is already provided with R libraries (<kbd class="calibre61">MASS</kbd>), as we will see later, so we do not have to worry about retrieving the data.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Neural network regression with the Boston dataset</h1>
                
            
            <article>
                
<p class="calibre2">In this section, we will run a regression neural network for the <kbd class="calibre13">Boston</kbd> dataset. The <kbd class="calibre13">medv</kbd> value is predicted for the test data. The train to test split is <em class="calibre14">70:30</em>. The <kbd class="calibre13">neuralnet</kbd><em class="calibre14"> </em>function is used to model the data with a neural network:</p>
<pre class="calibre24"><strong class="calibre1">#####################################################################</strong><br class="title-page-name"/><strong class="calibre1">###Chapter 2 - Introduction to Neural Networks - using R ############</strong><br class="title-page-name"/><strong class="calibre1">###Simple R program to build, train, test regression neural networks#</strong><br class="title-page-name"/><strong class="calibre1">#########################flename: Boston.r###########################</strong><br class="title-page-name"/><strong class="calibre1">#####################################################################</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">library("neuralnet")</strong><br class="title-page-name"/><strong class="calibre1">library(MASS)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">set.seed(1)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">data = Boston</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">max_data &lt;- apply(data, 2, max) </strong><br class="title-page-name"/><strong class="calibre1">min_data &lt;- apply(data, 2, min)</strong><br class="title-page-name"/><strong class="calibre1">data_scaled &lt;- scale(data,center = min_data, scale = max_data - min_data) </strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">index = sample(1:nrow(data),round(0.70*nrow(data)))</strong><br class="title-page-name"/><strong class="calibre1">train_data &lt;- as.data.frame(data_scaled[index,])</strong><br class="title-page-name"/><strong class="calibre1">test_data &lt;- as.data.frame(data_scaled[-index,])</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">n = names(data)</strong><br class="title-page-name"/><strong class="calibre1">f = as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))</strong><br class="title-page-name"/><strong class="calibre1">net_data = neuralnet(f,data=train_data,hidden=10,linear.output=T)</strong><br class="title-page-name"/><strong class="calibre1">plot(net_data)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">predict_net_test &lt;- compute(net_data,test_data[,1:13])</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">predict_net_test_start &lt;- predict_net_test$net.result*(max(data$medv)-min(data$medv))+min(data$medv)</strong><br class="title-page-name"/><strong class="calibre1">test_start &lt;- as.data.frame((test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv))</strong><br class="title-page-name"/><strong class="calibre1">MSE.net_data &lt;- sum((test_start - predict_net_test_start)^2)/nrow(test_start)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">Regression_Model &lt;- lm(medv~., data=data)</strong><br class="title-page-name"/><strong class="calibre1">summary(Regression_Model)</strong><br class="title-page-name"/><strong class="calibre1">test &lt;- data[-index,]</strong><br class="title-page-name"/><strong class="calibre1">predict_lm &lt;- predict(Regression_Model,test)</strong><br class="title-page-name"/><strong class="calibre1">MSE.lm &lt;- sum((predict_lm - test$medv)^2)/nrow(test)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">MSE.net_data</strong><br class="title-page-name"/><strong class="calibre1">MSE.lm</strong><br class="title-page-name"/><strong class="calibre1">###########################################################################</strong><br class="title-page-name"/><br class="title-page-name"/></pre>
<p class="calibre2">Don't worry, now we will explain in detail the whole code,<span> line by line.</span></p>
<pre class="calibre24"><strong class="calibre1">library("neuralnet")</strong><br class="title-page-name"/><strong class="calibre1">library(MASS)</strong></pre>
<p class="calibre2">The first two lines of the code are simple, as they load the libraries we will use for later calculations. Specifically, the <kbd class="calibre13">neuralnet</kbd> library will help us to build and train the network, while the <kbd class="calibre13">MASS</kbd> library will serve us to load the <kbd class="calibre13">Boston</kbd> dataset that we have previously introduced in detail.</p>
<div class="packt_tip">Remember, to install a library that is not present in the initial distribution of R, you must use the <kbd class="calibre61">install.package</kbd> function. <span class="calibre62">This is the main function to install packages. It takes a vector of names and a destination library, downloads the packages from the repositories and installs them.</span></div>
<p class="calibre2">In our case, for example, <span>to install the </span><kbd class="calibre13">neuralnet</kbd><span> package, </span>we should write:</p>
<pre class="calibre24"><strong class="calibre1">install.neuralnet</strong></pre>
<p class="calibre2">Finally, it should be emphasized that this function should be used only once and not every time you run the code. Instead, load the library through the following command and <span>must be repeated every time you run the code</span>:</p>
<pre class="calibre24"><strong class="calibre1">library (neuralnet)</strong></pre>
<p class="calibre2"><span>The function </span><kbd class="calibre13">set.seed</kbd><span> sets the seed of R's random number generator, which is useful for creating simulations or random objects that can be reproduced:</span></p>
<pre class="calibre24"><strong class="calibre1">set.seed(1)</strong></pre>
<div class="packt_tip">You have to use this function every time you want to get a reproducible random result. In this case, the random numbers are the same, and they would continue to be the same no matter how far out in the sequence we go.</div>
<p class="calibre2">The following<span> command loads the </span><kbd class="calibre13">Boston</kbd><span> dataset, </span>which<span>, as we anticipated, is contained in the </span><kbd class="calibre13">MASS</kbd><span> library and saves it in a given frame:</span></p>
<pre class="calibre24"><strong class="calibre1">data = Boston</strong></pre>
<p class="calibre2">Use the <kbd class="calibre13">str</kbd> function to view a compactly display the structure of an arbitrary R object. In our case, using <kbd class="calibre13">str(data)</kbd>, we will obtain the following results:</p>
<pre class="calibre24"><strong class="calibre1">&gt; str(data)</strong><br class="title-page-name"/><strong class="calibre1">'data.frame': 506 obs. of 14 variables:</strong><br class="title-page-name"/><strong class="calibre1"> $ crim : num 0.00632 0.02731 0.02729 0.03237 0.06905 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ zn : num 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ indus : num 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ chas : int 0 0 0 0 0 0 0 0 0 0 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ nox : num 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ rm : num 6.58 6.42 7.18 7 7.15 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ age : num 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ dis : num 4.09 4.97 4.97 6.06 6.06 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ rad : int 1 2 2 3 3 3 5 5 5 5 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ tax : num 296 242 242 222 222 222 311 311 311 311 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ ptratio: num 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ black : num 397 397 393 395 397 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ lstat : num 4.98 9.14 4.03 2.94 5.33 ...</strong><br class="title-page-name"/><strong class="calibre1"> $ medv : num 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...</strong></pre>
<p class="calibre2">The result obtained for the given object is shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00057.jpeg"/></div>
<p class="calibre2">Let's go back to parse the code:</p>
<pre class="calibre24"><strong class="calibre1">max_data &lt;- apply(data, 2, max) </strong><br class="title-page-name"/><strong class="calibre1">min_data &lt;- apply(data, 2, min)</strong><br class="title-page-name"/><strong class="calibre1">data_scaled &lt;- scale(data,center = min_data, scale = max_data - min_data)</strong> </pre>
<p class="calibre2"><span>We need t</span>his snippet of code to normalize the data.</p>
<div class="packt_tip">Remember, it is good practice to normalize the data before training a neural network. With normalization, data units are eliminated, allowing you to easily compare data from different locations.</div>
<p class="calibre2">This is an extremely important procedure in building a neural network, as it avoids unnecessary results or very difficult training processes resulting in algorithm convergence problems. You can choose different methods for scaling the data (<strong class="calibre1">z-normalization</strong>, <strong class="calibre1">min-max scale</strong>, and so on). For this example, we will use the min-max method (usually called feature scaling) to get all the scaled data in the range <em class="calibre14">[0,1]</em>. The formula to achieve this is the following:</p>
<div class="calibre28"><img src="../images/00058.gif" class="calibre63"/></div>
<p class="calibre2"> </p>
<p class="calibre2">Before applying the method chosen for <span>normalization</span>, you must calculate the minimum and maximum values of each database column. To do this, we use the <kbd class="calibre13">apply</kbd> function. This function returns a vector or an array or a list of values obtained by applying a function to margins of an array or matrix. Let's understand the meaning of the arguments used.</p>
<pre class="calibre24"><strong class="calibre1">max_data &lt;- apply(data, 2, max)</strong> </pre>
<p class="calibre2">The first argument of the <kbd class="calibre13">apply</kbd> function specifies the dataset to apply the function, in our case, the dataset named <kbd class="calibre13">data</kbd>. The second argument must contain a vector giving the subscripts which the function will be applied over. In our case, one indicates rows and <kbd class="calibre13">2</kbd> indicates columns. The third argument must contain the function to be applied; in our case, the <kbd class="calibre13">max</kbd> function.</p>
<p class="calibre2">To normalize the data, we use the <kbd class="calibre13">scale</kbd> function, which is a generic function whose default method centers and/or scales the columns of a numeric matrix.</p>
<pre class="calibre24"><strong class="calibre1">index = sample(1:nrow(data),round(0.70*nrow(data)))</strong><br class="title-page-name"/><strong class="calibre1">train_data &lt;- as.data.frame(data_scaled[index,])</strong><br class="title-page-name"/><strong class="calibre1">test_data &lt;- as.data.frame(data_scaled[-index,])</strong></pre>
<p class="calibre2">In the first line of the code just suggested, the dataset is split into <em class="calibre14">70:30</em>, with the intention of using <em class="calibre14">70</em> percent of the data at our disposal to train the network and the remaining <em class="calibre14">30</em> percent to test the network. In the second and third lines, the data of the dataframe named <kbd class="calibre13">data</kbd> is subdivided into two new dataframes, called <kbd class="calibre13">train_data</kbd> and <kbd class="calibre13">test_data.</kbd></p>
<pre class="calibre24"><strong class="calibre1">n = names(data)</strong><br class="title-page-name"/><strong class="calibre1">f = as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))</strong><br class="title-page-name"/><strong class="calibre1">net_data = neuralnet(f,data=train_data,hidden=10,linear.output=T)</strong><br class="title-page-name"/><strong class="calibre1">plot(net_data)</strong></pre>
<p class="calibre2">Everything so far has only been used to prepare the data. It is now time to build the network. To do this, we first recover all the variable names using the <kbd class="calibre13">names</kbd> function. This function will get or set the name of an object.</p>
<p class="calibre2">Next, we build <kbd class="calibre13">formula</kbd> that we will use to build the network, so we use the <kbd class="calibre13">neuralnet</kbd> function to build and train the network. In this case, we will create a network with only one hidden layer with <kbd class="calibre13">10</kbd> nodes. Finally, we plot the neural network,as shown in the following figure:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00059.jpeg"/></div>
<p class="calibre2">Now that we have the network, what do we do? Of course, we use it to make predictions. We had set aside <em class="calibre14">30</em> percent of the available data to do this:</p>
<pre class="calibre24"><strong class="calibre1">predict_net_test &lt;- compute(net_data,test_data[,1:13])</strong></pre>
<p class="calibre2">In our case, we applied the function to the <kbd class="calibre13">test_data</kbd> dataset, using only the first <kbd class="calibre13">13</kbd> columns representing the input variables of the network:</p>
<pre class="calibre24"><strong class="calibre1">predict_net_test_start &lt;- predict_net_test$net.result*(max(data$medv)-       min(data$medv))+min(data$medv)</strong><br class="title-page-name"/><strong class="calibre1">test_start &lt;- as.data.frame((test_data$medv)*(max(data$medv)-min(data$medv))+min(data$medv))</strong><br class="title-page-name"/><strong class="calibre1">MSE.net_data &lt;- sum((predict_net_test_start - test_start)^2)/nrow(test_start)</strong></pre>
<p class="calibre2">But how do we figure out whether the forecasts the network is able to perform are accurate? We can use the <strong class="calibre1">Mean Squared Error</strong> (<strong class="calibre1">MSE</strong>) as a measure of how far away our predictions are from the real data.</p>
<p class="calibre2">In this regard, it is worth remembering that before we built the network we had normalized the data. Now, in order to be able to compare, we need to step back and return to the starting position. Once the values of the dataset are restored, we can calculate the <em class="calibre14">MSE</em> through the following equation:</p>
<div class="calibre47"><img src="../images/00060.jpeg" class="calibre64"/></div>
<p class="calibre2"> </p>
<p class="calibre2">Well, we have calculated <em class="calibre14">MSE</em> now with what do we compare it to? To get an idea of the accuracy of the network prediction, we can build a linear regression model:</p>
<pre class="calibre24"><strong class="calibre1">Regression_Model &lt;- lm(medv~., data=data)</strong><br class="title-page-name"/><strong class="calibre1">summary(Regression_Model)</strong><br class="title-page-name"/><strong class="calibre1">test &lt;- data[-index,]</strong><br class="title-page-name"/><strong class="calibre1">predict_lm &lt;- predict(Regression_Model,test)</strong><br class="title-page-name"/><strong class="calibre1">MSE.lm &lt;- sum((predict_lm - test$medv)^2)/nrow(test)</strong></pre>
<p class="calibre2">We build a linear regression model using the <kbd class="calibre13">lm</kbd> function. This function is used to fit linear models. It can be used to perform regression, single stratum analysis of variance, and analysis of covariance. To produce result summaries of the results of model fitting obtained, we have used the <kbd class="calibre13">summary</kbd> function, which returns the following results:</p>
<pre class="calibre24"><strong class="calibre1">&gt; summary(Regression_Model)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">Call:</strong><br class="title-page-name"/><strong class="calibre1">lm(formula = medv ~ ., data = data)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">Residuals:</strong><br class="title-page-name"/><strong class="calibre1"> Min 1Q Median 3Q Max </strong><br class="title-page-name"/><strong class="calibre1">-15.5944739 -2.7297159 -0.5180489 1.7770506 26.1992710</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">Coefficients:</strong><br class="title-page-name"/><strong class="calibre1"> Estimate Std. Error t value Pr(&gt;|t|) </strong><br class="title-page-name"/><strong class="calibre1">(Intercept) 36.4594883851 5.1034588106 7.14407 0.00000000000328344 ***</strong><br class="title-page-name"/><strong class="calibre1">crim -0.1080113578 0.0328649942 -3.28652 0.00108681 ** </strong><br class="title-page-name"/><strong class="calibre1">zn 0.0464204584 0.0137274615 3.38158 0.00077811 ***</strong><br class="title-page-name"/><strong class="calibre1">indus 0.0205586264 0.0614956890 0.33431 0.73828807 </strong><br class="title-page-name"/><strong class="calibre1">chas 2.6867338193 0.8615797562 3.11838 0.00192503 ** </strong><br class="title-page-name"/><strong class="calibre1">nox -17.7666112283 3.8197437074 -4.65126 0.00000424564380765 ***</strong><br class="title-page-name"/><strong class="calibre1">rm 3.8098652068 0.4179252538 9.11614 &lt; 0.000000000000000222 ***</strong><br class="title-page-name"/><strong class="calibre1">age 0.0006922246 0.0132097820 0.05240 0.95822931 </strong><br class="title-page-name"/><strong class="calibre1">dis -1.4755668456 0.1994547347 -7.39800 0.00000000000060135 ***</strong><br class="title-page-name"/><strong class="calibre1">rad 0.3060494790 0.0663464403 4.61290 0.00000507052902269 ***</strong><br class="title-page-name"/><strong class="calibre1">tax -0.0123345939 0.0037605364 -3.28001 0.00111164 ** </strong><br class="title-page-name"/><strong class="calibre1">ptratio -0.9527472317 0.1308267559 -7.28251 0.00000000000130884 ***</strong><br class="title-page-name"/><strong class="calibre1">black 0.0093116833 0.0026859649 3.46679 0.00057286 ***</strong><br class="title-page-name"/><strong class="calibre1">lstat -0.5247583779 0.0507152782 -10.34715 &lt; 0.000000000000000222 ***</strong><br class="title-page-name"/><strong class="calibre1">---</strong><br class="title-page-name"/><strong class="calibre1">Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">Residual standard error: 4.745298 on 492 degrees of freedom</strong><br class="title-page-name"/><strong class="calibre1">Multiple R-squared: 0.7406427, Adjusted R-squared: 0.7337897 </strong><br class="title-page-name"/><strong class="calibre1">F-statistic: 108.0767 on 13 and 492 DF, p-value: &lt; 0.00000000000000022204</strong></pre>
<p class="calibre2">Also, for the regression model, we calculate the <span>mean MSE. Finally, i</span><span>n order to assess the performance of the network, it is compared with a multiple linear regression model calculated with the same database as follows:</span></p>
<pre class="calibre24"><strong class="calibre1">MSE.net_data</strong><br class="title-page-name"/><strong class="calibre1">MSE.lm</strong></pre>
<p class="calibre2">The results are:</p>
<pre class="calibre24"><strong class="calibre1">&gt; MSE.net_data</strong><br class="title-page-name"/><strong class="calibre1">[1] 12.0692812</strong><br class="title-page-name"/><strong class="calibre1">&gt; MSE.lm</strong><br class="title-page-name"/><strong class="calibre1">[1] 26.99265692</strong></pre>
<p class="calibre2">From the analysis of the results, it is possible to note that the neural network has a lower <kbd class="calibre13">MSE</kbd> than the linear regression model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Unsupervised learning in neural networks </h1>
                
            
            <article>
                
<p class="calibre2">In this section, we present unsupervised learning models in neural network, named competitive learning and Kohonen SOM. Kohonen SOM was invented by a professor named Teuvo Kohonen and is a way to represent multidimensional data in much lower dimensions: <em class="calibre14">1D</em> or <em class="calibre14">2D</em>. It can classify data without supervision. Unsupervised learning aims at finding hidden patterns within the dataset and clustering them into different classes of data.</p>
<p class="calibre2">There are many unsupervised learning techniques, namely K-means clustering, dimensionality reduction, EM, and so on. The common feature is that there is no input-output mapping and we work only on the input values to create a group or set of outputs. </p>
<p class="calibre2">For the case of neural networks, they can be used for unsupervised learning. They can group data into different buckets (clustering) or abstract original data into a different set of output data points (feature abstraction or dimensionality reduction). Unsupervised techniques require less processing power and memory than supervised technique.</p>
<p class="calibre2">In unsupervised neural networks, there is no target variable and we cannot do backpropagation. Instead, we keep adjusting the weights without the error measure and try to group similar data together. There are two methods we will see for unsupervised neural networks:</p>
<ul class="calibre16">
<li class="calibre17">Competitive learning</li>
<li class="calibre17">Kohonen SOMs</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Competitive learning</h1>
                
            
            <article>
                
<p class="calibre2">Here, <span>the neural network nodes compete with each other for the right to respond to a subset of the input data. The hidden layer is called the</span> <strong class="calibre1">competitive layer</strong>. <span>Every competitive neuron has its own weight and we calculate the similarity measure between the individual input vector and the neuron weight. For each input vector, the hidden neurons compete with each other to see which one is the <em class="calibre14">most </em>similar to the particular input vector:</span></p>
<div class="cdpaligncenter"><img class="image-border11" src="../images/00061.gif"/></div>
<p class="calibre2">The output neurons are said to be in competition for input patterns.</p>
<ul class="calibre16">
<li class="calibre17">During training, the output neuron that provides the highest activation to a given input pattern is declared the weights of the winner and is moved closer to the input pattern, whereas the rest of the neurons are left unchanged</li>
<li class="calibre17">This strategy is also called <strong class="calibre1">winner-takes-all</strong>, since only the winning neuron is updated:</li>
</ul>
<div class="cdpaligncenter"><img class="image-border12" src="../images/00062.gif"/></div>
<p class="calibre2">Let us see a simple competitive learning algorithm example to find three neurons within the given input data:</p>
<ol class="calibre19">
<li value="1" class="calibre17">We will have three input neurons in the input layer. Each input to the neuron is a continuous variable and let the weight at each input neuron be a random number between <em class="calibre14">0.0</em> and <em class="calibre14">1.0</em>. The output of each node is the product of the three weights and its input.</li>
<li value="2" class="calibre17">Each competitive layer neuron receives the sum of the product of weights and inputs.</li>
<li value="3" class="calibre17">The competitive layer node with the highest output is regarded as the winner. The input is then categorized as being within the cluster corresponding to that node.</li>
<li value="4" class="calibre17">The winner updates each of its weights, moving the weight from the connections that gave it weaker signals to the ones that gave it stronger signals.</li>
</ol>
<p class="calibre2">Thus, as we receive more data, each node converges on the center of the cluster that it has come to represent. It activates more strongly for inputs belonging to this cluster and more weakly for inputs that belong to other clusters.</p>
<p class="calibre2">There are basically two stopping conditions of competitive learning:</p>
<ul class="calibre16">
<li class="calibre17"><strong class="calibre1">Predefined number of epochs</strong>: Only <em class="calibre14">N </em>epochs are run and this prevents the algorithm from running for a relatively long time without convergence</li>
<li class="calibre17"><strong class="calibre1">Minimum value of weight update</strong>: The algorithm is run until we have a minimum value of weight update</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Kohonen SOM</h1>
                
            
            <article>
                
<p class="mce-root">The concept of competitive learning combined with neighborhood neurons gives us Kohonen SOMs. Every neuron in the output layer has two neighbors. The neuron that fires the greatest value updates its weights in competitive learning, but in SOM, the neighboring neurons also update their weights at a relatively slow rate. The number of neighborhood neurons that the network updates the weights is based on the dimension of the problem.</p>
<p class="calibre2">For a <em class="calibre14">2D</em> problem, the SOM is represented as follows:</p>
<div class="cdpaligncenter"><img class="image-border13" src="../images/00063.jpeg"/></div>
<p class="calibre2">Diagrammatically, this is how the SOM maps different colors into different clusters:</p>
<div class="cdpaligncenter"><img class="image-border14" src="../images/00064.jpeg"/></div>
<p class="calibre2">Let us understand the working of Kohonen SOM step-by-step:</p>
<ol class="calibre19">
<li value="1" class="calibre17">The number of inputs and the clusters that define the SOM structure and each node's weights are initialized.</li>
<li value="2" class="calibre17">A vector is chosen at random from the set of training data and is presented to the network.</li>
<li value="3" class="calibre17">Every node in the network is examined to calculate which one's weights are most similar to the input vector. The winning node is commonly known as the <strong class="calibre1">Best Matching Unit</strong> (<strong class="calibre1">BMU</strong>).</li>
<li value="4" class="calibre17">The radius of the neighborhood of the BMU is calculated. This value starts large and is typically set to be the radius of the network, diminishing each time-step.</li>
<li value="5" class="calibre17">Any neurons found within the radius of the BMU, calculated in step 4, are adjusted to make them more like the input vector. The closer a neuron is to the BMU, the more its weights are altered.</li>
<li value="6" class="calibre17">Repeat from step 2 for <em class="calibre14">N</em> iterations.</li>
</ol>
<p class="calibre2">The steps are repeated for a set of <em class="calibre14">N</em> epochs or until a minimum weight update is obtained.</p>
<p class="calibre2">SOMs are used in the fields of clustering (grouping of data into different buckets), data abstraction (deriving output data from inputs), and dimensionality reduction (reducing the number of input features). SOMs handle the problem in a way similar to <strong class="calibre1">Multi Dimensional Scaling</strong> (<strong class="calibre1">MDS</strong>), but instead of minimizing the distances, they try regroup topology, or in other words, they try to keep the same neighbors.</p>
<p class="calibre2">Let us see an example of SOM implementation in R. The <kbd class="calibre13">kohonen</kbd> package is a package to be installed to use the functions offered in R for SOM.</p>
<p class="calibre2">The following R program explains some functions from the <kbd class="calibre13">kohonen</kbd> package :</p>
<pre class="calibre24"><strong class="calibre1">######################################################################</strong><br class="title-page-name"/><strong class="calibre1">###Chapter 2 - Introduction to Neural Networks - using R    ##########</strong><br class="title-page-name"/><strong class="calibre1">###Usuervised ML technique using Kohonen package  ####################</strong><br class="title-page-name"/><strong class="calibre1">######################filename: kohonen.r#############################</strong><br class="title-page-name"/><strong class="calibre1">######################################################################</strong><br class="title-page-name"/><strong class="calibre1">library("kohonen")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">data("wines")</strong><br class="title-page-name"/><strong class="calibre1">str(wines)</strong><br class="title-page-name"/><strong class="calibre1">head(wines)</strong><br class="title-page-name"/><strong class="calibre1">View (wines)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">set.seed(1)</strong><br class="title-page-name"/><strong class="calibre1">som.wines = som(scale(wines), grid = somgrid(5, 5, "hexagonal"))</strong><br class="title-page-name"/><strong class="calibre1">som.wines</strong><br class="title-page-name"/><strong class="calibre1">dim(getCodes(som.wines))</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">plot(som.wines, main = "Wine data Kohonen SOM")</strong><br class="title-page-name"/><strong class="calibre1">par(mfrow = c(1, 1))</strong><br class="title-page-name"/><strong class="calibre1">plot(som.wines, type = "changes", main = "Wine data: SOM")</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">training = sample(nrow(wines), 150)</strong><br class="title-page-name"/><strong class="calibre1">Xtraining = scale(wines[training, ])</strong><br class="title-page-name"/><strong class="calibre1">Xtest = scale(wines[-training, ],</strong><br class="title-page-name"/><strong class="calibre1">              center = attr(Xtraining, "scaled:center"),</strong><br class="title-page-name"/><strong class="calibre1">              scale = attr(Xtraining, "scaled:scale"))</strong><br class="title-page-name"/><strong class="calibre1">trainingdata = list(measurements = Xtraining,</strong><br class="title-page-name"/><strong class="calibre1">              vintages = vintages[training])</strong><br class="title-page-name"/><strong class="calibre1">testdata = list(measurements = Xtest, vintages = vintages[-training])</strong><br class="title-page-name"/><strong class="calibre1">mygrid = somgrid(5, 5, "hexagonal")</strong><br class="title-page-name"/><strong class="calibre1">som.wines = supersom(trainingdata, grid = mygrid)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">som.prediction = predict(som.wines, newdata = testdata)</strong><br class="title-page-name"/><strong class="calibre1">table(vintages[-training], som.prediction$predictions[["vintages"]])</strong><br class="title-page-name"/><strong class="calibre1">######################################################################</strong></pre>
<p class="calibre2">The code uses a wine dataset, which contains a data frame with <kbd class="calibre13">177</kbd> rows and <kbd class="calibre13">13</kbd> columns; the object <kbd class="calibre13">vintages</kbd> contains the class labels. This data is obtained from the chemical analyses of wines grown in the same region in Italy (Piemonte) but derived from three different cultivars, namely, the <kbd class="calibre13">Nebbiolo</kbd>, <kbd class="calibre13">Barberas</kbd>, and <kbd class="calibre13">Grignolino</kbd> grapes. The wine from the <kbd class="calibre13">Nebbiolo</kbd> grape is called <strong class="calibre1">Barolo</strong>. The data consists of the amounts of several constituents found in each of the three types of wines, as well as some spectroscopic variables.</p>
<p class="calibre2">Now, let's see the outputs at each section of the code.</p>
<pre class="calibre24"><strong class="calibre1">library("kohonen")</strong></pre>
<p class="calibre2">The first line of the code is simple, as it loads the library we will use for later calculations. Specifically, the <kbd class="calibre13">kohonen</kbd> library will help us to train SOMs. Also, interrogation of the maps and prediction using trained maps are supported.</p>
<div class="packt_tip">Remember, to install a library that is not present in the initial distribution of R, you must use the <kbd class="calibre61">install.package</kbd> function. This is the main function to install packages. It takes a vector of names and a destination library, downloads the packages from the repositories and installs them.</div>
<pre class="calibre24"><strong class="calibre1">data("wines")</strong><br class="title-page-name"/><strong class="calibre1">str(wines)</strong><br class="title-page-name"/><strong class="calibre1">head(wines)</strong><br class="title-page-name"/><strong class="calibre1">view (wines)</strong></pre>
<p class="calibre2">These lines <span>load the <kbd class="calibre13">wines</kbd> dataset, which, as we anticipated, is contained in the R distribution, and saves it in a dataframe named <kbd class="calibre13">data</kbd>. Then, we use the <kbd class="calibre13">str</kbd> function to view a compactly display the structure of the dataset. The function <kbd class="calibre13">head</kbd> is used to return the first or last parts of the dataframe. Finally, the <kbd class="calibre13">view</kbd> function is used to invoke a spreadsheet-style data viewer on the dataframe object, as shown in the following figure:</span></p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00065.jpeg"/></div>
<p class="calibre2">We will continue to analyze the code:</p>
<pre class="calibre24"><strong class="calibre1">set.seed(1)</strong><br class="title-page-name"/><strong class="calibre1">som.wines = som(scale(wines), grid = somgrid(5, 5, "hexagonal"))</strong><br class="title-page-name"/><strong class="calibre1">dim(getCodes(som.wines))</strong><br class="title-page-name"/><strong class="calibre1">plot(som.wines, main = "Wine data Kohonen SOM")</strong></pre>
<p class="calibre2">After loading the wine data and setting <kbd class="calibre13">seed</kbd> for reproducibility, we call <kbd class="calibre13">som</kbd> to create a <em class="calibre14">5x5</em> matrix, in which the features have to be clustered. The function internally does the <kbd class="calibre13">kohonen</kbd> processing and the result can be seen by the clusters formed with the features. There are <em class="calibre14">25</em> clusters created, each of which has a combined set of features having common pattern, as shown in the following image:</p>
<div class="cdpaligncenter"><img class="image-border4" src="../images/00066.jpeg"/></div>
<p class="calibre2">The next part of the code plots the mean distance to the closest unit versus the number of iterations done by <kbd class="calibre13">som</kbd>:</p>
<pre class="calibre24"><strong class="calibre1">graphics.off()</strong><br class="title-page-name"/><strong class="calibre1">par(mfrow = c(1, 1))</strong><br class="title-page-name"/><strong class="calibre1">plot(som.wines, type = "changes", main = "Wine data: SOM")</strong></pre>
<p class="calibre2">In the following figure is shown mean distance to closest unit versus the number of iterations:</p>
<div class="cdpaligncenter"><img class="image-border15" src="../images/00067.jpeg"/></div>
<p class="calibre2">Next, we create a <kbd class="calibre13">training</kbd> dataset with <kbd class="calibre13">150</kbd> rows and <kbd class="calibre13">test</kbd> dataset with <kbd class="calibre13">27</kbd> rows. We run the SOM and predict with the test data. The <kbd class="calibre13">supersom</kbd><em class="calibre14"> </em>function is used here. Here, the model is supervised SOM:</p>
<pre class="calibre24"><strong class="calibre1">training = sample(nrow(wines), 150)</strong><br class="title-page-name"/><strong class="calibre1">Xtraining = scale(wines[training, ])</strong><br class="title-page-name"/><strong class="calibre1">Xtest = scale(wines[-training, ],</strong><br class="title-page-name"/><strong class="calibre1">              center = attr(Xtraining, "scaled:center"),</strong><br class="title-page-name"/><strong class="calibre1">              scale = attr(Xtraining, "scaled:scale"))</strong><br class="title-page-name"/><strong class="calibre1">trainingdata = list(measurements = Xtraining,</strong><br class="title-page-name"/><strong class="calibre1">                    vintages = vintages[training])</strong><br class="title-page-name"/><strong class="calibre1">testdata = list(measurements = Xtest, vintages = vintages[-training])</strong><br class="title-page-name"/><strong class="calibre1">mygrid = somgrid(5, 5, "hexagonal")</strong><br class="title-page-name"/><strong class="calibre1">som.wines = supersom(trainingdata, grid = mygrid)</strong><br class="title-page-name"/><br class="title-page-name"/><strong class="calibre1">som.prediction = predict(som.wines, newdata = testdata)</strong><br class="title-page-name"/><strong class="calibre1">table(vintages[-training], som.prediction$predictions[["vintages"]])</strong></pre>
<p class="calibre2">Finally, we invoke the <kbd class="calibre13">table</kbd> function that uses the cross-classifying factors to build a contingency table of the counts at each combination of factor levels, as shown next:</p>
<pre class="calibre24"><strong class="calibre1">&gt; table(vintages[-training], som.prediction$predictions[["vintages"]])</strong><br class="title-page-name"/> <br class="title-page-name"/><strong class="calibre1">              Barbera Barolo Grignolino</strong><br class="title-page-name"/><strong class="calibre1"> Barbera            5      0          0</strong><br class="title-page-name"/><strong class="calibre1"> Barolo             0     11          0</strong><br class="title-page-name"/><strong class="calibre1"> Grignolino         0      0         11</strong></pre>
<p class="calibre2">The <kbd class="calibre13">kohonen</kbd> package features standard SOMs and two extensions: for classification and regression purposes, and for data mining. Also, it has extensive graphics capability for visualization.</p>
<p class="calibre2">The following table lists the functions available in the <kbd class="calibre13">kohonen</kbd> package:</p>
<table class="calibre65">
<tbody class="calibre6">
<tr class="calibre7">
<td class="calibre8"><strong class="calibre1">Function name</strong></td>
<td class="calibre8"><strong class="calibre1">Description</strong></td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">som</kbd></td>
<td class="calibre8">Standard SOM</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">xyf</kbd>, <kbd class="calibre13">bdk</kbd></td>
<td class="calibre8">Supervised SOM; two parallel maps</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">supersom</kbd></td>
<td class="calibre8">SOM with multiple parallel maps</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">plot.kohonen</kbd></td>
<td class="calibre8">Generic plotting function</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">summary.kohonen</kbd></td>
<td class="calibre8">Generic summary function</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">map.kohonen</kbd></td>
<td class="calibre8">Map data to the most similar neuron</td>
</tr>
<tr class="calibre7">
<td class="calibre8"><kbd class="calibre13">predict.kohonen</kbd></td>
<td class="calibre8">Generic function to predict properties</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2"><span>In this chapter, we explored the machine learning field and we saw the learning process in a neural network. We learned to distinguish between supervised learning, unsupervised learning, and reinforcement learning. To understand in detail the necessary procedures, we also learned how to train and test the model.</span></p>
<p class="calibre2"><span>Afterwards, we discovered the meaning of the data cycle and how the data must be collected, cleaned, converted, and then fed to the model for learning. </span>So we went deeper into the evaluation model to see if the expected value is equal to the actual value during the test phase. We analyzed the different metrics available to control the model that depends on the status of the target variable.</p>
<p class="calibre2"><span>Then we discovered one of the concepts important for understanding the neural networks, the backpropagation algorithm, that is based on computing to update weights and bias ions at each level.</span></p>
<p class="calibre2"><span>Finally, we covered two practical programs in R for the learning process, by applying the <kbd class="calibre13">neuralnet</kbd> and the <kbd class="calibre13">kohonen</kbd> libraries. We can systematically use these basics for further building of complex networks.</span></p>
<p class="calibre2"><span>In the next chapter, we will discover the <strong class="calibre1">Deep Neural Network</strong> (<strong class="calibre1">DNN</strong>). We will see some basics of the <kbd class="calibre13">H2O</kbd> package. Overall, <kbd class="calibre13">H2O</kbd> is a highly user-friendly package that can be used to train feed-forward networks or deep auto-encoders. It supports distributed computations and provides a web interface. By including the <kbd class="calibre13">H2O</kbd> package, like any other package in R, we can do all kind of modeling and processing of DNN.</span></p>
<p class="calibre2"> </p>


            </article>

            
        </section>
    </body></html>