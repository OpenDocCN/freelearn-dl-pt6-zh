<html><head></head><body><div id="book-columns"><div id="book-inner"><div class="chapter" title="Chapter 7. Miscellaneous Deep Learning Operations using Hadoop"><div class="titlepage"><div><div><h1 class="title"><a id="ch07"/><span class="koboSpan" id="kobo.1.1">Chapter 7. Miscellaneous Deep Learning Operations using Hadoop</span></h1></div></div></div><div class="blockquote"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td valign="top"><span class="koboSpan" id="kobo.2.1"> </span></td><td valign="top"><p>
<span class="emphasis"><em><span class="koboSpan" id="kobo.3.1">"In pioneer days they used oxen for heavy pulling, and when one ox couldn't budge a log, they didn't try to grow a larger ox. </span><span class="koboSpan" id="kobo.3.2">We shouldn't be trying for bigger computers, but for more systems of computers."</span></em></span>
</p></td><td valign="top"><span class="koboSpan" id="kobo.4.1"> </span></td></tr><tr><td valign="top"><span class="koboSpan" id="kobo.5.1"> </span></td><td colspan="2" align="right" valign="top" style="text-align: center"><span class="koboSpan" id="kobo.6.1">--</span><span class="attribution"><span class="emphasis"><em><span class="koboSpan" id="kobo.7.1">Grace Hopper</span></em></span></span></td></tr></table></div><p><span class="koboSpan" id="kobo.8.1">So far in this book, we discussed various deep neural network models and their concepts, applications, and implementation of the models in distributed environments. </span><span class="koboSpan" id="kobo.8.2">We have also explained why it is difficult for a centralized computer to store and process vast amounts of data and extract information using these models. </span><span class="koboSpan" id="kobo.8.3">Hadoop has been used to overcome the limitations caused by large-scale data.</span></p><p><span class="koboSpan" id="kobo.9.1">As we have now reached the final chapter of this book, we will mainly discuss the design of the three most commonly used machine learning applications. </span><span class="koboSpan" id="kobo.9.2">We will explain the general concept of large-scale video processing, large-scale image processing, and natural language processing using the Hadoop framework.</span></p><p><span class="koboSpan" id="kobo.10.1">The organization of this chapter is as follows:</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.11.1">Large-scale distributed video processing using Hadoop</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.12.1">Large-scale image processing using Hadoop</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.13.1">Natural language processing using Hadoop</span></li></ul></div><p><span class="koboSpan" id="kobo.14.1">The large amount of videos available in the digital world are contributing to the lion's share of the big data generated in recent days. </span><span class="koboSpan" id="kobo.14.2">In </span><a class="link" href="ch02.html" title="Chapter 2.  Distributed Deep Learning for Large-Scale Data"><span class="koboSpan" id="kobo.15.1">
Chapter 2
</span></a><span class="koboSpan" id="kobo.16.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.17.1">Distributed Deep Learning for Large-Scale Data</span></em></span><span class="koboSpan" id="kobo.18.1"> we discussed how millions of videos are uploaded to various social media websites such as YouTube and Facebook. </span><span class="koboSpan" id="kobo.18.2">Apart from this, surveillance cameras installed for security purposes in various shopping malls, airports, or government organizations generate loads of videos on a daily basis. </span><span class="koboSpan" id="kobo.18.3">Most of these videos are typically stored as compressed video files due to their huge storage consumption. </span><span class="koboSpan" id="kobo.18.4">In most of these enterprises, the security cameras operate for the whole day and later store the important videos, to be investigated in future.</span></p><p><span class="koboSpan" id="kobo.19.1">These videos contain hidden "hot data" or information, which needs to be processed and extracted quickly. </span><span class="koboSpan" id="kobo.19.2">As a consequence, the need to process and analyze these large-scale videos has become one of the priorities for data enthusiasts. </span><span class="koboSpan" id="kobo.19.3">Also, in many different fields of studies, such as bio-medical engineering, geology, and educational research, there is a need to process these large-scale videos and make them available at different locations for detailed analysis.</span></p><p><span class="koboSpan" id="kobo.20.1">In this section, we will look into the processing of large-scale video datasets using the Hadoop framework. </span><span class="koboSpan" id="kobo.20.2">The primary challenge of large-scale video processing is to transcode the videos from compressed to uncompressed format. </span><span class="koboSpan" id="kobo.20.3">For this reason, we need a distributed video transcoder that will write the video in the </span><span class="strong"><strong><span class="koboSpan" id="kobo.21.1">Hadoop Distributed File System</span></strong></span><span class="koboSpan" id="kobo.22.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.23.1">HDFS</span></strong></span><span class="koboSpan" id="kobo.24.1">), decode the bit stream chunks in parallel, and generate a sequence file.</span></p><p><span class="koboSpan" id="kobo.25.1">When a block of the input data is processed in the HDFS, each mapper process accesses the lines in each split separately. </span><span class="koboSpan" id="kobo.25.2">However, in case of a large-scale video dataset, when it is split into multiple blocks of predefined sizes, each mapper process is supposed to interpret the blocks of bit-stream separately. </span><span class="koboSpan" id="kobo.25.3">The mapper process will then provide access to the decoded video frames for subsequent analysis. </span><span class="koboSpan" id="kobo.25.4">In the following subsections, we will discuss how each block of the HDFS containing the video bit-stream can be transcoded into sets of images to be processed for the further analyses.</span></p><div class="section" title="Distributed video decoding in Hadoop"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec46"/><span class="koboSpan" id="kobo.26.1">Distributed video decoding in Hadoop</span></h1></div></div></div><p><span class="koboSpan" id="kobo.27.1">Most of the popular video compression formats, such as MPEG-2 and MPEG-4, follow a hierarchical structure in the bit-stream. </span><span class="koboSpan" id="kobo.27.2">In this subsection, we will assume that the compression format used has a hierarchical structure for its bit-stream. </span><span class="koboSpan" id="kobo.27.3">For simplicity, we have divided the decoding task into two different Map-reduce jobs:</span></p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="strong"><strong><span class="koboSpan" id="kobo.28.1">Extraction of video sequence level information</span></strong></span><span class="koboSpan" id="kobo.29.1">: From the outset, it can be easily predicted that the header information of all the video dataset can be found in the first block of the dataset. </span><span class="koboSpan" id="kobo.29.2">In this phase, the aim of the map-reduce job is to collect the sequence level information from the first block of the video dataset and output the result as a text file in the HDFS. </span><span class="koboSpan" id="kobo.29.3">The sequence header information is needed to set the format for the decoder object.</span><p><span class="koboSpan" id="kobo.30.1">For the video files, a new </span><code class="literal"><span class="koboSpan" id="kobo.31.1">FileInputFormat</span></code><span class="koboSpan" id="kobo.32.1"> should be implemented with its own record reader. </span><span class="koboSpan" id="kobo.32.2">Each record reader will then provide a </span><code class="literal"><span class="koboSpan" id="kobo.33.1">&lt;key, value&gt;</span></code><span class="koboSpan" id="kobo.34.1"> pair in this format to each map process: </span><code class="literal"><span class="koboSpan" id="kobo.35.1">&lt;LongWritable, BytesWritable&gt;</span></code><span class="koboSpan" id="kobo.36.1">. </span><span class="koboSpan" id="kobo.36.2">The input key denotes the byte offset within the file; the value that corresponds to </span><code class="literal"><span class="koboSpan" id="kobo.37.1">BytesWritable</span></code><span class="koboSpan" id="kobo.38.1"> is a byte array containing the video bit-stream for the whole block of data.</span></p><p><span class="koboSpan" id="kobo.39.1">For each map process, the key value is compared with </span><code class="literal"><span class="koboSpan" id="kobo.40.1">0</span></code><span class="koboSpan" id="kobo.41.1"> to identify if it is the first block of the video file. </span><span class="koboSpan" id="kobo.41.2">Once the first block is identified, the bit-stream is parsed to determine the sequence level information. </span><span class="koboSpan" id="kobo.41.3">This information is then dumped to a </span><code class="literal"><span class="koboSpan" id="kobo.42.1">.txt</span></code><span class="koboSpan" id="kobo.43.1"> file to be written to  HDFS. </span><span class="koboSpan" id="kobo.43.2">Let's denote the name of the </span><code class="literal"><span class="koboSpan" id="kobo.44.1">.txt</span></code><span class="koboSpan" id="kobo.45.1"> file as </span><code class="literal"><span class="koboSpan" id="kobo.46.1">input_filename_sequence_level_header_information.txt</span></code><span class="koboSpan" id="kobo.47.1">. </span><span class="koboSpan" id="kobo.47.2">As only the map process can provide us the desired output, the reducer count for this method is set to </span><code class="literal"><span class="koboSpan" id="kobo.48.1">0</span></code><span class="koboSpan" id="kobo.49.1">.</span></p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note25"/><span class="koboSpan" id="kobo.50.1">Note</span></h3><p><span class="koboSpan" id="kobo.51.1">Assume a text file with the following data:
</span><span class="strong"><strong><span class="koboSpan" id="kobo.52.1">Deep Learning
</span></strong></span>
<span class="strong"><strong><span class="koboSpan" id="kobo.53.1">with Hadoop
</span></strong></span><span class="koboSpan" id="kobo.54.1">Now the offset for the first line is </span><code class="literal"><span class="koboSpan" id="kobo.55.1">0</span></code><span class="koboSpan" id="kobo.56.1"> and the input to the Hadoop job will be </span><code class="literal"><span class="koboSpan" id="kobo.57.1">&lt;0,Deep Learning&gt;</span></code><span class="koboSpan" id="kobo.58.1"> and for the second line the offset will be </span><code class="literal"><span class="koboSpan" id="kobo.59.1">&lt;14,with Hadoop&gt;</span></code><span class="koboSpan" id="kobo.60.1">.
</span><span class="koboSpan" id="kobo.60.2">Whenever we pass the text file to the Hadoop job, it internally calculates the byte offset.</span></p></div></div></li><li class="listitem"><span class="strong"><strong><span class="koboSpan" id="kobo.61.1">Decode and convert the blocks of videos into sequence files</span></strong></span><span class="koboSpan" id="kobo.62.1">: The aim of this Map-reduce job is to decode  each block  of the video datasets and generate a corresponding sequence file. </span><span class="koboSpan" id="kobo.62.2">The sequence file will contain the decoded video frames of each block of data in JPEG format. </span><span class="koboSpan" id="kobo.62.3">The </span><code class="literal"><span class="koboSpan" id="kobo.63.1">InputFileFormat</span></code><span class="koboSpan" id="kobo.64.1"> file and record reader should be kept same as the first Map-reduce job. </span><span class="koboSpan" id="kobo.64.2">Therefore, the </span><code class="literal"><span class="koboSpan" id="kobo.65.1">&lt;key, value&gt;</span></code><span class="koboSpan" id="kobo.66.1"> pairs of the mapper input is </span><code class="literal"><span class="koboSpan" id="kobo.67.1">&lt;LongWritable, BytesWritable&gt;</span></code><span class="koboSpan" id="kobo.68.1">.</span><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.69.1"><img src="graphics/B05883_07_01-1.jpg" alt="Distributed video decoding in Hadoop"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.70.1">Figure 7.1: The overall representation of video decoding with Hadoop</span></p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.71.1">In this second phase, the output of the first job is considered as the input to this second Map-reduce job. </span><span class="koboSpan" id="kobo.71.2">Therefore, each mapper of this job will read the sequence information file in the HDFS and pass this information along with the bit-stream buffer, which comes as the </span><code class="literal"><span class="koboSpan" id="kobo.72.1">BytesWritable</span></code><span class="koboSpan" id="kobo.73.1"> input.</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.74.1">The map process basically converts the decoded video frames to JPEG images and generates a </span><code class="literal"><span class="koboSpan" id="kobo.75.1">&lt;key, value&gt;</span></code><span class="koboSpan" id="kobo.76.1"> pair as the output of the map process. </span><span class="koboSpan" id="kobo.76.2">The key of this output of the map process encodes the input video filename and the block number as </span><code class="literal"><span class="koboSpan" id="kobo.77.1">video_filename_block_number</span></code><span class="koboSpan" id="kobo.78.1">. </span><span class="koboSpan" id="kobo.78.2">The output value that corresponds to this key is </span><code class="literal"><span class="koboSpan" id="kobo.79.1">BytesWritable</span></code><span class="koboSpan" id="kobo.80.1">, and it stores the JPEG bit-stream of the decoded video block.</span></li><li class="listitem" style="list-style-type: disc"><span class="koboSpan" id="kobo.81.1">The reducers will then take the blocks of data as input and simply write the decoded frames into a sequence file containing JPEG images as output format for further processing. </span><span class="koboSpan" id="kobo.81.2">A simple format and overview of the whole process is shown in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.82.1">Figure 7.1</span></em></span><span class="koboSpan" id="kobo.83.1">. </span><span class="koboSpan" id="kobo.83.2">We have taken an input video </span><code class="literal"><span class="koboSpan" id="kobo.84.1">sample.m2v</span></code><span class="koboSpan" id="kobo.85.1"> for illustration purposes. </span><span class="koboSpan" id="kobo.85.2">Further, in this chapter, we will discuss how to process the large-scale image files (from the sequence files) with the HDFS.</span></li></ul></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note26"/><span class="koboSpan" id="kobo.86.1">Note</span></h3><p>
</p><p><span class="koboSpan" id="kobo.87.1">Input </span><code class="literal"><span class="koboSpan" id="kobo.88.1">&lt;key,value&gt;</span></code><span class="koboSpan" id="kobo.89.1"> for Mapper: </span><code class="literal"><span class="koboSpan" id="kobo.90.1">&lt;LongWritable, BytesWritable&gt;</span></code>
</p><p>
</p><p><span class="koboSpan" id="kobo.91.1">For example: </span><code class="literal"><span class="koboSpan" id="kobo.92.1">&lt;17308965, BytesWritable&gt;
</span></code><span class="koboSpan" id="kobo.93.1">
Output </span><code class="literal"><span class="koboSpan" id="kobo.94.1">&lt;key,value&gt;</span></code><span class="koboSpan" id="kobo.95.1"> from Mapper: </span><code class="literal"><span class="koboSpan" id="kobo.96.1">&lt;Text, BytesWritable&gt;
</span></code><span class="koboSpan" id="kobo.97.1">
For example: </span><code class="literal"><span class="koboSpan" id="kobo.98.1">&lt;sample.m2v_3, BytesWritable&gt;</span></code>
</p><p>
</p></div></div></li></ol></div></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Large-scale image processing using Hadoop"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec47"/><span class="koboSpan" id="kobo.1.1">Large-scale image processing using Hadoop</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">We have already mentioned in the earlier chapters how the size and volume of images are increasing day by day; the need to store and process these vast amount of images is difficult for centralized computers. </span><span class="koboSpan" id="kobo.2.2">Let's consider an example to get a practical idea of such situations. </span><span class="koboSpan" id="kobo.2.3">Let's take a large-scale image of size 81025 pixels by 86273 pixels. </span><span class="koboSpan" id="kobo.2.4">Each pixel is composed of three values:red, green, and blue. </span><span class="koboSpan" id="kobo.2.5">Consider that, to store each of these values, a 32-bit precision floating point number is required. </span><span class="koboSpan" id="kobo.2.6">Therefore, the total memory consumption of that image can be calculated as follows:</span></p><p>
<span class="emphasis"><em><span class="koboSpan" id="kobo.3.1">86273 * 81025 * 3 * 32 bits = 78.12 GB</span></em></span>
</p><p><span class="koboSpan" id="kobo.4.1">Leave aside doing any post processing on this image, as it can be clearly concluded that it is impossible for a traditional computer to even store this amount of data in its main memory. </span><span class="koboSpan" id="kobo.4.2">Even though some advanced computers come with higher configurations, given the return on investment, most companies do not opt for these computers as they are much too expensive to be acquired and maintained. </span><span class="koboSpan" id="kobo.4.3">Therefore, the proper solution should be to run the images in commodity hardware so that the images can be stored in their memory. </span><span class="koboSpan" id="kobo.4.4">In this section, we will explain the use of Hadoop to process these vast amounts of images in a distributed manner.</span></p><div class="section" title="Application of Map-Reduce jobs"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec63"/><span class="koboSpan" id="kobo.5.1">Application of Map-Reduce jobs</span></h2></div></div></div><p><span class="koboSpan" id="kobo.6.1">In this section, we will discuss how to process large image files using Map-reduce jobs with Hadoop. </span><span class="koboSpan" id="kobo.6.2">Before the job starts, all the input images to be processed are loaded to the HDFS. </span><span class="koboSpan" id="kobo.6.3">During the operation, the client sends a job request, which goes through NameNode. </span><span class="koboSpan" id="kobo.6.4">NameNode collects that request from the client, searches its metadata mapping, and then sends the data block information of the filesystem as well as location of the data block back to the client. </span><span class="koboSpan" id="kobo.6.5">Once the client gets the block's metadata, it automatically accesses the DataNodes, where the requested data block resides, then processes this data via the applicable commands.</span></p><p><span class="koboSpan" id="kobo.7.1">The Map-reduce jobs used for large-scale image processing are primarily responsible for controlling the whole task. </span><span class="koboSpan" id="kobo.7.2">Basically, here we explain the concept of an executable shell script file, which is responsible for collecting the executable file's input data from the HDFS.</span></p><p><span class="koboSpan" id="kobo.8.1">The best way to use the Map-reduce programming model is to design our own Hadoop data types for processing large numbers of image files directly. </span><span class="koboSpan" id="kobo.8.2">The system will use Hadoop Streaming technology, which helps the users to create and run special kinds of Map-reduce jobs. </span><span class="koboSpan" id="kobo.8.3">These special kinds of jobs can be performed through an executable file mentioned earlier, which will act as a mapper or reducer. </span><span class="koboSpan" id="kobo.8.4">The mapper implementation of the program will use a shell script to perform the necessary operation. </span><span class="koboSpan" id="kobo.8.5">The shell script is responsible for calling the executable files of the image processing. </span><span class="koboSpan" id="kobo.8.6">The lists of image files are taken as the input to these executable files for further processing. </span><span class="koboSpan" id="kobo.8.7">The results of this processing or output are later written back to the HDFS.</span></p><p><span class="koboSpan" id="kobo.9.1">So, the input image files should be written to the HDFS first, and then a file list is generated in a particular directory of Hadoop Streaming's input. </span><span class="koboSpan" id="kobo.9.2">The directory will store a collection of file lists. </span><span class="koboSpan" id="kobo.9.3">Each line of the file list will contain the HDFS address of the images files to be processed. </span><span class="koboSpan" id="kobo.9.4">The input of the mapper will be </span><code class="literal"><span class="koboSpan" id="kobo.10.1">Inputsplit</span></code><span class="koboSpan" id="kobo.11.1"> class, which is a text file. </span><span class="koboSpan" id="kobo.11.2">The shell script manager reads the files line by line and retrieves the images from the metadata. </span><span class="koboSpan" id="kobo.11.3">It then calls the image processing executable file for further processing of the images, and then write the result back to the HDFS. </span><span class="koboSpan" id="kobo.11.4">Hence, the output of the mapper is the final desired result. </span><span class="koboSpan" id="kobo.11.5">The mapper thus does all the jobs, retrieving the image file from the HDFS, image processing, and then writing it back to the HDFS. </span><span class="koboSpan" id="kobo.11.6">The number of reducers in this process can be set to zero.</span></p><p><span class="koboSpan" id="kobo.12.1">This is a simple design of how to process large numbers of images using Hadoop by the binary image processing method. </span><span class="koboSpan" id="kobo.12.2">Other complex image processing methods can also be deployed to process large-scale image datasets.</span></p></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Natural language processing using Hadoop"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec48"/><span class="koboSpan" id="kobo.1.1">Natural language processing using Hadoop</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">The exponential growth of information in the Web has increased the intensity of diffusion of large-scale unstructured natural language textual resources. </span><span class="koboSpan" id="kobo.2.2">Hence, in the last few years, the interest to extract, process, and share this information has increased substantially. </span><span class="koboSpan" id="kobo.2.3">Processing these sources of knowledge within a stipulated time frame has turned out to be a major challenge for various research and commercial industries. </span><span class="koboSpan" id="kobo.2.4">In this section, we will describe the process used to crawl the web documents, discover the information and run natural language processing in a distributed manner using Hadoop.</span></p><p><span class="koboSpan" id="kobo.3.1">To design architecture for </span><span class="strong"><strong><span class="koboSpan" id="kobo.4.1">natural language processing</span></strong></span><span class="koboSpan" id="kobo.5.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.6.1">NLP</span></strong></span><span class="koboSpan" id="kobo.7.1">), the first task to be performed is the extraction of annotated keywords and key phrases from the large-scale unstructured data. </span><span class="koboSpan" id="kobo.7.2">To perform the NLP on a distributed architecture, the Apache Hadoop framework can be chosen for its efficient and scalable solution, and also to improve the failure handling and data integrity. </span><span class="koboSpan" id="kobo.7.3">The large-scale web crawler can be set to extract all the unstructured data from the Web and write it in the Hadoop Distributed File System for further processing. </span><span class="koboSpan" id="kobo.7.4">To perform the particular NLP tasks, we can use the open source GATE application as shown in the paper [136]. </span><span class="koboSpan" id="kobo.7.5">An overview of the tentative design of a distributed natural language processing architecture is shown in </span><span class="emphasis"><em><span class="koboSpan" id="kobo.8.1">Figure 7.2</span></em></span><span class="koboSpan" id="kobo.9.1">.</span></p><p><span class="koboSpan" id="kobo.10.1">To distribute the working of the web crawler, map-reduce can be used and run across multiple nodes. </span><span class="koboSpan" id="kobo.10.2">The execution of the NLP tasks and also the writing of the final output is performed with Map-reduce. </span><span class="koboSpan" id="kobo.10.3">The whole architecture will depend on two input files i) the </span><code class="literal"><span class="koboSpan" id="kobo.11.1">seedurls</span></code><span class="koboSpan" id="kobo.12.1"> given for crawling a particular web page stored in </span><code class="literal"><span class="koboSpan" id="kobo.13.1">seed_urls.txt</span></code><span class="koboSpan" id="kobo.14.1"> and ii) the path location of the NLP application (such as where GATE is installed). </span><span class="koboSpan" id="kobo.14.2">The web crawler will take </span><code class="literal"><span class="koboSpan" id="kobo.15.1">seedurls</span></code><span class="koboSpan" id="kobo.16.1"> from the </span><code class="literal"><span class="koboSpan" id="kobo.17.1">.txt</span></code><span class="koboSpan" id="kobo.18.1"> file and run the crawler for those in parallel. </span><span class="koboSpan" id="kobo.18.2">Asynchronously, an extraction plugin searches the keywords and key phrases on the crawled web pages and executes independently along with the web pages crawled. </span><span class="koboSpan" id="kobo.18.3">At the last step, a dedicated program stores the extracted keywords and key phrases in an external SQL database or a NoSQL database such as </span><code class="literal"><span class="koboSpan" id="kobo.19.1">Elasticsearch</span></code><span class="koboSpan" id="kobo.20.1">, as per the requirements. </span><span class="koboSpan" id="kobo.20.2">All these modules mentioned in the architecture are described in the following subsections.</span></p><div class="section" title="Web crawler"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec64"/><span class="koboSpan" id="kobo.21.1">Web crawler</span></h2></div></div></div><p><span class="koboSpan" id="kobo.22.1">To explain this phase, we won't go into a deep explanation, as it's almost out of the scope of this book. </span><span class="koboSpan" id="kobo.22.2">Web crawling has a few different phases. </span><span class="koboSpan" id="kobo.22.3">The first phase is the URL discovery stage, where the process takes each seed URL as the input of the </span><code class="literal"><span class="koboSpan" id="kobo.23.1">seed_urls.txt</span></code><span class="koboSpan" id="kobo.24.1"> file and navigates through the pagination URLs to discover relevant URLs. </span><span class="koboSpan" id="kobo.24.2">This phase defines the set of URLs that are going to be fetched in the next phase.</span></p><p><span class="koboSpan" id="kobo.25.1">The next phase is fetching the page content of the URLs and saving in the disk. </span><span class="koboSpan" id="kobo.25.2">The operation is done segment-wise, where each segment will contain some predefined numbers of URLs. </span><span class="koboSpan" id="kobo.25.3">The operation will run in parallel on different </span><code class="literal"><span class="koboSpan" id="kobo.26.1">DataNodes</span></code><span class="koboSpan" id="kobo.27.1">. </span><span class="koboSpan" id="kobo.27.2">The final outcome of the phases is stored in the Hadoop Distributed File System. </span><span class="koboSpan" id="kobo.27.3">The Keyword extractor will work on these saved page contents for the next phase.</span></p><p>
</p><div class="mediaobject"><span class="koboSpan" id="kobo.28.1"><img src="graphics/B05883_07_02-1.jpg" alt="Web crawler"/></span></div><p>
</p><p><span class="koboSpan" id="kobo.29.1">Figure 7.2: The representation of  how natural language processing is performed in Hadoop that is going to be fetched in the next phase. </span><span class="koboSpan" id="kobo.29.2">The next phase is fetching the page content of the URLs and saving in the disk. </span><span class="koboSpan" id="kobo.29.3">The operation is done segment wise, where each segment will contain some pre-defined numbers of URLs. </span><span class="koboSpan" id="kobo.29.4">The operation will run in parallel on different DataNodes. </span><span class="koboSpan" id="kobo.29.5">The final outcome of the phases is stored in Hadoop Distributed File System. </span><span class="koboSpan" id="kobo.29.6">The keyword extractor will work on these saved page contents for the next phase.</span></p></div><div class="section" title="Extraction of keyword and module for natural language processing"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec65"/><span class="koboSpan" id="kobo.30.1">Extraction of keyword and module for natural language processing</span></h2></div></div></div><p><span class="koboSpan" id="kobo.31.1">For the page content of each URL, a </span><span class="strong"><strong><span class="koboSpan" id="kobo.32.1">Document Object Model</span></strong></span><span class="koboSpan" id="kobo.33.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.34.1">DOM</span></strong></span><span class="koboSpan" id="kobo.35.1">) is created and stored back in the HDFS. </span><span class="koboSpan" id="kobo.35.2">In the </span><span class="emphasis"><em><span class="koboSpan" id="kobo.36.1">DOM</span></em></span><span class="koboSpan" id="kobo.37.1">, documents have a logical structure like a tree. </span><span class="koboSpan" id="kobo.37.2">Using DOM, one can write the </span><code class="literal"><span class="koboSpan" id="kobo.38.1">xpath</span></code><span class="koboSpan" id="kobo.39.1"> to collect the required keywords and phrases in the natural language processing phase. </span><span class="koboSpan" id="kobo.39.2">In this module, we will define the Map-reduce job for executing the natural language processing application for the next phase. </span><span class="koboSpan" id="kobo.39.3">The map function defined as a </span><code class="literal"><span class="koboSpan" id="kobo.40.1">&lt;key, value&gt;</span></code><span class="koboSpan" id="kobo.41.1"> pair key is the URL, and values are a corresponding DOM of the URL. </span><span class="koboSpan" id="kobo.41.2">The </span><span class="emphasis"><em><span class="koboSpan" id="kobo.42.1">reduce</span></em></span><span class="koboSpan" id="kobo.43.1"> function will perform the configuration and execution of the natural language processing part. </span><span class="koboSpan" id="kobo.43.2">The subsequent estimation of the extracted keywords and phrases at the web domain level will be performed in the </span><code class="literal"><span class="koboSpan" id="kobo.44.1">reduce</span></code><span class="koboSpan" id="kobo.45.1"> method. </span><span class="koboSpan" id="kobo.45.2">For this purpose, we can write a custom plugin to generate the rule files to perform various string manipulations to filter out the noisy, undesired words from the extracted texts. </span><span class="koboSpan" id="kobo.45.3">The rule files can be a JSON file or any other easy to load and interpret file based on the use case. </span><span class="koboSpan" id="kobo.45.4">Preferably, the common nouns and adjectives are identified as common keywords from the texts.</span></p></div><div class="section" title="Estimation of relevant keywords from a page"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec66"/><span class="koboSpan" id="kobo.46.1">Estimation of relevant keywords from a page</span></h2></div></div></div><p><span class="koboSpan" id="kobo.47.1">The paper [136] has presented a very important formulation to find the relevant keywords and key phrases from a web document. </span><span class="koboSpan" id="kobo.47.2">They have provided the </span><span class="strong"><strong><span class="koboSpan" id="kobo.48.1">Term Frequency - Inverse Document Frequency</span></strong></span><span class="koboSpan" id="kobo.49.1"> (</span><span class="strong"><strong><span class="koboSpan" id="kobo.50.1">TF-IDF</span></strong></span><span class="koboSpan" id="kobo.51.1">) metric to estimate the relevant information from the whole corpus, composed of all the documents and pages that belong to a single web domain. </span><span class="koboSpan" id="kobo.51.2">Computing the value of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.52.1">TD-IDF</span></em></span><span class="koboSpan" id="kobo.53.1"> and assigning it a threshold value for discarding other keywords allows us to generate the most relevant words from the corpus. </span><span class="koboSpan" id="kobo.53.2">In other words, it discards the common articles and conjunctions that might have a high frequency of occurrence in the text, but generally do not possess any meaningful information. </span><span class="koboSpan" id="kobo.53.3">The </span><span class="emphasis"><em><span class="koboSpan" id="kobo.54.1">TF-IDF</span></em></span><span class="koboSpan" id="kobo.55.1"> metric is basically the product of two functions, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.56.1">TF</span></em></span><span class="koboSpan" id="kobo.57.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.58.1">IDF</span></em></span><span class="koboSpan" id="kobo.59.1">.</span></p><p>
<span class="emphasis"><em><span class="koboSpan" id="kobo.60.1">TF</span></em></span><span class="koboSpan" id="kobo.61.1"> provides the frequency of each word in the corpus, that is, how many times a word is present in the corpus. </span><span class="koboSpan" id="kobo.61.2">Whereas </span><span class="emphasis"><em><span class="koboSpan" id="kobo.62.1">IDF</span></em></span><span class="koboSpan" id="kobo.63.1"> behaves as a balance term, showing higher values for terms having the lower frequency in the whole corpus.</span></p><p><span class="koboSpan" id="kobo.64.1">Mathematically, the metric </span><span class="emphasis"><em><span class="koboSpan" id="kobo.65.1">TF-IDF</span></em></span><span class="koboSpan" id="kobo.66.1"> for a keyword or key phrase </span><span class="emphasis"><em><span class="koboSpan" id="kobo.67.1">i</span></em></span><span class="koboSpan" id="kobo.68.1"> in a document </span><span class="emphasis"><em><span class="koboSpan" id="kobo.69.1">d</span></em></span><span class="koboSpan" id="kobo.70.1"> contained in the document </span><span class="emphasis"><em><span class="koboSpan" id="kobo.71.1">D</span></em></span><span class="koboSpan" id="kobo.72.1"> is given by the following equation:</span></p><p>
<span class="emphasis"><em><span class="koboSpan" id="kobo.73.1">(TF-IDF)</span><sub><span class="koboSpan" id="kobo.74.1">i</span></sub><span class="koboSpan" id="kobo.75.1"> = TF</span><sub><span class="koboSpan" id="kobo.76.1">i</span></sub><span class="koboSpan" id="kobo.77.1"> . </span><span class="koboSpan" id="kobo.77.2">IDF</span><sub><span class="koboSpan" id="kobo.78.1">i</span></sub></em></span>
</p><p><span class="koboSpan" id="kobo.79.1">Here </span><span class="emphasis"><em><span class="koboSpan" id="kobo.80.1">TF</span><sub><span class="koboSpan" id="kobo.81.1">i</span></sub><span class="koboSpan" id="kobo.82.1"> = f</span><sub><span class="koboSpan" id="kobo.83.1">i</span></sub><span class="koboSpan" id="kobo.84.1">/n</span><sub><span class="koboSpan" id="kobo.85.1">d</span></sub></em></span><span class="koboSpan" id="kobo.86.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.87.1">IDF</span><sub><span class="koboSpan" id="kobo.88.1">i</span></sub><span class="koboSpan" id="kobo.89.1"> = log N</span><sub><span class="koboSpan" id="kobo.90.1">d</span></sub><span class="koboSpan" id="kobo.91.1">/N</span><sub><span class="koboSpan" id="kobo.92.1">i</span></sub></em></span>
</p><p><span class="koboSpan" id="kobo.93.1">Here </span><span class="emphasis"><em><span class="koboSpan" id="kobo.94.1">f</span><sub><span class="koboSpan" id="kobo.95.1">i</span></sub></em></span><span class="koboSpan" id="kobo.96.1"> is the frequency of the candidate keyword or key phrase </span><span class="emphasis"><em><span class="koboSpan" id="kobo.97.1">i</span></em></span><span class="koboSpan" id="kobo.98.1"> in the document </span><span class="emphasis"><em><span class="koboSpan" id="kobo.99.1">d</span></em></span><span class="koboSpan" id="kobo.100.1"> and </span><span class="emphasis"><em><span class="koboSpan" id="kobo.101.1">n</span><sub><span class="koboSpan" id="kobo.102.1">d</span></sub></em></span><span class="koboSpan" id="kobo.103.1"> is the total number of terms in the document </span><span class="emphasis"><em><span class="koboSpan" id="kobo.104.1">d</span></em></span><span class="koboSpan" id="kobo.105.1">. </span><span class="koboSpan" id="kobo.105.2">In </span><span class="emphasis"><em><span class="koboSpan" id="kobo.106.1">IDF</span></em></span><span class="koboSpan" id="kobo.107.1">, </span><span class="emphasis"><em><span class="koboSpan" id="kobo.108.1">N</span><sub><span class="koboSpan" id="kobo.109.1">D</span></sub><span class="koboSpan" id="kobo.110.1"> </span></em></span><span class="koboSpan" id="kobo.111.1">denotes the total number of documents present in the corpus </span><span class="emphasis"><em><span class="koboSpan" id="kobo.112.1">D</span></em></span><span class="koboSpan" id="kobo.113.1">, whereas </span><span class="emphasis"><em><span class="koboSpan" id="kobo.114.1">N</span><sub><span class="koboSpan" id="kobo.115.1">i</span></sub></em></span><span class="koboSpan" id="kobo.116.1"> denotes the number of documents in which the keyword or key phrase </span><span class="emphasis"><em><span class="koboSpan" id="kobo.117.1">i</span></em></span><span class="koboSpan" id="kobo.118.1"> is present.</span></p><p><span class="koboSpan" id="kobo.119.1">Based on the use cases, one should define a generic threshold frequency for </span><span class="emphasis"><em><span class="koboSpan" id="kobo.120.1">TF-IDF</span></em></span><span class="koboSpan" id="kobo.121.1">. </span><span class="koboSpan" id="kobo.121.2">For a keyword or key phrase </span><span class="emphasis"><em><span class="koboSpan" id="kobo.122.1">i</span></em></span><span class="koboSpan" id="kobo.123.1"> if the value of </span><span class="emphasis"><em><span class="koboSpan" id="kobo.124.1">TF-IDF</span></em></span><span class="koboSpan" id="kobo.125.1"> becomes higher than the threshold value, that keyword or key phrase is accepted as final as written directly to the HDFS. </span><span class="koboSpan" id="kobo.125.2">On the other hand, if the corresponding value is less than the threshold value, that keyword is dropped from the final collection. </span><span class="koboSpan" id="kobo.125.3">In that way, finally, all the desired keywords will be written to the HDFS.</span></p></div></div></div></div>
<div id="book-columns"><div id="book-inner"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec49"/><span class="koboSpan" id="kobo.1.1">Summary</span></h1></div></div></div><p><span class="koboSpan" id="kobo.2.1">This chapter discussed the most widely used applications of Machine learning and how they can be designed in the Hadoop framework. </span><span class="koboSpan" id="kobo.2.2">First, we started with a large video set and showed how the video can be decoded in the HDFS and later converted into a sequence file containing images for later processing. </span><span class="koboSpan" id="kobo.2.3">Large-scale image processing was discussed next in the chapter. </span><span class="koboSpan" id="kobo.2.4">The mapper used for this purpose has a shell script which performs all the tasks necessary. </span><span class="koboSpan" id="kobo.2.5">So, no reducer is necessary to perform this operation. </span><span class="koboSpan" id="kobo.2.6">Finally, we discussed how the natural language processing model can be deployed in Hadoop.</span></p></div></div></div></body></html>