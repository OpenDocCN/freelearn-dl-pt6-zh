<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Transfer Learning for Image Classification</h1>
                </header>
            
            <article>
                
<p>In <a href="5e051ef6-a3bf-4fe3-b888-8cebdb4240e6.xhtml" target="_blank">Chapter 3</a>, <em>Multi-Label</em> <em>Image Classification using Convolutional Neural Networks</em>, we saw how to develop an end-to-end project for handling multi-label image classification problems using CNN based on Java and the <strong>Deeplearning4J</strong> (<strong><span>DL4J</span></strong>) framework on real Yelp image datasets. For that purpose, we developed a CNN model from scratch.</p>
<p>Unfortunately, developing such a model from scratch is very time <span>consuming </span>and requires a significant amount of computational resources. Secondly, sometimes, we may not <span>even </span>have enough data to train such deep networks. For example, ImageNet is one of the largest image datasets at the moment and has millions of labeled images.</p>
<p>Therefore, we will develop an end-to-end project to solve dog versus cat image classification using a pretrained VGG-16 model, which is already trained with ImageNet. In the end, we will wrap up everything in a Java JFrame and JPanel application to make the overall pipeline understandable. Concisely, we will learn the following topics throughout an end-to-end project:</p>
<ul>
<li>Transfer learning for image classification</li>
<li>Developing an image classifier using transfer learning</li>
<li>Dataset collection and description</li>
<li>Developing a dog versus cat detector UI</li>
<li><strong>Frequently Asked Questions</strong> (<strong>FAQs</strong>)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image classification with pretrained VGG16</h1>
                </header>
            
            <article>
                
<p>One of the most useful and emerging applications in the ML domain nowadays is using the transfer learning technique; it provides high portability between different frameworks and platforms.</p>
<p>Once you've trained a neural network, what you get is a set of trained hyperparameters' values. For example, <strong>LeNet-5</strong> has 60k parameter values, <strong>AlexNet</strong> has 60 million, and <strong>VGG- 16</strong> has about 138 million parameters. These architectures are trained using anything from 1,000 to millions of images and typically have very deep architectures, having hundreds of layers that contribute toward so many hyperparameters.</p>
<p>There are many open source community guys or even tech giants who have made those pretrained models publicly available for research (and also industry) so that they can be restored and reused to solve similar problems. For example, suppose we want to classify new images into one of 1,000 classes in the case of AlexNet and 10 for LeNet-5. We typically do not need to deal with so many parameters but only a few selected ones (we will see an example soon).</p>
<p>In short, we do not need to train such a deep network from scratch, but we reuse the existing pre-trained model; still, we manage to achieve acceptable classification accuracy. More technically, we can use the weights of that pre-trained model as a feature extractor, or we can just initialize our architecture with it and then fine-tune them to our new task.</p>
<p>In this regard, while using the TL technique to solve your own problem, there might be three options available:</p>
<ul>
<li><strong>Use a Deep CNN as a fixed feature extractor</strong>: We can reuse a pre-trained ImageNet having a fully connected layer by removing the output layer if we are no longer interested in the 1,000 categories it has. This way, we can treat all other layers, as a feature extractor. Even once you have extracted the features using the pre-trained model, you can feed these features to any linear classifier, such as the softmax classifier, or even linear SVM!</li>
<li><strong>Fine-tune the Deep CNN</strong>: Trying to fine-tune the whole network, or even most of the layers, may result in overfitting. Therefore, with some extra effort to fine-tune the pre-trained weights on your new task using backpropagation.</li>
<li><strong>Reuse pre-trained models with checkpointing</strong>: The third widely used scenario is to download checkpoints that people have made available on the internet. You may go for this scenario if you do not have big computational power to train the model from scratch, so you just initialize the model with the released checkpoints and then do a little fine-tuning.</li>
</ul>
<p>Now at this point, you may have an interesting question come to mind: what is the difference between traditional ML and ML using transfer learning? Well, in traditional ML, you do not transfer any knowledge or representations to any other task, which is not the case in <span>transfer learning</span>.</p>
<p>Unlike traditional machine learning, the source and target task or domains do not have to come from the same distribution, but they have to be similar. Moreover, you can use <span>transfer learning</span> in case of fewer training samples or if you do not have the necessary computational power.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/792d1f27-123a-4af5-b400-7bd0cb749722.png" style=""/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Traditional machine learning versus transfer learning</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DL4J and transfer learning</h1>
                </header>
            
            <article>
                
<p>Now, let's take a look how the DL4J provides us with these functionalities through the <span>transfer learning</span> API it has. The DL4J <span>transfer learning</span> API enables users to (see more at <a href="https://deeplearning4j.org/transfer-learning">https://deeplearning4j.org/transfer-learning</a>):</p>
<ul>
<li>Modify the architecture of an existing model</li>
<li>Fine-tune learning configurations of an existing model</li>
<li>Hold parameters of a specified layer (also called a <strong>frozen layer</strong>) constant during training</li>
</ul>
<p>These functionalities are depicted in the following diagram, where we solve task B (similar to task A) using the transfer learning technique:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/74b69371-b7bf-47e7-8f0a-9ec9bde897ac.png" style=""/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Working principle of transfer learning</div>
<p>In the next section, we will provide more insights into how to use such a pretrained model with DL4J to help us in transfer learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Developing an image classifier using transfer learning</h1>
                </header>
            
            <article>
                
<p>In the next section, we will see how to distinguish between dogs and cats based on their raw images. We will also see how to implement our first CNN model to deal with the raw and color image having three channels.</p>
<div class="packt_infobox">This project is highly inspired (but extended significantly) by the "Java Image Cat&amp;Dog Recognition with Deep Neural Networks" article by Klevis Ramo (<a href="http://ramok.tech/">http://ramok.tech/</a>). </div>
<p>The <kbd>code</kbd> folder has three packages with a few Java files in each. Their functionalities are outlined as follows:</p>
<ul>
<li><kbd>com.packt.JavaDL.DogvCatClassification.Train</kbd>:
<ul>
<li><kbd>TrainCatvsDogVG16.java</kbd>: It is used to train the network and the trained model is saved to a user specific location. Finally, it prints the results.</li>
<li><kbd>PetType.java</kbd>: Contains an <kbd>enum</kbd> type that specifies pet types (that is, cat, dog, and unknown).</li>
<li><kbd>VG16CatvDogEvaluator.java</kbd>: Restores the trained model saved in a specified location by the <kbd>TrainCatvsDogVG16.java</kbd> class. Then it evaluates on both test and validation sets. Finally, it prints the results.</li>
</ul>
</li>
<li><kbd>com.packt.JavaDL.DogvCatClassification.Classifier</kbd>:
<ul>
<li><kbd>PetClassfier.java</kbd>: Gives the user the opportunity to upload a sample image (that is, either dog or cat). Then, the user can make the detection from a high-level UI.</li>
</ul>
</li>
<li><kbd>com.packt.JavaDL.DogvCatClassification.UI</kbd>:
<ul>
<li><kbd>ImagePanel.java</kbd>: Acts as the image panel by extending the Java JPanel</li>
<li><kbd>UI.java</kbd>: Creates the user interface for uploading the image and shows the result</li>
<li><kbd>ProgressBar.java</kbd>: Shows the progress bar</li>
</ul>
</li>
</ul>
<p>We will explore them step by step. First, let us look at the dataset description.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dataset collection and description</h1>
                </header>
            
            <article>
                
<p>For this end-to-end project, we will use the dog versus cat dataset from Microsoft that was provided for the infamous dogs versus cats classification problem as a playground competition. The dataset can be downloaded from <a href="https://www.microsoft.com/en-us/download/details.aspx?id=54765.">https://www.microsoft.com/en-us/download/details.aspx?id=54765.</a></p>
<p>The train folder contains 25k images of both dogs and cats, where the labels are part of the filename. However, the test folder contains 12.5k images named according to numeric IDs. Now let's take a look at some sample snaps from the 25k images:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/56ec1ba5-7445-4ac4-82ae-dc4d575a03ae.png" style=""/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Showing the true labels of images that are randomly selected</div>
<p>For each image in the test set, we have to predict whether an image contains a dog (<em>1 = dog, 0 = cat</em>). In short, this is a binary classification problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Architecture choice and adoption</h1>
                </header>
            
            <article>
                
<p>As mentioned earlier, we will be reusing the VGG-16 pretrained model, which is already trained with different images of cat and dog breeds from ImageNet (see the list here at <a href="http://www.image-net.org/challenges/LSVRC/2014/results#clsloc">http://www.image-net.org/challenges/LSVRC/2014/results#clsloc</a>). The original VGG-16 model had 1,000 classes of images to be predicted as outlined in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8de28bf5-65da-4908-9b80-d3edef23304d.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Original VGG-16 model architecture</div>
<p>Fortunately, the trained model and network weights are already available on the DL4J website (see <a href="http://blob.deeplearning4j.org/models/vgg16_dl4j_inference.zip">http://blob.deeplearning4j.org/models/vgg16_dl4j_inference.zip</a>) and the size is about 500 MB.</p>
<p>You can manually download and restore, or a better way is to do it the DL4J way, where you just need to specify the pretrained type (up to DL4J 1.0.0 alpha, there were only four pretrained types available, such as ImageNet, CIFAR, MNIST, and VGG-Face).</p>
<p>The latter is very straightforward; just use the following lines of code and the trained model will be downloaded automatically (it will take a while depending on Internet speed though):</p>
<pre><strong>ZooModel</strong> zooModel = <strong>new</strong> VGG16();<br/><strong>LOGGER</strong>.info(" VGG16 model is getting downloaded...");<br/><strong>ComputationGraph</strong> preTrainedNet = (<strong>ComputationGraph</strong>) zooModel.initPretrained(PretrainedType.<strong>IMAGENET</strong>);</pre>
<p>In the preceding code snippet, the <kbd>ComputationGraph</kbd> class is used to instantiate a computation graph, which is a neural network with an arbitrary (that is, a directed, acyclic graph) connection structure. This graph structure may also have an arbitrary number of inputs and outputs.</p>
<pre><strong>LOGGER</strong>.info(preTrainedNet.summary());</pre>
<p>Now, let's take a look at the network architecture including the number of neurons in/out, the parameter shape, and the number of parameters:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/913bd9a8-7cd0-4b01-8494-d4f46594a1ea.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">VGG-16 model architecture as a computational graph</div>
<p>Now that we have the pretrained model, using this, we will predict as many as 1,000 classes. And the trainable parameters are equal to total parameters: 138 million. It is a difficult job to train so many parameters.</p>
<p>Nevertheless, since we need only two classes to be predicted, we need to modify the model architecture slightly such that it outputs only two classes instead of 1,000. So we leave everything unchanged. The modified VGG-16 network will then look like this:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7a85deba-fab7-4f1c-91ec-adacd11c161a.png" style=""/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">From the input to the last fully connected layer (that is, fc2) is freeze</div>
<p>In the preceding diagram, we freeze until the last pooling layer and use initial weights. The green part is the topic of interest that we want to train, so we are going to train only the last layer for the two classes. In other words, in our case, we are going to freeze from the input to the last fully connected layer, which is <kbd>fc2</kbd>. That is, the <kbd>featurizeExtractionLayer</kbd> variable value would be <kbd>fc2</kbd>.</p>
<p>However, before that, let us define some properties such as seed, the number of classes, and up to which layer we want to freeze:</p>
<pre><strong>private </strong><strong>static </strong><strong>final </strong><strong>long </strong><strong>seed</strong> = 12345;<br/><strong>private </strong><strong>static </strong><strong>final</strong><span> String </span><strong>FREEZE_UNTIL_LAYER</strong><span> = "fc2";<br/></span><strong>private </strong><strong>static </strong><strong>final </strong><strong>int </strong><strong>NUM_CLASS</strong><span> = 2;</span></pre>
<p>Then we instantiate the configuration for fine-tuning, which will override the values for all non-frozen layers with the values set here:</p>
<pre><strong>FineTuneConfiguration</strong> fineTuneConf = <strong>new</strong> FineTuneConfiguration.Builder()    <br/>         .optimizationAlgo(OptimizationAlgorithm.<strong>STOCHASTIC_GRADIENT_DESCENT</strong>)<br/>         .updater(<strong>new</strong> Adam(0.001))<br/>         .seed(<strong>seed</strong>)<br/>         .build();</pre>
<div class="title packt_infobox"><strong>FineTuneConfiguration</strong> is the configuration for fine-tuning. Values set in this configuration will override the values in each non-frozen layer. Interested readers can take a look at <a href="https://deeplearning4j.org/doc/org/deeplearning4j/nn/transferlearning/FineTuneConfiguration.html">https://deeplearning4j.org/doc/org/deeplearning4j/nn/transferlearning/FineTuneConfiguration.html</a>.</div>
<p>Then we create a configuration graph that will do the trick: it will work as the transfer learner  using pretrained VGG-16 model:</p>
<pre><strong>ComputationGraph</strong> vgg16Transfer = <strong>new</strong> <strong>TransferLearning</strong>.GraphBuilder(preTrainedNet)<br/>       .fineTuneConfiguration(fineTuneConf)<br/>       .setFeatureExtractor(<strong>FREEZE_UNTIL_LAYER</strong>)<br/>       .removeVertexKeepConnections("predictions")<br/>       .setWorkspaceMode(WorkspaceMode.<strong>SEPARATE</strong>)<br/>       .addLayer("predictions", <strong>new</strong> OutputLayer<br/>                  .Builder(LossFunctions.LossFunction.<strong>NEGATIVELOGLIKELIHOOD</strong>)<br/>                  .nIn(4096).nOut(<strong>NUM_CLASS</strong>)<br/>                  .weightInit(WeightInit.<strong>XAVIER</strong>)<br/>                  .activation(Activation.<strong>SOFTMAX</strong>).build(), <strong>FREEZE_UNTIL_LAYER</strong>)<br/>       .build();<br/>vgg16Transfer.setListeners(<strong>new</strong> ScoreIterationListener(5));<br/><strong>LOGGER</strong>.info(vgg16Transfer.summary());</pre>
<p>The following screenshot shows the output of the previous code snippet:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/55a7cae8-7caf-4ffe-9693-5f9134fd91f5.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The frozen network has only 8,194 trainable parameters</div>
<p>In the preceding code, we removed previously computed predictions and instead used our way so that the modified network predicts only two classes by re-adding a new predictions layer.</p>
<p>In addition, the <kbd>setFeatureExtractor</kbd> method freezes the weights by specifying a layer vertex to set as a feature extractor. Then, the specified layer vertex and the layers on the path from an input vertex to it will be frozen, with the parameters staying constant.</p>
<p>Thus, we are going to train only 8,192 parameters (out of 138 million parameters) from the last layer to the two outputs; two extra parameters are for the biases for two classes. In short, by freezing until the fc2 layer, now the trainable parameters are drastically reduced from 138 million to 8,194 (that is <em>8,192 network params + 2 bias params</em>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Train and test set preparation</h1>
                </header>
            
            <article>
                
<p>Now that we have created a <strong>ComputationGraph</strong>, we need to prepare the training and test sets for the fine-tuning stage. But even before that, we define some parameters, such as allowable format and data paths:</p>
<pre><strong>public </strong><strong>static </strong><strong>final</strong> Random <strong>RAND_NUM_GEN</strong> = <strong>new</strong> Random(<strong><em>seed</em></strong>);<br/><strong>public </strong><strong>static </strong><strong>final</strong> String[] <strong>ALLOWED_FORMATS</strong> = BaseImageLoader.<strong><em>ALLOWED_FORMATS</em></strong>;<br/><strong>public </strong><strong>static</strong> ParentPathLabelGenerator <em>LABEL_GENERATOR_MAKER</em> = <strong>new</strong> ParentPathLabelGenerator();<br/><strong>public </strong><strong>static</strong> BalancedPathFilter <em>PATH_FILTER</em> = <strong>new</strong> BalancedPathFilter(<strong>RAND_NUM_GEN</strong>, <strong>ALLOWED_FORMATS</strong>, LABEL_GENERATOR_MAKER);</pre>
<p>Let's briefly discuss the difference between MultiLayerNetwork and ComputationGraph. In DL4J, there are two types of network composed of multiple layers:</p>
<ul>
<li><strong>MultiLayerNetwork</strong>: A stack of neural network layers we've used so far.</li>
<li><strong>ComputationGraph</strong>: This allows networks to be built with the following features: multiple network input arrays and multiple network outputs (for both classification and regression). In this network type, layers connected with each other using a directed acyclic graph connection structure.</li>
</ul>
<p>Anyway, let's come to the point. Once the params are set, the next task is defining the file paths. Readers should follow this path or show an accurate path during training:</p>
<pre><strong>public </strong><strong>static</strong> String DATA_PATH = "data/DoG_CaT/data";<br/><strong>public </strong><strong>static </strong><strong>final</strong> String <strong>TRAIN_FOLDER</strong> = DATA_PATH + "/train";<br/><strong>public </strong><strong>static </strong><strong>final</strong> String <strong><em>TEST_FOLDER</em></strong> = DATA_PATH + "/test";<br/><strong>File</strong> trainData = <strong>new</strong> File(<strong>TRAIN_FOLDER</strong>);</pre>
<p>Then we will use the <kbd>NativeImageLoader</kbd> class based on the <kbd>JavaCV</kbd> library for loading images, where the allowed formats are <kbd>.bmp</kbd>, <kbd>.gif</kbd>, <kbd>.jpg</kbd>, <kbd>.jpeg</kbd>, <kbd>.jp2</kbd>, <kbd>.pbm</kbd>, <kbd>.pgm</kbd>, <kbd>.ppm</kbd>, <kbd>.pnm</kbd>, <kbd>.png</kbd>, <kbd>.tif</kbd>, <kbd>.tiff</kbd>, <kbd>.exr</kbd>, and <kbd>.webp</kbd>:</p>
<div class="packt_tip"><strong>JavaCV</strong> uses wrappers from the JavaCPP presets of several libraries for computer vision (for example, OpenCV and FFmpeg). More details can be found at <a href="https://github.com/bytedeco/javacv">https://github.com/bytedeco/javacv</a>.</div>
<pre><strong>FileSplit</strong> train = <strong>new</strong> <strong>FileSplit</strong>(trainData, <strong>NativeImageLoader</strong>.<strong>ALLOWED_FORMATS</strong>, <strong>RAND_NUM_GEN</strong>);</pre>
<p>Once the features are extracted from images, we randomly split the features space into 80% for training and the remaining 20% for validating the training itself to prevent overfitting:</p>
<pre><strong>private </strong><strong>static </strong><strong>final </strong><strong>int </strong>TRAIN_SIZE = 80;<br/><strong>InputSplit</strong>[] sample = train.sample(<em>PATH_FILTER</em>, <strong>TRAIN_SIZE</strong>, 100 - <strong>TRAIN_SIZE</strong>);</pre>
<p>In addition, our DL4J network will not be able to consume the data in this format, but we need to convert it to <kbd>DataSetIterator</kbd> format:</p>
<pre><strong>DataSetIterator</strong> trainIterator = getDataSetIterator(sample[0]);<br/><strong>DataSetIterator</strong> devIterator = getDataSetIterator(sample[1]);</pre>
<p>In the preceding lines, we converted both training and validation sets into <kbd>DataSetIterator</kbd> through the <kbd>getDataSetIterator()</kbd> method. The signature of this method can be seen as follows:</p>
<pre><strong>public </strong><strong>static</strong> <strong>DataSetIterator</strong> getDataSetIterator(InputSplit sample) <strong>throws</strong> IOException {<br/>    <strong>ImageRecordReader</strong> imageRecordReader = <strong>new</strong> ImageRecordReader(224, 224, 3, <em>LABEL_GENERATOR_MAKER</em>);<br/>    imageRecordReader.initialize(sample);<br/><br/>    <strong>DataSetIterator</strong> iterator = <strong>new</strong> RecordReaderDataSetIterator(imageRecordReader, <br/>                               BATCH_SIZE, 1, NUM_CLASS);<br/>    iterator.setPreProcessor(<strong>new</strong> VGG16ImagePreProcessor());<br/>    <strong>return</strong> iterator;<br/>}</pre>
<p>Fantastic! Up to this point, we have managed to prepare the training sets. Nevertheless, remember that this will take a while since it has to process 12,500 images.</p>
<p>Now we can start the training. However, you might be wondering why we did not talk about the test set. Well, yes! Definitely we will need the test set, too. However, let's discuss this in the network evaluation step.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Network training and evaluation</h1>
                </header>
            
            <article>
                
<p>Now that training and test sets are prepared, we can start the training. However, before that, we define some hyperparameters for the dataset preparation:</p>
<pre><strong>private </strong><strong>static </strong><strong>final </strong><strong>int </strong>EPOCH = 100;<br/><strong>private </strong><strong>static </strong><strong>final </strong><strong>int </strong>BATCH_SIZE = 128;<br/><strong>private </strong><strong>static </strong><strong>final </strong><strong>int </strong>SAVING_INTERVAL = 100;</pre>
<p>Additionally, we specify the path where the trained model will be saved for future reuse:</p>
<pre><strong>private </strong><strong>static</strong><strong> final</strong> <strong>String</strong> SAVING_PATH = "bin/CatvsDog_VG16_TrainedModel_Epoch100_v1.zip";</pre>
<p>Now we can start training the network. We will do the training combined such that training is carried out with the training set and validation is effected by using the validation set. Finally, the network will evaluate the network performance using the test set. Therefore, for this, we need to prepare the test set too:</p>
<pre><strong>File</strong> testData = <strong>new</strong> File(<strong>TEST_FOLDER</strong>);<br/><strong>FileSplit</strong> test = <strong>new</strong> FileSplit(testData, NativeImageLoader.<strong>ALLOWED_FORMATS</strong>, <strong>RAND_NUM_GEN</strong>);<br/><strong>DataSetIterator</strong> testIterator = <em>getDataSetIterator</em>(test.sample(<em>PATH_FILTER</em>, 1, 0)[0]);</pre>
<p>Then we start the training; we used a batch size of 128 and 100 epochs. Therefore, the first while loop will be executed 100 times. Then, the second inner <kbd>while</kbd> loop will be executed 196 times (25,000 cat and dog images/128):</p>
<pre><strong>int</strong> iEpoch = 0;<br/><strong>int</strong> i = 0;<br/><strong>while</strong> (iEpoch &lt; <strong>EPOCH</strong>) {<br/><strong>    while</strong> (trainIterator.hasNext()) {<br/>        <strong>DataSet</strong> trained = trainIterator.next();<br/>        vgg16Transfer.fit(trained);<br/><strong>        if</strong> (i % <strong>SAVING_INTERVAL</strong> == 0 &amp;&amp; i != 0) {<br/>            <strong>ModelSerializer</strong>.<em>writeModel</em>(vgg16Transfer, <strong>new</strong> File(<strong>SAVING_PATH</strong>), <strong>false</strong>);<br/><em>            evaluateOn</em>(vgg16Transfer, devIterator, i);<br/>        }<br/>        i++;<br/>    }<br/>    trainIterator.reset();<br/>    iEpoch++;<br/>    evaluateOn(vgg16Transfer, testIterator, iEpoch);<br/>}</pre>
<p>This way, we've already tried to make the training faster, but still it might take several hours or even days depending on a number of an epoch that is set. And, if the training is carried out on a CPU rather than GPU, it might take several days. For me, it took 48 hours for 100 epochs. By the way, my machine has a Core i7 processor, 32 GB of RAM, and GeForce GTX 1050 GPU.</p>
<div class="packt_infobox"><span class="packt_screen">Epoch versus iteration</span><br/>
An epoch is a full traversal through the data, and one iteration is one forward and one back propagation on the batch size specified.</div>
<p>Anyway, once the training is complete, the trained model will be saved in the location specified previously. Now let us take a look at how the training went. For this, we will see the performance on the validation set (as stated earlier, we used 15% of the total training set as a validation set, that is, 5,000 images):</p>
<pre class="mce-root">&gt;&gt;&gt;<br/> Cat classified by model as cat: 2444 times<br/> Cat classified by model as dog: 56 times<br/> Dog classified by model as cat: 42 times<br/> Dog classified by model as dog: 2458 times<br/> ==========================Scores==========================<br/> # of classes: 2<br/> Accuracy: 0.9800<br/> Precision: 0.9804<br/> Recall: 0.9806<br/> F1 Score: 0.9800<br/> ========================================================<span class="packt_screen"><br/></span></pre>
<p class="mce-root">Then, when we evaluated our model on a full test set (that is, 12,500 images), I experienced the following performance metrics:<strong><span class="packt_screen"><br/></span></strong></p>
<pre class="mce-root">&gt;&gt;&gt;<br/> Cat classified by model as cat: 6178 times<br/> Cat classified by model as dog: 72 times<br/> Dog classified by model as cat: 261 times<br/> Dog classified by model as dog: 5989 times<br/> ==========================Scores===================<br/> # of classes: 2<br/> Accuracy: 0.9693<br/> Precision: 0.9700<br/> Recall: 0.9693<br/> F1 Score: 0.9688<br/> ==================================================</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Restoring the trained model and inferencing</h1>
                </header>
            
            <article>
                
<p>Now that we have seen how our model performed, it would be worth exploring the feasibility of restoring the already trained model. In other words, we will restore the trained model and evaluate the network performance on both validation and test sets:</p>
<pre><strong>private </strong><strong>static</strong><strong> </strong><strong>final</strong> String TRAINED_PATH_MODEL = "bin/CatvsDog_VG16_TrainedModel_Epoch100_v1.zip";<br/><strong>ComputationGraph</strong> computationGraph = <strong>ModelSerializer</strong>.restoreComputationGraph(<strong>new</strong> File(<strong>TRAINED_PATH_MODEL</strong>));<br/><br/>VG16CatvDogEvaluator().runOnTestSet(computationGraph);<br/>VG16CatvDogEvaluator().runOnValidationSet(computationGraph);</pre>
<p>In the previous line, of code, first, we restored the trained model from the disk; then we performed the evaluation on both the test set (full test set) and validation set (on 20% of the training set).</p>
<p>Now, let's take a look at the signature of the <kbd>runOnTestSet()</kbd> method, which is straightforward, in the sense that we already described a similar workflow in the previous subsection:</p>
<pre><strong>private </strong><strong>void</strong> runOnTestSet(ComputationGraph computationGraph) <strong>throws</strong> IOException {<br/>        <strong>File</strong> trainData = <strong>new</strong> File(TrainCatvsDogVG16.<strong>TEST_FOLDER</strong>);<br/>        <strong>FileSplit</strong> test = <strong>new</strong> FileSplit(trainData, NativeImageLoader.<strong>ALLOWED_FORMATS</strong>,             <br/>                                       TrainCatvsDogVG16.<strong>RAND_NUM_GEN</strong>);<br/><br/>        <strong>InputSplit</strong> inputSplit = test.sample(TrainCatvsDogVG16.<em>PATH_FILTER</em>, 100, 0)[0];<br/>        <strong>DataSetIterator</strong> dataSetIterator = TrainCatvsDogVG16.getDataSetIterator(inputSplit);<br/>        TrainCatvsDogVG16.evaluateOn(computationGraph, dataSetIterator, 1);<br/>}</pre>
<p>Now, let's take a look at the signature of the <kbd>runOnValidationSet</kbd> method:</p>
<pre><strong>private </strong><strong>void</strong> runOnValidationSet(ComputationGraph computationGraph) <strong>throws</strong> IOException {<br/>        <strong>File</strong> trainData = <strong>new</strong> File(TrainCatvsDogVG16.<strong>TRAIN_FOLDER</strong>);<br/>        <strong>FileSplit</strong> test = <strong>new</strong> FileSplit(trainData, NativeImageLoader.<strong>ALLOWED_FORMATS</strong>,     <br/>                                       TrainCatvsDogVG16.<strong>RAND_NUM_GEN</strong>);<br/><br/>        <strong>InputSplit</strong> inputSplit = test.sample(TrainCatvsDogVG16.<em>PATH_FILTER</em>, 15, 80)[0];<br/>        <strong>DataSetIterator</strong> dataSetIterator = TrainCatvsDogVG16.getDataSetIterator(inputSplit);<br/>        TrainCatvsDogVG16.evaluateOn(computationGraph, dataSetIterator, 1);<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Making simple inferencing</h1>
                </header>
            
            <article>
                
<p>Now we have seen that our trained model shows outstanding accuracy on both test and validation sets. So why don't we develop a UI that would help us make the thing easier? As outlined previously, we will develop a simple UI that will allow us to unload a sample image, and then we should be able to detect it through a simple button press. This part is pure Java, so I'm not going to discuss the details here.</p>
<p>If we run the <kbd>PetClassifier.java</kbd> class, it first loads our trained model and acts as the backend deployed the model. Then it calls the <kbd>UI.java</kbd> class to load the user interface, which looks as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/dd775f02-53c6-4827-b0b2-5b3bdc1593f6.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">UI for the cat versus dog recognizer</div>
<p>In the console, you should experience the following logs/messages:</p>
<pre><strong>19:54:52.496 [pool-1-thread-1] INFO org.nd4j.linalg.factory.Nd4jBackend - Loaded [CpuBackend] backend</strong><br/><strong>19:54:52.534 [pool-1-thread-1] WARN org.reflections.Reflections - given scan urls are empty. set urls in the configuration</strong><br/><strong>19:54:52.865 [pool-1-thread-1] INFO org.nd4j.nativeblas.NativeOpsHolder - Number of threads used for NativeOps: 4</strong><br/><strong>19:54:53.249 [pool-1-thread-1] INFO org.nd4j.nativeblas.Nd4jBlas - Number of threads used for BLAS: 4</strong><br/><strong>19:54:53.252 [pool-1-thread-1] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Backend used: [CPU]; OS: [Windows 10]</strong><br/><strong>19:54:53.252 [pool-1-thread-1] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Cores: [8]; Memory: [7.0GB];</strong><br/><strong>19:54:53.252 [pool-1-thread-1] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Blas vendor: [OPENBLAS]</strong><br/><strong>19:55:09.015 [pool-1-thread-1] DEBUG org.reflections.Reflections - going to scan these urls:</strong><br/><strong> ...</strong><br/><strong>9:55:13.394 [pool-1-thread-1] INFO org.deeplearning4j.nn.graph.ComputationGraph - Starting ComputationGraph with WorkspaceModes set to [training: NONE; inference: SEPARATE]</strong><br/><strong>19:55:13.394 [pool-1-thread-1] DEBUG org.reflections.Reflections - going to scan these urls:</strong><br/><strong>19:55:13.779 [pool-1-thread-1] INFO com.packt.JavaDL.DogvCatClassification.UI.UI - Model loaded successfully!</strong></pre>
<p>Now, let's upload a few photos from the test set (it makes more sense since we are reusing the trained model, which is trained to recognize only the training set, so the test set is still unseen):</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/78c5ac06-bae8-46c2-85f3-8f7c5afb8c4b.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Our cat versus dog recognizer recognizes dogs from the images having dogs of different shapes and colors</div>
<p>Therefore, our trained model has been able to recognize dogs having a different shape, size, and color in terms of images. Now, let us try to upload a few cat images and see if it works:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-749 image-border" src="assets/399ca5c8-189f-4585-b8be-d13a23744b0f.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Our cat versus dog recognizer recognizes cats from the images having cats of different shape and colors</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Frequently asked questions (FAQs)</h1>
                </header>
            
            <article>
                
<p>Now that we have solved the dog <span>versus</span> cat classification problem with outstanding accuracy, there are other practical aspects of transfer learning and overall deep learning phenomena that need to be considered too. In this section, we will see some frequently asked questions that might already be on your mind. Answers to these questions can be found in Appendix A.</p>
<ol>
<li>Can I train the model with my own animal images?</li>
<li>Training using all the images is taking too long. What can I do?</li>
<li>Can I wrap up this application as a web app?</li>
<li>Can I use VGG-19 for this task?</li>
<li>How many hyperparameters do we have? I also want to see for each layer.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we solved an interesting dog <span>versus</span> cat classification problem using the transfer learning technique. We used a pre-trained VGG16 model and its weights, and subsequently we fine-tuned the training with a real-life cat <span>versus</span> dog dataset from Kaggle.</p>
<p>Once the training was complete, we saved the trained model for model persistence and subsequent reuse. We saw that the trained model can successfully detect and differentiate both cat and dog images having very different sizes, qualities, and shapes.</p>
<p>Even the trained model/classifier can be used in solving a <span>real-life </span>cat <span>versus</span> dog problem. The takeaway is that this technique with some minimal effort can be extended and used for solving similar image classification problems, which applies to both binary and multiclass classification problems.</p>
<p>In the next chapter, we will see how to develop an end-to-end project that will detect objects from video frames when a video clip plays continuously. We will also see how to utilize a pre-trained <kbd>TinyYOLO</kbd> model, which is a smaller variant of the original YOLOv2 model.</p>
<p>Furthermore, some typical challenges in object detection from both still images and videos will be discussed. Then we will demonstrate how to solve them using bounding box and non-max suppression techniques. Nevertheless, we will see how to process a video clip using the JavaCV library on top of DL4J. Finally, we will see some frequently asked questions that should be useful for adopting and extending this project.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Answers to questions</h1>
                </header>
            
            <article>
                
<p><strong>Answer</strong> <strong>to question 1</strong>: Yes, of course, you can. However, please note that you have to provide a sufficient number of images, preferably at least a few thousand images for each animal type. Otherwise, the model will not be trained well.</p>
<p><strong>Answer</strong> <strong>to question 2</strong>: A possible reason could be you are trying to feed all the images at once or you are training on CPU (and your machine does not have a good configuration). The former can be addressed easily; we can undertake the training in batch mode, which is recommended for the era of deep learning.</p>
<p>The latter case can be addressed by migrating your training from CPU to GPU. However, if your machine does not have a GPU, you can try migrating to Amazon GPU instance to get the support for a single (p2.xlarge) or multiple GPUs (for example, p2.8xlarge).</p>
<p><strong>Answer</strong> <strong>to question 3</strong>: The application provided should be enough to understand the effectiveness of the application. However, this application can still be wrapped up as a web application where the trained model can be served at the backend.</p>
<p>I often use Spring Boot Framework (see more at <a href="https://projects.spring.io/spring-boot/">https://projects.spring.io/spring-boot/</a>) for this purpose. Apart from this, Java CUBA studio can be used too (see <a href="https://www.cuba-platform.com/">https://www.cuba-platform.com/</a>).</p>
<p>As mentioned earlier in this chapter, VGG-16 is a small variant of VGG-19. Unfortunately, there is no way to use VGG-19 directly. However, readers can try to load VGG-19 can be imported with Keras import.</p>
<p><strong>Answer to question 6:</strong> Just use the following code immediately after the network initialization:</p>
<pre>//Print the number of parameters in the network (and for each layer)<br/><strong>Layer</strong>[] layers = model.getLayers();<br/><strong>int</strong> totalNumParams = 0;<br/><br/><strong>for</strong>( <strong>int</strong> i=0; i&lt;layers.length; i++ ){<br/>         <strong>int</strong> nParams = layers[i].numParams();<br/>         System.<strong><em>out</em></strong>.println("Number of parameters in layer " + i + ": " + nParams);<br/>         totalNumParams += nParams;<br/>}<br/>System.<strong><em>out</em></strong>.println("Total number of network parameters: " + totalNumParams);</pre>
<pre><strong>&gt;&gt;&gt;</strong><br/><strong> Number of parameters in layer 0: 1792</strong><br/><strong> Number of parameters in layer 1: 36928</strong><br/><strong> Number of parameters in layer 2: 0</strong><br/><strong> Number of parameters in layer 3: 73856</strong><br/><strong> Number of parameters in layer 4: 147584</strong><br/><strong> Number of parameters in layer 5: 0</strong><br/><strong> Number of parameters in layer 6: 295168</strong><br/><strong> Number of parameters in layer 7: 590080</strong><br/><strong> Number of parameters in layer 8: 590080</strong><br/><strong> Number of parameters in layer 9: 0</strong><br/><strong> Number of parameters in layer 10: 1180160</strong><br/><strong> Number of parameters in layer 11: 2359808</strong><br/><strong> Number of parameters in layer 12: 2359808</strong><br/><strong> Number of parameters in layer 13: 0</strong><br/><strong> Number of parameters in layer 14: 2359808</strong><br/><strong> Number of parameters in layer 15: 2359808</strong><br/><strong> Number of parameters in layer 16: 2359808</strong><br/><strong> Number of parameters in layer 17: 0</strong><br/><strong> Number of parameters in layer 18: 102764544</strong><br/><strong> Number of parameters in layer 19: 16781312</strong><br/><strong> Number of parameters in layer 20: 8194</strong><br/><strong> Total number of network parameters: 134268738</strong></pre>


            </article>

            
        </section>
    </body></html>