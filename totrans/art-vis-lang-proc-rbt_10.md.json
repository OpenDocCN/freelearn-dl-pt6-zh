["```py\nfrom math import pi\ndef wheel_distance(diameter, encoder, encoder_time, wheel, movement_time):\n    time = movement_time / encoder_time\n    wheel_encoder = wheel * time\n    wheel_distance = (wheel_encoder * diameter * pi) / encoder\n\n    return wheel_distance\nfrom math import cos,sin\ndef final_position(initial_pos,wheel_axis,angle):\n    final_x=initial_pos[0]+(wheel_axis*cos(angle))\n    final_y=initial_pos[1]+(wheel_axis*sin(angle))\n    final_angle=initial_pos[2]+angle\n\n    return(final_x,final_y,final_angle)\ndef position(diameter,base,encoder,encoder_time,left,right,initial_pos,movement_time):\n#First step: Wheels completed distance\n    left_wheel=wheel_distance(diameter,encoder,encoder_time,left,movement_time)\n    right_wheel=wheel_distance(diameter,encoder,encoder_time,right,movement_time)\n#Second step: Wheel's central axis completed distance\n    wheel_axis=(left_wheel+right_wheel)/2\n#Third step: Robot's rotation angle\n    angle=(right_wheel-left_wheel)/base\n#Final step: Final position calculus\n    final_pos=final_position(initial_pos,wheel_axis,angle)\n\n    returnfinal_pos\nposition(10,80,76,5,600,900,(0,0,0),5)\n```", "```py\n    from google.colab import drive\n    drive.mount('/content/drive')\n    ```", "```py\n    cd /content/drive/My Drive/C13550/Lesson02/Activity02/\n    ```", "```py\n    from keras.datasets import fashion_mnist\n    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n    ```", "```py\n    import random\n    from sklearn import metrics\n    from sklearn.utils import shuffle\n    random.seed(42)\n    from matplotlib import pyplot as plt\n    for idx in range(5):\n        rnd_index = random.randint(0, 59999)\n        plt.subplot(1,5,idx+1),plt.imshow(x_train[idx],'gray')\n        plt.xticks([]),plt.yticks([])\n    plt.show()\n    ```", "```py\n    import numpy as np\n    from keras import utils as np_utils\n    x_train = (x_train.astype(np.float32))/255.0\n    x_test = (x_test.astype(np.float32))/255.0\n    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n    y_train = np_utils.to_categorical(y_train, 10)\n    y_test = np_utils.to_categorical(y_test, 10)\n    input_shape = x_train.shape[1:]\n    ```", "```py\n    from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n    from keras.layers import Input, Dense, Dropout, Flatten\n    from keras.preprocessing.image import ImageDataGenerator\n    from keras.layers import Conv2D, MaxPooling2D, Activation, BatchNormalization\n    from keras.models import Sequential, Model\n    from keras.optimizers import Adam, Adadelta\n    def DenseNN(inputh_shape):\n        model = Sequential()\n        model.add(Dense(128, input_shape=input_shape))\n        model.add(BatchNormalization())\n        model.add(Activation('relu'))\n        model.add(Dropout(0.2))\n        model.add(Dense(128))\n        model.add(BatchNormalization())\n        model.add(Activation('relu'))\n        model.add(Dropout(0.2))\n        model.add(Dense(64))\n        model.add(BatchNormalization())\n        model.add(Activation('relu'))\n        model.add(Dropout(0.2))\n        model.add(Flatten())\n        model.add(Dense(64))\n        model.add(BatchNormalization())\n        model.add(Activation('relu'))\n        model.add(Dropout(0.2))\n        model.add(Dense(10, activation=\"softmax\"))\n        return model\n    model = DenseNN(input_shape)\n    ```", "```py\n    optimizer = Adadelta()\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    ckpt = ModelCheckpoint('model.h5', save_best_only=True,monitor='val_loss', mode='min', save_weights_only=False)\n    model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(x_test, y_test), callbacks=[ckpt])\n    ```", "```py\n    import cv2\n    images = ['ankle-boot.jpg', 'bag.jpg', 'trousers.jpg', 't-shirt.jpg']\n    for number in range(len(images)):\n        imgLoaded = cv2.imread('Dataset/testing/%s'%(images[number]),0)\n        img = cv2.resize(imgLoaded, (28, 28))\n        img = np.invert(img)\n    cv2.imwrite('test.jpg',img)\n        img = (img.astype(np.float32))/255.0\n        img = img.reshape(1, 28, 28, 1)\n        plt.subplot(1,5,number+1),plt.imshow(imgLoaded,'gray')\n        plt.title(np.argmax(model.predict(img)[0]))\n        plt.xticks([]),plt.yticks([])\n    plt.show()\n    ```", "```py\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn.decomposition import TruncatedSVD\n    ```", "```py\n    docs = []\n    ndocs = [\"doc1\", \"doc2\", \"doc3\"]\n    for n in ndocs:\n        aux = open(\"dataset/\"+ n +\".txt\", \"r\", encoding=\"utf8\")\n        docs.append(aux.read())\n    ```", "```py\n    import spacy\n    import en_core_web_sm\n    from spacy.lang.en.stop_words import STOP_WORDS\n    nlp = en_core_web_sm.load()\n    nlp.vocab[\"\\n\\n\"].is_stop = True\n    nlp.vocab[\"\\n\"].is_stop = True\n    nlp.vocab[\"the\"].is_stop = True\n    nlp.vocab[\"The\"].is_stop = True\n    newD = []\n    for d, i in zip(docs, range(len(docs))):\n        doc = nlp(d)\n        tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]\n        newD.append(' '.join(tokens))\n    ```", "```py\n    vectorizer = TfidfVectorizer(use_idf=True, \n                                ngram_range=(1,2), \n                                smooth_idf=True,\n                                max_df=0.5)\n    X = vectorizer.fit_transform(newD)\n    ```", "```py\n    lsa = TruncatedSVD(n_components=100,algorithm='randomized',n_iter=10,random_state=0)\n    lsa.fit_transform(X)\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    dic1 = {\"Terms\": terms, \"Components\": lsa.components_[0]}\n    dic2 = {\"Terms\": terms, \"Components\": lsa.components_[1]}\n    dic3 = {\"Terms\": terms, \"Components\": lsa.components_[2]}\n    f1 = pd.DataFrame(dic1)\n    f2 = pd.DataFrame(dic2)\n    f3 = pd.DataFrame(dic3)\n    f1.sort_values(by=['Components'], ascending=False)\n    f2.sort_values(by=['Components'], ascending=False)\n    f3.sort_values(by=['Components'], ascending=False)\n    ```", "```py\n    import tensorflow as tf\n    from keras.models import Sequential\n    from keras.layers import LSTM, Dense, Activation, LeakyReLU\n    import numpy as np\n    ```", "```py\n    char_seq = 'qwertyuiopasdfghjklñzxcvbnm' * 100\n    char_seq = list(char_seq)\n    ```", "```py\n    char2id = dict([(char, idx) for idx, char in enumerate(set(char_seq))])\n    ```", "```py\n    maxlen = 5\n    sequences = []\n    next_char = []\n\n    for i in range(0,len(char_seq)-maxlen):\n        sequences.append(char_seq[i:i+maxlen])\n        next_char.append(char_seq[i+maxlen])\n\n    y_labels = len(char2id)\n    print(\"5 first sequences: {}\".format(sequences[:5]))\n    print(\"5 first next characters: {}\".format(next_char[:5]))\n    print(\"Total sequences: {}\".format(len(sequences)))\n    print(\"Total output labels: {}\".format(y_labels))\n    ```", "```py\n    def one_hot_encoder(seq, ids):\n        encoded_seq = np.zeros([len(seq),len(ids)])\n        for i,s in enumerate(seq):\n            encoded_seq[i][ids[s]] = 1\n        return encoded_seq\n    ```", "```py\n    x = np.array([one_hot_encoder(item, char2id) for item in sequences])\n    y = np.array(one_hot_encoder(next_char, char2id))\n    x = x.astype(np.int32)\n    y = y.astype(np.int32)\n\n    print(\"Shape of x: {}\".format(x.shape))\n    print(\"Shape of y: {}\".format(y.shape))\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=False)\n    print('x_train shape: {}'.format(x_train.shape)) \n    print('y_train shape: {}'.format(y_train.shape))  \n    print('x_test shape: {}'.format(x_test.shape)) \n    print('y_test shape: {}'.format(y_test.shape))\n    ```", "```py\n    model = Sequential()\n    model.add(LSTM(8,input_shape=(maxlen,y_labels)))\n    model.add(Dense(y_labels))\n    model.add(LeakyReLU(alpha=.01)) \n\n    model.compile(loss='mse', optimizer='rmsprop')\n    ```", "```py\n    history = model.fit(x_train, y_train, batch_size=32, epochs=25, verbose=1)\n    ```", "```py\n    print('MSE: {:.5f}'.format(model.evaluate(x_test, y_test)))\n    ```", "```py\n    prediction = model.predict(x_test)\n\n    errors = 0\n    for pr, res in zip(prediction, y_test):\n        if not np.array_equal(np.around(pr),res):\n            errors+=1\n\n    print(\"Errors: {}\".format(errors))\n    print(\"Hits: {}\".format(len(prediction) - errors))\n    print(\"Hit average: {}\".format((len(prediction) - errors)/len(prediction)))\n    ```", "```py\n    def decode(vec):\n        val = np.argmax(vec)\n        return list(char2id.keys())[list(char2id.values()).index(val)]\n    ```", "```py\n    def pred_seq(seq):\n        seq = list(seq)\n        x = one_hot_encoder(seq,char2id)\n        x = np.expand_dims(x, axis=0)\n        prediction = model.predict(x, verbose=0)\n        return decode(list(prediction[0]))\n    ```", "```py\n    pred_seq('tyuio')\n    ```", "```py\n    import numpyasnp\n    classes=['daisy','dandelion','rose','sunflower','tulip']\n    X=np.load(\"Dataset/flowers/%s_x.npy\"%(classes[0]))\n    y=np.load(\"Dataset/flowers/%s_y.npy\"%(classes[0]))\n    print(X.shape)\n    forflowerinclasses[1:]:\n        X_aux=np.load(\"Dataset/flowers/%s_x.npy\"%(flower))\n        y_aux=np.load(\"Dataset/flowers/%s_y.npy\"%(flower))\n        print(X_aux.shape)\n        X=np.concatenate((X,X_aux),axis=0)\n        y=np.concatenate((y,y_aux),axis=0)\n\n    print(X.shape)\n    print(y.shape)\n    ```", "```py\n    import random \n    random.seed(42) \n    from matplotlib import pyplot as plt\n    import cv2\n    for idx in range(5): \n        rnd_index = random.randint(0, 4000) \n        plt.subplot(1,5,idx+1),plt.imshow(cv2.cvtColor(X[rnd_index],cv2.COLOR_BGR2RGB)) \n        plt.xticks([]),plt.yticks([])\n        plt.savefig(\"flowers_samples.jpg\", bbox_inches='tight')\n    plt.show() \n    ```", "```py\n    from keras import utils as np_utils\n    X = (X.astype(np.float32))/255.0 \n    y = np_utils.to_categorical(y, len(classes))\n    print(X.shape)\n    print(y.shape)\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n    input_shape = x_train.shape[1:]\n    print(x_train.shape)\n    print(y_train.shape)\n    print(x_test.shape)\n    print(y_test.shape)\n    print(input_shape)\n    ```", "```py\n    from keras.models import Sequential\n    from keras.callbacks import ModelCheckpoint\n    from keras.layers import Input, Dense, Dropout, Flatten\n    from keras.layers import Conv2D, Activation, BatchNormalization\n    def CNN(input_shape):\n        model = Sequential()\n\n        model.add(Conv2D(32, kernel_size=(5, 5), padding='same',  strides=(2,2), input_shape=input_shape))\n        model.add(Activation('relu')) \n        model.add(BatchNormalization()) \n        model.add(Dropout(0.2))\n        model.add(Conv2D(64, kernel_size=(3, 3), padding='same', strides=(2,2))) \n        model.add(Activation('relu')) \n        model.add(BatchNormalization()) \n        model.add(Dropout(0.2))\n        model.add(Conv2D(128, kernel_size=(3, 3), padding='same', strides=(2,2))) \n        model.add(Activation('relu')) \n        model.add(BatchNormalization()) \n        model.add(Dropout(0.2))\n\n        model.add(Conv2D(256, kernel_size=(3, 3), padding='same', strides=(2,2))) \n        model.add(Activation('relu')) \n        model.add(BatchNormalization()) \n        model.add(Dropout(0.2))\n\n        model.add(Flatten())\n        model.add(Dense(512))\n        model.add(Activation('relu'))\n        model.add(BatchNormalization())\n        model.add(Dropout(0.5))\n        model.add(Dense(5, activation = \"softmax\"))\n        return model\n    ```", "```py\n    from keras.preprocessing.image import ImageDataGenerator\n    datagen = ImageDataGenerator(\n            rotation_range=10,\n            zoom_range = 0.2,\n            width_shift_range=0.2,\n            height_shift_range=0.2,\n            shear_range=0.1,\n            horizontal_flip=True\n            )\n    ```", "```py\n    datagen.fit(x_train)\n    model = CNN(input_shape)\n    model.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\n    ckpt = ModelCheckpoint('Models/model_flowers.h5', save_best_only=True,monitor='val_loss', mode='min', save_weights_only=False) \n    //{…}##the detailed code can be found on Github##\n    model.fit_generator(datagen.flow(x_train, y_train,\n                                batch_size=32),\n                        epochs=200,\n                        validation_data=(x_test, y_test),\n                        callbacks=[ckpt],\n                        steps_per_epoch=len(x_train) // 32,\n                        workers=4)\n    ```", "```py\n    from sklearn import metrics\n    model.load_weights('Models/model_flowers.h5')\n    y_pred = model.predict(x_test, batch_size=32, verbose=0)\n    y_pred = np.argmax(y_pred, axis=1)\n    y_test_aux = y_test.copy()\n    y_test_pred = list()\n    for i in y_test_aux:\n        y_test_pred.append(np.argmax(i))\n    //{…}\n    ##the detailed code can be found on Github##\n    print (y_pred)\n    # Evaluate the prediction\n    accuracy = metrics.accuracy_score(y_test_pred, y_pred)\n    print('Acc: %.4f' % accuracy)\n    ```", "```py\n    classes = ['daisy','dandelion','rose','sunflower','tulip']\n    images = ['sunflower.jpg','daisy.jpg','rose.jpg','dandelion.jpg','tulip .jpg']\n    model.load_weights('Models/model_flowers.h5')\n    for number in range(len(images)):\n        imgLoaded = cv2.imread('Dataset/testing/%s'%(images[number])) \n        img = cv2.resize(imgLoaded, (150, 150)) \n        img = (img.astype(np.float32))/255.0 \n        img = img.reshape(1, 150, 150, 3)\n        plt.subplot(1,5,number+1),plt.imshow(cv2.cvtColor(imgLoaded,cv2.COLOR_BGR2RGB)) \n        plt.title(np.argmax(model.predict(img)[0])) \n        plt.xticks([]),plt.yticks([]) \n    plt.show()\n    ```", "```py\n    cd ~/catkin_ws/src\n    catkin_create_pkg activity1 rospy sensor_msgs\n    cd  activity1\n    mkdir scripts\n    cd scripts\n    touch observer.py\n    touch movement.py\n    chmod +x observer.py\n    chmod +x movement.py\n    ```", "```py\n    #!/usr/bin/env python\n    import rospy\n    from sensor_msgs.msg import Image\n    import cv2\n    from cv_bridge import CvBridge\n    class Observer:\n        bridge = CvBridge()\n        counter = 0\n        def callback(self, data):\n            if self.counter == 20:\n                cv_image = self.bridge.imgmsg_to_cv2(data, \"bgr8\")\n                cv2.imshow('Image',cv_image)\n                cv2.waitKey(1000)\n                cv2.destroyAllWindows()\n                self.counter = 0\n            else:\n                self.counter += 1\n        def observe(self):\n            rospy.Subscriber('/camera/rgb/image_raw', Image, self.callback)\n            rospy.init_node('observer', anonymous=True)\n            rospy.spin()\n    if __name__ == '__main__':\n        obs = Observer()\n        obs.observe()\n    ```", "```py\n    #!/usr/bin/env python\n    import rospy\n    from geometry_msgs.msg import Twist\n    def move():\n        pub = rospy.Publisher('/mobile_base/commands/velocity', Twist, queue_size=1)\n        rospy.init_node('movement', anonymous=True)\n        move = Twist()\n        move.angular.z = 0.5\n        rate = rospy.Rate(10)\n        while not rospy.is_shutdown():\n            pub.publish(move)\n            rate.sleep()\n    if __name__ == '__main__':\n        try:\n            move()\n        except rospy.ROSInterruptException:\n            pass\n    ```", "```py\n    cd ~/catkin_ws\n    source devel/setup.bash\n    roscore\n    roslaunch turtlebot_gazebo turtlebot_world.launch\n    rosrun activity1 observer.py\n    rosrun activity1 movement.py\n    ```", "```py\n    from google.colab import drive\n    drive.mount('/content/drive')\n    ```", "```py\n    cd /content/drive/My Drive/C13550/Lesson07/Activity01\n    ```", "```py\n    from chatbot_intro import *\n    ```", "```py\n    filename = '../utils/glove.6B.50d.txt.word2vec'\n    model = KeyedVectors.load_word2vec_format(filename, binary=False)\n    ```", "```py\n    intent_route = 'training/'\n    response_route = 'responses/'\n    intents = listdir(intent_route)\n    responses = listdir(response_route)\n    print(\"Documents: \", intents)\n    ```", "```py\n    doc_vectors = np.empty((0,50), dtype='f')\n    for i in intents:\n        with open(intent_route + i) as f:\n            sentences = f.readlines()\n        sentences = [x.strip() for x in sentences]\n        sentences = pre_processing(sentences)\n    doc_vectors= np.append(doc_vectors,doc_vector(sentences,model),axis=0)\n    print(\"Shape of doc_vectors:\",doc_vectors.shape)\n    print(\" Vector representation of backward.txt:\\n\",doc_vectors)\n    ```", "```py\n    user_sentence = \"Look to the right\"\n    user_sentence = pre_processing([user_sentence])\n    user_vector = doc_vector(user_sentence,model).reshape(100,)\n    intent = intents[select_intent(user_vector, doc_vectors)]\n    intent\n    ```", "```py\n    from google.colab import drive\n    drive.mount('/content/drive')\n    cd /content/drive/My Drive/C13550/Lesson08/\n    ```", "```py\n    pip3 install https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.2/imageai-2.0.2-py3-none-any.whl\n    ```", "```py\n    from imageai.Detection import VideoObjectDetection\n    from matplotlib import pyplot as plt\n    ```", "```py\n    video_detector = VideoObjectDetection()\n    video_detector.setModelTypeAsYOLOv3()\n    video_detector.setModelPath(\"Models/yolo.h5\")\n    video_detector.loadModel()\n    ```", "```py\n    color_index = {'bus': 'red', 'handbag': 'steelblue', 'giraffe': 'orange', 'spoon': 'gray', 'cup': 'yellow', 'chair': 'green', 'elephant': 'pink', 'truck': 'indigo', 'motorcycle': 'azure', 'refrigerator': 'gold', 'keyboard': 'violet', 'cow': 'magenta', 'mouse': 'crimson', 'sports ball': 'raspberry', 'horse': 'maroon', 'cat': 'orchid', 'boat': 'slateblue', 'hot dog': 'navy', 'apple': 'cobalt', 'parking meter': 'aliceblue', 'sandwich': 'skyblue', 'skis': 'deepskyblue', 'microwave': 'peacock', 'knife': 'cadetblue', 'baseball bat': 'cyan', 'oven': 'lightcyan', 'carrot': 'coldgrey', 'scissors': 'seagreen', 'sheep': 'deepgreen', 'toothbrush': 'cobaltgreen', 'fire hydrant': 'limegreen', 'remote': 'forestgreen', 'bicycle': 'olivedrab', 'toilet': 'ivory', 'tv': 'khaki', 'skateboard': 'palegoldenrod', 'train': 'cornsilk', 'zebra': 'wheat', 'tie': 'burlywood', 'orange': 'melon', 'bird': 'bisque', 'dining table': 'chocolate', 'hair drier': 'sandybrown', 'cell phone': 'sienna', 'sink': 'coral', 'bench': 'salmon', 'bottle': 'brown', 'car': 'silver', 'bowl': 'maroon', 'tennis racket': 'palevilotered', 'airplane': 'lavenderblush', 'pizza': 'hotpink', 'umbrella': 'deeppink', 'bear': 'plum', 'fork': 'purple', 'laptop': 'indigo', 'vase': 'mediumpurple', 'baseball glove': 'slateblue', 'traffic light': 'mediumblue', 'bed': 'navy', 'broccoli': 'royalblue', 'backpack': 'slategray', 'snowboard': 'skyblue', 'kite': 'cadetblue', 'teddy bear': 'peacock', 'clock': 'lightcyan', 'wine glass': 'teal', 'frisbee': 'aquamarine', 'donut': 'mincream', 'suitcase': 'seagreen', 'dog': 'springgreen', 'banana': 'emeraldgreen', 'person': 'honeydew', 'surfboard': 'palegreen', 'cake': 'sapgreen', 'book': 'lawngreen', 'potted plant': 'greenyellow', 'toaster': 'ivory', 'stop sign': 'beige', 'couch': 'khaki'}\n    def forFrame(frame_number, output_array, output_count, returned_frame):\n        plt.clf()\n        this_colors = []\n        labels = []\n        sizes = []\n        counter = 0\n        for eachItem in output_count:\n            counter += 1\n            labels.append(eachItem + \" = \" + str(output_count[eachItem]))\n            sizes.append(output_count[eachItem])\n            this_colors.append(color_index[eachItem])\n        plt.subplot(1, 2, 1)\n        plt.title(\"Frame : \" + str(frame_number))\n        plt.axis(\"off\")\n        plt.imshow(returned_frame, interpolation=\"none\")\n        plt.subplot(1, 2, 2)\n        plt.title(\"Analysis: \" + str(frame_number))\n        plt.pie(sizes, labels=labels, colors=this_colors, shadow=True, startangle=140, autopct=\"%1.1f%%\")\n        plt.pause(0.01)\n    ```", "```py\n    plt.show()\n    video_detector.detectObjectsFromVideo(input_file_path=\"Dataset/videos/street.mp4\", output_file_path=\"output-video\" ,  frames_per_second=20, per_frame_function=forFrame,  minimum_percentage_probability=30, return_detected_frame=True, log_progress=True)\n    ```", "```py\n    cd ~/catkin_ws/\n    source devel/setup.bash\n    roscore\n    cd src\n    catkin_create_pkg activity1 rospy cv_bridge geometry_msgs image_transport sensor_msgs std_msgs\n    ```", "```py\n    cd activity1\n    mkdir scripts\n    cd scripts\n    touch activity.py\n    touch activity_sub.py\n    chmod +x activity.py\n    chmod +x activity_sub.py\n    ```", "```py\n    #!/usr/bin/env python\n    import rospy\n    import cv2\n    import sys\n    import os\n    from cv_bridge import CvBridge, CvBridgeError\n    from sensor_msgs.msg import Image\n    from std_msgs.msg import String\n    sys.path.append(os.path.join(os.getcwd(), '/home/alvaro/Escritorio/tfg/darknet/python/'))\n    import darknet as dn\n    ```", "```py\n    class Activity():\n        def __init__(self):\n    ```", "```py\n            rospy.init_node('Activity', anonymous=True)\n            self.bridge = CvBridge()\n            self.image_sub = rospy.Subscriber(\"camera/rgb/image_raw\", Image, self.imageCallback)\n            self.pub = rospy.Publisher('yolo_topic', String, queue_size=10)\n            self.imageToProcess = None\n            cfgPath =  \"/home/alvaro/Escritorio/tfg/darknet/cfg/yolov3.cfg\"\n            weightsPath = \"/home/alvaro/Escritorio/tfg/darknet/yolov3.weights\"\n            dataPath = \"/home/alvaro/Escritorio/tfg/darknet/cfg/coco2.data\"\n            self.net = dn.load_net(cfgPath, weightsPath, 0)\n            self.meta = dn.load_meta(dataPath)\n            self.fileName = 'predict.jpg'\n            self.rate = rospy.Rate(10)\n    ```", "```py\n        def imageCallback(self, data):\n            self.imageToProcess = self.bridge.imgmsg_to_cv2(data, \"bgr8\")\n    ```", "```py\n        def run(self): \n            print(\"The robot is recognizing objects\")\n            while not rospy.core.is_shutdown():\n                if(self.imageToProcess is not None):\n                    cv2.imwrite(self.fileName, self.imageToProcess)\n    ```", "```py\n                    r = dn.detect(self.net, self.meta, self.fileName)\n                    objects = \"\"\n                    for obj in r:\n                        objects += obj[0] + \" \"\n    ```", "```py\n                    self.pub.publish(objects)\n                    self.rate.sleep()\n    ```", "```py\n    if __name__ == '__main__':\n        dn.set_gpu(0)\n        node = Activity()\n        try:\n            node.run()\n        except rospy.ROSInterruptException:\n            pass\n    ```", "```py\n    #!/usr/bin/env python\n    import rospy\n    from std_msgs.msg import String\n    ```", "```py\n    class ActivitySub():\n        yolo_data = \"\"\n\n        def __init__(self):\n    ```", "```py\n            rospy.init_node('ThiefDetector', anonymous=True)\n            rospy.Subscriber(\"yolo_topic\", String, self.callback)\n\n    ```", "```py\n        def callback(self, data):\n            self.yolo_data = data\n        def run(self):\n            while True:\n    ```", "```py\n                if \"person\" in str(self.yolo_data):\n                    print(\"ALERT: THIEF DETECTED\")\n                    break\n    ```", "```py\n    if __name__ == '__main__':\n        node = ActivitySub()\n        try:\n            node.run()\n        except rospy.ROSInterruptException:\n            pass\n    ```", "```py\n    cd ../../\n    cd ..\n    cd src/activity1/scripts/\n    ```", "```py\n    touch movement.py\n    chmod +x movement.py\n    cd ~/catkin_ws\n    source devel/setup.bash\n    roslaunch turtlebot_gazebo turtlebot_world.launch\n    ```", "```py\n    cd ~/catkin_ws\n    source devel/setup.bash\n    rosrun activity1 activity.py\n    cd ~/catkin_ws\n    source devel/setup.bash\n    rosrun activity1 activity_sub.py\n    cd ~/catkin_ws\n    source devel/setup.bash\n    rosrun activity1 movement.py\n    ```"]