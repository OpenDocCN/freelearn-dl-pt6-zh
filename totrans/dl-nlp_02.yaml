- en: '*Chapter 2*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Applications of Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Describe POS tagging and its applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differentiate between rule-based and stochastic POS taggers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform POS tagging, chunking, and chinking on text data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform named entity recognition for information extraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop and train your own POS tagger and named entity recognizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use NLTK and spaCy to perform POS tagging, chunking, chinking, and named entity
    recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter aims to introduce you to the plethora of applications of NLP and
    the various techniques involved within.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This chapter begins with a quick recap of what natural language processing
    is and what services it can help provide. Then, it discusses two applications
    of natural language processing: **Parts of Speech Tagging (POS tagging)** and
    **Named Entity Recognition**. The functioning, necessity, and purposes of both
    of these algorithms are explained. Additionally, there are exercises and activities
    that perform POS tagging and named entity recognition and build and develop these
    algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Natural language processing consists of aiding machines to understand the natural
    language of humans in order to communicate with them effectively and automate
    a large number of tasks. The previous chapter discussed the applications of natural
    language processing along with examples of real-life use cases where these techniques
    could simplify the lives of humans. This chapter will specifically look into two
    of these algorithms and their real-life applications.
  prefs: []
  type: TYPE_NORMAL
- en: Every aspect of natural language processing can be seen to follow the same analogy
    of teaching a language. In the last chapter, we saw how machines need to be told
    what parts of a corpus to pay attention to and what parts are irrelevant and unimportant.
    They need to be trained to remove stop words and noisy elements and focus on key
    words to reduce various forms of the same word to the word's root form so that
    it's easier to search for and interpret. In a similar fashion, the two algorithms
    discussed in this chapter also teach machines particular things about languages
    in the way we humans have been taught.
  prefs: []
  type: TYPE_NORMAL
- en: POS Tagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we dive straight into the algorithm, let's understand what parts of speech
    are. Parts of speech are something most of us are taught in our early years of
    learning the English language. They are categories assigned to words based on
    their syntactic or grammatical functions. These functions are the functional relationships
    that exist between different words.
  prefs: []
  type: TYPE_NORMAL
- en: Parts of Speech
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The English language has nine main parts of speech:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Nouns*: Things or people'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples: table, dog, piano, London, towel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Pronouns*: Words that replace nouns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples: I, you, he, she, it'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Verbs*: Action words'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples: to be, to have, to study, to learn, to play'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Adjectives*: Words that describe nouns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples: intelligent, small, silly, intriguing, blue'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Determiners*: Words that limit nouns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples: a few, many, some, three'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: For more examples of determiners, visit https://www.ef.com/in/english-resources/english-grammar/determiners/.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Adverbs*: Words that describe verbs, adjectives, or adverbs themselves'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples: quickly, shortly, very, really, drastically'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Prepositions*: Words that link nouns to other words'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples: to, on, in, under, beside'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Conjunctions*: Words that join two sentences or words'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples: and, but, yet'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Interjections*: Words that are exclamations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples: ouch! Ow! Wow!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, each word falls under a specific Parts of speech tag assigned
    to it that helps us understand the meaning and purpose of the word, enabling us
    to better understand the context in which it is being used.
  prefs: []
  type: TYPE_NORMAL
- en: POS Tagger
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: POS tagging is the process of assigning a tag to a word. This is done by an
    algorithm known as a POS tagger. The aim of the algorithm is really just as simple
    as this.
  prefs: []
  type: TYPE_NORMAL
- en: Most POS taggers are supervised learning algorithms. If you don't remember what
    supervised learning algorithms are, they are machine learning algorithms that
    learn to perform a task based on previously labeled data. The algorithms take
    rows of data as input. This data contains feature columns—data used to predict
    something—and usually one label column—the something that needs to be predicted.
    The models are trained on this input to learn and understand what features correspond
    to which label, thus learning how to perform the task of predicting the labels.
    Ultimately, they are given unlabeled data (data that just consists of feature
    columns), for which they must predict labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is a general illustration of a supervised learning model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fig 2.1: Supervised learning'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_02_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig 2.1: Supervised learning'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For more information on supervised learning, go to https://www.packtpub.com/big-data-and-business-intelligence/applied-supervised-learning-python.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, POS taggers hone their predictive abilities by learning from previously
    labeled datasets. In this case, the datasets can consist of a variety of features,
    such as the word itself (obviously), the definition of the word, the relationships
    of the word with its preceding, proceeding, and other related word(s) that are
    present within the same sentence, phrase, or paragraph. These features together
    help the tagger predict what POS tag should be assigned to a word. The corpus
    used to train a supervised POS tagger is known as a pre-tagged corpus. Such corpora
    serve as the basis for the creation of a system for the POS tagger to tag untagged
    words. These systems/types of POS taggers will be discussed in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-tagged corpora, however, are not always readily available, and to accurately
    train a tagger, the corpus must be large. Thus, recently there have been iterations
    of the POS tagger that can be considered as unsupervised learning algorithms.
    These are algorithms that take data consisting solely of features as input. These
    features aren't associated with labels and thus the algorithm, instead of predicting
    labels, forms groups or clusters of the input data.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of POS tagging, the models use computational methods to automatically
    generate sets of POS tags. While, pre-tagged corpora are responsible for aiding
    the process of creating a system for the tagger in the case of supervised POS
    taggers, with unsupervised POS taggers, these computational methods serve as the
    basis for the creation of such systems. The drawback of unsupervised learning
    methods is that the cluster of POS tags generated automatically may not always
    be as accurate as those found in the pre-tagged corpora used to train supervised
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, the key differences between supervised and unsupervised learning
    methods are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised POS taggers take pre-tagged corpora as input to be trained, while
    unsupervised POS taggers take untagged corpora as input to create a set of POS
    tags.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised POS taggers create dictionaries of words with their respective POS
    tags based on the tagged corpora, while unsupervised POS taggers generate these
    dictionaries using the self-created POS tag set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Several Python libraries (such as NLTK and spaCy) have trained POS taggers
    of their own. You will learn how to use one in the following sections, but let''s
    understand the input and output of a POS tagger with an example for now. An important
    thing to remember is that since a POS tagger assigns a POS tag to each word in
    the given corpus, the input needs to be in the form of word tokens. Therefore,
    before performing POS tagging, tokenization needs to be carried out on the corpus.
    Let''s say we give the trained POS tagger the following tokens as an input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After POS tagging, the output would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, **PRO** = pronoun, **V** = verb, **DT** = determiner, and **N** = noun.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input and output for both a trained supervised and unsupervised POS tagger
    are the same: tokens, and tokens with POS tags, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is not the exact syntax of the output; you'll see the proper output later
    when you perform the exercise. This is just to give you an idea of what POS taggers
    do.
  prefs: []
  type: TYPE_NORMAL
- en: 'The aforementioned parts of speech are very basic tags, and to ease the process
    of understanding natural language, POS algorithms create much more complicated
    tags that are variations of these basic ones. Here''s a full list of the POS tags
    with their descriptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2: POS tags with descriptions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_02_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.2: POS tags with descriptions'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These tags are from the Penn Treebank tagset (https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html),
    which is one of the most popular tagsets. A majority of the pre-trained taggers
    for the English language are trained on this tagset, including NLTK's POS tagger.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of Parts of Speech Tagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just like text pre-processing techniques help the machine understand natural
    language better by encouraging it to focus on only the important details, POS
    tagging helps the machine actually interpret the context of text and thus make
    sense of it. While text pre-processing is more of a cleaning phase, parts of speech
    tagging is actually the part where the machine is beginning to output valuable
    information about corpora on its own.
  prefs: []
  type: TYPE_NORMAL
- en: 'Understanding what words correspond to which parts of speech can be beneficial
    in processing natural language in several ways for a machine:'
  prefs: []
  type: TYPE_NORMAL
- en: POS tagging is useful in differentiating between homonyms – words that have
    the same spelling but mean different things. For example, the word "play" can
    mean the verb to play, as in engage in an activity, and also the noun, as in a
    dramatic work to be performed on stage. A POS tagger can help the machine understand
    what context the word "play" is being used in by determining its POS tag.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: POS tagging builds on the need for sentence and word segmentation – one of the
    basic tasks of natural language processing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: POS tags are used in performing higher-level tasks by other algorithms, one
    of which we will be discussing in this chapter, named entity recognition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: POS tags contribute to the process of sentiment analysis and question answering
    too. For example, in the sentence "Tim Cook is the CEO of this technology company,"
    you want the machine to be able to replace "this technology company" with the
    name of the company. POS tagging can help the machine recognize that the phrase
    "this technology company" is a determiner ((this) + a noun phrase (technology
    company)). It can use this information to, for example, search articles online
    and check how many times "Tim Cook is the CEO of Apple" appears in them to then
    decide whether Apple is the correct answer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, POS tagging is an important step in the process of understanding natural
    language because it contributes to other tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Types of POS Taggers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we saw in the previous section, POS taggers can be both of the supervised
    and unsupervised learning type. This difference largely affects how a tagger is
    trained. There is another distinction that impacts how the tagger actually assigns
    a tag to an untagged word, which is the approach used to train the taggers.
  prefs: []
  type: TYPE_NORMAL
- en: The two types of POS taggers are rule-based and stochastic. Let's take a look
    at both of them.
  prefs: []
  type: TYPE_NORMAL
- en: Rule-Based POS Taggers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These POS taggers work pretty much exactly as their name states – by rules.
    The purpose for giving the taggers sets of rules is to ensure that they tag an
    ambiguous/unknown word accurately most of the times, thus most of the rules are
    applied only when the taggers come across an ambiguous/unknown word.
  prefs: []
  type: TYPE_NORMAL
- en: 'These rules are often known as context frame rules and provide the taggers
    with contextual information to understand what tag to give an ambiguous word.
    An example of a rule is as follows: If an ambiguous/unknown word, x, is preceded
    by a determiner and followed by a noun, then assign it the tag of an adjective.
    An example of this would be "one small girl," where "one" is a determiner and
    "girl" is a noun, therefore the tagger will assign adjective to the word "small."'
  prefs: []
  type: TYPE_NORMAL
- en: The rules depend on your theory of grammar. Additionally, they also often include
    rules such as capitalization and punctuation. This can help you recognize pronouns
    and differentiate them from words found at the start of a sentence (following
    a full stop).
  prefs: []
  type: TYPE_NORMAL
- en: Most rule-based POS taggers are supervised learning algorithms, in order to
    be able to learn the correct rules and apply them to properly tag ambiguous words.
    Recently, though, there have been experiments with training these taggers the
    unsupervised way. Untagged text is given to the tagger to tag, and humans go through
    the output tags, correcting whatever tags are inaccurate. This correctly tagged
    text is then given to the tagger so that it can develop correction rules between
    the two different tagsets and learn how to accurately tag words.
  prefs: []
  type: TYPE_NORMAL
- en: An example of this correction rule-based POS tagger is Brill's tagger, which
    follows the process mentioned earlier. Its functioning can be compared with the
    art of painting – when painting a house, it is easier to first paint the background
    of the house (for example, a brown square) and then paint the details, such as
    a door and windows, on top of that background using a finer brush. Similarly,
    Brill's rule-based POS tagger aims to first generally tag an untagged corpus,
    even if some of the tags may be wrong, and then revisit those tags to understand
    why some are wrong and learn from them.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Exercises 10-16 can be performed in the same Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 10: Performing Rule-Based POS Tagging'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'NLTK has a POS tagger that is a rule-based tagger. In this exercise, we will
    perform POS tagging using NLTK''s POS tagger. The following steps will help you
    with the solution:'
  prefs: []
  type: TYPE_NORMAL
- en: Open cmd or terminal, depending on your operating system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to the desired path and use the following command to initiate a `Jupyter`
    Notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `nltk` and `punkt`, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Store an input string in a variable called `s`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Tokenize the sentence, as demonstrated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply the POS tagger on the tokens and then print the tagset, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Your output will look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Fig 2.3: Tagged output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_02_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fig 2.3: Tagged output'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To understand what the "`NN`" POS tag stands for, you can use the following
    line of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Fig 2.4: Noun details'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_02_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fig 2.4: Noun details'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: You can do this for each POS tag by substituting "NN" with it.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's try this out with a sentence containing homonyms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Store an input string containing homonyms in a variable called sent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Tokenize this sentence and then apply the POS tagger on the tokens, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Expected output:**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Fig 2.5: Tagged output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_02_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig 2.5: Tagged output'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, the first instance of the word play has been tagged as '**VB**',
    which stands for verb, base form, and the second instance of the word play has
    been tagged as '**NN**', which stands for noun. Thus, POS taggers are able to
    differentiate between homonyms and different instances of the same word. This
    helps machines understand natural language better.
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic POS Taggers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Stochastic POS taggers are taggers that use any method other than rule-based
    methods to assign tags to words. Thus, there are a large number of approaches
    that fall into the stochastic category. All models that incorporate statistical
    methods, such as probability and frequency, when determining the POS tags for
    words are stochastic models.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will discuss three models:'
  prefs: []
  type: TYPE_NORMAL
- en: The Unigram or Word Frequency Approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The n – gram approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hidden Markov Model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Unigram or Word Frequency Approach*'
  prefs: []
  type: TYPE_NORMAL
- en: The simplest stochastic POS taggers assign POS tags to ambiguous words solely
    based on the probability that a word occurs with a tag. This basically means that
    whatever tag the tagger found linked with a word most often in the training set
    is the tag that it will assign to an ambiguous instance of the same word. For
    example, let's say the training set has the word "beautiful" tagged as an adjective
    a majority of the time. When the POS tagger encounters "beaut", it won't be able
    to tag this directly because it isn't a proper word. This will be an ambiguous
    word, and so it will calculate the probability of it being each of the POS tags,
    based on how many times different instances of this word have been tagged with
    each of those POS tags. "beaut" can be seen as an ambiguous form of "beautiful",
    and since "beautiful" has been tagged as an adjective a majority of the time,
    the POS tagger will tag "beaut" as an adjective too. This is called the word frequency
    approach because the tagger is checking the frequency of the POS tags assigned
    to words.
  prefs: []
  type: TYPE_NORMAL
- en: '*The n – gram Approach*'
  prefs: []
  type: TYPE_NORMAL
- en: This builds on the previous approach. The **n** in the name stands for how many
    words are considered when determining the probability of a word belonging to a
    particular POS tag. In the Unigram tagger, **n = 1**, and thus only the word itself
    is taken into consideration. Increasing the value of n results in taggers calculating
    the probability of a specific sequence of n POS tags occurring together and assigning
    a word a tag based on this probability.
  prefs: []
  type: TYPE_NORMAL
- en: When assigning a tag to a word, these POS taggers create a context of the word
    by factoring in the type of token it is, along with the POS tags of the n preceding
    words. Based on the context, the taggers select the tag that is most likely to
    be in sequence with the tags of the preceding words and assigns this to the word
    in question. The most popular n – gram tagger is known as the Viterbi algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '*Hidden Markov Model*'
  prefs: []
  type: TYPE_NORMAL
- en: '*The hidden Markov model combines both the word frequency approach and the
    n – gram approach. A Markov model is one that describes a sequence of events or
    states. The probability of each state occurring depends solely on the state attained
    by the previous event. These events are based on observations. The "hidden" aspect
    of the hidden Markov model is that the set of states that an event could possibly
    be is hidden.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*In the case of POS tagging, the observations are the word tokens, and the
    hidden set of states are the POS tags. The way this works is that the model calculates
    the probability of a word having a particular tag based on what the tag of the
    previous word was. For example, P (V | NN) is the probability of the current word
    being a verb given that the previous word is a noun.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Note*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*This is a very basic explanation of the hidden Markov model. To learn more,
    go to* https://medium.freecodecamp.org/an-introduction-to-part-of-speech-tagging-and-the-hidden-markov-model-953d45338f24.'
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about stochastic models, go to http://ccl.pku.edu.cn/doubtfire/NLP/Lexical_Analysis/Word_Segmentation_Tagging/POS_Tagging_Overview/POS%20Tagging%20Overview.htm.
  prefs: []
  type: TYPE_NORMAL
- en: The three approaches mentioned earlier have been explained in an order where
    each model builds upon and improves the accuracy of the preceding model. However,
    each model that builds upon a preceding model involves more calculations of probability
    and thus will take more time to perform computations, depending on the size of
    the training corpus. Therefore, the decision of which approach to use depends
    on the size of the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 11: Performing Stochastic POS Tagging'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'spaCy''s POS tagger is a stochastic one. In this exercise, we will use spaCy''s
    POS tagger on some sentences to see the difference in the results of rule-based
    and stochastic tagging. The following steps will help you with the solution:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To install spaCy, click on the following link and follow the instructions:
    https://spacy.io/usage'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `spaCy`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load spaCy''s ''`en_core_web_sm`'' model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: spaCy has models that are specific to different languages. The 'en_core_web_sm'
    model is the English language model and has been trained on written web text,
    such as blogs and news articles, and includes vocabulary, syntax, and entities.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: To learn more about spaCy models, click on https://spacy.io/models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit the model on the sentence you want to assign POS tags to. Let''s use the
    sentence we gave NLTK''s POS tagger:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s tokenize this sentence, assign the POS tags, and print them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Expected output:**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.6: Output for POS tags'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_02_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.6: Output for POS tags'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To understand what a POS tag stands for, use the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Replace "VBZ" with the POS tag you''d like to know about. In this case, your
    output will be this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the results are pretty much the same as the ones obtained from
    the NLTK POS tagger. This is the case due to the simplicity of our input.
  prefs: []
  type: TYPE_NORMAL
- en: Chunking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: POS taggers work on individual tokens of words. Tagging individual words isn't
    always the best way to understand corpora, though. For example, the words 'United'
    and 'Kingdom' don't make a lot of sense when they're separated, but 'United Kingdom'
    together tells the machine that this is a country, thus providing it with more
    context and information. This is where the process of chunking comes into the
    picture.
  prefs: []
  type: TYPE_NORMAL
- en: Chunking is an algorithm that takes words and their POS tags as input. It processes
    these individual tokens and their tags to see whether they can be combined. The
    combination of one or more individual tokens is known as a chunk, and the POS
    tag assigned to such a chunk is known as a chunk tag.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chunk tags are combinations of basic POS tags. They are easier to define phrases
    by and are more efficient than simple POS tags. These phrases are chunks. There
    will be instances where a single word is considered a chunk and assigned a chunk
    tag too. There are five major chunk tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Noun Phrase* (*NP*): These are phrases that have nouns as the head word. They
    act as a subject or an object to the verb or verb phrase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Verb Phrase* (*VP*): These are phrases that have verbs as the head word.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Adjective Phrase* (*ADJP*): These are phrases that have adjectives as the
    head word. Describing and qualifying nouns or pronouns is the main function of
    adjective phrases. They are found either directly before or after the noun or
    pronoun.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Adverb Phrase* (*ADVP*): These are phrases that have adverbs as the head word.
    They''re used as modifiers for nouns and verbs by providing details that describe
    and qualify them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Prepositional Phrase* (*PP*): These are phrases that have prepositions as
    the head word. They position an action or an entity in time or space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, in the sentence ''the yellow bird is slow and is flying into the
    brown house'', the following phrases will be assigned the following chunk tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '''the yellow bird'' – NP'
  prefs: []
  type: TYPE_NORMAL
- en: '''is'' – VP'
  prefs: []
  type: TYPE_NORMAL
- en: '''slow'' – ADJP'
  prefs: []
  type: TYPE_NORMAL
- en: '''is flying'' – VP'
  prefs: []
  type: TYPE_NORMAL
- en: '''into'' – PP'
  prefs: []
  type: TYPE_NORMAL
- en: '''the brown house'' – NP'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, chunking is performed after POS tagging has been applied on a corpus.
    This allows the text to be broken down into its simplest form (tokens of words),
    have its structure analyzed, and then be grouped back together into meaningful
    higher-level chunks. Chunking also benefits the process of named entity recognition.
    We'll see how in the coming section.
  prefs: []
  type: TYPE_NORMAL
- en: The chunk parser present within the NLTK library is rule based and thus needs
    to be given a regular expression as a rule to output a chunk with its chunk tag.
    **spaCy** can perform chunking without the presence of rules. Let's take a look
    at both these approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 12: Performing Chunking with NLTK'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will generate chunks and chunk tags. **nltk** has a regular
    expression parser. This requires an input of a regular expression of a phrase
    and the corresponding chunk tag. It then searches the corpus for this expression
    and assigns it the tag.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since chunking works with POS tags, we can add on to our code from the POS
    tagging exercise. We saved the tokens with their respective POS tags in ''tagset''.
    Let''s use this. The following steps will help you with the solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a regular expression that will search for a noun phrase, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This regular expression is searching for a determiner (optional), followed by
    one or more adjectives and then a single noun. This will form a chunk called `Noun
    Phrase`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you don''t know how to write Regular Expressions, check out these quick
    tutorials: https://www.w3schools.com/python/python_regex.asp https://pythonprogramming.net/regular-expressions-regex-tutorial-python-3/'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create an instance of `RegexpParser` and feed it the rule:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Give `chunkParser` the `tagset` containing the tokens with their respective
    POS tags so that it can perform chunking, and then draw the chunks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: matplotlib needs to be installed on your machine for the `.draw()` function
    to work.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Your output will look something like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.7: Parse tree.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_02_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.7: Parse tree.'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: This is a parse tree. As you can see, the chunking process has recognized the
    noun phrases and labeled them, and the remaining tokens are shown with their POS
    tags.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s try the same thing out with another sentence. Store an input sentence
    in another variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Tokenize the sentence and perform POS tagging using NLTK''s POS tagger:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Repeat step 3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Expected output:**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.8: Output for chunking.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_02_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.8: Output for chunking.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 13: Performing Chunking with spaCy'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will implement chunking with spaCy. **spaCy** doesn''t
    require us to formulate rules to recognize chunks; it identifies chunks on its
    own and tells us what the head word is, thus telling us what the chunk tag is.
    Let''s identify some noun chunks using the same sentence from Exercise 12\. The
    following steps will help you with the solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fit `spaCy`''s English model on the sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply `noun_chunks` on this model, and for each chunk, print the text of the
    chunk, the root word of the chunk, and the dependency relation that connects the
    root word to its head:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Expected output:**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.9: Output for chunking with spaCy'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_02_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.9: Output for chunking with spaCy'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, chunking with **spaCy** is a lot simpler than with NLTK.
  prefs: []
  type: TYPE_NORMAL
- en: Chinking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chinking is an extension of chunking, as you've probably guessed already from
    its name. It's not a mandatory step in processing natural language, but it can
    be beneficial.
  prefs: []
  type: TYPE_NORMAL
- en: Chinking is performed after chunking. Post chunking, you have chunks with their
    chunk tags, along with individual words with their POS tags. Often, these extra
    words are unnecessary. They don't contribute to the final result or the entire
    process of understanding natural language and thus are a nuisance. The process
    of chinking helps us deal with this issue by extracting the chunks, and their
    chunk tags form the tagged corpus, thus getting rid of the unnecessary bits. These
    useful chunks are called chinks once they have been extracted from the tagged
    corpus.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you need only the nouns or noun phrases from a corpus to answer
    questions such as "what is this corpus talking about?", you would apply chinking
    because it would extract just what you want and present it in front of your eyes.
    Let's check this out with an exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 14: Performing Chinking'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Chinking is basically altering the things that you''re looking for in a corpus.
    Thus, applying chinking involves altering the rule (regular expression) provided
    to `chinkParser`. The following steps will help you with the solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a rule that chunks the entire corpus and only creates chinks out of
    the words or phrases tagged as nouns or noun phrases:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This rule is in the form of a regular expression. Basically, this regular expression
    is telling the machine to ignore all words that are not nouns or noun phrases.
    When it comes across a noun or a noun phrase, this rule will ensure that it is
    extracted as a chink.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create an instance of `RegexpParser` and feed it the rule:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Give `chinkParser` the `tagset` containing the tokens with their respective
    POS tags so that it can perform chinking, and then draw the chinks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Expected output:**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.10: Output for chinking'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_02_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.10: Output for chinking'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, the chinks have been highlighted and contain only nouns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 2: Building and Training Your Own POS Tagger'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We''ve already looked at POS tagging words using the existing and pre-trained
    POS taggers. In this activity, we will train our own POS tagger. This is like
    training any other machine learning algorithm. The following steps will help you
    with the solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pick a corpus to train the tagger on. You can use the nltk treebank to work
    on. The following code should help you import the treebank corpus:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Determine what features the tagger will consider when assigning a tag to a word.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a function to strip the tagged words of their tags so that we can feed
    them into our tagger.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the dataset and split the data into training and testing sets. Assign
    the features to 'X' and append the POS tags to 'Y'. Apply this function on the
    training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the decision tree classifier to train the tagger.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the classifier, initialize it, fit the model on the training data, and
    print the accuracy score.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The accuracy score in the output may vary, depending on the corpus used.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Expected output:**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.11: Expected accuracy score.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_02_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.11: Expected accuracy score.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for the activity can be found on page 297.
  prefs: []
  type: TYPE_NORMAL
- en: Named Entity Recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is one of the first steps in the process of information extraction. Information
    extraction is the task of a machine extracting structured information from unstructured
    or semi-structured text. This furthers the comprehension of natural language by
    machines.
  prefs: []
  type: TYPE_NORMAL
- en: After text preprocessing and POS tagging, our corpus becomes semi-structured
    and machine-readable. Thus, information extraction is performed after we've readied
    our corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is an example of named entity recognition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12: Example for named entity recognition'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_02_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.12: Example for named entity recognition'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Named Entities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Named entities are real-world objects that can be classified into categories,
    such as people, places, and things. Basically, they are words that can be denoted
    by a proper name. Named entities can also include quantities, organizations, monetary
    values, and many more things.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples of named entities and the categories they fall under are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Donald Trump, person
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Italy, location
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bottle, object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 500 USD, money
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Named entities can be viewed as instances of entities. In the previous examples,
    the categories are basically entities in their own and the named entities are
    instances of those. For example, London is an instance of city, which is an entity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common named entity categories are as listed:'
  prefs: []
  type: TYPE_NORMAL
- en: ORGANIZATION
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PERSON
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LOCATION
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DATE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TIME
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MONEY
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PERCENT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FACILITY
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPE (which stands Geo-Political Entity)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Named Entity Recognizers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Named entity recognizers are algorithms that identify and extract named entities
    from corpora and assign them a category. The input provided to a trained named
    entity recognizer consists of tokenized words with their respective POS tags.
    The output of named entity recognition is named entities along with their categories,
    among the other tokenized words and their POS tags.
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem of named entity recognition takes place in two phases:'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying and recognizing named entities (for example, 'London')
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Classifying these names entities (for example, 'London' is a 'location')
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first phase of identifying named entities is quite similar to the process
    of chunking, because the aim is to recognize things that are denoted by proper
    names. The named entity recognizer needs to look out for continuous sequences
    of tokens to be able to correctly spot named entities. For example, 'Bank of America'
    should be identified as a single named entity, despite the phrase containing the
    word 'America', which in itself is a named entity.
  prefs: []
  type: TYPE_NORMAL
- en: Much like POS taggers, most named entity recognizers are supervised learning
    algorithms. They are trained on input that contains named entities along with
    the categories that they fall under, thus enabling the algorithm to learn how
    to classify unknown named entities in the future.
  prefs: []
  type: TYPE_NORMAL
- en: This input containing named entities with their respective categories is often
    known as a knowledge base. Once a named entity recognizer has been trained and
    is given an unrecognized corpus, it refers to this knowledge base to search for
    the most accurate classification to assign to a named entity.
  prefs: []
  type: TYPE_NORMAL
- en: However, due to the fact that supervised learning requires an excessive amount
    of labeled data, unsupervised learning versions of named entity recognizers are
    also being researched. These are trained on unlabeled corpora – text that doesn't
    have named entities categorized. Like POS taggers, named entity recognizers categorize
    the named entities, and then the incorrect categories are corrected manually by
    humans. This corrected data is fed back to the NERs so that they can simply learn
    from their mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of Named Entity Recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As mentioned earlier, named entity recognition is one of the first steps of
    information extraction and thus plays a major role in enabling machines to understand
    natural language and perform a variety of tasks based on it. Named entity recognition
    is and can be used in various industries and scenarios to simplify and automate
    processes. Let''s take a look at a few use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Online content*, including articles, reports, and blog posts, are often tagged
    to enable users to search for it more easily and also to get a quick overview
    of what exactly the content is about. Named entity recognizers can be used to
    scour through this content and extract named entities to automatically generate
    these tags. These tags help categorize articles into predefined hierarchies as
    well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Search algorithms* also benefit from these tags. If a user were to enter a
    keyword into a search algorithm, instead of scouring through all the words of
    every article (which will take forever), the algorithm just needs to refer to
    the tags produced by named entity recognition to pull up articles containing or
    pertaining to the entered keyword. This reduces the computational time and operations
    by a lot.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another purpose for these tags is to create an *efficient recommendation system*.
    If you read an article that discusses the current political situation in India,
    and is thus maybe tagged as 'Indian Politics' (this is just an example), the news
    website can use this tag to suggest different articles with the same or similar
    tags. This also works in the case of visual entertainment such as movies and shows.
    Online streaming websites use tags assigned to content (for example, genres such
    as 'action', 'adventure', 'thriller', and so on) to understand your taste better
    and thus recommend similar content to you.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Customer feedback* is important for any service or product providing company.
    Running customer complaints and reviews through named entity recognizers produces
    tags that can help classify them based on location, type of product, and type
    of feedback (positive or negative). These reviews and complaints can then be sent
    to the people responsible for that particular product or that particular area
    and can be dealt with based on whether the feedback is positive or negative. The
    same thing can be done with tweets, Instagram captions, Facebook posts, and so
    on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, there are many applications of named entity recognition. Thus,
    it is important to understand how it works and how to implement it.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Named Entity Recognizers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As is the case with POS taggers, there are two broad methods to design a named
    entity recognizer: a linguistic approach by defining rules to recognize entities,
    or a stochastic approach using statistical models to accurately determine which
    category a named entity falls into best.'
  prefs: []
  type: TYPE_NORMAL
- en: Rule-Based NERs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Rule-based NERs work in the same way that rule-based POS taggers do.
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic NERs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'These include any and all models that use statistics to name and recognize
    entities. There are several approaches to stochastic named entity recognition.
    Let''s take a look at two of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Maximum Entropy Classification*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a machine learning classification model. It calculates the probability
    of a named entity falling into a particular category solely on the basis of the
    information provided to it (the corpus).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: For more information on Maximum Entropy Classification, go to http://blog.datumbox.com/machine-learning-tutorial-the-max-entropy-text-classifier/.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Hidden Markov Model*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*This method is the same as the one explained in the POS tagging section, but
    instead of the hidden set of states being the POS tags, they are the categories
    of the named entities.*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: For more information on stochastic named entity recognition and when to use
    which approach, go to http://www.datacommunitydc.org/blog/2013/04/a-survey-of-stochastic-and-gazetteer-based-approaches-for-named-entity-recognition-part-2.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Exercise 15: Perform Named Entity Recognition with NLTK'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we''ll use the `ne_chunk` algorithm of `NLTK` to perform
    named entity recognition on a sentence. Instead of using the sentences we used
    in the previous exercises, create a new sentence that contains proper names that
    can be classified into categories so that you can actually see the results:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Store an input sentence in a variable, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Tokenize the sentence and assign `POS tags` to the tokens:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply the `ne_chunk()` algorithm on the tagged words and either print or draw
    the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Assigning the value of ''`True`'' to the ''`binary`'' parameter tells the algorithm
    to just recognize the named entities and not classify them. Thus, your results
    will look something like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.13: Output for named entity recognition with POS tags'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_02_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.13: Output for named entity recognition with POS tags'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, the named entities have been highlighted as '`NE`'.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To know which categories the algorithm has assigned to these named entities,
    simply assign the value of ''`False`'' to the ''`binary`'' parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.14: Output with named entities'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_02_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.14: Output with named entities'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The algorithm has accurately categorized 'Shubhangi' and 'SpiceJet'. 'Taj Mahal',
    however, shouldn't be an ORGANIZATION, it should be a FACILITY. Thus, NLTK's `ne_chunk()`
    algorithm isn't the best one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 16: Performing Named Entity Recognition with spaCy'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we''ll be implementing **spaCy**''s named entity recognizer
    on the sentence from the previous exercise and compare the results. spaCy has
    several NERs that have been trained on different corpora. Each model has a different
    set of categories; here''s a list of all the categories spaCy can recognize:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.15: Categories of spaCy'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_02_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.15: Categories of spaCy'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following steps will help you with the solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fit `spaCy`''s English model on the sentence we used in the previous exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For each entity in this sentence, print the text of the entity and the label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Your output will look something like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.16: Output for named entity'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_02_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.16: Output for named entity'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: It's only recognizing 'SpiceJet' and 'Pune' as named entities, and not 'Shubhangi'
    and 'Taj Mahal'. Let's try adding a last name to 'Shubhangi' and check whether
    that makes a difference.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit the model on the new sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Repeat step 2:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Expected output:**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.17: Output for named entity recognition with spaCy.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_02_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.17: Output for named entity recognition with spaCy.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So now that we've added a last name, "Shubhangi Hora" is recognized as a PERSON,
    and "Taj Mahal" is recognized as a **WORK_OF ART**. The latter is incorrect, since
    if you check the table of categories, **WORK_OF_ART** is used to describe songs
    and books.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the recognition and categorization of named entities strongly depends
    on the data that the recognizer has been trained on. This is something to keep
    in mind when implementing named entity recognition; it is often better to train
    and develop your own recognizer for specific use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 3: Performing NER on a Tagged Corpus'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we''ve seen how to perform named entity recognition on a sentence,
    in this activity, we''ll perform named entity recognition on a corpus that has
    been through POS tagging. Imagine that you''re given a corpus that you''ve identified
    the POS tags for and now your job is to extract entities from it so that you can
    provide an overall summary of what the corpus is discussing. The following steps
    will help you with the solution:'
  prefs: []
  type: TYPE_NORMAL
- en: Import NLTK and other necessary packages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print `nltk.corpus.treebank.tagged_sents()` to see the tagged corpus that you
    need extract named entities from.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Store the first sentence of the tagged sentences in a variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `nltk.ne_chunk` to perform NER on the sentence. Set **binary** to **True**
    and print the named entities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 3 and 4 on any number of sentences to see the different entities
    that exist in the corpus. Set the **binary** parameter to **False** to see what
    the named entities are categorized as.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Expected output:**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.18: Expected output for NER on tagged corpus'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_02_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.18: Expected output for NER on tagged corpus'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for the activity can be found on page 300.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Natural language processing enables a machine to understand the language of
    humans, and just as we learned how to comprehend and process language, machines
    are taught as well. Two ways of better understanding language that allow machines
    to contribute to the real world are POS tagging and named entity recognition.
  prefs: []
  type: TYPE_NORMAL
- en: The former is the process of assigning POS tags to individual words so that
    the machine can learn context, and the latter is recognizing and categorizing
    named entities to extract valuable information from corpora.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are distinctions in the way these processes are performed: the algorithms
    can be supervised or unsupervised, and the approach can be rule-based or stochastic.
    Either way, the goal is the same, that is, to comprehend and communicate with
    humans in their natural language.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be discussing neural networks, how they work, and
    how they can be used for natural language processing.
  prefs: []
  type: TYPE_NORMAL
