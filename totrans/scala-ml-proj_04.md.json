["```py\n1 15211 rs78601809 T G 100 PASS AC=3050;\n AF=0.609026;\n AN=5008;\n NS=2504;\n DP=32245;\n EAS_AF=0.504;\n AMR_AF=0.6772;\n AFR_AF=0.5371;\n EUR_AF=0.7316;\n SAS_AF=0.6401;\n AA=t|||;\n VT=SNP\n```", "```py\n<properties>\n    <spark.version>2.2.1</spark.version>\n    <scala.version>2.11.12</scala.version>\n    <h2o.version>3.16.0.2</h2o.version>\n    <sparklingwater.version>2.2.6</sparklingwater.version>\n    <adam.version>0.23.0</adam.version>\n</properties>\n```", "```py\n<dependency>\n    <groupId>org.apache.spark</groupId>\n    <artifactId>spark-core_2.11</artifactId>\n    <version>${spark.version}</version>\n</dependency>\n```", "```py\n<dependency>\n    <groupId>ai.h2o</groupId>\n    <artifactId>sparkling-water-core_2.11</artifactId>\n    <version>2.2.6</version>\n</dependency>\n<dependency>\n    <groupId>ai.h2o</groupId>\n    <artifactId>sparkling-water-examples_2.11</artifactId>\n    <version>2.2.6</version>\n</dependency>\n<dependency>\n    <groupId>ai.h2o</groupId>\n    <artifactId>h2o-core</artifactId>\n    <version>${h2o.version}</version>\n</dependency>\n<dependency>\n    <groupId>ai.h2o</groupId>\n    <artifactId>h2o-scala_2.11</artifactId>\n    <version>${h2o.version}</version>\n</dependency>\n<dependency>\n    <groupId>ai.h2o</groupId>\n    <artifactId>h2o-algos</artifactId>\n    <version>${h2o.version}</version>\n</dependency>\n<dependency>\n    <groupId>ai.h2o</groupId>\n    <artifactId>h2o-app</artifactId>\n    <version>${h2o.version}</version>\n</dependency>\n<dependency>\n    <groupId>ai.h2o</groupId>\n    <artifactId>h2o-persist-hdfs</artifactId>\n    <version>${h2o.version}</version>\n</dependency>\n<dependency>\n    <groupId>ai.h2o</groupId>\n    <artifactId>google-analytics-java</artifactId>\n    <version>1.1.2-H2O-CUSTOM</version>\n</dependency>\n```", "```py\n<dependency>\n    <groupId>org.bdgenomics.adam</groupId>\n    <artifactId>adam-core_2.11</artifactId>\n    <version>0.23.0</version>\n</dependency>\n```", "```py\n<dependency>\n    <groupId>joda-time</groupId>\n    <artifactId>joda-time</artifactId>\n    <version>2.9.9</version>\n</dependency>\n```", "```py\nval spark:SparkSession = SparkSession\n    .builder()\n    .appName(\"PopStrat\")\n    .master(\"local[*]\")\n    .config(\"spark.sql.warehouse.dir\", \"C:/Exp/\")\n    .getOrCreate()\n```", "```py\nval genotypeFile = \"<path>/ALL.chrY.phase3_integrated_v2a.20130502.genotypes.vcf\"\nval panelFile = \"<path>/integrated_call_samples_v3.20130502.ALL.panel \"\n```", "```py\nval populations = Set(\"FIN\", \"GBR\", \"ASW\", \"CHB\", \"CLM\")\n```", "```py\ndef extract(file: String,\nfilter: (String, String) => Boolean): Map[String, String] = {\nSource\n    .fromFile(file)\n    .getLines()\n    .map(line => {\nval tokens = line.split(Array('t', ' ')).toList\ntokens(0) -> tokens(1)\n}).toMap.filter(tuple => filter(tuple._1, tuple._2))\n}\n\nval panel: Map[String, String] = extract(\npanelFile,\n(sampleID: String, pop: String) => populations.contains(pop))\n```", "```py\nval allGenotypes: RDD[Genotype] = sc.loadGenotypes(genotypeFile).rdd\nval genotypes: RDD[Genotype] = allGenotypes.filter(genotype => {\n    panel.contains(genotype.getSampleId)\n    })\n```", "```py\ncase class SampleVariant(sampleId: String,\n        variantId: Int,\n        alternateCount: Int)\n```", "```py\ndef variantId(genotype: Genotype): String = {\n val name = genotype.getVariant.getContigName\n val start = genotype.getVariant.getStart\n val end = genotype.getVariant.getEnd\ns\"$name:$start:$end\"\n}\n```", "```py\ndef alternateCount(genotype: Genotype): Int = {\n      genotype.getAlleles.asScala.count(_ != GenotypeAllele.REF)\n   }\n```", "```py\ndef toVariant(genotype: Genotype): SampleVariant = {\n new SampleVariant(genotype.getSampleId.intern(),\n            variantId(genotype).hashCode(),\n            alternateCount(genotype))\n        }\n```", "```py\nval variantsRDD: RDD[SampleVariant] = genotypes.map(toVariant)\n```", "```py\nval variantsBySampleId: RDD[(String, Iterable[SampleVariant])] =\nvariantsRDD.groupBy(_.sampleId)\n\nval sampleCount: Long = variantsBySampleId.count()\nprintln(\"Found \" + sampleCount + \" samples\")\n\nval variantsByVariantId: RDD[(Int, Iterable[SampleVariant])] =\nvariantsRDD.groupBy(_.variantId).filter {\n case (_, sampleVariants) => sampleVariants.size == sampleCount\n    }\n```", "```py\nval variantFrequencies: collection.Map[Int, Int] = variantsByVariantId\n.map {\n case (variantId, sampleVariants) =>\n        (variantId, sampleVariants.count(_.alternateCount > 0))\n        }.collectAsMap()\n```", "```py\nval permittedRange = inclusive(11, 11)\nval filteredVariantsBySampleId: RDD[(String, Iterable[SampleVariant])] =\n    variantsBySampleId.map {\n case (sampleId, sampleVariants) =>\n val filteredSampleVariants = sampleVariants.filter(\n        variant =>\n        permittedRange.contains(\n        variantFrequencies.getOrElse(variant.variantId, -1)))\n    (sampleId, filteredSampleVariants)\n    }\n```", "```py\nval sortedVariantsBySampleId: RDD[(String, Array[SampleVariant])] =\n    filteredVariantsBySampleId.map {\n case (sampleId, variants) =>\n        (sampleId, variants.toArray.sortBy(_.variantId))\n        }\n    println(s\"Sorted by Sample ID RDD: \" + sortedVariantsBySampleId.first())\n```", "```py\nval rowRDD: RDD[Row] = sortedVariantsBySampleId.map {\n case (sampleId, sortedVariants) =>\n val region: Array[String] = Array(panel.getOrElse(sampleId, \"Unknown\"))\n val alternateCounts: Array[Int] = sortedVariants.map(_.alternateCount)\n        Row.fromSeq(region ++ alternateCounts)\n        }\n```", "```py\nval header = StructType(\n        Seq(StructField(\"Region\", StringType)) ++\n        sortedVariantsBySampleId\n            .first()\n            ._2\n            .map(variant => {\n                StructField(variant.variantId.toString, IntegerType)\n        }))\n```", "```py\nval sqlContext = spark.sqlContext\nval schemaDF = sqlContext.createDataFrame(rowRDD, header)\nschemaDF.printSchema()\nschemaDF.show(10)\n>>>\n```", "```py\nval sqlContext = sparkSession.sqlContext\nval schemaDF = sqlContext.createDataFrame(rowRDD, header).drop(\"Region\")\nschemaDF.printSchema()\nschemaDF.show(10)\n>>>\n```", "```py\nval featureCols = schemaDF.columns\n```", "```py\nval assembler = \nnew VectorAssembler()\n    .setInputCols(featureCols)\n    .setOutputCol(\"features\")\nval assembleDF = assembler.transform(schemaDF).select(\"features\")\n```", "```py\nassembleDF.show()\n>>>\n```", "```py\nval pca = \nnew PCA()\n    .setInputCol(\"features\")\n    .setOutputCol(\"pcaFeatures\")\n    .setK(50)\n    .fit(assembleDF)\n```", "```py\nval pcaDF = pca.transform(assembleDF)\n            .select(\"pcaFeatures\")\n            .withColumnRenamed(\"pcaFeatures\", \"features\")\npcaDF.show()\n>>>\n```", "```py\nval kmeans = \nnew KMeans().setK(5).setSeed(12345L)\nval model = kmeans.fit(pcaDF)\n```", "```py\nval WSSSE = model.computeCost(pcaDF)\nprintln(\"Within-Cluster Sum of Squares for k = 5 is\" + WSSSE)\n>>>\n```", "```py\nval iterations = 20\nfor (i <- 2 to iterations) {\n val kmeans = new KMeans().setK(i).setSeed(12345L)\n val model = kmeans.fit(pcaDF)\n val WSSSE = model.computeCost(pcaDF)\n        println(\"Within-Cluster Sum of Squares for k = \" + i + \" is \" +\n                WSSSE)\n    }\n```", "```py\nWithin-Cluster Sum of Squares for k = 2 is 453.161838161838\nWithin-Cluster Sum of Squares for k = 3 is 438.2392344497606\nWithin-Cluster Sum of Squares for k = 4 is 390.2278787878787\nWithin-Cluster Sum of Squares for k = 5 is 397.72112098427874\nWithin-Cluster Sum of Squares for k = 6 is 367.8890909090908\nWithin-Cluster Sum of Squares for k = 7 is 362.3360347662672\nWithin-Cluster Sum of Squares for k = 8 is 347.49306362861336\nWithin-Cluster Sum of Squares for k = 9 is 327.5002901103624\nWithin-Cluster Sum of Squares for k = 10 is 327.29376873556436\nWithin-Cluster Sum of Squares for k = 11 is 315.2954156954155\nWithin-Cluster Sum of Squares for k = 12 is 320.2478696814693\nWithin-Cluster Sum of Squares for k = 13 is 308.7674242424241\nWithin-Cluster Sum of Squares for k = 14 is 314.64784054938576\nWithin-Cluster Sum of Squares for k = 15 is 297.38523698523704\nWithin-Cluster Sum of Squares for k = 16 is 294.26114718614707\nWithin-Cluster Sum of Squares for k = 17 is 284.34890572390555\nWithin-Cluster Sum of Squares for k = 18 is 280.35662525879917\nWithin-Cluster Sum of Squares for k = 19 is 272.765762015762\nWithin-Cluster Sum of Squares for k = 20 is 272.05702362771336\n```", "```py\nval dataFrame = h2oContext.asH2OFrame(schemaDF)\n```", "```py\ndataFrame.replace(dataFrame.find(\"Region\"),\ndataFrame.vec(\"Region\").toCategoricalVec()).remove()\ndataFrame.update()\n```", "```py\nval frameSplitter = new FrameSplitter(\n        dataFrame, Array(.8, .1), Array(\"training\", \"test\", \"validation\")\n        .map(Key.make[Frame]),null)\n\nwater.H2O.submitTask(frameSplitter)\nval splits = frameSplitter.getResult\nval training = splits(0)\nval test = splits(1)\nval validation = splits(2)\n```", "```py\n// Set the parameters for our deep learning model.\nval deepLearningParameters = new DeepLearningParameters()\n        deepLearningParameters._train = training\n        deepLearningParameters._valid = validation\n        deepLearningParameters._response_column = \"Region\"\n        deepLearningParameters._epochs = 200\n        deepLearningParameters._l1 = 0.01\n        deepLearningParameters._seed = 1234567\n        deepLearningParameters._activation = Activation.RectifierWithDropout\n        deepLearningParameters._hidden = Array[Int](128, 256, 512)\n```", "```py\nval deepLearning = new DeepLearning(deepLearningParameters)\nval deepLearningTrained = deepLearning.trainModel\nval trainedModel = deepLearningTrained.get\n```", "```py\nval error = trainedModel.classification_error()\nprintln(\"Training Error: \" + error)\n>>>\nTraining Error: 0.5238095238095238\n```", "```py\nval trainMetrics = ModelMetricsSupport.modelMetrics[ModelMetricsMultinomial](trainedModel, test)\nval met = trainMetrics.cm()\n\nprintln(\"Accuracy: \"+ met.accuracy())\nprintln(\"MSE: \"+ trainMetrics.mse)\nprintln(\"RMSE: \"+ trainMetrics.rmse)\nprintln(\"R2: \" + trainMetrics.r2)\n>>>\nAccuracy: 0.42105263157894735\nMSE: 0.49369297490740655\nRMSE: 0.7026328877211816\nR2: 0.6091597281983032\n```", "```py\nval deepLearningParameters = new DeepLearningParameters()\n        deepLearningParameters._train = training\n        deepLearningParameters._valid = validation\n        deepLearningParameters._response_column = \"Region\"\n        deepLearningParameters._epochs = 100\n        deepLearningParameters._l2 = 0.01\n        deepLearningParameters._seed = 1234567\n        deepLearningParameters._activation = Activation.RectifierWithDropout\n        deepLearningParameters._hidden = Array[Int](32, 64, 128)\n>>>\nTraining Error: 0.47619047619047616\nAccuracy: 0.5263157894736843\nMSE: 0.39112548936806274\nRMSE: 0.6254002633258662\nR2: 0.690358987583617\n```", "```py\nval featureCols = schemaDF.columns.drop(1)\nval assembler = \nnew VectorAssembler()\n    .setInputCols(featureCols)\n    .setOutputCol(\"features\")\nval assembleDF = assembler.transform(schemaDF).select(\"features\", \"Region\")\nassembleDF.show()\n```", "```py\nval indexer = \nnew StringIndexer()\n    .setInputCol(\"Region\")\n    .setOutputCol(\"label\")\n\nval indexedDF =  indexer.fit(assembleDF)\n                .transform(assembleDF)\n                .select(\"features\", \"label\") \n```", "```py\nval seed = 12345L\nval splits = indexedDF.randomSplit(Array(0.75, 0.25), seed)\nval (trainDF, testDF) = (splits(0), splits(1))\n```", "```py\ntrainDF.cache\ntestDF.cache\nval rf = new RandomForestClassifier()\n    .setLabelCol(\"label\")\n    .setFeaturesCol(\"features\")\n    .setSeed(1234567L)\n```", "```py\nval paramGrid =\nnew ParamGridBuilder()\n    .addGrid(rf.maxDepth, 3 :: 5 :: 15 :: 20 :: 25 :: 30 :: Nil)\n    .addGrid(rf.featureSubsetStrategy, \"auto\" :: \"all\" :: Nil)\n    .addGrid(rf.impurity, \"gini\" :: \"entropy\" :: Nil)\n    .addGrid(rf.maxBins, 3 :: 5 :: 10 :: 15 :: 25 :: 35 :: 45 :: Nil)\n    .addGrid(rf.numTrees, 5 :: 10 :: 15 :: 20 :: 30 :: Nil)\n    .build()\n\nval evaluator = new MulticlassClassificationEvaluator()\n    .setLabelCol(\"label\")\n    .setPredictionCol(\"prediction\")\n```", "```py\nval numFolds = 10\nval crossval = \nnew CrossValidator()\n    .setEstimator(rf)\n    .setEvaluator(evaluator)\n    .setEstimatorParamMaps(paramGrid)\n    .setNumFolds(numFolds)\n```", "```py\nval cvModel = crossval.fit(trainDF)\n```", "```py\nval predictions = cvModel.transform(testDF)\npredictions.show(10)\n>>>\n```", "```py\nval metric = \nnew MulticlassClassificationEvaluator()\n    .setLabelCol(\"label\")\n    .setPredictionCol(\"prediction\")\n\nval evaluator1 = metric.setMetricName(\"accuracy\")\nval evaluator2 = metric.setMetricName(\"weightedPrecision\")\nval evaluator3 = metric.setMetricName(\"weightedRecall\")\nval evaluator4 = metric.setMetricName(\"f1\")\n```", "```py\nval accuracy = evaluator1.evaluate(predictions)\nval precision = evaluator2.evaluate(predictions)\nval recall = evaluator3.evaluate(predictions)\nval f1 = evaluator4.evaluate(predictions)\n```", "```py\nprintln(\"Accuracy = \" + accuracy);\nprintln(\"Precision = \" + precision)\nprintln(\"Recall = \" + recall)\nprintln(\"F1 = \" + f1)\nprintln(s\"Test Error = ${1 - accuracy}\")\n>>>\nAccuracy = 0.7196470196470195\nPrecision = 0.7196470196470195\nRecall = 0.7196470196470195\nF1 = 0.7196470196470195\nTest Error = 0.28035298035298046\n```"]