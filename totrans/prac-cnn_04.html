<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Popular CNN Model Architectures</h1>
                </header>
            
            <article>
                
<p>In this chapter, will introduce the ImageNet image database and also cover the architectures of the following popular CNN models:</p>
<ul>
<li>LeNet</li>
<li>AlexNet</li>
<li>VGG</li>
<li>GoogLeNet</li>
<li>ResNet </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to ImageNet</h1>
                </header>
            
            <article>
                
<p class="mce-root">ImageNet is a database of over 15 million <span>hand-labeled, high-resolution </span>images in roughly 22,000 categories. This database is organized just like the WordNet hierarchy, where each concept is also called a <strong>synset</strong> (that is, <strong>synonym set</strong>). Each synset is a node in the ImageNet hierarchy. Each node has more than 500 images. </p>
<p class="mce-root"><span>The <strong>ImageNet Large Scale Visual Recognition Challenge</strong> (<strong>ILSVRC</strong>)</span> was founded in 2010 to improve state-of-the-art technology for object detection and image classification on a large scale:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/20756743-ac07-44cb-a804-05763571d81d.png"/></div>
<p>Following this overview of ImageNet, we will now take a look at various CNN model architectures.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">LeNet</h1>
                </header>
            
            <article>
                
<p>In 2010, a <span>challenge f</span>rom ImageNet (known as <strong>ILSVRC 2010</strong>) came out with a CNN architecture, LeNet 5, built by Yann Lecun. This network takes a 32 x 32 image as input, which goes to the convolution layers (<strong>C1</strong>) and then to the subsampling layer (<strong>S2</strong>). Today, the subsampling layer is replaced by a pooling layer. Then, there is another sequence of convolution layers (<strong>C3</strong>) followed by a pooling (that is, subsampling) layer (<strong>S4</strong>). Finally, there are three fully connected layers, including the <strong>OUTPUT</strong> layer at the end. This network was used for zip code recognition in post offices. Since then, every year various CNN architectures were introduced with the help of this competition:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cb2b54bf-cb43-4e1f-897c-862f85f06423.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">LeNet 5 – CNN architecture from Yann Lecun's article in 1998</div>
<p>Therefore, we can conclude the following points:</p>
<ul>
<li>The input to this network is a grayscale 32 x 32 image</li>
<li>The architecture implemented is a CONV layer, followed by POOL and a fully connected layer</li>
<li>CONV filters are 5 x 5, applied at a stride of 1</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AlexNet architecture</h1>
                </header>
            
            <article>
                
<p><span>The first breakthrough in the architecture of CNN came in the year 2012. This award-winning CNN architecture is called <strong>AlexNet</strong>. It was developed at the University of Toronto by Alex Krizhevsky and his professor, Jeffry Hinton. </span></p>
<p>In the first run, a ReLU activation function and a dropout of 0.5 were used in this network to fight overfitting. As we can see in the following image, there is a normalization layer used in the architecture, but this is not used in practice anymore as it used heavy data augmentation. AlexNet is still used today even though there are more accurate networks available, because of its relative simple structure and small depth. It is widely used in computer vision:</p>
<div class="CDPAlignCenter CDPAlign"><img height="207" src="assets/dd1ba84c-d1ae-4f6f-87b6-0e641600eab4.png" width="530"/></div>
<p><span>AlexNet is trained on the ImageNet database using two separate GPUs, possibly due to processing limitations with inter-GPU connections at the time, as shown in the following figure:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="252" src="assets/1100c791-9be1-4e7b-8039-3172ad50606d.png" width="529"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Traffic sign classifiers using AlexNet</h1>
                </header>
            
            <article>
                
<p>In this example, we will use transfer learning for feature extraction and a German traffic sign dataset to develop a classifier. Used here is an AlexNet implementation by<strong> </strong>Michael Guerzhoy and Davi Frossard, and AlexNet weights are from the Berkeley vision and Learning center. The complete code and dataset can be downloaded from here.</p>
<p>AlexNet expects a 227 x 227 x 3 pixel image, whereas the traffic sign images are 32 x 32 x 3 pixels. In order to feed the traffic sign images into AlexNet, we'll need to resize the images to the dimensions that AlexNet expects, that is, 227 x 227 x 3:</p>
<pre>original_image = tf.placeholder(tf.float32, (<span class="hljs-keyword">None</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>))
resized_image = tf.image.resize_images(<span>original_imag</span>, (<span class="hljs-number">227</span>, <span class="hljs-number">227</span>))</pre>
<p>We can do so with the help of the <span><kbd>tf.image.resize_images</kbd> method by </span>TensorFlow. Another issue here is that AlexNet was trained on the ImageNet dataset, which has 1,000 classes of images. So, we will replace this layer with a 43-neuron classification layer. <span>To do this, figure out the size of the output from the last fully connected layer; since this is a fully connected layer and so is a 2D shape, the last element will be the size of the output.</span> <kbd>fc7.get_shape().as_list()[-1]</kbd><span> does the trick; combine this with the number of classes for the traffic sign dataset to get the shape of the final fully connected layer: </span><kbd>shape = (fc7.get_shape().as_list()[-1], 43)</kbd><span>. The rest of the code is just the standard way to define a fully connected layer in TensorFlow. Finally, calculate the probabilities with <kbd>softmax</kbd>:</span></p>
<pre><span class="hljs-comment">#Refer AlexNet implementation code, returns last fully connected layer</span>
fc7 = AlexNet(resized, feature_extract=<span class="hljs-keyword">True</span>)
shape = (fc7.get_shape().as_list()[-<span class="hljs-number">1</span>], 43)
fc8_weight = tf.Variable(tf.truncated_normal(shape, stddev=<span class="hljs-number">1e-2</span>))
fc8_b = tf.Variable(tf.zeros(43))
logits = tf.nn.xw_plus_b(fc7, fc8_weight, fc8_b)
probs = tf.nn.softmax(logits)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">VGGNet architecture</h1>
                </header>
            
            <article>
                
<p>The runner-up in the 2014 ImageNet challenge was VGGNet from the visual geometric group at Oxford University. This convolutional neural network is a simple and elegant architecture with a 7.3% error rate. It has two versions: VGG16 and VGG19.</p>
<p>VGG16 is a 16-layer neural network, not counting the max pooling layer and the softmax layer. Hence, it is known as VGG16. VGG19 consists of 19 layers. A pre-trained model is available in Keras for both Theano and TensorFlow backends.</p>
<p>The key design consideration here is depth. Increases in the depth of the network were achieved by adding more convolution layers, and it was done due to the small 3 x 3 convolution filters in all the layers. The default input size of an image for this model is 224 x 224 x 3. The image is passed through a stack of convolution layers with a stride of 1 pixel and padding of 1. It uses 3 x 3 convolution throughout the network. Max pooling is done over a 2 x 2 pixel window with a stride of 2, then another stack of convolution layers followed by three fully connected layers. The first two fully connected layers have 4,096 neurons each, and the third fully connected layers are responsible for classification with 1,000 neurons. The final layer is a softmax layer. VGG16 uses a much smaller 3 x 3 convolution window, compared to AlexNet's much larger 11 x 11 convolution window. All hidden layers are built with the ReLU activation function. The architecture looks like this:</p>
<div class="CDPAlignCenter CDPAlign"><img height="375" src="assets/0a1c03f6-5c37-4e49-9b3d-ef56e005b84d.png" width="389"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">VGG16 network architecture</div>
<div class="packt_tip">Due to the small 3 x 3 convolution filter, the depth of VGGNet is increased. The number of parameters in this network is approximately 140 million, mostly from the first fully connected layer. In latter-day architectures, fully connected layers of VGGNet are replaced with <strong>global average pooling</strong> (<strong>GAP</strong>) layers in order to minimize the number of parameters.</div>
<p class="mce-root">Another observation is that the number of filters increases as the image size decreases.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">VGG16 image classification code example</h1>
                </header>
            
            <article>
                
<p>The Keras Applications module has pre-trained neural network models, along with its pre-trained weights trained on ImageNet. These models can be used <span>directly</span><span> </span>for <span>prediction, feature extraction, and fine-tuning:</span></p>
<pre>#import VGG16 network model and other necessary libraries <br/><br/>from keras.applications.vgg16 import VGG16<br/>from keras.preprocessing import image<br/>from keras.applications.vgg16 import preprocess_input<br/>import numpy as np<br/><br/><span>#Instantiate VGG16 and returns a vgg16 model instance <br/>vgg16_model = VGG16(weights=</span><span class="hljs-string">'imagenet'</span><span>, include_top=</span><span class="hljs-keyword">False</span><span>) <br/></span>#include_top: whether to include the 3 fully-connected layers at the top of the network.<br/>#This has to be True for classification and False for feature extraction. Returns a model instance<br/>#weights:'imagenet' means model is pre-training on ImageNet data.<br/>model = VGG16(weights='imagenet', include_top=True)<br/>model.summary()<br/><br/>#image file name to classify<br/>image_path = 'jumping_dolphin.jpg'<br/>#load the input image with keras helper utilities and resize the image. <br/>#Default input size for this model is 224x224 pixels.<br/>img = image.load_img(image_path, target_size=(224, 224))<br/>#convert PIL (Python Image Library??) image to numpy array<br/>x = image.img_to_array(img)<br/>print (x.shape)<br/><br/>#image is now represented by a NumPy array of shape (224, 224, 3),<br/># but we need to expand the dimensions to be (1, 224, 224, 3) so we can<br/># pass it through the network -- we'll also preprocess the image by<br/># subtracting the mean RGB pixel intensity from the ImageNet dataset<br/>#Finally, we can load our Keras network and classify the image:<br/><br/>x = np.expand_dims(x, axis=0)<br/>print (x.shape)<br/><br/>preprocessed_image = preprocess_input(x)<br/><br/>preds = model.predict(preprocessed_image)<br/>print('Prediction:', decode_predictions(preds, top=2)[0])</pre>
<p>The first time it executes the preceding script, Keras will automatically<span> </span>download and cache the architecture weights to disk in the<span> </span><kbd><span class="crayon-syntax crayon-syntax-inline crayon-theme-classic crayon-theme-classic-inline crayon-font-monaco"><span class="crayon-pre crayon-code"><span class="crayon-o">~</span><span class="crayon-o">/</span><span class="crayon-sy">.</span><span class="crayon-v">keras</span><span class="crayon-o">/</span><span class="crayon-v">models</span></span></span></kbd> directory. Subsequent runs will be faster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">GoogLeNet architecture</h1>
                </header>
            
            <article>
                
<p>In 2014, ILSVRC, Google published its own network known as <strong>GoogLeNet</strong>. Its performance is a little better than VGGNet; GoogLeNet's performance is 6.7% compared to VGGNet's performance of 7.3%. The main attractive feature of GoogLeNet is that it runs very fast due to the introduction of a new concept called <strong>inception module</strong>, thus reducing the number of parameters to only 5 million; that's 12 times less than AlexNet. It has lower memory use and lower power use too.</p>
<p class="mce-root"><span>It has 22 layers, so it is a very deep network. Adding more layers increases the number of parameters and it is likely that the network overfits. There will be more computation, because a linear increase in filters results in a quadratic increase in computation. So, the designers use the inception module and GAP. The fully connected layer at the end of the network is replaced with a GAP layer because fully connected layers are generally prone to overfitting. GAP has no parameters to learn or optimize. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Architecture insights</h1>
                </header>
            
            <article>
                
<p>Instead of choosing a particular filter size as in the previous architectures, the GoogLeNet designers applied all the three filters of sizes 1 x 1, 3 x 3, and 5 x 5 on the same patch, with a 3 x 3 max pooling and concatenation into a single output vector.</p>
<p>The use of 1 x 1 convolutions decreases the dimensions wherever the computation is increased by the expensive 3 x 3 and 5 x 5 convolutions. 1 x 1 convolutions with the ReLU activation function are used before the expensive 3 x 3 and 5 x 5 convolutions.</p>
<p>In GoogLeNet, inception modules are stacked one over the other. This stacking allows us to modify each module without affecting the later layers. For example, you can increase or decrease the width of any layer:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="239" src="assets/2636db13-c7fb-49c9-b20d-544287eacf55.png" width="472"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">GoogLeNet architecture</div>
<p class="mce-root"><span>Deep networks also suffer from the fear of what is known as the <strong>vanishing gradient</strong> problem during backpropagation. This is avoided by adding auxiliary classifiers to intermediate layers. Also, during training, the intermediate loss was added to the total loss with a discounted factor of 0.3.</span></p>
<p>Since fully connected layers are prone to overfitting, it is replaced with a GAP layer. Average pooling does not exclude use of dropout, a regularization method for overcoming overfitting in deep neural networks. GoogLeNet added a linear layer after 60, a <span>GAP layer to help others swipe for their own classifier using transfer learning techniques.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Inception module</h1>
                </header>
            
            <article>
                
<p>The following image is an example of an inception module:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="198" src="assets/ac511227-54cd-4a2a-a9ef-077ec291ec0f.png" width="344"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ResNet architecture</h1>
                </header>
            
            <article>
                
<p>After a certain depth, adding additional layers to feed-forward convNets results in a higher training error and higher validation error. When adding layers, performance increases only up to a certain depth, and then it rapidly decreases. In the <strong>ResNet</strong> (<strong><span>Residual Network</span></strong><span>)</span> paper, the authors argued that this underfitting is unlikely due to the vanishing gradient problem, because this happens even when using the batch normalization technique.<span> Therefore, they have added a new concept called <strong>residual block</strong>. The ResNet team added connections that can skip layers:</span></p>
<div class="packt_tip">ResNet uses standard convNet and adds connections that skip a few convolution layers at a time. Each bypass gives a residual block.</div>
<div class="CDPAlignCenter CDPAlign"><img height="369" src="assets/c7c96246-7f86-4e6c-89d0-22a25e655493.png" width="190"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Residual block</div>
<p>In the 2015 ImageNet ILSVRC <span>competition, the winner was ResNet from Microsoft, with an error rate of 3.57%. ResNet is a kind of VGG in the sense that the same structure is repeated again and again to make the network deeper. Unlike VGGNet, it has different depth variations, such as 34, 50, 101, and 152 layers. It has a whopping 152 layers compared to AlexNet 8, VGGNet's 19 layers, and GoogLeNet's 22 layers. The ResNet architecture is a stack of residual blocks. The main idea is to skip layers by adding connections to the neural network. Every residual block has 3 x 3 convolution layers. After the last conv layer, a GAP layer is added. There is only one fully connected layer to classify 1,000 classes. It has different depth varieties, such as 34, 50, 101, or 152 layers for the ImageNet dataset. For a deeper network, say more than 50 layers, it uses the <strong>bottleneck</strong> features concept to improve efficiency. No dropout is used in this network.</span></p>
<p>Other network architectures to be aware of include:</p>
<ul>
<li>Network in Network</li>
<li>Beyond ResNet</li>
<li>FractalNet, an ultra-deep neural network without residuals</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned about the different CNN architectures. These models are pre-trained existing models and differ in network architecture. Each of these networks is designed to solve a problem specific to its architecture. So, here we described their architectural differences.</p>
<p>We also understood how our own CNN architecture, as defined in the previous chapter, differs from these advanced ones.</p>
<p>In the next chapter, we will learn how these pre-trained models can be used for transfer learning.</p>


            </article>

            
        </section>
    </body></html>