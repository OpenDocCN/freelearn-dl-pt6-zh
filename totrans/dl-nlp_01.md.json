["```py\n    jupyter notebook\n    ```", "```py\n    s = s.lower()\n    print(s)\n    ```", "```py\n    words = ['indiA', 'India', 'india', 'iNDia']\n    ```", "```py\n    words = [word.lower() for word in words]\n    print(words)\n    ```", "```py\n    import re\n    ```", "```py\n    def clean_words(text):\n\n      #remove html markup\n      text = re.sub(\"(<.*?>)\",\"\",text)\n      #remove non-ascii and digits\n      text=re.sub(\"(\\W|\\d+)\",\" \",text)\n      #remove whitespace\n      text=text.strip()\n      return text \n    ```", "```py\n    raw = ['..sleepy', 'sleepy!!', '#sleepy', '>>>>>sleepy>>>>', '<a>sleepy</a>']\n    ```", "```py\n    clean = [clean_words(r) for r in raw]\n    print(clean)\n    ```", "```py\n    import nltk\n    import pandas as pd\n    from nltk.stem import PorterStemmer as ps\n    ```", "```py\n    stemmer = ps()\n    ```", "```py\n    words=['annoying', 'annoys', 'annoyed', 'annoy']\n    ```", "```py\n    stems =[stemmer.stem(word = word) for word in words]\n    ```", "```py\n    sdf = pd.DataFrame({'raw word': words,'stem': stems})\n    sdf\n    ```", "```py\n    from nltk.stem import WordNetLemmatizer as wnl\n    nltk.download('wordnet')\n    ```", "```py\n    lemmatizer = wnl()\n    ```", "```py\n    words = ['troubling', 'troubled', 'troubles', 'trouble']\n    ```", "```py\n    # v denotes verb in \"pos\"\n    lemmatized = [lemmatizer.lemmatize(word = word, pos = 'v') for word in words]\n    ```", "```py\n    ldf = pd.DataFrame({'raw word': words,'lemmatized': lemmatized})\n    ldf = ldf[['raw word','lemmatized']]\n    ldf\n    ```", "```py\n    import nltk\n    ```", "```py\n    nltk.download('punkt')\n    from nltk import word_tokenize\n    ```", "```py\n    s = \"hi! my name is john.\"\n    tokens = word_tokenize(s)\n    tokens\n    ```", "```py\n    from nltk import sent_tokenize\n    ```", "```py\n    s = \"hi! my name is shubhangi.\"\n    tokens = sent_tokenize(s)\n    tokens\n    ```", "```py\n    nltk.download('stopwords')\n    ```", "```py\n    s = \"the weather is really hot and i want to go for a swim\"\n    ```", "```py\n    from nltk.corpus import stopwords\n    stop_words = set(stopwords.words('english'))\n    ```", "```py\n    tokens = word_tokenize(s)\n    tokens = [word for word in tokens if not word in stop_words]\n    print(tokens)\n    ```", "```py\n    from gensim.models import Word2Vec as wtv\n    from nltk import word_tokenize\n    ```", "```py\n    s1 = \"Ariana Grande is a singer\"\n    s2 = \"She has been a singer for many years\"\n    s3 = \"Ariana is a great singer\"\n    sentences = [word_tokenize(s1), word_tokenize(s2), word_tokenize(s3)]\n    ```", "```py\n    model = wtv(sentences, min_count = 1)\n    ```", "```py\n    print('this is the summary of the model: ')\n    print(model)\n    ```", "```py\n    words = list(model.wv.vocab)\n    print('this is the vocabulary for our corpus: ')\n    print(words)\n    ```", "```py\nprint(\"the vector for the word singer: \")\nprint(model['singer'])\n```", "```py\n#lookup top 6 similar words to great\nw1 = [\"great\"]\nmodel.wv.most_similar (positive=w1, topn=6)\n```", "```py\n#lookup top 6 similar words to singer\nw1 = [\"singer\"]\nmodel.wv.most_similar (positive=w1, topn=6)\n```", "```py\n    import itertools\n    ```", "```py\n    from gensim.models.word2vec import Text8Corpus\n    from glove import Corpus, Glove\n    ```", "```py\n    sentences = list(itertools.islice(Text8Corpus('text8'),None))\n    ```", "```py\n    corpus = Corpus()\n    corpus.fit(sentences, window=10)\n    ```", "```py\n    glove = Glove(no_components=100, learning_rate=0.05)\n    ```", "```py\n    glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n    ```", "```py\n    glove.add_dictionary(corpus.dictionary)\n    ```", "```py\n    glove.most_similar('man')\n    ```", "```py\nglove.most_similar('queen', number = 10)\n```"]