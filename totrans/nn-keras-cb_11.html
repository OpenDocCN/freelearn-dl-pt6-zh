<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building a Recurrent Neural Network</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we looked at multiple ways of representing text as a vector and then performed sentiment classification on top of those representations.</p>
<p>One of the drawbacks of this approach is that we did not take the order of words into consideration—for example, the sentence <em>A is faster than B</em> would have the same representation as <em>B is faster than A</em>, as the words in both sentences are exactly the same, while the order of words is different.</p>
<p><strong>Recurrent neural networks</strong> (<strong>RNNs</strong>) come in handy in scenarios when the word order needs to be preserved. In this chapter, you will learn about the following topics:</p>
<ul>
<li>Building RNN and LSTM from scratch in Python</li>
<li>Implementing RNN for sentiment classification</li>
<li>Implementing LSTM for sentiment classification</li>
<li>Implementing stacked LSTM for sentiment classification</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p><span>RNN</span> can be architected in multiple ways. Some of the possible ways are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1089 image-border" src="Images/79db1776-f471-4fe6-89b0-67cbae844bfc.png" style="width:127.00em;height:42.92em;" width="1524" height="515"/></p>
<p>The box in the bottom is the input, followed by the hidden layer (as the middle box), and the box on top is the output layer. The one-to-one architecture is the typical neural network with a hidden layer between the input and the output layer. The examples of different architectures are as follows:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td><strong>Architecture</strong></td>
<td><strong>Example</strong></td>
</tr>
<tr>
<td>One-to-many</td>
<td>Input is image and output is caption of image</td>
</tr>
<tr>
<td>Many-to-one</td>
<td>Input is a movie's review (multiple words in input) and output is sentiment associated with the review</td>
</tr>
<tr>
<td>Many-to-many</td>
<td>Machine translation of a sentence in one language to a sentence in another language</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Intuition of RNN architecture</h1>
                </header>
            
            <article>
                
<p>RNN is useful when we want to predict the next event given a sequence of events.</p>
<p>An example of that could be to predict the word that comes after <em>This is an _____</em>.</p>
<p>Let's say, in reality, the sentence is <em>This is an example</em>.</p>
<p>Traditional text-mining techniques would solve the problem in the following way:</p>
<ol>
<li>Encode each word while having an additional index for potential new words:</li>
</ol>
<pre style="padding-left: 60px">This: {1,0,0,0}<br/>is: {0,1,0,0}<br/>an: {0,0,1,0}</pre>
<ol start="2">
<li>Encode the phrase <kbd>This is an</kbd>:</li>
</ol>
<pre style="padding-left: 60px">This is an: {1,1,1,0}</pre>
<ol start="3">
<li>Create the training dataset:</li>
</ol>
<pre style="padding-left: 60px">Input --&gt; {1,1,1,0}<br/>Output --&gt; {0,0,0,1}</pre>
<ol start="4">
<li>Build a model with input and output</li>
</ol>
<p>One of the major drawbacks of the model is that the input representation does not change in the input sentence; it is either <kbd>this is an</kbd>, or <kbd>an is this</kbd>, or <kbd>this an is</kbd>.</p>
<p>However, intuitively, we know that each of the preceding sentences is different and cannot be represented by the same structure mathematically. This calls for having a different architecture, which looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1090 image-border" src="Images/a4da81fc-763e-4ed2-b638-f3694aebd985.png" style="width:7.83em;height:14.58em;" width="325" height="605"/></p>
<p>In the preceding architecture, each of the individual words from the sentence enter into individual box among the input boxes. However the structure of the sentence will be preserved, for example <kbd>this</kbd> enters the first box, <kbd>is</kbd> enters second box and <kbd>an</kbd> enters the third box.</p>
<p>The output box at the top will be the output that is <kbd>example</kbd>.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Interpreting an RNN</h1>
                </header>
            
            <article>
                
<p>You can think of RNN as a mechanism to hold memory—where the memory is contained within the hidden layer. It can be visualized as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1091 image-border" src="Images/ef7343c2-b306-43b0-8e2d-3dc8f10fa443.png" style="width:42.00em;height:16.92em;" width="1492" height="601"/></p>
<p>The network on the right is an unrolled version of the network on the left. The network on the right takes one input in each time step and extracts the output at each time step. However, if we are interested in the output in the fourth time step, we'll provide input in the previous three time steps and the output of the third time step is the predicted value for fourth time step.</p>
<p>Note that, while predicting the output of third time step, we are incorporating values from the first three time steps through the hidden layer, which is connecting the values across time steps.</p>
<p>Let's explore the preceding diagram:</p>
<ul>
<li>The <strong>U</strong> weight represents the weights that connect the input layer to the hidden layer</li>
<li>The <strong>W</strong> weight represents the hidden-layer-to-hidden-layer connection</li>
<li>The <strong>V</strong> weight represents the hidden-layer-to-output-layer connection</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Why store memory?</h1>
                </header>
            
            <article>
                
<p>There is a need to store memory, as in the preceding example or even in text-generation in general, the next word does not necessarily depend only on the preceding word, but the context of the words preceding the word to predict.</p>
<p>Given that we are looking at the preceding words, there should be a way to keep them in memory, so that we can predict the next word more accurately.</p>
<p>Moreover, we should also have the memory in order; that is, more often than not, the recent words are more useful in predicting the next word than the words that are far away from the word to predict.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building an RNN from scratch in Python</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will build an RNN from scratch using a toy example, so that you gain a solid intuition of how RNN helps in solving the problem of taking the order of events (words) into consideration.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Note that a typical NN has an input layer, followed by an activation in the hidden layer, and then a softmax activation at the output layer.</p>
<p>RNN follows a similar structure, with modifications done in such a way that the hidden layers of the previous time steps are considered in the current time step.</p>
<p>We'll build the working details of RNN with a simplistic example before implementing it on more practical use cases.</p>
<p>Let's consider an example text that looks as follows: <kbd>This is an example</kbd>.</p>
<p>The task at hand is to predict the third word given a sequence of two words.</p>
<p>So, the dataset translates as follows:</p>
<table style="border-collapse: collapse;width: 35.3508%" border="1">
<tbody>
<tr>
<td style="width: 17%">
<p><strong>Input</strong></p>
</td>
<td style="width: 16.3994%">
<p><strong>Output</strong></p>
</td>
</tr>
<tr>
<td style="width: 17%">
<p><kbd>this, is</kbd></p>
</td>
<td style="width: 16.3994%">
<p><kbd>an</kbd></p>
</td>
</tr>
<tr>
<td style="width: 17%">
<p><kbd>is, an</kbd></p>
</td>
<td style="width: 16.3994%">
<p><kbd>example</kbd></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Given an input of <kbd>this is</kbd>, we are expected to predict <kbd>example</kbd> as the output.</p>
<p class="mce-root"/>
<p>The strategy that we'll adopt to build an RNN is as follows:</p>
<ol>
<li>One-hot encode the words</li>
<li>Identify the maximum length of input:
<ul>
<li>Pad the rest of input to the maximum length so that all the inputs are of the same length</li>
</ul>
</li>
<li>Convert the words in the input into a one-hot-encoded version</li>
<li>Convert the words in the output into a one-hot-encoded version</li>
<li>Process the input and the output data, then fit the RNN model</li>
</ol>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The strategy discussed above is coded as follows (the code file is available as <kbd>Building_a_Recurrent_Neural_Network_from_scratch-in_Python.ipynb</kbd> in GitHub):</p>
<ol>
<li>Let's define the input and output in code, as follows:</li>
</ol>
<pre style="padding-left: 60px">#define documents<br/>docs = ['this, is','is an']<br/># define class labels<br/>labels = ['an','example']</pre>
<ol start="2">
<li>Let's preprocess our dataset so that it can be passed to an RNN:</li>
</ol>
<pre style="padding-left: 60px">from collections import Counter<br/>counts = Counter()<br/>for i,review in enumerate(docs+labels):<br/>      counts.update(review.split())<br/>words = sorted(counts, key=counts.get, reverse=True)<br/>vocab_size=len(words)<br/>word_to_int = {word: i for i, word in enumerate(words, 1)}</pre>
<p style="padding-left: 60px">In the preceding step, we are identifying all the unique words and their corresponding frequency (counts) in a given dataset, and we are assigning an ID number to each word. The output of <kbd>word_to_int</kbd> in the preceding code looks as follows:</p>
<pre style="padding-left: 60px">print(word_to_int)<br/><span># {'an': 2, 'example': 4, 'is': 1, 'this': 3}</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="3">
<li>Modify the input and output words with their corresponding IDs, as follows:</li>
</ol>
<pre style="padding-left: 60px">encoded_docs = []<br/>for doc in docs:<br/>      encoded_docs.append([word_to_int[word] for word in doc.split()])<br/>encoded_labels = []<br/>for label in labels:<br/>      encoded_labels.append([word_to_int[word] for word in label.split()])<br/>print('encoded_docs: ',encoded_docs)<br/>print('encoded_labels: ',encoded_labels)<br/><span># encoded_docs: [[3, 1], [1, 2]]<br/></span><span># encoded_labels: [[2], [4]]</span></pre>
<p style="padding-left: 60px">In the preceding code, we are appending the ID of each word of an input sentence into a list, thus making the input (<kbd>encoded_docs</kbd>) a list of lists.</p>
<p style="padding-left: 60px">Similarly, we are <span>appending the ID of each word of the output into a list.</span></p>
<ol start="4">
<li>One additional factor to take care of while encoding the input is the input length. In cases of sentiment analysis, the input text length can vary from one review to another. However, the neural network expects the input size to be fixed. To get around this problem, we perform padding on top of the input. Padding ensures that all inputs are encoded to have a similar length. While the lengths of both examples in our case is 2, in practice, we are very likely to face the scenario of differing lengths between input. In code, we perform padding as follows:</li>
</ol>
<pre style="padding-left: 60px"># pad documents to a max length of 2 words<br/>max_length = 2<br/>padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='pre')</pre>
<p style="padding-left: 60px">In the preceding code, we are passing <kbd>encoded_docs</kbd> to the <kbd>pad_sequences</kbd> function, which ensures that all the input data points have the same length—which is equal to the <kbd>maxlen</kbd> parameter. Additionally, for those parameters that are shorter than <kbd>maxlen</kbd>, it pads those data points with a value of 0 to achieve a total length of <kbd>maxlen</kbd> and the zero padding is done <kbd>pre</kbd>—that is, to the left of the original encoded sentence.</p>
<p style="padding-left: 60px">Now that the input dataset is created, let's preprocess the output dataset so that it can be passed to the model-training step.</p>
<p class="mce-root"/>
<ol start="5">
<li>The typical processing for outputs is to make them into dummy values, that is, make a one-hot-encoded version of the output labels, which is done as follows:</li>
</ol>
<pre style="padding-left: 60px">one_hot_encoded_labels = to_categorical(encoded_labels, num_classes=5)<br/>print(one_hot_encoded_labels)<br/><span># [[0. 0. 1. 0. 0.] [0. 0. 0. 0. 1.]]</span></pre>
<p style="padding-left: 60px">Note that, given the output values (<kbd>encoded_labels</kbd>) are <kbd>{2, 4}</kbd>, the output vectors have a value of 1 at the second and fourth positions, respectively.</p>
<ol start="6">
<li>Let's build the model:
<ol>
<li>An RNN expects the input to be (<kbd>batch_size</kbd>, <kbd>time_steps</kbd>, and <kbd>features_per_timestep</kbd>) in shape. Hence, we first reshape the <kbd>padded_docs</kbd><span> input into the following format:</span></li>
</ol>
</li>
</ol>
<pre style="padding-left: 60px">padded_docs = padded_docs.reshape(2,2,1)</pre>
<div class="packt_infobox">Note that ideally we would have created a word embedding for each word (ID in this specific case). However, given that the intent of this recipe is only to understand the working details of RNN, we will exclude the embedding of IDs and assume that each input is not an ID but a value. Having said that, we will learn how to perform ID-embedding in the next recipe.</div>
<ol>
<li style="list-style-type: none">
<ol start="2">
<li>Define the model—where we are specifying that we will initialize an RNN by using the <kbd>SimpleRNN</kbd> method:</li>
</ol>
</li>
</ol>
<pre style="padding-left: 120px"># define the model<br/>embed_length=1<br/>max_length=2<br/>model = Sequential()<br/>model.add(SimpleRNN(1,activation='tanh', return_sequences=False,recurrent_initializer='Zeros',input_shape=(max_length,embed_length),unroll=True))</pre>
<div class="packt_infobox">In the preceding step, we explicitly specified the <kbd>recurrent_initializer</kbd> to be zero so that we understand the working details of RNN more easily. In practice, we would not be initializing the recurrent initializer to 0.</div>
<p style="padding-left: 120px">The <kbd>return_sequences</kbd> parameter specifies whether we want to obtain the hidden layer values at each time step. A false value for <kbd>return_sequences</kbd> specifies that we want the hidden layer output only at the final timestep.</p>
<div class="packt_infobox">Typically, in a many-to-one task, where there are many inputs (one input in each time step) and outputs, <kbd>return_sequences</kbd> will be false, resulting in the output being obtained only in the final time step. An example of this could be the stock price on the next day, given a sequence of historical five-day stock prices.</div>
<p style="padding-left: 120px">However, in cases where we try to obtain hidden-layer values in each time step, <kbd>return_sequences</kbd> will be set to <kbd>True</kbd>. An example of this could be machine translation, where there are many inputs and many outputs.</p>
<ol>
<li style="list-style-type: none">
<ol start="3">
<li>Connect the RNN output to five nodes of the output layer:</li>
</ol>
</li>
</ol>
<pre style="padding-left: 120px">model.add(Dense(5, activation='softmax'))</pre>
<p style="padding-left: 120px">We have performed a <kbd>Dense(5)</kbd>, as there are five possible classes of output (the output of each example has 5 values, where each value corresponds to the probability of it belonging to word ID <kbd>0</kbd> to word ID <kbd>4</kbd>).</p>
<ol>
<li style="list-style-type: none">
<ol start="4">
<li>Compile and summarize the model:</li>
</ol>
</li>
</ol>
<pre style="padding-left: 120px"># compile the model<br/>model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])<br/># summarize the model<br/>print(model.summary())</pre>
<p style="padding-left: 120px">A summary of model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/54388466-b7f3-4314-a618-964e8acfd193.png" style="width:34.50em;height:13.75em;" width="520" height="208"/></p>
<p style="padding-left: 120px">Now that we have defined the model, before we fit the input and output, let's understand the reason why there are certain number of parameters in each layer.</p>
<p style="padding-left: 120px">The simple RNN part of the model has an output shape of <kbd>(None, 1)</kbd>. The <kbd>None</kbd> in the output shape represents the <kbd>batch_size</kbd>. None is a way of specifying that the <kbd>batch_size</kbd> could be any number. Given that we have specified that there shall be one unit of hidden that is to be outputted from simple RNN, the number of columns is one. </p>
<p style="padding-left: 120px">Now that we understand the output shape of <kbd>simpleRNN</kbd>, let's understand why the number of parameters is three in the <kbd>simpleRNN</kbd> layer. Note that the hidden-layer value is outputted at the final time step. Given that the input has a value  of one in each time step (one feature per time step) and the output is also of one value, the input is essentially multiplied by a weight that is of a single value. Had the output (hidden-layer value) been 40 units of hidden-layer values, the input should have been multiplied by 40 units to get the output (more on this in the <em>Implementing RNN for sentiment classification</em><br/>
recipe). Apart from the one weight connecting the input to the hidden-layer value, there is a bias term that accompanies the weight. The other 1 parameter comes from the connection of the previous time step's hidden layer value to the current time step's hidden layer, resulting in a total of three parameters.</p>
<p style="padding-left: 120px">There are 10 parameters from the hidden layer to the final output as there are five possible classes resulting in five weights and five biases connecting the hidden-layer value (which is of one unit)—a total of 10 parameters.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol>
<li style="list-style-type: none">
<ol start="5">
<li>Fit the model to predict the output from the input:</li>
</ol>
</li>
</ol>
<pre style="padding-left: 120px">model.fit(padded_docs,np.array(one_hot_encoded_labels),epochs=500)</pre>
<ol start="7">
<li>Extract prediction on the first input data point</li>
</ol>
<pre style="padding-left: 60px">model.predict(padded_docs[0].reshape(1,2,1))</pre>
<p style="padding-left: 60px">The extracted output is as follows:</p>
<pre style="padding-left: 60px">array([[0.3684635, 0.33566403, 0.61344165, 0.378485, 0.4069949 ]],      dtype=float32)</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Validating the output</h1>
                </header>
            
            <article>
                
<p>Now that the model is fit, let's gain an understanding of how an RNN works by working backward—that is, extract the weights of model, feed forward the input through the weights to match the predicted value using NumPy <span>(the code file is available as</span> <kbd>Building_a_Recurrent_Neural_Network_from_scratch_in_Python.ipynb</kbd> in GitHub).</p>
<ol start="1">
<li>Inspect the weights:</li>
</ol>
<pre style="padding-left: 60px">model.weights<br/>[&lt;tf.Variable 'simple_rnn_2/kernel:0' shape=(1, 1) dtype=float32_ref&gt;,<br/> &lt;tf.Variable 'simple_rnn_2/recurrent_kernel:0' shape=(1, 1) dtype=float32_ref&gt;,<br/> &lt;tf.Variable 'simple_rnn_2/bias:0' shape=(1,) dtype=float32_ref&gt;,<br/> &lt;tf.Variable 'dense_2/kernel:0' shape=(1, 5) dtype=float32_ref&gt;,<br/> &lt;tf.Variable 'dense_2/bias:0' shape=(5,) dtype=float32_ref&gt;]</pre>
<p style="padding-left: 60px">The preceding gives us an intuition of the order in which weights are presented in the output.</p>
<p style="padding-left: 60px">In the preceding example, <kbd>kernel</kbd> represents the weights and <kbd>recurrent</kbd> represents the connection of the hidden layer from one step to another.</p>
<p style="padding-left: 60px">Note that a <kbd>simpleRNN</kbd> has weights that connect the input to the hidden layer and also weights that connect the previous time step's hidden layer to the current time step's hidden layer.</p>
<p class="mce-root"/>
<p style="padding-left: 60px">The kernel and bias in the <kbd>dense_2</kbd> layer represent the layer that connects the hidden layer value to the final output:</p>
<ol>
<li style="list-style-type: none">
<ol>
<li>Extract weights:</li>
</ol>
</li>
</ol>
<pre style="padding-left: 120px" class="mce-root">model.get_weights()</pre>
<p style="padding-left: 120px">The preceding line of code gives us the computed values of each of the weights.</p>
<ol start="2">
<li>Pass the input through the first time step—<span>the</span> input is as follows:</li>
</ol>
<pre style="padding-left: 60px">padded_docs[0]<br/>#array([3, 1], dtype=int32)</pre>
<p style="padding-left: 60px">In the preceding code, the first time step has a value of <kbd>3</kbd> and the second time step has a value of <kbd>1</kbd>. We'll initialize the value at first time step as follows:</p>
<pre style="padding-left: 60px">input_t0 = 3</pre>
<ol>
<li style="list-style-type: none">
<ol>
<li>The value at the first time step is multiplied by the weight connecting the input to the hidden layer, then the bias value is added:</li>
</ol>
</li>
</ol>
<pre style="padding-left: 120px">input_t0_kernel_bias = input_t0*model.get_weights()[0] + model.get_weights()[2]</pre>
<ol>
<li style="list-style-type: none">
<ol start="2">
<li>The hidden layer value at this time step is calculated by passing the preceding output through the <kbd>tanh</kbd> activation (as that is the activation we specified when we defined the model):</li>
</ol>
</li>
</ol>
<pre style="padding-left: 120px">hidden_layer0_value = np.tanh(input_t0_kernel_bias)</pre>
<ol start="4">
<li>Calculate the hidden-layer value at time step 2;<span> where t</span>he input has a value of <kbd>1</kbd> <span>(note the value of</span> <kbd>padded_docs[0]</kbd> <span>is</span> <kbd>[3, 1]</kbd><span>):</span></li>
</ol>
<pre style="padding-left: 60px">input_t1 = 1</pre>
<ol>
<li style="list-style-type: none">
<ol>
<li>The output value when the input at the second time step is passed through the weight and bias is as follows:</li>
</ol>
</li>
</ol>
<pre style="padding-left: 120px">input_t1_kernel_bias = input_t1*model.get_weights()[0] + model.get_weights()[2]</pre>
<p style="padding-left: 120px">Note that the weights that multiply the input remain the same, regardless of the time step being considered.</p>
<ol>
<li style="list-style-type: none">
<ol start="2">
<li>The calculation for the hidden-layer at various time steps is performed as follows:</li>
</ol>
</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/0fe5cacd-023b-479f-8a90-874d5b0f3c42.png" style="width:28.00em;height:2.08em;" width="3760" height="280"/></p>
<p style="padding-left: 60px" class="mce-root"><span>Where <em>Φ</em> is an activation that is performed (In general,</span> <kbd>tanh</kbd> <span>activation is used).</span></p>
<p style="padding-left: 60px">The calculation from the input layer to the hidden-layer constitutes two components:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li>Matrix multiplication of the input layer value and kernel weights</li>
<li>Matrix multiplication of the hidden layer of the previous time step and recurrent weights</li>
</ul>
</li>
</ul>
<p style="padding-left: 60px">The final calculation of the hidden-layer value at a given time step would be the summation of the preceding two matrix multiplications. Pass the result through a tanh activation function:</p>
<pre style="padding-left: 60px">input_t1_recurrent = hidden_layer0_value*model.get_weights()[1]</pre>
<p style="padding-left: 60px">The total value before passing through the <kbd>tanh</kbd> activation is as follows:</p>
<pre style="padding-left: 60px">total_input_t1 = input_t1_kernel_bias + input_t1_recurrent</pre>
<p style="padding-left: 60px">The output of the hidden-layer value is calculated by passing the preceding output through the <kbd>tanh</kbd> activation, as follows:</p>
<pre style="padding-left: 60px">output_t1 = np.tanh(total_input_t1)</pre>
<ol start="5">
<li>Pass the hidden layer output from the final time step through the dense layer, which connects the hidden layer to the output layer:</li>
</ol>
<pre style="padding-left: 60px">final_output = output_t1*model.get_weights()[3] + model.get_weights()[4]</pre>
<p style="padding-left: 60px">Note that the fourth and fifth output of the <kbd>model.get_weights()</kbd> method correspond to the connection from the hidden layer to the output layer.</p>
<ol start="6">
<li>Pass the preceding output through the softmax activation (as defined in the model) to obtain the final output:</li>
</ol>
<pre style="padding-left: 60px">np.exp(final_output)/np.sum(np.exp(final_output))<br/># array([[0.3684635, 0.33566403, 0.61344165, 0.378485, 0.40699497]], dtype=float32)</pre>
<p class="mce-root"/>
<p>You should notice that the output we obtained through the forward pass of input through the network is the same as what the <kbd>model.predict</kbd> function gave as output.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementing RNN for sentiment classification</h1>
                </header>
            
            <article>
                
<p>To understand how RNN is implemented in Keras, let's implement the airline-tweet sentiment classification exercise that we performed in the <a href="79b072ff-23b6-47ef-80bc-d9400a855714.xhtml" target="_blank"/><a href="79b072ff-23b6-47ef-80bc-d9400a855714.xhtml" target="_blank">Chapter 10</a>, <em>Text Analysis Using Word Vectors</em> chapter.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The task would be performed as follows (the code file is available as <kbd>RNN_and_LSTM_sentiment_classification.ipynb</kbd> in GitHub):</p>
<ol>
<li>Import the relevant packages and dataset:</li>
</ol>
<pre style="padding-left: 60px">from keras.layers import Dense, Activation<br/>from keras.layers.recurrent import SimpleRNN<br/>from keras.models import Sequential<br/>from keras.utils import to_categorical<br/>from keras.layers.embeddings import Embedding<br/>from sklearn.cross_validation import train_test_split<br/>import numpy as np<br/>import nltk<br/>from nltk.corpus import stopwords<br/>import re<br/>import pandas as pd<br/>data=pd.read_csv('https://www.dropbox.com/s/8yq0edd4q908xqw/airline_sentiment.csv')<br/>data.head()</pre>
<ol start="2">
<li>Preprocess the text to remove punctuation, normalize all words to lowercase, and remove the stopwords, as follows:</li>
</ol>
<pre style="padding-left: 60px">import nltk<br/>nltk.download('stopwords')<br/>stop = nltk.corpus.stopwords.words('english')<br/>def preprocess(text):<br/>    text=text.lower()<br/>    text=re.sub('[^0-9a-zA-Z]+',' ',text)<br/>    words = text.split()<br/>    words2=[w for w in words if (w not in stop)]<br/>    #words3=[ps.stem(w) for w in words]<br/>    words4=' '.join(words2)<br/>    return(words4)<br/>data['text'] = data['text'].apply(preprocess)</pre>
<ol start="3">
<li>Extract the word-to-integer mapping of all the words that constitute the dataset:</li>
</ol>
<pre style="padding-left: 60px">from collections import Counter<br/>counts = Counter()<br/>for i,review in enumerate(t['text']):<br/>    counts.update(review.split())<br/>words = sorted(counts, key=counts.get, reverse=True</pre>
<p style="padding-left: 60px">In the preceding step, we are extracting the frequency of all the words in the dataset. A sample of extracted words are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/f9b4e5b9-a37a-4944-941f-6647af1ba595.jpg" width="130" height="177"/></p>
<pre style="padding-left: 60px">nb_chars = len(words)<br/>word_to_int = {word: i for i, word in enumerate(words, 1)}<br/>int_to_word = {i: word for i, word in enumerate(words, 1)}</pre>
<p style="padding-left: 60px">In the preceding code, we are looping through all the words and are assigning an index for each word. A sample of integer to word dictionary is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/c12eb915-d624-4999-b83c-6daeff8e36e0.jpg" width="150" height="106"/></p>
<p class="mce-root"/>
<ol start="4">
<li>Map each word in a given sentence to the corresponding word associated with it:</li>
</ol>
<pre style="padding-left: 60px">mapped_reviews = []<br/>for review in data['text']:<br/>    mapped_reviews.append([word_to_int[word] for word in review.split()])</pre>
<p style="padding-left: 60px">In the preceding step, we are converting a text review into a list of lists where each list constitutes the ID of words contained in a sentence. A sample of original and mapped review is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/04c3a122-9b47-4e60-87a5-bf3bed019521.jpg" width="584" height="37"/></p>
<ol start="5">
<li>Extract the maximum length of a sentence and normalize all sentences to the same length by padding them. <span>In the following code, we are looping through all the reviews and storing the length corresponding to each review. Additionally, we are also calculating the maximum length of a review (tweet text):</span></li>
</ol>
<pre style="padding-left: 60px">length_sent = []<br/>for i in range(len(mapped_reviews)):<br/>      length_sent.append(len(mapped_reviews[i]))<br/>sequence_length = max(length_sent)</pre>
<p style="padding-left: 60px"><span>We should note that different tweets have different lengths. However, RNN expects the number of time steps for each input to be the same. In the code below, we are padding a mapped review</span> <span> </span>with<span> a value of 0, if the length of the review is less than the maximum length of all reviews in dataset. This way, all inputs will have the same length.</span></p>
<pre style="padding-left: 60px">from keras.preprocessing.sequence import pad_sequences<br/>X = pad_sequences(maxlen=sequence_length, sequences=mapped_reviews, padding="post", value=0)</pre>
<ol start="6">
<li>Prepare the training and test datasets:</li>
</ol>
<pre style="padding-left: 60px">y=data['airline_sentiment'].values<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30,random_state=42)<br/>y_train2 = to_categorical(y_train)<br/>y_test2 = to_categorical(y_test)</pre>
<p style="padding-left: 60px">In the preceding step, we are splitting the original data into the train and test datasets, and are converting the dependent variable into a one-hot-encoded variable.</p>
<ol start="7">
<li>Build the RNN architecture and compile the model:</li>
</ol>
<pre style="padding-left: 60px">embedding_vecor_length=32<br/>max_review_length=26<br/>model = Sequential()<br/>model.add(Embedding(input_dim=12533, output_dim=32, input_length = 26))</pre>
<p style="padding-left: 60px"><span>Note that embedding takes the total number of distinct words as input, and creates a vector for each word, where <kbd>output_dim</kbd> represents the number of dimensions in which the word is to be represented. <kbd>input_length</kbd> represents the number of words in each sentence:</span></p>
<pre style="padding-left: 60px">model.add(SimpleRNN(40, return_sequences=False))</pre>
<p style="padding-left: 60px" class="mce-root"><span>Note that, in the RNN layer, if we want to extract the output of each time step, we say the <kbd>return_sequences</kbd> parameter is <kbd>True</kbd>. However, in the use case that we are solving now, we extract the output only after reading through all the input words and thus <kbd>return_sequences = False</kbd>:</span></p>
<pre style="padding-left: 60px">model.add(Dense(2, activation='softmax'))<br/>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>print(model.summary())</pre>
<p style="padding-left: 60px">The summary of model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/7d2328cb-7549-4f74-9844-a2f5f3d89b04.png" style="width:34.50em;height:14.58em;" width="465" height="196"/></p>
<p class="mce-root"/>
<p style="padding-left: 60px">Let's understand why there are <kbd>401056</kbd> parameters to be estimated in the embedding layer. There are a total of 12, 532 unique words, and if we consider that there is no word with an index of 0, it results in a total of 12,533 possible words where each is represented in 32 dimensions and hence (12,533 x 32 = 401,056) parameters to be estimated.</p>
<p style="padding-left: 60px">Now, let's try to understand why there are 2,920 parameters in the <kbd>simpleRNN</kbd> layer.</p>
<p style="padding-left: 60px">There is a set of weights that connect the input to the 40 units of RNN. Given that there are 32 inputs at each time step (where the same set of weights is repeated for each time step), a total of 32 x 40 weights is used to connect the input to the hidden layer. This gives an output that is of 1 x 40 in dimension for each input.</p>
<p style="padding-left: 60px">Additionally, for the summation between the <em>X * W<sub>xh</sub></em> and <em>h<sub>(t-1) *</sub> W<sub>hh</sub></em> to happen (where <em>X</em> is is the input values, <em>W</em><sub><em>xh</em> </sub>is the weights-connecting input layer to the hidden layer, <em>W<sub>hh</sub></em> is the weights-connecting the previous time step's hidden layer to the current time step's hidden layer, and <em>h<sub>(t-1)</sub></em> is the hidden layer of previous time step)—given that the output of the <em>X W<sub>xh</sub></em> input is 1 x 40—the output of <em>h<sub>(t-1) </sub>X W<sub>hh</sub></em> should also be 1 x 40 in size. Thus, the <em>W<sub>hh</sub></em> matrix will be 40 x 40 in dimension, as the dimensions of  <em>h<sub>(t-1)</sub></em> are 1 x 40.</p>
<p style="padding-left: 60px">Along with weights, we would also have 40 bias terms associated with each of the 40 output and thus a total of (32 x 40 + 40 x 40 + 40 = 2,920) weights.</p>
<p style="padding-left: 60px">There are a total of 82 weights in the final layer, as the 40 units of the final time step are connected to the two possible output, resulting 40 x 2 weights and 2 biases, and thus a total of 82 units.</p>
<ol start="8">
<li>Fit the model:</li>
</ol>
<pre style="padding-left: 60px">model.fit(X_train, y_train2, validation_data=(X_test, y_test2), epochs=10, batch_size=32)</pre>
<p style="padding-left: 60px">The plot of accuracy and loss values in training, test dataset are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/afc6244e-0a9d-463a-956d-c6b4956c784d.jpg" style="width:31.33em;height:25.25em;" width="388" height="313"/></p>
<p>The output of the preceding model is also ~89%, and does not offer any significant improvement over the word-vector-based network that we built in <em>Text analysis using word vectors</em> chapter.</p>
<p>However, it is expected to have better accuracy as the number of data points increases.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>A traditional RNN that takes multiple time steps into account for giving predictions can be visualized as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1092 image-border" src="Images/a7fcb1be-8d05-49f7-811f-8f716f209092.png" style="width:24.92em;height:14.25em;" width="987" height="567"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Notice that, as time step increases, the impact of input at a much earlier layer would be lower. An intuition of that can be seen here (for a moment, let's ignore the bias terms):</p>
<p class="CDPAlignCenter CDPAlign"><em>h<sub>5</sub> = WX<sub>5</sub> + Uh<sub>4</sub> = WX5 + UWX<sub>4</sub> + U<sup>2</sup>WX<sub>3</sub> + U<sup>3</sup>WX<sub>2</sub> + U<sup>4</sup>WX<sub>1</sub></em></p>
<p>You can see that, as the time step increases, the value of hidden layer is highly dependent on <em>X<sub>1</sub></em> if <em>U&gt;1</em>, or much less dependent on <em>X<sub>1</sub></em> if <em>U&lt;1</em>.</p>
<p>The dependency on U matrix can also result in vanishing gradient when the value of <em>U</em> is very small and can result in exploding gradient when the value of <em>U</em> is very high.</p>
<p>The above phenomenon results in an issue when there is a long-term dependency in predicting the next word. To solve this problem, we'll use the <strong>Long Short Term Memory</strong> (<strong>LSTM</strong>) architecture in the next recipe.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building a LSTM Network from scratch in Python</h1>
                </header>
            
            <article>
                
<p>In the previous section on issues with traditional RNN, we learned about how RNN does not help when there is a long-term dependency. For example, imagine the input sentence is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em>I live in India. I speak ____.</em></p>
<p>The blank space in the preceding statement could be filled by looking at the key word, <em>India</em>, which is three time steps prior to the word we are trying to predict.</p>
<p>In a similar manner, if the key word is far away from the word to predict, vanishing/exploding gradient problems need to be solved.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we'll learn how LSTM helps in overcoming the long-term dependency drawback of the RNN architecture and also build a toy example so that we understand the various components of LSTM.</p>
<p class="mce-root"/>
<p>LSTM looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1094 image-border" src="Images/f2243925-4b0f-400b-bb2d-6d485e0d1401.png" style="width:41.33em;height:17.75em;" width="948" height="408"/></p>
<p>You can see that while the input, <strong>X</strong>, and the output of the hidden layer, (<strong>h</strong>), remain the same, there are different activations that happen in the hidden layer (sigmoid activation in certain cases and tanh activation in others).</p>
<p>Let's closely examine the various activations that happen in one of the time steps:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1095 image-border" src="Images/4c51bfa7-310d-4ee5-97dd-aff0de7c9669.png" style="width:22.75em;height:22.75em;" width="403" height="403"/></p>
<p>In the preceding diagram, <strong>X</strong> and <strong>h</strong> represent the input layer and the hidden layer.</p>
<p>The longterm memory is stored in cell state <strong>C</strong>.</p>
<p><span><span>The content that needs to be forgotten is obtained using the forget gate</span></span>:</p>
<p class="CDPAlignCenter CDPAlign"><em>f<sub>t</sub><span>=σ</span>(W<sub>xf</sub>x<sup>(t)</sup>+W<sub>hf</sub>h<sup>(t-1)</sup>+b<sub>f</sub>)</em></p>
<p>Sigmoid activation enables the network to selectively identify the content that needs to be forgotten.</p>
<p>The updated cell state after we determine the content that needs to be forgotten is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em>c<sub>t</sub>=(c<sub>t-1</sub> <img class="fm-editor-equation" src="Images/598b937a-e234-4d91-a30e-ae8b81fbf55e.png" style="width:1.17em;height:1.25em;" width="140" height="150"/> f)</em></p>
<p>Note that <img class="fm-editor-equation" src="Images/dc04ccba-f496-4601-93a6-3f1f35655156.png" style="width:1.17em;height:1.25em;" width="140" height="150"/> represents element-to-element multiplication.</p>
<p>For example, if the input input sequence of the sentence is <em>I live in India. I speak ___</em>, the blank space can be filled based on the input word <em>India</em>. After filling in the blank, we do not necessarily need the specific information of the name of the country.</p>
<p>We update the cell state based on what needs to be forgotten at the current time step.</p>
<p>In the next step, we will be adding additional information to the cell state based on the input provided in the current time step. Additionally, the magnitude of the update (either positive or negative) is obtained through the tanh activation.</p>
<p>The input can be specified as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em>i<sub>t</sub><span>=σ</span>(W<sub>xi</sub>x<sup>(t)</sup>+W<sub>hi</sub>h<sup>(t-1)</sup>+b<span>i</span>)</em></p>
<p>The modulation (magnitude of the input update) can be specified as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em>g<sub>t</sub><span>=tanh</span>(W<sub>xg</sub>x<sup>(t)</sup>+W<sub>hg</sub>h<sup>(t-1)</sup>+b<span>g</span>)</em></p>
<p>The cell state—where we are forgetting certain things in a time step and also adding additional information in the same time step—gets updated as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/b2688aa0-958e-4f11-9c52-f6661cacac20.png" style="width:18.25em;height:1.83em;" width="2590" height="260"/></p>
<p>In the final gate, we need to specify what part of the combination of input (combination of current time step input and previous time step's hidden layer value) and the cell state needs to be outputted to the next hidden layer:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/9f52fca7-fe04-4a8f-a16a-ae6386c325f1.png" style="width:18.75em;height:1.83em;" width="2660" height="260"/></p>
<p>The final hidden layer is represented as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/962e9401-a248-417f-8f8d-0ebdfff4aff3.png" style="width:12.17em;height:1.83em;" width="1730" height="260"/></p>
<p>This way, we are in a position to leverage the various gates in LSTM to selectively identify the information that needs to be stored in memory and thus overcome the limitation of RNN.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>To gain a practical intuition of how this theory works, let's look at the same example we worked out in understanding RNN but this time using LSTM.</p>
<p>Note that the data preprocessing steps are common between the two examples. Hence, we will reuse the preprocessing part (<em>step 1</em> to <em>step 4</em> in the <em>Building an RNN from scratch in Python</em> recipe) and directly head over to the model-building part (the code file is available as <kbd>LSTM_working_details.ipynb</kbd> in GitHub):</p>
<ol>
<li>Define the model:</li>
</ol>
<pre style="padding-left: 60px">embed_length=1<br/>max_length=2<br/>model = Sequential()<br/>model.add(LSTM(1,activation='tanh',return_sequences=False,<br/>recurrent_initializer='Zeros',recurrent_activation='sigmoid',<br/>input_shape=(max_length,embed_length),unroll=True))</pre>
<div class="packt_infobox"><span>Note that, in the preceding code, we initialized the recurrent initializer and recurrent activation to certain values only to make this example simpler; the purpose is only to help you understand what is happening in the backend.</span></div>
<pre style="padding-left: 60px">model.add(Dense(5, activation='softmax'))</pre>
<pre style="padding-left: 60px"># compile the model<br/>model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])<br/># summarize the model<br/>print(model.summary())</pre>
<p style="padding-left: 60px">A summary of the model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/04377b35-d2b0-4091-9dae-5d5558039585.png" style="width:37.75em;height:13.67em;" width="514" height="186"/></p>
<p style="padding-left: 60px">The number of parameters is <kbd>12</kbd> in the LSTM layer as there are four gates (forget, input, cell, and output), which results in four weights and four biases connecting the input to the hidden layer. Additionally, the recurrent layer contains weight values that correspond to the four gates, which gives us a total of <kbd>12</kbd> parameters.</p>
<p style="padding-left: 60px">The dense layer has a total of 10 parameters as there are five possible classes as output, and thus <span>five</span> weights and <span>five</span> biases that correspond to each connection from the hidden layer to the output layer.</p>
<ol start="2">
<li>Let's fit the model:</li>
</ol>
<pre style="padding-left: 60px">model.fit(padded_docs.reshape(2,2,1),np.array(one_hot_encoded_labels),epochs=500)</pre>
<p class="mce-root"/>
<ol start="3">
<li>The order of weights of this model are as follows:</li>
</ol>
<pre style="padding-left: 60px">model.weights[&lt;tf.Variable 'lstm_19/kernel:0' shape=(1, 4) dtype=float32_ref&gt;,<br/> &lt;tf.Variable 'lstm_19/recurrent_kernel:0' shape=(1, 4) dtype=float32_ref&gt;,<br/> &lt;tf.Variable 'lstm_19/bias:0' shape=(4,) dtype=float32_ref&gt;,<br/> &lt;tf.Variable 'dense_18/kernel:0' shape=(1, 5) dtype=float32_ref&gt;,<br/> &lt;tf.Variable 'dense_18/bias:0' shape=(5,) dtype=float32_ref&gt;]</pre>
<p style="padding-left: 60px">The weights can be obtained as follows:</p>
<pre style="padding-left: 60px">model.get_weights()</pre>
<p style="padding-left: 60px">From the preceding code (<kbd>model.weights</kbd>), we can see that the order of weights in the LSTM layer is as follows:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li>Weights of the input (kernel)</li>
<li>Weights corresponding to the hidden layer (<kbd>recurrent_kernel</kbd>)</li>
<li>Bias in the LSTM layer</li>
</ul>
</li>
</ul>
<p style="padding-left: 60px">Similarly, in the dense layer (the layer that connects the hidden layer to the output), the order of weights is as follows:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li>Weight to be multiplied with the hidden layer</li>
<li>Bias</li>
</ul>
</li>
</ul>
<p style="padding-left: 60px">Here is the order in which the weights and biases appear (not provided in the preceding output, but available in the GitHub repository of Keras) in the LSTM layer:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li>Input gate</li>
<li>Forget gate</li>
<li>Modulation gate (cell gate)</li>
<li>Output gate</li>
</ul>
</li>
</ul>
<ol start="3">
<li>Calculate the predictions for the input.</li>
</ol>
<div class="packt_infobox">We are using raw-encoded input values (1,2,3) without converting them into embedding values—only to see how the calculation works. In practice, we would be converting the input into embedding values.</div>
<p class="mce-root"/>
<ol start="4">
<li>Reshape the input for the predict method, so that it is as per the data format expected by LSTM (batch size, number of time steps, features per time step):</li>
</ol>
<pre style="padding-left: 60px">model.predict(padded_docs[0].reshape(1,2,1))<br/><span># array([[0.05610514, 0.11013522, 0.38451442, 0.0529648, 0.39628044]], dtype=float32)<br/></span></pre>
<p style="padding-left: 60px"><span>The output of predict method is provided in commented line in the code above.</span><span> </span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Validating the output</h1>
                </header>
            
            <article>
                
<p>Now that we have a predicted probability from the model, let's run the input through the forward pass of weights using NumPy to obtain the same output as we just did.</p>
<p>This is done so that we validate our understanding of how LSTM works under the hood. The steps that we take to validate the output of model we built are as follows:</p>
<ol start="1">
<li>Update the forget gate in time step 1. This step looks at the input and then provides an estimate of how much of the cell state (memory) known so far is to be forgotten (note the usage of the sigmoid function):</li>
</ol>
<pre style="padding-left: 60px">input_t0 = 3<br/>cell_state0 = 0<br/>forget0 = input_t0*model.get_weights()[0][0][1] + model.get_weights()[2][1]<br/>forget1 = 1/(1+np.exp(-(forget0)))</pre>
<ol start="2">
<li>Update the cell state based on the updated forget gate. The output of the previous step is being used here to direct the amount of values to be forgotten from the cell state (memory):</li>
</ol>
<pre style="padding-left: 60px">cell_state1 = forget1 * cell_state0</pre>
<ol start="3">
<li>Update the input gate value in time step 1. This step gives an estimate of how much new information is to be injected into the cell state based on the current input:</li>
</ol>
<pre style="padding-left: 60px">input_t0_1 = input_t0*model.get_weights()[0][0][0] + model.get_weights()[2][0]<br/>input_t0_2 = 1/(1+np.exp(-(input_t0_1)))</pre>
<ol start="4">
<li>Update the cell state based on the updated input value. This is the step where the output from the previous step is being used to dictate the amount of information update that is to happen to cell state (memory):</li>
</ol>
<pre style="padding-left: 60px">input_t0_cell1 = input_t0*model.get_weights()[0][0][2] +model.get_weights()[2][2]<br/>input_t0_cell2 = np.tanh(input_t0_cell1)</pre>
<p style="padding-left: 60px">The preceding <kbd>tanh</kbd> activation helps to determine whether the update from the input will add to or subtract from the cell state (memory). This provides an additional lever, as, if certain information is already conveyed in the current time step and is not useful in future time steps, we are better off wiping it off from the cell state so that this extra information (which might not be helpful in the next step) is wiped from memory:</p>
<pre style="padding-left: 60px">input_t0_cell3 = input_t0_cell2*input_t0_2<br/>input_t0_cell4 = input_t0_cell3 + cell_state1</pre>
<ol start="5">
<li>Update the output gate. This step provides an estimate of how much information will be conveyed in the current time step (note the usage of the sigmoid function in this regard):</li>
</ol>
<pre style="padding-left: 60px">output_t0_1 = input_t0*model.get_weights()[0][0][3] + model.get_weights()[2][3]<br/>output_t0_2 = 1/(1+np.exp(-output_t0_1))</pre>
<ol start="6">
<li>Calculate the hidden layer value at time step 1. Note that the final hidden-layer value at a time step is a combination of how much memory and output in the current time step is used to convey for a single time step:</li>
</ol>
<pre style="padding-left: 60px">hidden_layer_1 = np.tanh(input_t0_cell4)*output_t0_2</pre>
<p style="padding-left: 60px">We are done calculating the hidden layer value output from the first time step. In the next steps, we will pass the updated cell state value from time step 1 and the output of the hidden layer value from time step 1 as inputs to time step 2.</p>
<ol start="7">
<li>Pass the input value at time step 2 and the cell state value going into time step 2:</li>
</ol>
<pre style="padding-left: 60px">input_t1 = 1<br/>cell_state1 = input_t0_cell4</pre>
<ol start="8">
<li> Update the forget gate value:</li>
</ol>
<pre style="padding-left: 60px">forget21 = hidden_layer_1*model.get_weights()[1][0][1] + model.get_weights()[2][1] + input_t1*model.get_weights()[0][0][1]<br/>forget_22 = 1/(1+np.exp(-(forget21)))</pre>
<p class="mce-root"/>
<ol start="9">
<li>Update the cell state value in time step 2:</li>
</ol>
<pre style="padding-left: 60px">cell_state2 = cell_state1 * forget_22<br/>input_t1_1 = input_t1*model.get_weights()[0][0][0] + model.get_weights()[2][0] + hidden_layer_1*model.get_weights()[1][0][0]<br/>input_t1_2 = 1/(1+np.exp(-(input_t1_1)))<br/>input_t1_cell1 = input_t1*model.get_weights()[0][0][2] + model.get_weights()[2][2]+ hidden_layer_1*model.get_weights()[1][0][2]<br/>input_t1_cell2 = np.tanh(input_t1_cell1)<br/>input_t1_cell3 = input_t1_cell2*input_t1_2<br/>input_t1_cell4 = input_t1_cell3 + cell_state2</pre>
<ol start="10">
<li>Update the hidden layer output value based on the combination of updated cell state and the magnitude of output that is to be made:</li>
</ol>
<pre style="padding-left: 60px">output_t1_1 = input_t1*model.get_weights()[0][0][3] + model.get_weights()[2][3]+ hidden_layer_1*model.get_weights()[1][0][3]<br/>output_t1_2 = 1/(1+np.exp(-output_t1_1))<br/>hidden_layer_2 = np.tanh(input_t1_cell4)*output_t1_2</pre>
<ol start="11">
<li> Pass the hidden layer output through the dense layer:</li>
</ol>
<pre style="padding-left: 60px">final_output = hidden_layer_2 * model.get_weights()[3][0] +model.get_weights()[4]</pre>
<ol start="12">
<li>Run softmax on top of the output we just obtained:</li>
</ol>
<pre style="padding-left: 60px">np.exp(final_output)/np.sum(np.exp(final_output))</pre>
<pre style="padding-left: 60px"># array([0.05610514, 0.11013523, 0.3845144, 0.05296481, 0.39628044],dtype=float32)</pre>
<p>You should notice that the output obtained here is exactly the same as what we obtained from the <kbd>model.predict</kbd> method.</p>
<p>With this exercise, we are in a better position to appreciate the working details of LSTM.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementing LSTM for sentiment classification</h1>
                </header>
            
            <article>
                
<p>In <em>Implementing RNN for sentiment classification</em> recipe, we implemented sentiment classification using RNN. In this recipe, we will look at implementing it using LSTM.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The steps we'll adopt are as follows (the code file is available as <kbd>RNN_and_LSTM_sentiment_classification.ipynb</kbd> in GitHub):</p>
<ol>
<li>Define the model. The only change from the code we saw i<span>n </span><em>Implementing RNN for sentiment classification</em><span> recipe</span> will be the change from <kbd>simpleRNN</kbd> to LSTM in the model architecture part (we will be reusing the code from <em>step 1</em> to <em>step 6</em> in the <em>Implementing RNN for sentiment classification</em> recipe):</li>
</ol>
<pre style="padding-left: 60px">embedding_vecor_length=32<br/>max_review_length=26<br/>model = Sequential()<br/>model.add(Embedding(input_dim=12533, output_dim=32, input_length = 26))</pre>
<p style="padding-left: 60px">The input for the embedding layer is the total number of unique IDs present in the dataset and the expected dimension to which each word needs to be converted (<kbd>output_dim</kbd>).</p>
<p style="padding-left: 60px">Additionally, we'll also specify the maximum length of input, so that the LSTM layer in the next step has the required information—batch size, number of time steps (<kbd>input_length</kbd>), and the number of features per time (<kbd>step(output_dim)</kbd>):</p>
<pre style="padding-left: 60px">model.add(LSTM(40, return_sequences=False))<br/>model.add(Dense(2, activation='softmax'))<br/>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>print(model.summary())</pre>
<p style="padding-left: 60px">A summary of the model is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/b4e00271-4c1b-464a-bdc1-6dbc45594704.png" style="width:35.33em;height:15.83em;" width="471" height="211"/></p>
<p style="padding-left: 60px">While the parameters in the first and last layer are the same as what saw in the <em>Implementing RNN for sentiment classification</em><span> recipe</span>, the LSTM layer has a different number of parameters.</p>
<p style="padding-left: 60px">Let's understand how 11,680 parameters are obtained in the LSTM layer:</p>
<pre style="padding-left: 60px">W = model.layers[1].get_weights()[0]<br/>U = model.layers[1].get_weights()[1]<br/>b = model.layers[1].get_weights()[2]<br/>print(W.shape,U.shape,b.shape)</pre>
<p style="padding-left: 60px">The output will look like the following:</p>
<pre style="padding-left: 60px" class="mce-root">((32, 160), (40, 160), (160,))</pre>
<p style="padding-left: 60px" class="mce-root">Note that the total of the preceding weights has <em>(32*160) + (40*160) + 160 = 11,680</em> parameters.</p>
<p style="padding-left: 60px"><kbd>W</kbd> represents the weights that connect the input to each of the four cells (<kbd>i</kbd>, <kbd>f</kbd>, <kbd>c</kbd>, <kbd>o</kbd>), <kbd>U</kbd> represents the hidden-layer-to-hidden-layer connection, and <kbd>b</kbd> represents the bias in each gate.</p>
<p style="padding-left: 60px">Individual weights of the input, forget, cell state, and output gates can be obtained as follows:</p>
<pre style="padding-left: 60px">units = 40<br/>W_i = W[:, :units]<br/>W_f = W[:, units: units * 2]<br/>W_c = W[:, units * 2: units * 3]<br/>W_o = W[:, units * 3:]</pre>
<pre style="padding-left: 60px">U_i = U[:, :units]<br/>U_f = U[:, units: units * 2]<br/>U_c = U[:, units * 2: units * 3]<br/>U_o = U[:, units * 3:]</pre>
<pre style="padding-left: 60px">b_i = b[:units]<br/>b_f = b[units: units * 2]<br/>b_c = b[units * 2: units * 3]<br/>b_o = b[units * 3:]</pre>
<ol start="2">
<li>Fit the model as follows:</li>
</ol>
<pre style="padding-left: 60px">model.fit(X_train, y_train2, validation_data=(X_test, y_test2), epochs=50, batch_size=32)</pre>
<p>The variation of loss and accuracy over increasing epochs in training and test datasets are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/ffc8b5d6-d220-487e-b635-67ce7423dddf.jpg" style="width:32.17em;height:25.50em;" width="394" height="312"/></p>
<p>The prediction accuracy is 91% when using the LSTM layer, which is slightly better than the prediction accuracy when we are using the simpleRNN layer. Potentially, we can further improve upon the result by fine-tuning the number of LSTM units.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementing stacked LSTM for sentiment classification</h1>
                </header>
            
            <article>
                
<p>In the previous recipe, we implemented sentiment classification using LSTM in Keras. In this recipe, we will look at implementing the same thing but stack multiple LSTMs. Stacking multiple LSTMs is likely to capture more variation in the data and thus potentially a better accuracy.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Stacked LSTM is implemented as follows (the code file is available as <kbd>RNN_and_LSTM_sentiment_classification.ipynb</kbd> in GitHub):</p>
<ol>
<li>The only change in the code we saw earlier will be to change the <kbd>return_sequences</kbd> parameter to true. This ensures that the first LSTM returns a sequence of output (as many output as the number of LSTM units), which can then be passed to another LSTM as an input on top of the original LSTM in the model architecture part (more details on the <kbd>return_sequences</kbd> parameter can be found in the <em>Sequence to Sequence learning</em> chapter):</li>
</ol>
<pre style="padding-left: 60px">embedding_vecor_length=32<br/>max_review_length=26<br/>model = Sequential()<br/>model.add(Embedding(input_dim=12533, output_dim=32, input_length = 26))<br/>model.add(LSTM(40, return_sequences=True))<br/>model.add(LSTM(40, return_sequences=False))<br/>model.add(Dense(2, activation='softmax'))<br/>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>print(model.summary())</pre>
<p style="padding-left: 60px">A summary of model architecture is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/5e6ca342-8153-47a5-92ed-a04e9dcd64d8.png" style="width:36.50em;height:17.92em;" width="465" height="228"/></p>
<p style="padding-left: 60px">Note that, in the preceding architecture, there is an additional LSTM that is stacked on top of another LSTM. The output at each time step of the first LSTM is <kbd>40</kbd> values and thus an output shape of (<kbd>None</kbd>, <kbd>26</kbd>, <kbd>40</kbd>), where <kbd>None</kbd> represents the <kbd>batch_size</kbd>, <kbd>26</kbd> represents the number of time steps, and <kbd>40</kbd> represents the number of LSTM units considered.</p>
<p style="padding-left: 60px">Now that there are <kbd>40</kbd> input values, the number of parameters in the second LSTM is considered in the same fashion as in the previous recipe, as follows:</p>
<pre style="padding-left: 60px">W = model.layers[2].get_weights()[0]<br/>U = model.layers[2].get_weights()[1]<br/>b = model.layers[2].get_weights()[2]<br/>print(W.shape,U.shape,b.shape)</pre>
<p style="padding-left: 60px">The following are the values we get by executing preceding code:</p>
<pre style="padding-left: 60px">((40, 160), (40, 160), (160,))</pre>
<p style="padding-left: 60px">This results in a total of 12,960 parameters, as seen in the output.</p>
<p style="padding-left: 60px">W is 40 x 160 in shape, as it has 40 inputs that are mapped to 40 output and also 4 different gates to be controlled, and hence a total of 40 x 40 x 4 weights.</p>
<ol start="2">
<li>Implement the model, as follows:</li>
</ol>
<pre style="padding-left: 60px">model.fit(X_train, y_train2, validation_data=(X_test, y_test2), epochs=50, batch_size=32)</pre>
<p><span>The variation of loss and accuracy over increasing epochs in training and test datasets are as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/d2377a1a-3446-4819-bc41-7a6583e1c92e.jpg" style="width:30.33em;height:24.92em;" width="386" height="317"/></p>
<p>This results in an accuracy of 91%, as we saw with a single LSTM layer; however, with more data, stacked LSTM is likely to capture more variation in the data than the vanilla LSTM.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p><strong>Gated Recurrent Unit</strong> (<strong>GRU</strong>) is another architecture we can use and has accuracy that is similar to that of LSTM. For more information about GRU, visit <a href="https://arxiv.org/abs/1412.3555" target="_blank">https://arxiv.org/abs/1412.3555</a>.</p>


            </article>

            
        </section>
    </div>



  </body></html>