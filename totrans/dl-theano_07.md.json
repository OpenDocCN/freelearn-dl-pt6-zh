["```py\n    wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz -P /sharedfiles\n    tar xvzf /sharedfiles/cifar-10-python.tar.gz -C /sharedfiles/\n    ```", "```py\n    wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz -P /sharedfiles\n    tar xvzf food-101.tar.gz -C /sharedfiles/\n    ```", "```py\nl = NonlinearityLayer(\n      BatchNormLayer(\n        ConvLayer(l_in,\n          num_filters=n_filters[0],\n          filter_size=(3,3),\n          stride=(1,1),\n          nonlinearity=None,\n          pad='same',\n          W=he_norm)\n      ),\n      nonlinearity=rectify\n  )\n```", "```py\nInputLayer                       (None, 1, 28, 28)\nConv2DDNNLayer                   (None, 8, 28, 28)\nBatchNormLayer                   (None, 8, 28, 28)\nNonlinearityLayer                (None, 8, 28, 28)\nConv2DDNNLayer                   (None, 8, 28, 28)\nBatchNormLayer                   (None, 8, 28, 28)\nNonlinearityLayer                (None, 8, 28, 28)\nConv2DDNNLayer                   (None, 8, 28, 28)\nElemwiseSumLayer                 (None, 8, 28, 28)\nBatchNormLayer                   (None, 8, 28, 28)\nNonlinearityLayer                (None, 8, 28, 28)\nConv2DDNNLayer                   (None, 8, 28, 28)\nBatchNormLayer                   (None, 8, 28, 28)\nNonlinearityLayer                (None, 8, 28, 28)\nConv2DDNNLayer                   (None, 8, 28, 28)\nElemwiseSumLayer                 (None, 8, 28, 28)\nBatchNormLayer                   (None, 8, 28, 28)\nNonlinearityLayer                (None, 8, 28, 28)\nConv2DDNNLayer                   (None, 16, 14, 14)\nBatchNormLayer                   (None, 16, 14, 14)\nNonlinearityLayer                (None, 16, 14, 14)\nConv2DDNNLayer                   (None, 16, 14, 14)\nConv2DDNNLayer                   (None, 16, 14, 14)\nElemwiseSumLayer                 (None, 16, 14, 14)\nBatchNormLayer                   (None, 16, 14, 14)\nNonlinearityLayer                (None, 16, 14, 14)\nConv2DDNNLayer                   (None, 16, 14, 14)\nBatchNormLayer                   (None, 16, 14, 14)\nNonlinearityLayer                (None, 16, 14, 14)\nConv2DDNNLayer                   (None, 16, 14, 14)\nElemwiseSumLayer                 (None, 16, 14, 14)\nBatchNormLayer                   (None, 16, 14, 14)\nNonlinearityLayer                (None, 16, 14, 14)\nConv2DDNNLayer                   (None, 32, 7, 7)\nBatchNormLayer                   (None, 32, 7, 7)\nNonlinearityLayer                (None, 32, 7, 7)\nConv2DDNNLayer                   (None, 32, 7, 7)\nConv2DDNNLayer                   (None, 32, 7, 7)\nElemwiseSumLayer                 (None, 32, 7, 7)\nBatchNormLayer                   (None, 32, 7, 7)\nNonlinearityLayer                (None, 32, 7, 7)\nConv2DDNNLayer                   (None, 32, 7, 7)\nBatchNormLayer                   (None, 32, 7, 7)\nNonlinearityLayer                (None, 32, 7, 7)\nConv2DDNNLayer                   (None, 32, 7, 7)\nElemwiseSumLayer                 (None, 32, 7, 7)\nBatchNormLayer                   (None, 32, 7, 7)\nNonlinearityLayer                (None, 32, 7, 7)\nGlobalPoolLayer                  (None, 32)\nDenseLayer                       (None, 10)\n```", "```py\n# 8 -> 8 -> 16 -> 32\nn_filters = {0:8, 1:8, 2:16, 3:32}\n```", "```py\ndef residual_block(l, transition=False, first=False, filters=16):\n    if transition:\n        first_stride = (2,2)\n    else:\n        first_stride = (1,1)\n\n    if first:\n        bn_pre_relu = l\n    else:\n        bn_pre_conv = BatchNormLayer(l)\n        bn_pre_relu = NonlinearityLayer(bn_pre_conv, rectify)\n\n    conv_1 = NonlinearityLayer(BatchNormLayer(ConvLayer(bn_pre_relu, num_filters=filters, filter_size=(3,3), stride=first_stride, \n          nonlinearity=None, \n          pad='same', \n          W=he_norm)),nonlinearity=rectify)\n\n    conv_2 = ConvLayer(conv_1, num_filters=filters, filter_size=(3,3), stride=(1,1), nonlinearity=None, pad='same', W=he_norm)\n\n  # add shortcut connections\n    if transition:\n        # projection shortcut, as option B in paper\n        projection = ConvLayer(bn_pre_relu, num_filters=filters, filter_size=(1,1), stride=(2,2), nonlinearity=None, pad='same', b=None)\n    elif conv_2.output_shape == l.output_shape:\n        projection=l\n    else:\n        projection = ConvLayer(bn_pre_relu, num_filters=filters, filter_size=(1,1), stride=(1,1), nonlinearity=None, pad='same', b=None)\n\n    return ElemwiseSumLayer([conv_2, projection])\n```", "```py\nn_filters = {0:num_filters, 1:num_filters*width, 2:num_filters*2*width, 3:num_filters*4*width}\n```", "```py\ndef residual_bottleneck_block(l, transition=False, first=False, filters=16):\n    if transition:\n        first_stride = (2,2)\n    else:\n        first_stride = (1,1)\n\n    if first:\n        bn_pre_relu = l\n    else:\n        bn_pre_conv = BatchNormLayer(l)\n        bn_pre_relu = NonlinearityLayer(bn_pre_conv, rectify)\n\n    bottleneck_filters = filters / 4\n\n    conv_1 = NonlinearityLayer(BatchNormLayer(ConvLayer(bn_pre_relu, num_filters=bottleneck_filters, filter_size=(1,1), stride=(1,1), nonlinearity=None, pad='same', W=he_norm)),nonlinearity=rectify)\n\n    conv_2 = NonlinearityLayer(BatchNormLayer(ConvLayer(conv_1, num_filters=bottleneck_filters, filter_size=(3,3), stride=first_stride, nonlinearity=None, pad='same', W=he_norm)),nonlinearity=rectify)\n\n    conv_3 = ConvLayer(conv_2, num_filters=filters, filter_size=(1,1), stride=(1,1), nonlinearity=None, pad='same', W=he_norm)\n\n    if transition:\n        projection = ConvLayer(bn_pre_relu, num_filters=filters, filter_size=(1,1), stride=(2,2), nonlinearity=None, pad='same', b=None)\n    elif first:\n        projection = ConvLayer(bn_pre_relu, num_filters=filters, filter_size=(1,1), stride=(1,1), nonlinearity=None, pad='same', b=None)\n    else:\n        projection = l\n\n    return ElemwiseSumLayer([conv_3, projection])\n```", "```py\ndef model(shape, n=18, num_filters=16, num_classes=10, width=1, block='normal'):\n  l_in = InputLayer(shape=(None, shape[1], shape[2], shape[3]))\n  l = NonlinearityLayer(BatchNormLayer(ConvLayer(l_in, num_filters=n_filters[0], filter_size=(3,3), stride=(1,1), nonlinearity=None, pad='same', W=he_norm)),nonlinearity=rectify)\n\n  l = residual_block(l, first=True, filters=n_filters[1])\n for _ in range(1,n):\n      l = residual_block(l, filters=n_filters[1])\n\n  l = residual_block(l, transition=True, filters=n_filters[2])\n  for _ in range(1,n):\n      l = residual_block(l, filters=n_filters[2])\n\n  l = residual_block(l, transition=True, filters=n_filters[3])\n  for _ in range(1,n):\n      l = residual_block(l, filters=n_filters[3])\n\n  bn_post_conv = BatchNormLayer(l)\n  bn_post_relu = NonlinearityLayer(bn_post_conv, rectify)\n  avg_pool = GlobalPoolLayer(bn_post_relu)\n  return DenseLayer(avg_pool, num_units=num_classes, W=HeNormal(), nonlinearity=softmax)\n```", "```py\n python train.py --dataset=mnist --n=1 --num_filters=8 --batch_size=500\n```", "```py\n     python train.py --dataset=cifar10 --n=18 --num_filters=16 --batch_size=64\n    ```", "```py\n     python train.py --dataset=cifar10 --n=27 --num_filters=16 --batch_size=64\n    ```", "```py\n     python train.py --dataset=cifar10 --n=18 --num_filters=16 --width=4 --batch_size=64\n    ```", "```py\n     python train.py --dataset=cifar10 --n=18 --num_filters=16 --block=bottleneck --batch_size=64\n    ```", "```py\n     python train.py --dataset=food101 --batch_size=10 --n=18 --num_filters=16\n    ```", "```py\ndef dense_block(network, transition=False, first=False, filters=16):\n    if transition:\n        network = NonlinearityLayer(BatchNormLayer(network), nonlinearity=rectify)\n        network = ConvLayer(network,network.output_shape[1], 1, pad='same', W=he_norm, b=None, nonlinearity=None)\n        network = Pool2DLayer(network, 2, mode='average_inc_pad')\n\n    network = NonlinearityLayer(BatchNormLayer(network), nonlinearity=rectify)\n    conv = ConvLayer(network,filters, 3, pad='same', W=he_norm, b=None, nonlinearity=None)\n    return ConcatLayer([network, conv], axis=1)\n```", "```py\ndef dense_fast_block(network, transition=False, first=False, filters=16):\n    if transition:\n        network = NonlinearityLayer(BiasLayer(ScaleLayer(network)), nonlinearity=rectify)\n        network = ConvLayer(network,network.output_shape[1], 1, pad='same', W=he_norm, b=None, nonlinearity=None)\n        network = BatchNormLayer(Pool2DLayer(network, 2, mode='average_inc_pad'))\n\n    network = NonlinearityLayer(BiasLayer(ScaleLayer(network)), nonlinearity=rectify)\n    conv = ConvLayer(network,filters, 3, pad='same', W=he_norm, b=None, nonlinearity=None)\n    return ConcatLayer([network, BatchNormLayer(conv)], axis=1)\n```", "```py\npython train.py --dataset=cifar10 --n=13 --num_filters=16 --block=dense_fast --batch_size=64\n```"]