["```py\nimport org.apache.spark._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.Dataset\n```", "```py\ncase class CustomerAccount(state_code: String, \n    account_length: Integer, \n    area_code: String, \n    international_plan: String, \n    voice_mail_plan: String, \n    num_voice_mail: Double, \n    total_day_mins: Double, \n    total_day_calls: Double, \n    total_day_charge: Double,\n    total_evening_mins: Double, \n    total_evening_calls: Double, \n    total_evening_charge: Double,\n    total_night_mins: Double, \n    total_night_calls: Double, \n    total_night_charge: Double,\n    total_international_mins: Double, \n    total_international_calls: Double, \n    total_international_charge: Double,\n    total_international_num_calls: Double, \n    churn: String)\n```", "```py\nval schema = StructType(Array(\n    StructField(\"state_code\", StringType, true),\n    StructField(\"account_length\", IntegerType, true),\n    StructField(\"area_code\", StringType, true),\n    StructField(\"international_plan\", StringType, true),\n    StructField(\"voice_mail_plan\", StringType, true),\n    StructField(\"num_voice_mail\", DoubleType, true),\n    StructField(\"total_day_mins\", DoubleType, true),\n    StructField(\"total_day_calls\", DoubleType, true),\n    StructField(\"total_day_charge\", DoubleType, true),\n    StructField(\"total_evening_mins\", DoubleType, true),\n    StructField(\"total_evening_calls\", DoubleType, true),\n    StructField(\"total_evening_charge\", DoubleType, true),\n    StructField(\"total_night_mins\", DoubleType, true),\n    StructField(\"total_night_calls\", DoubleType, true),\n    StructField(\"total_night_charge\", DoubleType, true),\n    StructField(\"total_international_mins\", DoubleType, true),\n    StructField(\"total_international_calls\", DoubleType, true),\n    StructField(\"total_international_charge\", DoubleType, true),\n    StructField(\"total_international_num_calls\", DoubleType, true),\n    StructField(\"churn\", StringType, true)\n))\n```", "```py\nval spark: SparkSession = SparkSessionCreate.createSession(\"preprocessing\")\nimport spark.implicits._\n```", "```py\nval trainSet: Dataset[CustomerAccount] = spark.read.\n        option(\"inferSchema\", \"false\")\n        .format(\"com.databricks.spark.csv\")\n        .schema(schema)\n        .load(\"data/churn-bigml-80.csv\")\n        .as[CustomerAccount]\n```", "```py\ntrainSet.printSchema()\n>>>\n```", "```py\ntrainSet.show()\n>>>\n```", "```py\nval statsDF = trainSet.describe()\nstatsDF.show()\n>>>\n```", "```py\ntrainSet.cache()\n```", "```py\ntrainSet.groupBy(\"churn\").sum(\"total_international_num_calls\").show()\n>>>\n+-----+----------------------------------+\nchurn|sum(total_international_num_calls)|\n+-----+----------------------------------+\n|False| 3310.0|\n| True| 856.0|\n+-----+----------------------------------+\n```", "```py\ntrainSet.groupBy(\"churn\").sum(\"total_international_charge\").show()\n >>>\n+-----+-------------------------------+\n|churn|sum(total_international_charge)|\n+-----+-------------------------------+\n|False| 6236.499999999996|\n| True| 1133.63|\n+-----+-------------------------------+\n```", "```py\nval testSet: Dataset[CustomerAccount] = \n    spark.read.\n    option(\"inferSchema\", \"false\")\n    .format(\"com.databricks.spark.csv\")\n    .schema(schema)\n    .load(\"data/churn-bigml-20.csv\")\n    .as[CustomerAccount]\n```", "```py\ntestSet.cache()\n```", "```py\ntrainSet.createOrReplaceTempView(\"UserAccount\")\nspark.catalog.cacheTable(\"UserAccount\")\n```", "```py\ntrainSet.groupBy(\"churn\").count.show()\n>>>\n+-----+-----+\n|churn|count|\n+-----+-----+\n|False| 2278|\n| True| 388 |\n+-----+-----+\n```", "```py\nspark.sqlContext.sql(\"SELECT churn,SUM(international_num_calls) as Total_intl_call FROM UserAccount GROUP BY churn\").show()\n>>>\n```", "```py\nval fractions = Map(\"False\" -> 0.1675, \"True\" -> 1.0)\n```", "```py\nval churnDF = trainSet.stat.sampleBy(\"churn\", fractions, 12345L)\n```", "```py\nchurnDF.groupBy(\"churn\").count.show()\n>>>\n+-----+-----+\n|churn|count|\n+-----+-----+\n|False| 390|\n| True| 388|\n+-----+-----+\n```", "```py\nspark.sqlContext.sql(\"SELECT churn, SUM(total_day_charge) as TDC, SUM(total_evening_charge) as TEC,    \n                      SUM(total_night_charge) as TNC, SUM(total_international_charge) as TIC,  \n                      SUM(total_day_charge) + SUM(total_evening_charge) + SUM(total_night_charge) + \n                      SUM(total_international_charge) as Total_charge FROM UserAccount GROUP BY churn \n                      ORDER BY Total_charge DESC\")\n.show()\n>>>\n```", "```py\nspark.sqlContext.sql(\"SELECT churn, SUM(total_day_mins) \n                      + SUM(total_evening_mins) + SUM(total_night_mins) \n                      + SUM(total_international_mins) as Total_minutes \n                    FROM UserAccount GROUP BY churn\").show()\n>>>\n```", "```py\nval trainDF = churnDF\n    .drop(\"state_code\")\n    .drop(\"area_code\")\n    .drop(\"voice_mail_plan\")\n    .drop(\"total_day_charge\")\n    .drop(\"total_evening_charge\")\n```", "```py\ntrainDF.select(\"account_length\", \"international_plan\", \"num_voice_mail\",         \n               \"total_day_calls\",\"total_international_num_calls\", \"churn\")\n.show(10)\n>>>\n```", "```py\nval ipindexer = new StringIndexer()\n    .setInputCol(\"international_plan\")\n    .setOutputCol(\"iplanIndex\")\n\nval labelindexer = new StringIndexer()\n    .setInputCol(\"churn\")\n    .setOutputCol(\"label\")\n```", "```py\n* Label → churn: True or False\n* Features → {(\"account_length\", \"iplanIndex\", \"num_voice_mail\", \"total_day_mins\", \"total_day_calls\", \"total_evening_mins\", \"total_evening_calls\", \"total_night_mins\", \"total_night_calls\", \"total_international_mins\", \"total_international_calls\", \"total_international_num_calls\"}\n```", "```py\nval featureCols = Array(\"account_length\", \"iplanIndex\", \n                        \"num_voice_mail\", \"total_day_mins\", \n                        \"total_day_calls\", \"total_evening_mins\", \n                        \"total_evening_calls\", \"total_night_mins\", \n                        \"total_night_calls\", \"total_international_mins\", \n                        \"total_international_calls\", \"total_international_num_calls\")\n```", "```py\nval assembler = new VectorAssembler()\n    .setInputCols(featureCols)\n    .setOutputCol(\"features\")\n```", "```py\nimport org.apache.spark._\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.ml.classification.{BinaryLogisticRegressionSummary, LogisticRegression, LogisticRegressionModel}\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics \nimport org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n```", "```py\nval spark: SparkSession = SparkSessionCreate.createSession(\"ChurnPredictionLogisticRegression\")\nimport spark.implicits._\n```", "```py\nval numFolds = 10\nval MaxIter: Seq[Int] = Seq(100)\nval RegParam: Seq[Double] = Seq(1.0) // L2 regularization param, set 1.0 with L1 regularization\nval Tol: Seq[Double] = Seq(1e-8)// for convergence tolerance for iterative algorithms\nval ElasticNetParam: Seq[Double] = Seq(0.0001) //Combination of L1 & L2\n```", "```py\nval lr = new LogisticRegression()\n    .setLabelCol(\"label\")\n    .setFeaturesCol(\"features\")\n```", "```py\nval pipeline = new Pipeline()\n    .setStages(Array(PipelineConstruction.ipindexer,\n    PipelineConstruction.labelindexer,\n    PipelineConstruction.assembler, lr))\n```", "```py\nval paramGrid = new ParamGridBuilder()\n    .addGrid(lr.maxIter, MaxIter)\n    .addGrid(lr.regParam, RegParam)\n    .addGrid(lr.tol, Tol)\n    .addGrid(lr.elasticNetParam, ElasticNetParam)\n    .build()\n```", "```py\nval evaluator = new BinaryClassificationEvaluator()\n    .setLabelCol(\"label\")\n    .setRawPredictionCol(\"prediction\")\n```", "```py\nval crossval = new CrossValidator()\n    .setEstimator(pipeline)\n    .setEvaluator(evaluator)\n    .setEstimatorParamMaps(paramGrid)\n    .setNumFolds(numFolds)\n```", "```py\nval cvModel = crossval.fit(Preprocessing.trainDF)\n```", "```py\nval predictions = cvModel.transform(Preprocessing.testSet)\nal result = predictions.select(\"label\", \"prediction\", \"probability\")\nval resutDF = result.withColumnRenamed(\"prediction\", \"Predicted_label\")\nresutDF.show(10)\n>>>\n```", "```py\nval accuracy = evaluator.evaluate(predictions)\nprintln(\"Classification accuracy: \" + accuracy)\n>>>\nClassification accuracy: 0.7670592565329408\n```", "```py\nval predictionAndLabels = predictions\n    .select(\"prediction\", \"label\")\n    .rdd.map(x => (x(0).asInstanceOf[Double], x(1)\n    .asInstanceOf[Double]))\n```", "```py\nval metrics = new BinaryClassificationMetrics(predictionAndLabels)\nprintln(\"Area under the precision-recall curve: \" + metrics.areaUnderPR)\nprintln(\"Area under the receiver operating characteristic (ROC) curve : \" + metrics.areaUnderROC)\n>>>\nArea under the precision-recall curve: 0.5761887477313975\nArea under the receiver operating characteristic (ROC) curve: 0.7670592565329408\n```", "```py\nval lp = predictions.select(\"label\", \"prediction\")\nval counttotal = predictions.count()\nval correct = lp.filter($\"label\" === $\"prediction\").count()\n\nval wrong = lp.filter(not($\"label\" === $\"prediction\")).count()\nval ratioWrong = wrong.toDouble / counttotal.toDouble\nval ratioCorrect = correct.toDouble / counttotal.toDouble\n\nval truep = lp.filter($\"prediction\" === 0.0).filter($\"label\" ===\n$\"prediction\").count() / counttotal.toDouble\n\nval truen = lp.filter($\"prediction\" === 1.0).filter($\"label\" ===\n$\"prediction\").count() / counttotal.toDouble\n\nval falsep = lp.filter($\"prediction\" === 1.0).filter(not($\"label\" ===\n$\"prediction\")).count() / counttotal.toDouble\n\nval falsen = lp.filter($\"prediction\" === 0.0).filter(not($\"label\" ===\n$\"prediction\")).count() / counttotal.toDouble\n\nprintln(\"Total Count : \" + counttotal)\nprintln(\"Correct : \" + correct)\nprintln(\"Wrong: \" + wrong)\nprintln(\"Ratio wrong: \" + ratioWrong)\nprintln(\"Ratio correct: \" + ratioCorrect)\nprintln(\"Ratio true positive : \" + truep)\nprintln(\"Ratio false positive : \" + falsep)\nprintln(\"Ratio true negative : \" + truen)\nprintln(\"Ratio false negative : \" + falsen)\n>>>\n```", "```py\nimport org.apache.spark._\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.ml.classification.{LinearSVC, LinearSVCModel}\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions.max\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n```", "```py\nval spark: SparkSession = SparkSessionCreate.createSession(\"ChurnPredictionLogisticRegression\")\nimport spark.implicits._\n```", "```py\nval numFolds = 10\nval MaxIter: Seq[Int] = Seq(100)\nval RegParam: Seq[Double] = Seq(1.0) // L2 regularization param, set 0.10 with L1 reguarization\nval Tol: Seq[Double] = Seq(1e-8)\nval ElasticNetParam: Seq[Double] = Seq(1.0) // Combination of L1 and L2\n```", "```py\nval svm = new LinearSVC()\n```", "```py\nval pipeline = new Pipeline()\n     .setStages(Array(PipelineConstruction.ipindexer,\n                      PipelineConstruction.labelindexer,\n                      PipelineConstruction.assembler,svm)\n                      )\n```", "```py\nval paramGrid = new ParamGridBuilder()\n    .addGrid(svm.maxIter, MaxIter)\n    .addGrid(svm.regParam, RegParam)\n    .addGrid(svm.tol, Tol)\n    .addGrid(svm.elasticNetParam, ElasticNetParam)\n    .build()\n```", "```py\nval evaluator = new BinaryClassificationEvaluator()\n    .setLabelCol(\"label\")\n    .setRawPredictionCol(\"prediction\")\n```", "```py\nval crossval = new CrossValidator()\n    .setEstimator(pipeline)\n    .setEvaluator(evaluator)\n    .setEstimatorParamMaps(paramGrid)\n    .setNumFolds(numFolds)\n```", "```py\nval cvModel = crossval.fit(Preprocessing.trainDF)\n```", "```py\nval predictions = cvModel.transform(Preprocessing.testSet)\nprediction.show(10)\n>>>\n```", "```py\nval accuracy = evaluator.evaluate(predictions)\nprintln(\"Classification accuracy: \" + accuracy)\n>>>\nClassification accuracy: 0.7530180345969819\n```", "```py\nval predictionAndLabels = predictions\n    .select(\"prediction\", \"label\")\n    .rdd.map(x => (x(0).asInstanceOf[Double], x(1)\n    .asInstanceOf[Double]))\n```", "```py\nval metrics = new BinaryClassificationMetrics(predictionAndLabels)\nprintln(\"Area under the precision-recall curve: \" + metrics.areaUnderPR)\nprintln(\"Area under the receiver operating characteristic (ROC) curve : \" + metrics.areaUnderROC)\n>>>\nArea under the precision-recall curve: 0.5595712265324828\nArea under the receiver operating characteristic (ROC) curve: 0.7530180345969819\n```", "```py\nval lp = predictions.select(\"label\", \"prediction\")\nval counttotal = predictions.count()\n\nval correct = lp.filter($\"label\" === $\"prediction\").count()\n\nval wrong = lp.filter(not($\"label\" === $\"prediction\")).count()\nval ratioWrong = wrong.toDouble / counttotal.toDouble\n\nval ratioCorrect = correct.toDouble / counttotal.toDouble\n\nval truep = lp.filter($\"prediction\" === 0.0).filter($\"label\" ===\n$\"prediction\").count() / counttotal.toDouble\n\nval truen = lp.filter($\"prediction\" === 1.0).filter($\"label\" ===\n$\"prediction\").count() / counttotal.toDouble\n\nval falsep = lp.filter($\"prediction\" === 1.0).filter(not($\"label\" ===\n$\"prediction\")).count() / counttotal.toDouble\n\nval falsen = lp.filter($\"prediction\" === 0.0).filter(not($\"label\" ===\n$\"prediction\")).count() / counttotal.toDouble\n\nprintln(\"Total Count : \" + counttotal)\nprintln(\"Correct : \" + correct)\nprintln(\"Wrong: \" + wrong)\nprintln(\"Ratio wrong: \" + ratioWrong)\nprintln(\"Ratio correct: \" + ratioCorrect)\nprintln(\"Ratio true positive : \" + truep)\nprintln(\"Ratio false positive : \" + falsep)\nprintln(\"Ratio true negative : \" + truen)\nprintln(\"Ratio false negative : \" + falsen)\n>>>\n```", "```py\nimport org.apache.spark._\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql._\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.classification.{DecisionTreeClassifier, DecisionTreeClassificationModel}\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\n```", "```py\nval spark: SparkSession = SparkSessionCreate.createSession(\"ChurnPredictionDecisionTrees\")\nimport spark.implicits._\n```", "```py\nval dTree = new DecisionTreeClassifier()\n                .setLabelCol(\"label\")\n                .setFeaturesCol(\"features\")\n                .setSeed(1234567L)\n```", "```py\nval pipeline = new Pipeline()\n                .setStages(Array(PipelineConstruction.ipindexer,\n                PipelineConstruction.labelindexer,\n                PipelineConstruction.assembler,dTree))\n```", "```py\nvar paramGrid = new ParamGridBuilder()\n    .addGrid(dTree.impurity, \"gini\" :: \"entropy\" :: Nil)\n    .addGrid(dTree.maxBins, 2 :: 5 :: 10 :: 15 :: 20 :: 25 :: 30 :: Nil)\n    .addGrid(dTree.maxDepth, 5 :: 10 :: 15 :: 20 :: 25 :: 30 :: 30 :: Nil)\n    .build()\n```", "```py\nval evaluator = new BinaryClassificationEvaluator()\n    .setLabelCol(\"label\")\n    .setRawPredictionCol(\"prediction\")\n```", "```py\nval crossval = new CrossValidator()\n    .setEstimator(pipeline)\n    .setEvaluator(evaluator)\n    .setEstimatorParamMaps(paramGrid)\n    .setNumFolds(numFolds)\n```", "```py\nval cvModel = crossval.fit(Preprocessing.trainDF)\n```", "```py\nval predictions = cvModel.transform(Preprocessing.testSet)\nprediction.show(10)\n>>>\n```", "```py\nval accuracy = evaluator.evaluate(predictions)\nprintln(\"Classification accuracy: \" + accuracy)\n>>>\nAccuracy: 0.870334928229665\n```", "```py\nval predictionAndLabels = predictions\n    .select(\"prediction\", \"label\")\n    .rdd.map(x => (x(0).asInstanceOf[Double], x(1)\n    .asInstanceOf[Double]))\n```", "```py\nval metrics = new BinaryClassificationMetrics(predictionAndLabels)\nprintln(\"Area under the precision-recall curve: \" + metrics.areaUnderPR)\nprintln(\"Area under the receiver operating characteristic (ROC) curve : \" + metrics.areaUnderROC)\n>>>\nArea under the precision-recall curve: 0.7293101942399631\nArea under the receiver operating characteristic (ROC) curve: 0.870334928229665\n```", "```py\nval lp = predictions.select(\"label\", \"prediction\")\nval counttotal = predictions.count()\n\nval correct = lp.filter($\"label\" === $\"prediction\").count()\n\nval wrong = lp.filter(not($\"label\" === $\"prediction\")).count()\n\nval ratioWrong = wrong.toDouble / counttotal.toDouble\n\nval ratioCorrect = correct.toDouble / counttotal.toDouble\n\nval truep = lp.filter($\"prediction\" === 0.0).filter($\"label\" ===\n$\"prediction\").count() / counttotal.toDouble\n\nval truen = lp.filter($\"prediction\" === 1.0).filter($\"label\" ===\n$\"prediction\").count() / counttotal.toDouble\n\nval falsep = lp.filter($\"prediction\" === 1.0).filter(not($\"label\" ===\n$\"prediction\")).count() / counttotal.toDouble\n\nval falsen = lp.filter($\"prediction\" === 0.0).filter(not($\"label\" ===\n$\"prediction\")).count() / counttotal.toDouble\n\nprintln(\"Total Count : \" + counttotal)\nprintln(\"Correct : \" + correct)\nprintln(\"Wrong: \" + wrong)\nprintln(\"Ratio wrong: \" + ratioWrong)\nprintln(\"Ratio correct: \" + ratioCorrect)\nprintln(\"Ratio true positive : \" + truep)\nprintln(\"Ratio false positive : \" + falsep)\nprintln(\"Ratio true negative : \" + truen)\nprintln(\"Ratio false negative : \" + falsen)\n>>>\n```", "```py\nval bestModel = cvModel.bestModel\nprintln(\"The Best Model and Parameters:n--------------------\")\nprintln(bestModel.asInstanceOf[org.apache.spark.ml.PipelineModel].stages(3))\n>>>\n```", "```py\nDecisionTreeClassificationModel (uid=dtc_1fb45416b18b) of depth 5 with 53 nodes.\n```", "```py\nbestModel.asInstanceOf[org.apache.spark.ml.PipelineModel]\n    .stages(3)\n    .extractParamMap\n\nval treeModel = bestModel.asInstanceOf[org.apache.spark.ml.PipelineModel]\n    .stages(3)\n    .asInstanceOf[DecisionTreeClassificationModel]\nprintln(\"Learned classification tree model:n\" + treeModel.toDebugString)\n>>>\n```", "```py\nIf (feature 3 <= 245.2)\n    If (feature 11 <= 3.0)\n        If (feature 1 in {1.0})\n            If (feature 10 <= 2.0)\n                Predict: 1.0\n            Else (feature 10 > 2.0)\n            If (feature 9 <= 12.9)\n                Predict: 0.0\n            Else (feature 9 > 12.9)\n                Predict: 1.0\n        ...\n    Else (feature 7 > 198.0)\n        If (feature 2 <= 28.0)\n            Predict: 1.0\n        Else (feature 2 > 28.0)\n            If (feature 0 <= 60.0)\n                Predict: 0.0\n            Else (feature 0 > 60.0)\n                Predict: 1.0\n```", "```py\nprintln(\"Feature 11:\" + Preprocessing.trainDF.filter(PipelineConstruction.featureCols(11)))\nprintln(\"Feature 3:\" + Preprocessing.trainDF.filter(PipelineConstruction.featureCols(3)))\n>>>\nFeature 11: [total_international_num_calls: double]\nFeature 3: [total_day_mins: double]\n```", "```py\nimport org.apache.spark._\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql._\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.classification.{RandomForestClassifier, RandomForestClassificationModel}\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\n```", "```py\nval spark: SparkSession = SparkSessionCreate.createSession(\"ChurnPredictionRandomForest\")\nimport spark.implicits._\n```", "```py\nval rf = new RandomForestClassifier()\n    .setLabelCol(\"label\")\n    .setFeaturesCol(\"features\")\n    .setSeed(1234567L)// for reproducibility\n```", "```py\nval pipeline = new Pipeline()\n    .setStages(Array(PipelineConstruction.ipindexer,\n    PipelineConstruction.labelindexer,\n    PipelineConstruction.assembler,rf))\n```", "```py\nval paramGrid = new ParamGridBuilder()\n    .addGrid(rf.maxDepth, 3 :: 5 :: 15 :: 20 :: 50 :: Nil)\n    .addGrid(rf.featureSubsetStrategy, \"auto\" :: \"all\" :: Nil)\n    .addGrid(rf.impurity, \"gini\" :: \"entropy\" :: Nil)\n    .addGrid(rf.maxBins, 2 :: 5 :: 10 :: Nil)\n    .addGrid(rf.numTrees, 10 :: 50 :: 100 :: Nil)\n    .build()\n```", "```py\nval evaluator = new BinaryClassificationEvaluator()\n    .setLabelCol(\"label\")\n    .setRawPredictionCol(\"prediction\")\n```", "```py\nval crossval = new CrossValidator()\n    .setEstimator(pipeline)\n    .setEvaluator(evaluator)\n    .setEstimatorParamMaps(paramGrid)\n    .setNumFolds(numFolds)\n```", "```py\nval cvModel = crossval.fit(Preprocessing.trainDF)\n```", "```py\nval predictions = cvModel.transform(Preprocessing.testSet)\nprediction.show(10)\n>>>\n```", "```py\nval accuracy = evaluator.evaluate(predictions)\nprintln(\"Classification accuracy: \" + accuracy)\n>>>\nAccuracy: 0.870334928229665\n```", "```py\nval predictionAndLabels = predictions\n    .select(\"prediction\", \"label\")\n    .rdd.map(x => (x(0).asInstanceOf[Double], x(1)\n    .asInstanceOf[Double]))\n```", "```py\nval metrics = new BinaryClassificationMetrics(predictionAndLabels)\n\nprintln(\"Area under the precision-recall curve: \" + metrics.areaUnderPR)\nprintln(\"Area under the receiver operating characteristic (ROC) curve : \" + metrics.areaUnderROC)\n>>>\nArea under the precision-recall curve: 0.7293101942399631\nArea under the receiver operating characteristic (ROC) curve: 0.870334928229665\n```", "```py\nval lp = predictions.select(\"label\", \"prediction\")\nval counttotal = predictions.count()\n\nval correct = lp.filter($\"label\" === $\"prediction\").count()\n\nval wrong = lp.filter(not($\"label\" === $\"prediction\")).count()\n\nval ratioWrong = wrong.toDouble / counttotal.toDouble\n\nval ratioCorrect = correct.toDouble / counttotal.toDouble\n\nval truep = lp.filter($\"prediction\" === 0.0).filter($\"label\" ===\n$\"prediction\").count() / counttotal.toDouble\n\nval truen = lp.filter($\"prediction\" === 1.0).filter($\"label\" ===\n$\"prediction\").count() / counttotal.toDouble\n\nval falsep = lp.filter($\"prediction\" === 1.0).filter(not($\"label\" ===\n$\"prediction\")).count() / counttotal.toDouble\n\nval falsen = lp.filter($\"prediction\" === 0.0).filter(not($\"label\" ===\n$\"prediction\")).count() / counttotal.toDouble\n\nprintln(\"Total Count : \" + counttotal)\nprintln(\"Correct : \" + correct)\nprintln(\"Wrong: \" + wrong)\nprintln(\"Ratio wrong: \" + ratioWrong)\nprintln(\"Ratio correct: \" + ratioCorrect)\nprintln(\"Ratio true positive : \" + truep)\nprintln(\"Ratio false positive : \" + falsep)\nprintln(\"Ratio true negative : \" + truen)\nprintln(\"Ratio false negative : \" + falsen)\n>>>\n```", "```py\n// Save the workflow\ncvModel.write.overwrite().save(\"model/RF_model_churn\")\n```", "```py\n// Load the workflow back\nval cvModel = CrossValidatorModel.load(\"model/ RF_model_churn/\")\n```", "```py\nval predictions = cvModel.transform(Preprocessing.testSet)\n```", "```py\nval evaluator = new BinaryClassificationEvaluator()\n    .setLabelCol(\"label\")\n    .setRawPredictionCol(\"prediction\")\n\nval accuracy = evaluator.evaluate(predictions)\n    println(\"Accuracy: \" + accuracy)\n    evaluator.explainParams()\n\nval predictionAndLabels = predictions\n    .select(\"prediction\", \"label\")\n    .rdd.map(x => (x(0).asInstanceOf[Double], x(1)\n    .asInstanceOf[Double]))\n\nval metrics = new BinaryClassificationMetrics(predictionAndLabels)\nval areaUnderPR = metrics.areaUnderPR\nprintln(\"Area under the precision-recall curve: \" + areaUnderPR)\nval areaUnderROC = metrics.areaUnderROC\nprintln(\"Area under the receiver operating characteristic (ROC) curve: \" + areaUnderROC)\n>>>\n```"]