<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deep Neural Networks for Regression</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we worked with a dataset that had a categorical target variable, and we went over the steps for developing a classification model using Keras. In situations where the response variable is numeric, supervised learning problems are categorized as regression problems. In this chapter, we will develop a prediction model for numeric response variables. To illustrate the process of developing the prediction model, we will make use of the Boston Housing dataset, which is available within the <kbd>mlbench</kbd> package.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Understanding the Boston Housing dataset</li>
<li>Preparing the data</li>
<li>Creating and fitting a deep neural network model for regression</li>
<li>Model evaluation and prediction</li>
<li>Performance optimization tips and best practices</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the Boston Housing dataset</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will use six libraries. These libraries are as listed in the following code:</p>
<pre># Libraries<br/>library(keras)<br/>library(mlbench)<br/>library(psych)<br/>library(dplyr)<br/>library(magrittr)<br/>library(neuralnet)</pre>
<p>The structure of the <kbd>BostonHousing</kbd> data is as follows:</p>
<pre># Data structure<br/>data(BostonHousing)<br/>str(BostonHousing)<br/><br/><span>OUTPUT<br/><strong>'data.frame':        506 obs. of  14 variables:
 $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...
 $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...
 $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...
 $ chas   : Factor w/ 2 levels "0","1": 1 1 1 1 1 1 1 1 1 1 ...
 $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...
 $ rm     : num  6.58 6.42 7.18 7 7.15 ...
 $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...
 $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...
 $ rad    : num  1 2 2 3 3 3 5 5 5 5 ...
 $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...
 $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...
 $ b      : num  397 397 393 395 397 ...
 $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...
 $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...</strong></span></pre>
<p>As you can see from the preceding output, this dataset has <kbd>506</kbd> observations and <kbd>14</kbd> variables. Out of the 14 variables, 13 are numeric and 1 variable (<kbd>chas</kbd>) is of the factor type. The last variable, <kbd>medv</kbd> (the <span>median value of owner-occupied homes in thousand-USD units),</span> is the dependent, or target, variable. The remaining 13 variables are independent. The following is a brief description of all the variables, drawn up in a table for easy reference:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Variables</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Description</strong></td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>crim</kbd></td>
<td class="CDPAlignCenter CDPAlign">Per-capita crime rate by town</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>zn</kbd></td>
<td class="CDPAlignCenter CDPAlign">Proportion of residential land zoned for lots over 25,000 sq ft</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>indus</kbd></td>
<td class="CDPAlignCenter CDPAlign">Proportion of nonretail business acres per town</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>chas</kbd></td>
<td class="CDPAlignCenter CDPAlign">Charles River dummy variable (1 if the tract bounds a river; 0 otherwise)</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>nox</kbd></td>
<td class="CDPAlignCenter CDPAlign">Nitric-oxides concentration (parts per 10 million)</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>rm</kbd></td>
<td class="CDPAlignCenter CDPAlign">Average number of rooms per dwelling</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>age</kbd></td>
<td class="CDPAlignCenter CDPAlign">Proportion of owner-occupied units built prior to 1940</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>dis</kbd></td>
<td class="CDPAlignCenter CDPAlign">Weighted distances to five Boston employment centers</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>rad</kbd></td>
<td class="CDPAlignCenter CDPAlign">Index of accessibility to radial highways</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>tax</kbd></td>
<td class="CDPAlignCenter CDPAlign">Full-value property-tax rate per 10,000 USD</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>ptratio</kbd></td>
<td class="CDPAlignCenter CDPAlign">Pupil–teacher ratio by town</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>lstat</kbd></td>
<td class="CDPAlignCenter CDPAlign">Percentage of lower-income status members of the population</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><kbd>medv</kbd></td>
<td class="CDPAlignCenter CDPAlign"><span>Median value of owner-occupied homes in thousand-USD units</span></td>
</tr>
</tbody>
</table>
<p> </p>
<p class="mce-root">This data is based on the 1970 census. A detailed statistical study using this data was published by <span>Harrison and Rubinfeld</span> in 1978 (reference: <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.926.5532&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.926.5532&amp;rep=rep1&amp;type=pdf</a>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the data</h1>
                </header>
            
            <article>
                
<p>We start by changing the name of the <kbd>BostonHousing</kbd> data to simply <kbd>data</kbd> for ease of use. Independent variables that are of the factor type are then converted to the numeric type using the <kbd>lapply</kbd> function.</p>
<div class="packt_tip">Note that for this data, the only factor variable is <kbd>chas</kbd>; however, for any other dataset with more factor variables, this code will work fine.</div>
<p>Take a look at the following code:</p>
<pre># Converting factor variables to numeric<br/>data &lt;- BostonHousing<br/>data %&gt;% lapply(function(x) as.numeric(as.character(x)))<br/>data &lt;- data.frame(data)</pre>
<p>In the preceding code, after converting factor variables to the <kbd>numeric</kbd> type, we also change the format of <kbd>data</kbd> to <kbd>data.frame</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visualizing the neural network</h1>
                </header>
            
            <article>
                
<p>To visualize a neural network with hidden layers, we will use the <kbd>neuralnet</kbd> function. For illustration, two hidden layers with 10 and 5 units will be used in this example. The input layer has 13 nodes based on 13 independent variables. The output layer has only one node for the target variable, <kbd>medv</kbd>. The code used is as follows:</p>
<pre># Neural network<br/>n &lt;- neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+b+lstat,<br/>                data = data,<br/>                hidden = c(10,5),<br/>                linear.output = F,<br/>                lifesign = 'full',<br/>                rep=1)<br/><br/># Plot<br/>plot(n, col.hidden = "darkgreen", <br/>      col.hidden.synapse = 'darkgreen',<br/>      show.weights = F, <br/>      information = F, <br/>      fill = "lightblue")</pre>
<p><span>As shown in the preceding code, the result is saved in <kbd>n</kbd>, and it is then used for plotting the architecture of the neural network, as shown in the following diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/411058c3-98fe-4dd2-b54c-865c61368359.png" style="width:36.50em;height:21.00em;"/></p>
<p>As you can see from the preceding diagram, the input layer has 13 nodes for 13 independent variables. There are two hidden layers: the first hidden layer has 10 nodes and the second hidden layer has 5 nodes. Each node in the hidden layer is connected to all the nodes in the previous and the following layer. The output layer has one node for the response variable, <kbd>medv</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data partitioning</h1>
                </header>
            
            <article>
                
<p>Next, we change the data into a matrix format. We also set dimension names to <kbd>NULL</kbd>, which changes the names of the variables to the default names, <kbd>V1</kbd>, <kbd>V2</kbd>, <kbd>V3</kbd>, ..., <kbd>V14</kbd>:</p>
<pre>data &lt;- as.matrix(data)<br/>dimnames(data) &lt;- NULL </pre>
<p>We then the partition data into training and test datasets using the following code:</p>
<pre># Data partitioning<br/>set.seed(1234)<br/>ind &lt;- sample(2, nrow(data), replace = T, prob=c(.7, .3))<br/>training &lt;- data[ind==1, 1:13]<br/>test &lt;- data[ind==2, 1:13]<br/>trainingtarget &lt;- data[ind==1, 14]<br/>testtarget &lt;- data[ind==2, 14]</pre>
<p>A data split of 70:30 is used in this example. To maintain the repeatability of the data split, we use a random seed of <kbd>1234</kbd>. This will allow the same samples to be included in the training and test data each time data partitioning is carried out on any computer. The data for the independent variables are stored in <kbd>training</kbd> for the training data and in <kbd>test</kbd> for the test data. Similarly, the data for the dependent variable, <kbd>medv</kbd>, based on the corresponding split data, are stored in <kbd>trainingtarget</kbd> and <kbd>testtarget</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Normalization</h1>
                </header>
            
            <article>
                
<p>To normalize the data, the mean and standard deviations are obtained for all independent variables in the training data. Normalization is then carried out using the <kbd>scale</kbd> function:</p>
<div class="packt_tip">For both the train and test data, the mean and standard deviations are based on the training data used.</div>
<pre># Normalization<br/>m &lt;- colMeans(training)<br/>sd &lt;- apply(training, 2, sd)<br/>training &lt;- scale(training, center = m, scale = sd)<br/>test &lt;- scale(test, center = m, scale = sd)</pre>
<p>This concludes the data preparation step for this data. It should be noted that different datasets may need extra steps that are unique to that dataset—for example, many large datasets may have very high amounts of missing data values, and they may require additional data preparation steps in the form of arriving at a strategy for handling missing values and inputting missing values wherever necessary.</p>
<p>In the next section, we will create a deep neural network architecture and then fit a model for the accurate prediction of the numeric target variable.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating and fitting a deep neural network model for regression</h1>
                </header>
            
            <article>
                
<p>To create and fit a deep neural network model for a regression problem, we will make use of Keras. The code used for the model architecture is as follows:</p>
<div class="packt_tip">Note that the input layer having 13 units and the output layer having 1 unit is fixed based on the data; however, to arrive at a suitable number of hidden layers and the number of units in each layer, you need to experiment.</div>
<pre class="mce-root"># Model architecture<br/>model &lt;- keras_model_sequential()<br/>model %&gt;%<br/>   layer_dense(units = 10, activation = 'relu', input_shape = c(13)) %&gt;%  <br/>   layer_dense(units = 5, activation = 'relu') %&gt;%<br/>   layer_dense(units = 1) <br/>summary(model)<br/><br/><span>OUTPUT<br/><strong>___________________________________________________________________________
Layer (type)                     Output Shape                Param #      
===========================================================================
dense_1 (Dense)                   (None, 10)                   140          
___________________________________________________________________________
dense_2 (Dense)                   (None, 5)                    55           
___________________________________________________________________________
dense_3 (Dense)                   (None, 1)                     6            
===========================================================================
Total params: 201
Trainable params: 201
Non-trainable params: 0
___________________________________________________________________________</strong></span></pre>
<p>As you can see from the preceding code, we use the <kbd>keras_model_sequential</kbd> function to create a sequential model. The structure of the neural network is defined using the <kbd>layer_dense</kbd> function. Since there are 13 independent variables, <kbd>input_shape</kbd> is used to specify 13 units. The first hidden layer has <kbd>10</kbd> units and the rectified linear unit, or <kbd>relu</kbd>, is used as the activation function in this first hidden layer. The second hidden layer has <kbd>5</kbd> units, with <kbd>relu</kbd> as the activation function. The last, <kbd>layer_dense</kbd>, has <kbd>1</kbd> unit<span><span>, which </span></span>represents one dependent variable, <kbd>medv</kbd>. Using the <kbd>summary</kbd> function, you can print a model summary, which shows 201 total parameters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Calculating the total number of parameters</h1>
                </header>
            
            <article>
                
<p class="mce-root">Let's now see how a total of 201 parameters are obtained for this model. The <kbd>dense_1</kbd> <span>layer</span><span> </span><span>shows <kbd>140</kbd> parameters. These parameters are based on there being 13 units in the input layer that connect with each of the 10 units in the first hidden layer, meaning that there are 130 parameters (</span><span>13 x 10)</span><span>. The remaining 10 parameters come from the bias term for each of the 10 units in the first hidden layer. Similarly, 50 parameters (</span><span>10 x 5) </span><span>are from the connections between two hidden layers and the remaining 5 parameters come from the bias term from each of the 5 units in the second hidden layer. Finally,</span> <kbd>dense_3</kbd> <span>has <kbd>6</kbd> parameters ((</span><span>5 x 1) + 1)</span><span>. Thus, in all, there are 201 parameters based on the architecture of the neural network model that was chosen in this example.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compiling the model</h1>
                </header>
            
            <article>
                
<p class="mce-root">After the model architecture is defined, the model is compiled to configure the learning process using the following code:</p>
<pre class="mce-root"># Compile model<br/>model %&gt;% compile(loss = 'mse', <br/>   optimizer = 'rmsprop', <br/>   metrics = 'mae')</pre>
<p>As shown in the preceding code, we define the loss function as the mean square error, or <kbd>mse</kbd>. At this step, the <kbd>rmsprop</kbd> optimizer and mean absolute error, or <kbd>mae</kbd>, metric is also defined. We choose these because our response variable is numeric.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fitting the model</h1>
                </header>
            
            <article>
                
<p>Next, the model is trained using the <kbd>fit</kbd> function. Note that, as the training of the model proceeds, we get a visual as well as a numerical summary after each epoch. The output from the last three epochs is shown in the following code. We get the mean absolute error and loss values for both the training and the validation data. Note that, as pointed out in <a href="db6a812d-2bad-4f40-9e99-0e20abbe665c.xhtml">Chapter 1</a>, <em>Revisiting Deep Learning Architecture and Techniques</em>, each time we train a network, the training and validation errors can vary because of the random initialization of network weights. Such an outcome is expected even when the data is partitioned using the same random seed. To obtain repeatable results, it is always a good idea to save the model using the <kbd>save_model_hdf5</kbd> function and then reload it when needed.</p>
<p>The code used for training the network is as follows:</p>
<pre class="mce-root"># Fit model<br/>model_one &lt;- model %&gt;%  <br/>   fit(training,<br/>   trainingtarget,<br/>   epochs = 100,<br/>   batch_size = 32,<br/>   validation_split = 0.2)<br/><br/>OUTPUT from last 3 epochs
<strong>Epoch 98/100
284/284 [==============================] - 0s 74us/step - loss: 24.9585 - mean_absolute_error: 3.6937 - val_loss: 86.0545 - val_mean_absolute_error: 8.2678
Epoch 99/100
284/284 [==============================] - 0s 78us/step - loss: 24.6357 - mean_absolute_error: 3.6735 - val_loss: 85.4038 - val_mean_absolute_error: 8.2327
Epoch 100/100
284/284 [==============================] - 0s 92us/step - loss: 24.3293 - mean_absolute_error: 3.6471 - val_loss: 84.8307 - val_mean_absolute_error: 8.2015</strong></pre>
<p>As you can see from the preceding code, the model is trained in small batches of size <kbd>32</kbd>, and 20% of the data is reserved for validation to avoid overfitting. Here, <kbd>100</kbd> epochs or iterations are run to train the network. Once the training process is completed, information related to the training process is saved in <kbd>model_one</kbd>, which can then be used to plot the loss and mean absolute error based on the training and validation data for all epochs:</p>
<pre>plot(model_one)</pre>
<p>The preceding line of code will return the following output. Let's have a look at the l<span>oss and mean absolute error for training and validation data (<kbd>model_one</kbd>) </span>plot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/37680ec9-e55b-4c9a-9f21-607ae941db74.png"/></p>
<p>From the preceding plot, we can make the following observations:</p>
<ul>
<li>The <kbd>mae</kbd> and <kbd>loss</kbd> values decrease for both the training and validation data as the training proceeds.</li>
<li>The rate of decrease in errors for the training data reduces after about 60 epochs.</li>
</ul>
<p>After developing the prediction model, we can assess its performance by evaluating the prediction quality of the model, which we will look at in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model evaluation and prediction</h1>
                </header>
            
            <article>
                
<p>Model evaluation is an important step in the process of arriving at a suitable prediction model. A model may show good performance with training data that was used for developing the model; however, the real test of a model is with data that the model has not yet seen. Let's look at the model performance based on the test data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluation</h1>
                </header>
            
            <article>
                
<p>The performance of the model is evaluated using the <kbd>evaluate</kbd> function with the help of the test data shown in the following code:</p>
<pre># Model evaluation<br/>model %&gt;%  evaluate(test, testtarget) <br/><br/>OUTPUT<br/><strong> ## $loss</strong><br/><strong> ## [1] 31.14591 </strong><br/><strong> ##</strong><br/><strong> ## $mean_absolute_error</strong><br/><strong> ## [1] 3.614594</strong></pre>
<p>From the preceding output, we can see that the loss and mean absolute error for the test data are <kbd>31.15</kbd> and <kbd>3.61</kbd> respectively. We will use these numbers later to compare and assess whether or not the changes that we will make to the current model help to improve the prediction performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Prediction</h1>
                </header>
            
            <article>
                
<p>Let's predict the <kbd>medv</kbd> values for the <kbd>test</kbd> data and store the results in <kbd>pred</kbd> using the following code:</p>
<pre class="mce-root"># Prediction<br/>pred &lt;- model %&gt;%  predict(test)<br/>cbind(pred[1:10], testtarget[1:10])<br/><br/>OUTPUT<br/><strong>          [,1] [,2]</strong><br/><strong> [1,] 33.18942 36.2</strong><br/><strong> [2,] 18.17827 20.4</strong><br/><strong> [3,] 17.89587 19.9</strong><br/><strong> [4,] 13.07977 13.9</strong><br/><strong> [5,] 14.17268 14.8</strong><br/><strong> [6,] 19.09264 18.4</strong><br/><strong> [7,] 19.81316 18.9</strong><br/><strong> [8,] 21.00356 24.7</strong><br/><strong> [9,] 30.50263 30.8</strong><br/><strong>[10,] 19.75816 19.4</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We can take a look at the first 10 predicted and actual values using the <kbd>cbind</kbd> function. The first column in the output shows the predicted values based on the model and the second column shows the actual values. We can make the following observations from the output:</p>
<ul>
<li>The prediction for the first sample in the test data is about <kbd>33.19</kbd> and the actual value is <kbd>36.2</kbd>. The model underestimates the response by about <kbd>3</kbd> points.</li>
<li>For the second sample, the model underestimates the response by over <kbd>2</kbd> points.</li>
<li>For the tenth sample, the predicted and actual values are very close.</li>
<li>For the sixth sample, the model overestimates the response.</li>
</ul>
<p>To get an overall picture of the prediction performance, we can develop a scatter plot of the predicted versus the actual values. We will use the following code:</p>
<pre>plot(testtarget, pred,<br/>      xlab = 'Actual',<br/>      ylab = 'Prediction')<br/> abline(a=0,b=1)</pre>
<p class="mce-root">The scatter plot shows the predicted versus the actual response values based on the test data:</p>
<p class="CDPAlignCenter CDPAlign"><img class="details-image" src="assets/16a25a48-a15a-4d4f-9ac8-ce7dd907adc3.png" style="width:26.50em;height:27.50em;"/></p>
<p>From the preceding graph, we can see the overall performance of the prediction model. The relationship between the actual and predicted values is positive and approximately linear. Although we can see that the model has decent performance, clearly there is scope for further improvement that makes data points closer to the ideal line that has zero intercepts and a slope of 1. We will further explore making improvements to the model by developing a deeper neural network model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Improvements</h1>
                </header>
            
            <article>
                
<p>In the modified new model, we will build a deeper network by adding more layers. The additional layers are expected to show patterns in the data that the smaller network we used earlier was not able to show.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deeper network architecture</h1>
                </header>
            
            <article>
                
<p>The code used for this experiment is as follows:</p>
<pre class="mce-root"># Model Architecture<br/>model &lt;- keras_model_sequential()<br/>model %&gt;%<br/> layer_dense(units = 100, activation = 'relu', input_shape = c(13)) %&gt;% <br/> layer_dropout(rate = 0.4) %&gt;%<br/> layer_dense(units = 50, activation = 'relu') %&gt;%<br/> layer_dropout(rate = 0.3) %&gt;%<br/> layer_dense(units = 20, activation = 'relu') %&gt;%<br/> layer_dropout(rate = 0.2) %&gt;%<br/> layer_dense(units = 1)  <br/>summary(model)<br/><br/>OUTPUT<br/> <strong>## ___________________________________________________________________________</strong><br/><strong> ## Layer (type)                     Output Shape                  Param #    </strong><br/><strong> ## ===========================================================================</strong><br/><strong> ## dense_4 (Dense)                  (None, 100)                   1400       </strong><br/><strong> ## ___________________________________________________________________________</strong><br/><strong> ## dropout_1 (Dropout)              (None, 100)                   0          </strong><br/><strong> ## ___________________________________________________________________________</strong><br/><strong> ## dense_5 (Dense)                  (None, 50)                    5050       </strong><br/><strong> ## ___________________________________________________________________________</strong><br/><strong> ## dropout_2 (Dropout)              (None, 50)                    0          </strong><br/><strong> ## ___________________________________________________________________________</strong><br/><strong> ## dense_6 (Dense)                  (None, 20)                    1020       </strong><br/><strong> ## ___________________________________________________________________________</strong><br/><strong> ## dropout_3 (Dropout)              (None, 20)                    0          </strong><br/><strong> ## ___________________________________________________________________________</strong><br/><strong> ## dense_7 (Dense)                  (None, 1)                     21         </strong><br/><strong> ## ===========================================================================</strong><br/><strong> ## Total params: 7,491</strong><br/><strong> ## Trainable params: 7,491</strong><br/><strong> ## Non-trainable params: 0</strong><br/><strong> ## _________________________________________________________________________</strong><br/><br/># Compile model<br/>model %&gt;% compile(loss = 'mse', <br/>                   optimizer = 'rmsprop', <br/>                   metrics = 'mae')<br/><br/># Fit model<br/>model_two &lt;- model %&gt;%  <br/>   fit(training,<br/>       trainingtarget,<br/>       epochs = 100,<br/>       batch_size = 32, <br/>       validation_split = 0.2)<br/>plot(model_two)</pre>
<p><span>From the preceding code, we can observe that we now have three hidden layers with <kbd>100</kbd>, <kbd>50</kbd>, and <kbd>20</kbd> units respectively. W</span><span>e have also added a dropout layer after each hidden layer with rates of <kbd>0.4</kbd>, <kbd>0.3</kbd>, and <kbd>0.2</kbd> respectively. As an example of what a dropout layer's rate means, a rate of 0.4 means that 40% of the units in the first hidden layer are dropped to zero at the time of training, which helps to avoid overfitting. The total number of parameters in this model has now increased to <kbd>7,491</kbd>. Note that, in the previous model, the total number of parameters was <kbd>201</kbd>, and clearly we are going for a significantly bigger neural network. Next, we compile the model with the same settings that we used earlier, and subsequently, we will fit the model and store the results in <kbd>model_two</kbd>. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Results</h1>
                </header>
            
            <article>
                
<p><span>The following figure provides the loss and mean absolute error for <kbd>model_two</kbd> over 100 epochs:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b48f8055-750e-4560-9978-92c2fe0abfe3.png"/></p>
<p>From the preceding figure, we can make the following observations:</p>
<ul>
<li>The mean absolute error and loss values for the training and validation data drop very quickly to low values, and after about 30 epochs, we do not see any major improvement.</li>
<li>There is no evidence of overfitting as the training and validation errors seem closer to each other.</li>
</ul>
<p>We can obtain the loss and mean absolute error values for the test data using the following code:</p>
<pre class="mce-root"># Model evaluation<br/>model %&gt;%  evaluate(test, testtarget) <br/><br/>OUTPUT<br/> ## $loss<br/> ## [1] 24.70368 <br/> ##<br/> ## $mean_absolute_error<br/> ## [1] 3.02175 <br/><br/>pred &lt;- model %&gt;%  predict(test)<br/>plot(testtarget, pred,<br/>     xlab = 'Actual', <br/>     ylab = 'Prediction')<br/>abline(a=0,b=1)</pre>
<p><span>The loss and mean absolute error values using the <kbd>test</kbd> data and <kbd>model_two</kbd> are obtained as <kbd>24.70</kbd> and <kbd>3.02</kbd> respectively. This is a significant improvement compared to the results that we obtained from <kbd>model_one</kbd>.</span></p>
<p><span>We can visually see this improvement using the scatter plot for the predicted values versus the actual response values in the following graph:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="details-image" src="assets/6cd957b8-3c86-4bc4-bdc7-ab0ae18ccb17.png" style="width:29.50em;height:30.67em;"/></p>
<p>From the preceding graph, we can see that the spread in the scatter plot of actual versus predicted values is visibly less than that of the earlier scatter plot. This indicates better prediction performance compared to the previous model. Although <kbd>model_two</kbd> performs better than the previous model, at higher values, we can see the occurrence of significant underestimation of the target values. So, although we have developed a better model, we can also further explore the potential for the further improvement of this prediction model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance optimization tips and best practices</h1>
                </header>
            
            <article>
                
<p>Improving model performance can involve different strategies. Here, we will discuss two main strategies. One strategy is to make changes to the model architecture and observe the results to get any useful insights or indications of improvement. Another strategy could involve exploring the transformation of the target variable. In this section, we will try a combination of both of these strategies.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Log transformation on the output variable</h1>
                </header>
            
            <article>
                
<p>To overcome the issue of significant underestimation <span>of the target variable </span><span>at higher values, let's try log transformation on the target variable and see whether or not this helps us to further improve the model. Our next model has some minor changes to the architecture as well. In</span> <kbd>model_two</kbd><span>, we did not notice any major issue or evidence related to overfitting, and as a result, we can increase the number of units a little and also slightly reduce the percentages for dropout. The following is the code for this experiment:</span></p>
<pre># log transformation and model architecture <br/>trainingtarget &lt;- log(trainingtarget)<br/> testtarget &lt;- log(testtarget)<br/> model &lt;- keras_model_sequential()<br/> model %&gt;%<br/>   layer_dense(units = 100, activation = 'relu', input_shape = c(13)) %&gt;%  <br/>   layer_dropout(rate = 0.4) %&gt;% <br/>   layer_dense(units = 50, activation = 'relu') %&gt;%<br/>   layer_dropout(rate = 0.2) %&gt;%<br/>   layer_dense(units = 25, activation = 'relu') %&gt;%<br/>   layer_dropout(rate = 0.1) %&gt;%<br/>   layer_dense(units = 1)<br/> summary(model)<br/><br/>OUTPUT<br/><strong>## ___________________________________________________________________________</strong><br/><strong> ## Layer (type)                     Output Shape                  Param #    </strong><br/><strong> ## ===========================================================================</strong><br/><strong> ## dense_8 (Dense)                  (None, 100)                   1400       </strong><br/><strong> ## ___________________________________________________________________________</strong><br/><strong> ## dropout_4 (Dropout)              (None, 100)                   0           </strong><br/><strong> ## ___________________________________________________________________________</strong><br/><strong> ## dense_9 (Dense)                  (None, 50)                    5050       </strong><br/><strong> ## ___________________________________________________________________________</strong><br/><strong> ## dropout_5 (Dropout)              (None, 50)                    0          </strong><br/><strong> ## ___________________________________________________________________________</strong><br/><strong> ## dense_10 (Dense)                 (None, 25)                    1275       </strong><br/><strong> ## ___________________________________________________________________________</strong><br/><strong> ## dropout_6 (Dropout)              (None, 25)                    0          </strong><br/><strong> ## ___________________________________________________________________________</strong><br/><strong> ## dense_11 (Dense)                 (None, 1)                     26         </strong><br/><strong> ## ===========================================================================</strong><br/><strong> ## Total params: 7,751</strong><br/><strong> ## Trainable params: 7,751</strong><br/><strong> ## Non-trainable params: 0</strong><br/><strong> ## ___________________________________________________________________________</strong></pre>
<p class="mce-root">We will increase the number of units in the third hidden layer from <kbd>20</kbd> to <kbd>25</kbd>. Dropout rates for the second and third hidden layers are also reduced to <kbd>0.2</kbd> and <kbd>0.1</kbd> respectively. Note that the overall number of parameters has now increased to <kbd>7751</kbd>.</p>
<p class="mce-root">We next compile the model and then fit the model. The model results are stored in <kbd>model_three</kbd>, which we use for plotting the graph, as shown in the following code:</p>
<pre class="mce-root"># Compile model<br/>model %&gt;% compile(loss = 'mse', <br/>                   optimizer = optimizer_rmsprop(lr = 0.005),<br/>                   metrics = 'mae')<br/><br/># Fit model<br/> model_three &lt;- model %&gt;%  <br/>   fit(training,<br/>       trainingtarget,<br/>       epochs = 100,<br/>       batch_size = 32, <br/>       validation_split = 0.2)<br/>plot(model_three)</pre>
<p>The following shows the output of the <span>loss and mean absolute error for training and validation data (<kbd>model_three</kbd>)</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/840b6abb-532f-41be-9f1d-880f25acd57d.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"/>
<p>We can see from the preceding plot that although the values in the plot are not directly comparable to earlier figures because of the log transformation, we can see that the overall errors decrease and become stable after about 50 epochs for both the mean absolute error and the loss.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model performance</h1>
                </header>
            
            <article>
                
<p>We also obtain <kbd>loss</kbd> and <kbd>mae</kbd> values for this new model, but again, the numbers obtained are not directly comparable to the earlier two models for the log scale:</p>
<pre class="mce-root"># Model evaluation<br/>model %&gt;%  evaluate(test, testtarget) <br/><br/>OUTPUT<br/>## $loss<br/> ## [1] 0.02701566<br/> ##<br/> ## $mean_absolute_error<br/> ## [1] 0.1194756 <br/><br/>pred &lt;- model %&gt;%  predict(test)<br/> plot(testtarget, pred)</pre>
<p>We obtain a scatter plot of the actual values (log transformed) versus the predicted values based on the test data. We also get a scatter plot of the actual versus predicted values in the original scale for comparison with earlier plots. The s<span>catter plots for predicted versus actual response values (<kbd>model_three</kbd>)</span> are as shown in the following graph:</p>
<p class="CDPAlignCenter CDPAlign"><img class="details-image" src="assets/70341185-4756-4a9d-a62c-46d2eaf4dafb.png"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>From the preceding graph, we can see that the significant underestimation pattern observed in earlier models shows improvement, both in the log scale and in the original scale. In the original scale, the data points at higher values are relatively closer to the diagonal line, indicating improved prediction performance by the model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we went through the steps for developing a prediction model when the response variable is of a numeric type. We started with a neural network model that had 201 parameters and then developed deep neural network models with over 7,000 parameters. You may have noticed that, in this chapter, we made use of comparatively <span>deeper</span><span> </span><span>and</span><span> </span><span>more complex neural network models compared to the previous chapter, where we developed a classification model for the target variable that was of a categorical nature. In both</span> <a href="c5c236d5-fc58-4d90-95b0-2b05b148b187.xhtml">Chapter 2</a><span>, </span><em>Deep Neural Networks for Multiclass Classification</em><span>, and</span> <a href="07c9aa4a-1c93-490a-bfcd-7c4bcde639d5.xhtml">Chapter 3</a><span>,</span> <em>Deep Neural Networks for Regression</em><span>, we developed models based on data that was structured. In the next chapter, we move on to problems where the data type is unstructured. More specifically, we'll deal with the image type of data and go over the problem of image classification and recognition using deep neural network models.</span></p>
<p>In the next chapter, we will cover the steps required to develop an image recognition and prediction model using deep neural networks.</p>


            </article>

            
        </section>
    </body></html>