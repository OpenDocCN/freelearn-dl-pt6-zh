<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">High Frequency Bitcoin Price Prediction from Historical and Live Data</h1>
                </header>
            
            <article>
                
<p>Bitcoin is a worldwide cryptocurrency and digital payment system considered the <strong>first decentralized digital currency</strong>, since the system works without a central repository or single administrator. In recent times, it has gained much popularity and attention among people around the world.</p>
<p>In this chapter, we will see how to develop a real-life project using Scala, Spark ML, Cryptocompare API, and Bitcoin historical (and live) data to predict the price for an upcoming week, month, and so on that help us taking automated decision for online cryptocurrency. In addition to this, we will see how to generate a simple signal for online Bitcoin trading.</p>
<p>Briefly, we will learn the following topics throughout this end-to-end project:</p>
<ul>
<li>Bitcoin, cryptocurrency, and online trading</li>
<li>Historical and live-price data collection</li>
<li>High-level pipeline of the prototype</li>
<li>Gradient-boosted trees regression for Bitcoin price prediction</li>
<li>Demo prediction and signal generation using the Scala play framework</li>
<li>Future outlook—using the same technique for other datasets</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bitcoin, cryptocurrency, and online trading</h1>
                </header>
            
            <article>
                
<p>Bitcoin, the first cryptocurrency by date of launch and by market cap (as of December2017) has attracted investors and traders because of its ease of starting trading, ability to stay pseudo-anonymous, and, of course, dramatic growth during <span>its history </span>(see <em>Table 1</em> and <em>Figure 1</em> for some statistics). This lures long-term investors; its high volatility also attracts day traders.</p>
<p>However, it's hard predict the value of Bitcoin in the long term, as the value behind Bitcoin is less tangible. The price mostly reflects market perception and is highly dependent on news, regulations, collaboration of governments and banks, technical issues of the platform (such as transactions fee and <span>block</span> size), interest of institutional investors in including Bitcoin into their portfolio, and more:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/49e8ae72-5c00-48ac-8a1b-ded0004f484f.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 1: Bitcoin and its dramatic price increases</div>
<p>Nonetheless, from a short-term perspective, Bitcoin price is a by-product of market activity usually happening on a platform, called <strong>exchange</strong> (Bitstamp, Coinbase, Kraken, and Bitfinex among the most well-known <strong>exchanges</strong>). Users, after registration and after going through <strong>KYC</strong> (<strong>Know Your Customer</strong>) procedures, can trade Bitcoin in it for fiat currencies such as dollars and euros, as well as for other cryptocurrencies, called <strong>alt-coins</strong> or alternative coins (Ethereum, Litecoin, and Dash are well known):</p>
<p class="packt_figref CDPAlignLeft CDPAlign"><strong>Table 1 – Bitcoin historical price movement</strong></p>
<table class="MsoTableGrid">
<tbody>
<tr>
<td>
<p><strong>Date</strong></p>
</td>
<td>
<p><strong>USD: 1 BTC</strong></p>
</td>
</tr>
<tr>
<td>
<p>Jan 2009 to Mar 2010</p>
</td>
<td>
<p>Basically none</p>
</td>
</tr>
<tr>
<td>
<p>Mar 2010</p>
</td>
<td>
<p>$0.003</p>
</td>
</tr>
<tr>
<td>
<p>May 2010</p>
</td>
<td>
<p>Less than $0.01</p>
</td>
</tr>
<tr>
<td>
<p>Jul 2010</p>
</td>
<td>
<p>$0.08<img src="assets/90581ab4-0591-4dca-b5ed-443e38e1d7c0.png"/></p>
</td>
</tr>
<tr>
<td>
<p>Feb to Apr 2011</p>
</td>
<td>
<p>$1.00<img src="assets/90581ab4-0591-4dca-b5ed-443e38e1d7c0.png"/></p>
</td>
</tr>
<tr>
<td>
<p>8 Jul 2011</p>
</td>
<td>
<p>$31.00<img src="assets/90581ab4-0591-4dca-b5ed-443e38e1d7c0.png"/></p>
</td>
</tr>
<tr>
<td>
<p>Dec 2011</p>
</td>
<td>
<p>$2.00<img src="assets/a45968d6-6644-4ea0-b057-04ae1126bf6d.png"/></p>
</td>
</tr>
<tr>
<td>
<p>Dec 2012</p>
</td>
<td>
<p>$13.00</p>
</td>
</tr>
<tr>
<td>
<p>11 Apr 2013</p>
</td>
<td>
<p>$266<img src="assets/90581ab4-0591-4dca-b5ed-443e38e1d7c0.png"/></p>
</td>
</tr>
<tr>
<td>
<p>May 2013</p>
</td>
<td>
<p>$130<img src="assets/a45968d6-6644-4ea0-b057-04ae1126bf6d.png"/></p>
</td>
</tr>
<tr>
<td>
<p>Jun 2013</p>
</td>
<td>
<p>$100<img src="assets/a45968d6-6644-4ea0-b057-04ae1126bf6d.png"/></p>
</td>
</tr>
<tr>
<td>
<p>Nov 2013</p>
</td>
<td>
<p>$350 to $1,242<img src="assets/b6f31e88-ab89-497f-80de-74f781252588.png"/></p>
</td>
</tr>
<tr>
<td>
<p>Dec 2013</p>
</td>
<td>
<p>$600 to $1,000<img src="assets/5a96adb5-2395-4d3f-aafe-9a5611dcf169.png"/></p>
</td>
</tr>
<tr>
<td>
<p>Jan 2014</p>
</td>
<td>
<p>$750 to $1,000<img src="assets/32d8f6a6-d273-4f99-8151-9439ab44d07a.png"/></p>
</td>
</tr>
<tr>
<td>
<p>Feb 2014</p>
</td>
<td>
<p>$550 to $750<img src="assets/1ca1dfd2-722d-4c6b-96a9-2e0f148bca0b.png"/></p>
</td>
</tr>
<tr>
<td>
<p>Mar 2014</p>
</td>
<td>
<p>$450 to $700<img src="assets/5057f2fa-69b0-44ff-9c33-b7a637d18f48.png"/></p>
</td>
</tr>
<tr>
<td>
<p>Apr 2014</p>
</td>
<td>
<p>$340 to $530<img src="assets/56ee8714-fe1c-4fc1-9388-9f959cf22b1b.png"/></p>
</td>
</tr>
<tr>
<td>
<p>May 2014</p>
</td>
<td>
<p>$440 to $630<img src="assets/0cb1f1cf-928d-4ec2-8d06-23fad044b50a.png"/></p>
</td>
</tr>
<tr>
<td>
<p>Mar 2015</p>
</td>
<td>
<p>$200 to $300<img src="assets/9e3940a9-ff01-4626-be01-e8dbebe89654.png"/></p>
</td>
</tr>
<tr>
<td>
<p>Early Nov 2015</p>
</td>
<td>
<p>$395 to $504<img src="assets/d0532b1c-66e1-4127-b5a3-6528ce9a8fe7.png"/></p>
</td>
</tr>
<tr>
<td>
<p>May to Jun 2016</p>
</td>
<td>
<p>$450 to $750<img src="assets/bd67159a-b317-4476-9ad2-d26a9c13b868.png"/></p>
</td>
</tr>
<tr>
<td>
<p>Jul to Sept 2016</p>
</td>
<td>
<p>$600 to $630<img src="assets/0610538f-261e-4188-a43d-a0bca9d5be7c.png"/></p>
</td>
</tr>
<tr>
<td>
<p>Oct to Nov 2016</p>
</td>
<td>
<p>$600 to $780<img src="assets/45f5cbb4-1622-4d1a-b774-43651f28fa2d.png"/></p>
</td>
</tr>
<tr>
<td>
<p>Jan 2017</p>
</td>
<td>
<p>$800 to $1,150<img src="assets/68fe4e41-afe2-4443-970f-12f2f1828000.png"/></p>
</td>
</tr>
<tr>
<td>
<p>5-12 Jan 2017</p>
</td>
<td>
<p>$750 to $920<img src="assets/9fe99d9e-0593-41b5-8e75-2065a34c5e6e.png"/></p>
</td>
</tr>
<tr>
<td>
<p>2-3 Mar 2017</p>
</td>
<td>
<p>$1,290+ <img src="assets/de8a003e-b813-4e3c-9391-aff3af19b018.png"/></p>
</td>
</tr>
<tr>
<td>
<p>Apr 2017</p>
</td>
<td>
<p>$1,210 to $1,250<img src="assets/736f9408-d05a-43f0-b2cf-95381b850962.png"/></p>
</td>
</tr>
<tr>
<td>
<p>May 2017</p>
</td>
<td>
<p>$2,000 <img src="assets/e74ec91a-4184-4ab1-9baf-6381881e32d1.png"/></p>
</td>
</tr>
<tr>
<td>
<p>May to June 2017</p>
</td>
<td>
<p>$2,000 to $3,200+<img src="assets/3039f1b3-23ee-4a3f-86af-496ac578b85d.png"/></p>
</td>
</tr>
<tr>
<td>
<p>Aug 2017</p>
</td>
<td>
<p>$4,400 <img src="assets/090e34d8-4208-49d0-931f-2dc81ddd3f91.png"/></p>
</td>
</tr>
<tr>
<td>
<p>Sept 2017</p>
</td>
<td>
<p>$5,000<img src="assets/cc407ad7-fc7f-419c-9f30-9f77f5b14b48.png"/></p>
</td>
</tr>
<tr>
<td>
<p>12 Sept 2017</p>
</td>
<td>
<p>$2,900<img src="assets/3b77d917-ab15-4e2a-8fc6-a48fd221c8af.png"/></p>
</td>
</tr>
<tr>
<td>
<p>13 Oct 2017</p>
</td>
<td>
<p>$5,600<img src="assets/3afb31a2-f86b-4345-a3ef-c960f07632b7.png"/></p>
</td>
</tr>
<tr>
<td>
<p>21 Oct 2017</p>
</td>
<td>
<p>$6,180 <img src="assets/4646b186-f179-4514-abe8-74eef5a3aa0d.png"/></p>
</td>
</tr>
<tr>
<td>
<p>6 Nov 2017</p>
</td>
<td>
<p>$7,300 <img src="assets/ae5187a3-3bd3-4bcb-a9da-7f08c8ba90db.png"/></p>
</td>
</tr>
<tr>
<td>
<p>12 Nov 2017</p>
</td>
<td>
<p>$5,519 to 6,295 <img src="assets/b1721a3b-6f03-4de9-8551-2f2f9be53230.png"/></p>
</td>
</tr>
<tr>
<td>
<p>17-20 Nov 2017</p>
</td>
<td>
<p>$7,600 to 8,100 <img src="assets/2fdfb5d0-b68b-46a9-a6bc-85cb52ed1acb.png"/></p>
</td>
</tr>
<tr>
<td>
<p>15 Dec 2017</p>
</td>
<td>
<p>17,900 <img src="assets/c5c30f2c-62d2-4287-acbb-18d6e63baebc.png"/></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Exchanges maintain order books—lists of all buy and sell orders, with their quantities and prices<span>—</span>and execute when a match is found between somebody buying and somebody selling. In addition, exchanges also keep and provide statistics about the state of trading, often captured as OCHL and volume for both currencies of the trader pai. For this project, we will be using the BTC/USD cryptocurrency pair.</p>
<p>This data is presented as aggregated by period, from seconds to days, and even months. There are dedicated servers working on collecting Bitcoin data for professional traders and institutions. Although one cannot expect to have all orders data available free, some of it is accessible to the public and can be used.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">State-of-the-art automated trading of Bitcoin</h1>
                </header>
            
            <article>
                
<p>In the world of traditional securities, such as a company's stocks, it used to be humans who would do the analytics, predict the prices of stocks, and trade. Today, the development of <strong>machine learning </strong> (<strong>ML</strong>) and the growing availability of data has almost eliminated humans from high-frequency trading, as a regular person can't capture and process all data, and emotions affect one's decisions; so it's dominated by automated trading systems by investment institutions.</p>
<p>Currently, the volume of Bitcoin trading is relatively low compared to traditional exchanges; financial institutions, being traditionally careful and risk averse, haven't got their hands on Bitcoin trading yet (at least, it's not well-known). One of the reasons is high fees and uncertainty regarding regulations of cryptocurrencies.</p>
<p>So today, mostly individuals buy and sell Bitcoins, with all the consequences of irrational behavior connected to that, but some attempts to automate Bitcoin trading have been made. The most famous one was stated in a paper by MIT, and another one was by Stanford researchers, published in 2014. Many things have changed, and taking into account the massive Bitcoin price increase during these three years, anyone who just buys and holds on would be satisfied enough with the results:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f706ebab-7205-4b72-9e9b-b46121b5d0c3.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 2: Bitcoin buy and sell orders (until November 2017)</div>
<p>Definitely, some traders use ML for trading, and such applications look promising. So far, the best possible approach that was identified from research papers is as follows.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training</h1>
                </header>
            
            <article>
                
<p>Use order book data, instead of derived OHLC + volume data. Therefore, for training and prediction, use data that looks like this:</p>
<ul>
<li>Split the data into a time series of a certain <kbd>size</kbd> (<kbd>size</kbd> is a parameter to tune).</li>
<li>Cluster the time series data into <kbd>K</kbd> clusters (<kbd>K</kbd> is a parameter to tune). It's assumed that clusters with some natural trends would appear (sharp drop/rise in price and so on).</li>
<li>For each cluster, train the regression and classifier to predict the price and price change, respectively.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Prediction</h1>
                </header>
            
            <article>
                
<p>This approach considers the most recent time series with the size of a specific window and trains the model. Then it classifies the data as follows:</p>
<ul>
<li>Takes the most recent time series with window size used for training</li>
<li>Classifies it—which of the clusters does it belong to?</li>
<li>Uses the ML model for that cluster to predict the price or price change</li>
</ul>
<p>This solution dates back to 2014, but still it gives a certain level of robustness. By having many parameters to identify, and not having the order-book historical data available easily, in this project, we use a simpler approach and dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">High-level data pipeline of the prototype</h1>
                </header>
            
            <article>
                
<p>The goal of this chapter is to develop a prototype of a system that will predict the short-term change of Bitcoin price, using historical data to train the algorithm, and real-time data to predict and select algorithms that perform better. In the scope of this project, there is no attempt to predict the actual price in dollars, but only whether it would increase or not. This is because Bitcoin price, to some extent, is not actually about price but about market expectations. This can be seen as patterns in a trader's behavior, which, on a higher level, is represented by previous price itself.</p>
<div class="CDPAlignCenter CDPAlign"><img height="459" width="736" src="assets/4031f5ff-0aa6-415e-89a1-8303f1ab3c2c.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 3: High-level data pipeline of the prototype</div>
<p>Of course, there is an objective price associated with Bitcoin; miners are willing to sell Bitcoins for profit. So the base price can be estimated by knowing all bills that all miners have to pay for the Bitcoins they mine, but that is outside the scope of this project.</p>
<p>From this perspective, rather than trying to predict the price in dollars, it might make sense to look for trends of the price rising, dropping, or staying the same, and act accordingly. The second goal is to build a tool for experiments that allows us to try different approaches to predicting prices and evaluate it on real-life data easily. The code has to be flexible, robust, and easily extensible.</p>
<p>Therefore, in summary, there are three main components of the system:</p>
<ul>
<li>Scala script for preprocessing of historical data into the required format</li>
<li>Scala app to train the ML model</li>
<li>Scala web service to predict future prices</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Historical and live-price data collection</h1>
                </header>
            
            <article>
                
<p>As stated earlier, we will utilize both historical as well live data. We will be using the Bitcoin historical price data from Kaggle. For the real-time data, Cryptocompare API will be used.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Historical data collection</h1>
                </header>
            
            <article>
                
<p>For training the ML algorithm, there is a <kbd>Bitcoin Historical Price Data</kbd> dataset available to the public on Kaggle (version 10). The dataset can be downloaded from <a href="https://www.kaggle.com/mczielinski/bitcoin-historical-data/">https://www.kaggle.com/mczielinski/bitcoin-historical-data/</a>. It has 1 minute OHLC data for BTC-USD pairs from several exchanges.</p>
<p>At the beginning of the project, for most of them, data was available from January 1, 2012 to May 31, 2017; but for the Bitstamp exchange, it's available until October 20, 2017 (as well as for Coinbase, but that dataset became available later):</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3fcf81e6-a1ed-4dc7-b3b5-35d4c0f7ec32.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 4: The Bitcoin historical dataset on Kaggle</div>
<p>Note that you need to be a registered user and be logged in in order to download the file. The file that we are using is <kbd>bitstampUSD_1-min_data_2012-01-01_to_2017-10-20.csv</kbd><em>.</em> Now, let us get the data we have. It has eight columns:</p>
<ul>
<li><strong>Timestamp</strong>: The time elapsed in seconds since January 1, 1970. It is 1,325,317,920 for the first row and 1,325,317,920 for the second 1. (Sanity check! The difference is 60 seconds).</li>
<li><strong>Open</strong>: The price at the opening of the time interval. It is 4.39 dollars. Therefore it is the price of the first trade that happened after <strong>Timestamp</strong> (1,325,317,920 in the first row's case).</li>
<li><strong>Close</strong>: The price at the closing of the time interval.</li>
<li><strong>High</strong>: The highest price from all orders executed during the interval.</li>
<li><strong>Low</strong>: The same as <strong>High</strong> but it is the lowest price.</li>
<li><strong>Volume_(BTC)</strong>: The sum of all Bitcoins that were transferred during the time interval. So, take all transactions that happened during the selected interval and sum up the BTC values of each of them.</li>
<li><strong>Volume_(Currency)</strong>: The sum of all dollars transferred.</li>
<li><strong>Weighted_Price</strong>: This is derived from the volumes of BTC and USD. By dividing all dollars traded by all bitcoins, we can get the weighted average price of BTC during this minute. So <kbd>Weighted_Price=Volume_(Currency)/Volume_(BTC)</kbd>.</li>
</ul>
<p>One of the most important parts of the data-science pipeline after data collection (which is in a sense outsourced; we use data collected by others) is data preprocessing—clearing a dataset and transforming it to suit our needs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transformation of historical data into a time series</h1>
                </header>
            
            <article>
                
<p>Stemming from our goal—predict the direction of price change—we might ask ourselves, <em>does having an actual price in dollars help to achieve this?</em> Historically, the price of Bitcoin was usually rising, so if we try to fit a linear regression, it will show further exponential growth (whether in the long run this will be true is yet to be seen).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Assumptions and design choices</h1>
                </header>
            
            <article>
                
<p>One of the assumptions of this project is as follows: whether we are thinking about Bitcoin trading in November 2016 with a price of about <span>$</span>700, or trading in November 2017 with a price in the $6500-7000 range, patterns in how people trade are similar. Now, we have several other assumptions, as described in the following points:</p>
<ul>
<li><strong>Assumption one</strong>: From what has been said previously, we can ignore the actual price and rather look at its change. As a measure of this, we can take the delta between opening and closing prices. If it is positive, it means the price grew during that minute; the price went down if it is negative and stayed the same if delta = 0.<br/>
In the following figure, we can see that Delta was -1.25 for the first minute observed, -12.83 for the second one, and -0.23 for the third one. Sometimes, the open price can differ significantly from the close price of the previous minute (although Delta is negative during all three of the observed minutes, for the third minute the shown price was actually higher than close for a second). But such things are not very common, and usually the open price doesn't change significantly compared to the close price of the previous minute.</li>
<li><strong>Assumption two</strong>: The next need to consider...  is predicting the price change in a <strong>black box</strong> environment. We do not use other sources of knowledge such as news, Twitter feeds, and others to predict how the market would react to them. This is a more advanced topic. The only data we use is price and volume. For simplicity of the prototype, we can focus on price only and construct time series data.<br/>
Time series prediction is a prediction of a parameter based on the values of this parameter in the past. One of the most common examples is temperature prediction. Although there are many supercomputers using satellite and sensor data to predict the weather, a simple time series analysis can lead to some valuable results. We predict the price at T+60 seconds, for instance, based on the price at T, T-60s, T-120s and so on.</li>
<li><strong>Assumption three</strong>: Not all data in the dataset is valuable. The first 600,000 records are not informative, as price changes are rare and trading volumes are small. This can affect the model we are training and thus make end results worse. That is why the first 600,000 of rows are eliminated from the dataset.</li>
<li><strong>Assumption four</strong>: We need to <kbd>Label </kbd>our data so that we can use a supervised ML algorithm. This is the easiest measure, without concerns about transaction fees.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data preprocessing</h1>
                </header>
            
            <article>
                
<p>Taking into account the goals of data preparation, Scala was chosen as an easy and interactive way to manipulate data:</p>
<pre><strong>val</strong> priceDataFileName: String = "bitstampUSD_1-min_data_2012-01-01_to_2017-10-20.csv"<br/><br/><strong>val</strong> spark = SparkSession<br/>    .builder()<br/>    .master("local[*]")<br/>    .config("spark.sql.warehouse.dir", "E:/Exp/")<br/>    .appName("Bitcoin Preprocessing")<br/>    .getOrCreate()<br/><br/><strong>val</strong> data = spark.read.format("com.databricks.spark.csv").option("header", "true").load(priceDataFileName)<br/>data.show(10)<br/>&gt;&gt;&gt;</pre>
<div class="CDPAlignCenter CDPAlign"><img height="384" width="924" src="assets/c6304756-3dc9-45f6-af8d-71eef2a2a4c7.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5: A glimpse of the Bitcoin historical price dataset</div>
<pre>println((data.count(), data.columns.size))</pre>
<p class="mce-root">&gt;&gt;&gt;</p>
<pre>(3045857, 8)</pre>
<p>In the preceding code, we load data from the file downloaded from Kaggle and look at what is inside. There are <kbd>3045857</kbd> rows in the dataset and <kbd>8</kbd> columns, described before. Then we create the <kbd>Delta</kbd> column, containing the difference between closing and opening prices (that is, to consider only that data where meaningful trading has started to occur):</p>
<pre><strong>val</strong> dataWithDelta = data.withColumn("Delta", data("Close") - data("Open"))</pre>
<p>The following code labels our data by assigning 1 to the rows the <kbd>Delta</kbd> value of which was positive; it assigns <kbd>0</kbd> otherwise:</p>
<pre><strong>import</strong> org.apache.spark.sql.functions._<br/><strong>import</strong> spark.sqlContext.implicits._<br/><br/><strong>val</strong> dataWithLabels = dataWithDelta.withColumn("label", when($"Close" - $"Open" &gt; 0, 1).otherwise(0))<br/>rollingWindow(dataWithLabels, 22, outputDataFilePath, outputLabelFilePath)</pre>
<p>This code transforms the original dataset into time series data. It takes the Delta values of <kbd>WINDOW_SIZE</kbd> rows (<kbd>22</kbd> in this experiment) and makes a new row out of them. In this way, the first row has <kbd>Delta</kbd> values from <kbd>t0</kbd> to <kbd>t21</kbd>, and the second one has values from <kbd>t1</kbd> to <kbd>t22</kbd>. Then we create the corresponding array with labels (<kbd>1</kbd> or <kbd>0</kbd>).</p>
<p>Finally, we save <kbd>X</kbd> and <kbd>Y</kbd> into files where <kbd>612000</kbd> rows were cut off from the original dataset; <kbd>22</kbd> means rolling window size and 2 classes represents that labels are binary <kbd>0</kbd> and <kbd>1</kbd>:</p>
<pre><strong>val</strong> dropFirstCount: Int = 612000<br/><br/><strong>def</strong> rollingWindow(data: DataFrame, window: Int, xFilename: String, yFilename: String): Unit = {<br/><strong>    var</strong> i = 0<br/><strong>    val</strong> xWriter = <strong>new</strong> BufferedWriter(<strong>new</strong> FileWriter(<strong>new</strong> File(xFilename)))<br/><strong>    val</strong> yWriter = <strong>new</strong> BufferedWriter(<strong>new</strong> FileWriter(<strong>new</strong> File(yFilename)))<br/><strong>    val</strong> zippedData = data.rdd.zipWithIndex().collect()<br/>    System.gc()<br/><strong>    val</strong> dataStratified = zippedData.drop(dropFirstCount)//slice 612K<br/><br/><strong>    while</strong> (i &lt; (dataStratified.length - window)) {<br/><strong>        val</strong> x = dataStratified<br/>                .slice(i, i + window)<br/>                    .map(r =&gt; r._1.getAs[Double]("Delta")).toList<br/><strong>        val</strong> y = dataStratified.apply(i + window)._1.getAs[Integer]("label")<br/><strong>        val</strong> stringToWrite = x.mkString(",")<br/>        xWriter.write(stringToWrite + "n")<br/>        yWriter.write(y + "n")<br/>        i += 1<br/><br/><strong>        if</strong> (i % 10 == 0) {<br/>            xWriter.flush()<br/>            yWriter.flush()<br/>            }<br/>        }<br/>    xWriter.close()<br/>    yWriter.close()<br/>}</pre>
<p>In the preceding code segment:</p>
<pre><strong>val</strong> outputDataFilePath: String = "output/scala_test_x.csv"<br/><strong>val</strong> outputLabelFilePath: String = "output/scala_test_y.csv"</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Real-time data through the Cryptocompare API</h1>
                </header>
            
            <article>
                
<p>For real-time data, the Cryptocompare API is used (<a href="https://www.cryptocompare.com/api/">https://www.cryptocompare.com/api/#</a>), more specifically HistoMinute (<a href="https://www.cryptocompare.com/api/#-api-data-histominute-">https://www.cryptocompare.com/api/#-api-data-histominute-</a>), which gives us access to OHLC data for the past seven days at most. The details of the API will be discussed in a section devoted to implementation, but the API response is very similar to our historical dataset, and this data is retrieved using a regular HTTP request. For example, a simple JSON response from <a href="https://min-api.cryptocompare.com/data/histominute?fsym=BTC&amp;tsym=USD&amp;limit=23&amp;aggregate=1&amp;e=Bitstamp">https://min-api.cryptocompare.com/data/histominute?fsym=BTC&amp;tsym=USD&amp;limit=23&amp;aggregate=1&amp;e=Bitstamp</a> has the following structure:</p>
<pre>{<br/>    "Response":"Success",<br/>    "Type":100,<br/>    "Aggregated":false,<br/>    "Data":<br/>    [{"time":1510774800,"close":7205,"high":7205,"low":7192.67,"open":7198,                                             "volumefrom":81.73,"volumeto":588726.94},<br/>        {"time":1510774860,"close":7209.05,"high":7219.91,"low":7205,"open":7205,                                 "volumefrom":16.39,"volumeto":118136.61},<br/>        ... (other price data)<br/>        ],<br/>    "TimeTo":1510776180,<br/>    "TimeFrom":1510774800,<br/>    "FirstValueInArray":true,<br/>    "ConversionType":{"type":"force_direct","conversionSymbol":""}<br/>}</pre>
<p>Through Cryptocompare HistoMinute, we can get <kbd>open</kbd>, <kbd>high</kbd>, <kbd>low</kbd>, <kbd>close</kbd>, <kbd>volumefrom</kbd>, and <kbd>volumeto</kbd> from each minute of historical data. This data is stored for 7 days only; if you need more, use the hourly or daily path. It uses BTC conversion if data is not available because the coin is not being traded in the specified currency:</p>
<div class="CDPAlignCenter CDPAlign"><img height="562" width="898" src="assets/15833859-b3f2-4974-9f9d-3ae6f49d9630.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 6: Open, high, low, close, and volume values through Cryptocompare HistoMinute</div>
<p>Now, the following method fetches the correctly formed URL of <span>the</span><span> </span><span>Cryptocompare API (</span><a href="https://www.cryptocompare.com/api/#-api-data-histominute-">https://www.cryptocompare.com/api/#-api-data-histominute-</a><span>), which is a fully formed URL with all parameters, such as currency, limit, and aggregation specified. It finally returns the future that will have a response body parsed into the data model, with the price list to be processed at an upper level:</span></p>
<pre><strong>import</strong> javax.inject.Inject<br/><strong>import</strong> play.api.libs.json.{JsResult, Json}<br/><strong>import</strong> scala.concurrent.Future<br/><strong>import</strong> play.api.mvc._<br/><strong>import</strong> play.api.libs.ws._<br/><strong>import</strong> processing.model.CryptoCompareResponse<br/><br/><strong>class</strong> RestClient @Inject() (ws: WSClient) {<br/><strong>    def</strong> getPayload(url : String): Future[JsResult[CryptoCompareResponse]] = {<br/>        val request: WSRequest = ws.url(url)<br/>        val future = request.get()<br/>        implicit val context = play.api.libs.concurrent.Execution.Implicits.defaultContext<br/>        future.map {<br/>            response =&gt; response.json.validate[CryptoCompareResponse]<br/>            }<br/>        }<br/>    }</pre>
<p>In the preceding code segment, the <kbd>CryptoCompareResponse</kbd> class is the model of API, which takes the following parameters:</p>
<ul>
<li><kbd>Response</kbd></li>
<li><kbd>Type</kbd></li>
<li><kbd>Aggregated</kbd></li>
<li><kbd>Data</kbd></li>
<li><kbd>FirstValueInArray</kbd></li>
<li><kbd>TimeTo</kbd></li>
<li><kbd>TimeFrom</kbd></li>
</ul>
<p>Now, it has the following signature:</p>
<pre><strong>case class</strong> CryptoCompareResponse(Response : String,<br/>    Type : Int,<br/>    Aggregated : Boolean,<br/>    Data : List[OHLC],<br/>    FirstValueInArray : Boolean,<br/>    TimeTo : Long,<br/>    TimeFrom: Long)<br/><br/><strong>object</strong> CryptoCompareResponse {<br/>    implicit val cryptoCompareResponseReads = Json.reads[CryptoCompareResponse]<br/>    }</pre>
<p>Again, the preceding two code segments the <span class="st"><strong>open-high-low-close</strong> (also known as</span> <strong>OHLC</strong><span class="st">), are</span> <span class="st">a</span> model class for mapping with CryptoAPI response <kbd>data</kbd> array internals. It takes these parameters:</p>
<ul>
<li><kbd>Time</kbd>: Timestamp in seconds, <kbd>1508818680</kbd>, for instance.</li>
<li><kbd>Open</kbd>: Open price at a given minute interval.</li>
<li><kbd>High</kbd>: Highest price.</li>
<li><kbd>Low</kbd>: Lowest price.</li>
<li><kbd>Close</kbd>: Price at the closing of the interval.</li>
<li><kbd>Volumefrom</kbd>: Trading volume in the <kbd>from</kbd> currency. It's BTC in our case.</li>
<li><kbd>Volumeto</kbd>: The trading volume in the <kbd>to</kbd> currency, USD in our case.</li>
<li>Dividing <kbd>Volumeto</kbd> by <kbd>Volumefrom</kbd> gives us the weighted price of BTC.</li>
</ul>
<p>Now, it has the following signature:</p>
<pre><strong>case class</strong> OHLC(time: Long,<br/>    open: Double,<br/>    high: Double,<br/>    low: Double,<br/>    close: Double,<br/>    volumefrom: Double,<br/>    volumeto: Double)<br/><br/>    <strong>object</strong> OHLC {<br/>    implicit val implicitOHLCReads = Json.reads[OHLC]<br/>        }</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model training for prediction</h1>
                </header>
            
            <article>
                
<p>Inside the project, in the package folder <kbd>prediction.training</kbd>, there is a Scala object called <kbd>TrainGBT.scala</kbd>. Before launching, you have to specify/change four things:</p>
<ul>
<li>In the code, you need to set up <kbd>spark.sql.warehouse.dir</kbd> in some actual place on your computer that has several gigabytes of free space: <kbd>set("spark.sql.warehouse.dir", "/home/user/spark")</kbd></li>
<li>The <kbd>RootDir</kbd> is the main folder, where all files and train models will be <kbd>stored:rootDir = "/home/user/projects/btc-prediction/"</kbd></li>
<li>Make sure that the <kbd>x</kbd> filename matches the one produced by the Scala script in the preceding step: <kbd>x = spark.read.format("com.databricks.spark.csv ").schema(xSchema).load(rootDir + "scala_test_x.csv")</kbd></li>
<li>Make sure that the <kbd>y</kbd> filename matches the one produced by Scala script: <kbd>y_tmp=spark.read.format("com.databricks.spark.csv").schema(ySchema).load(rootDir + "scala_test_y.csv")</kbd></li>
</ul>
<p>The code for training uses the Apache Spark ML library (and libraries required for it) to train the classifier, which means they have to be present in your <kbd>class</kbd> path to be able to run it. The easiest way to do that (since the whole project uses SBT) is to run it from the project root folder by typing <kbd>sbt</kbd><kbd>run-main prediction.training.TrainGBT</kbd>, which will resolve all dependencies and launch training.</p>
<p>Depending on the number of iterations and depth, it can take several hours to train the model. Now let us see how training is performed on the example of the gradient-boosted trees model. First, we need to create a <kbd>SparkSession</kbd> object:</p>
<pre><strong>val</strong> spark = SparkSession<br/>        .builder()<br/>        .master("local[*]")<br/>        .config("spark.sql.warehouse.dir", ""/home/user/spark/")<br/>        .appName("Bitcoin Preprocessing")<br/>        .getOrCreate()</pre>
<p>Then, we define a schema of data for <kbd>x</kbd> and <kbd>y</kbd>. We rename the columns to <kbd>t0</kbd>-<kbd>t21,</kbd> to indicate that it's a time series:</p>
<pre><strong>val</strong> xSchema = StructType(Array(<br/>    StructField("t0", DoubleType, true),<br/>    StructField("t1", DoubleType, true),<br/>    StructField("t2", DoubleType, true),<br/>    StructField("t3", DoubleType, true),<br/>    StructField("t4", DoubleType, true),<br/>    StructField("t5", DoubleType, true),<br/>    StructField("t6", DoubleType, true),<br/>    StructField("t7", DoubleType, true),<br/>    StructField("t8", DoubleType, true),<br/>    StructField("t9", DoubleType, true),<br/>    StructField("t10", DoubleType, true),<br/>    StructField("t11", DoubleType, true),<br/>    StructField("t12", DoubleType, true),<br/>    StructField("t13", DoubleType, true),<br/>    StructField("t14", DoubleType, true),<br/>    StructField("t15", DoubleType, true),<br/>    StructField("t16", DoubleType, true),<br/>    StructField("t17", DoubleType, true),<br/>    StructField("t18", DoubleType, true),<br/>    StructField("t19", DoubleType, true),<br/>    StructField("t20", DoubleType, true),<br/>    StructField("t21", DoubleType, true))<br/>    )</pre>
<p>Then we read the files we defined for the schema. It was more convenient to generate two separate files in Scala for data and labels, so here we have to join them into a single DataFrame:</p>
<pre><strong>import</strong> spark.implicits._<br/><strong>val</strong> y = y_tmp.withColumn("y", 'y.cast(IntegerType))<br/><strong>import</strong> org.apache.spark.sql.functions._<br/><br/><strong>val</strong> x_id = x.withColumn("id", monotonically_increasing_id())<br/><strong>val</strong> y_id = y.withColumn("id", monotonically_increasing_id())<br/><strong>val</strong> data = x_id.join(y_id, "id")</pre>
<p>The next step is required by Spark—we need to vectorize the features:</p>
<pre><strong>val</strong> featureAssembler = new VectorAssembler()<br/>        .setInputCols(Array("t0", "t1", "t2", "t3",<br/>                            "t4", "t5", "t6", "t7",<br/>                            "t8", "t9", "t10", "t11",<br/>                            "t12", "t13", "t14", "t15",<br/>                            "t16", "t17", "t18", "t19",<br/>                            "t20", "t21"))<br/>        .setOutputCol("features")</pre>
<p>We split the data into train and test sets randomly in the proportion of 75% to 25%. We set the seed so that the splits would be equal among all times we run the training:</p>
<pre><strong>val</strong> Array(trainingData,testData) = dataWithLabels.randomSplit(Array(0.75, 0.25), 123)</pre>
<p>We then define the model. It tells which columns are features and which are labels. It also sets parameters:</p>
<pre><strong>val</strong> gbt = new GBTClassifier()<br/>        .setLabelCol("label")<br/>        .setFeaturesCol("features")<br/>        .setMaxIter(10)<br/>        .setSeed(123)</pre>
<p>Create a <kbd>pipeline</kbd> of steps—vector assembling of features and running GBT:</p>
<pre><strong>val</strong> pipeline = new Pipeline()<br/>            .setStages(Array(featureAssembler, gbt))</pre>
<p>Defining evaluator function—how the model knows whether it is doing well or not. As we have only two classes that are imbalanced, accuracy is a bad measurement; area under the ROC curve is better:</p>
<pre><strong>val</strong> rocEvaluator = new BinaryClassificationEvaluator()<br/>        .setLabelCol("label")<br/>        .setRawPredictionCol("rawPrediction")<br/>        .setMetricName("areaUnderROC")</pre>
<p>K-fold cross-validation is used to avoid overfitting; it takes out one-fifth of the data at each iteration, trains the model on the rest, and then tests on this one-fifth:</p>
<pre><strong>val</strong> cv = new CrossValidator()<br/>        .setEstimator(pipeline)<br/>        .setEvaluator(rocEvaluator)<br/>        .setEstimatorParamMaps(paramGrid)<br/>        .setNumFolds(numFolds)<br/>        .setSeed(123)<br/><strong>val</strong> cvModel = cv.fit(trainingData)</pre>
<p>After we get the trained model (which can take an hour or more depending on the number of iterations and parameters we want to iterate on, specified in <kbd>paramGrid</kbd>), we then compute the predictions on the test data:</p>
<pre><strong>val</strong> predictions = cvModel.transform(testData)</pre>
<p class="mce-root">In addition, evaluate quality of predictions:</p>
<pre><strong>val</strong> roc = rocEvaluator.evaluate(predictions)</pre>
<p>The trained model is saved for later usage by the prediction service:</p>
<pre><strong>val</strong> gbtModel = cvModel.bestModel.asInstanceOf[PipelineModel]<br/>gbtModel.save(rootDir + "__cv__gbt_22_binary_classes_" + System.nanoTime() / 1000000 + ".model")</pre>
<p>In summary, the code for model training is given as follows:</p>
<pre><strong>import</strong> org.apache.spark.{ SparkConf, SparkContext }<br/><strong>import</strong> org.apache.spark.ml.{ Pipeline, PipelineModel }<br/><br/><strong>import</strong> org.apache.spark.ml.classification.{ GBTClassificationModel, GBTClassifier, RandomForestClassificationModel, RandomForestClassifier}<br/><strong>import</strong> org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator, MulticlassClassificationEvaluator}<br/><strong>import</strong> org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorAssembler, VectorIndexer}<br/><strong>import</strong> org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}<br/><strong>import</strong> org.apache.spark.sql.types.{DoubleType, IntegerType, StructField, StructType}<br/><strong>import</strong> org.apache.spark.sql.SparkSession<br/><br/><strong>object</strong> TrainGradientBoostedTree {<br/>    <strong>def</strong> main(args: Array[String]): Unit = {<br/>        val maxBins = Seq(5, 7, 9)<br/>        val numFolds = 10<br/>        val maxIter: Seq[Int] = Seq(10)<br/>        val maxDepth: Seq[Int] = Seq(20)<br/>        val rootDir = "output/"<br/>        val spark = SparkSession<br/>            .builder()<br/>            .master("local[*]")<br/>            .config("spark.sql.warehouse.dir", ""/home/user/spark/")<br/>            .appName("Bitcoin Preprocessing")<br/>            .getOrCreate()<br/><br/>        val xSchema = StructType(Array(<br/>            StructField("t0", DoubleType, true),<br/>            StructField("t1", DoubleType, true),<br/>            StructField("t2", DoubleType, true),<br/>            StructField("t3", DoubleType, true),<br/>            StructField("t4", DoubleType, true),<br/>            StructField("t5", DoubleType, true),<br/>            StructField("t6", DoubleType, true),<br/>            StructField("t7", DoubleType, true),<br/>            StructField("t8", DoubleType, true),<br/>            StructField("t9", DoubleType, true),<br/>            StructField("t10", DoubleType, true),<br/>            StructField("t11", DoubleType, true),<br/>            StructField("t12", DoubleType, true),<br/>            StructField("t13", DoubleType, true),<br/>            StructField("t14", DoubleType, true),<br/>            StructField("t15", DoubleType, true),<br/>            StructField("t16", DoubleType, true),<br/>            StructField("t17", DoubleType, true),<br/>            StructField("t18", DoubleType, true),<br/>            StructField("t19", DoubleType, true),<br/>            StructField("t20", DoubleType, true),<br/>            StructField("t21", DoubleType, true)))<br/><br/>        val ySchema = StructType(Array(StructField("y", DoubleType,<br/>        true)))<br/>        val x = spark.read.format("csv").schema(xSchema).load(rootDir +<br/>        "scala_test_x.csv")<br/>        val y_tmp =<br/>        spark.read.format("csv").schema(ySchema).load(rootDir +<br/>        "scala_test_y.csv")<br/><br/>        import spark.implicits._<br/>        val y = y_tmp.withColumn("y", 'y.cast(IntegerType))<br/><br/>        import org.apache.spark.sql.functions._<br/>        //joining 2 separate datasets in single Spark dataframe<br/>        val x_id = x.withColumn("id", monotonically_increasing_id())<br/>        val y_id = y.withColumn("id", monotonically_increasing_id())<br/>        val data = x_id.join(y_id, "id")<br/>        val featureAssembler = new VectorAssembler()<br/>            .setInputCols(Array("t0", "t1", "t2", "t3", "t4", "t5", <br/>                                "t6", "t7", "t8", "t9", "t10", "t11", <br/>                                "t12", "t13", "t14", "t15", "t16",<br/>                                "t17","t18", "t19", "t20", "t21"))<br/>            .setOutputCol("features")<br/>        val encodeLabel = udf[Double, String] { case "1" =&gt; 1.0 case<br/>                                                "0" =&gt; 0.0 }<br/>        val dataWithLabels = data.withColumn("label",<br/>                                encodeLabel(data("y")))<br/><br/>        //123 is seed number to get same datasplit so we can tune<br/>        params<br/>        val Array(trainingData, testData) =<br/>        dataWithLabels.randomSplit(Array(0.75, 0.25), 123)<br/>        val gbt = new GBTClassifier()<br/>            .setLabelCol("label")<br/>            .setFeaturesCol("features")<br/>            .setMaxIter(10)<br/>            .setSeed(123)<br/>        val pipeline = new Pipeline()<br/>            .setStages(Array(featureAssembler, gbt))<br/>        // ***********************************************************<br/>        println("Preparing K-fold Cross Validation and Grid Search")<br/>        // ***********************************************************<br/>        val paramGrid = new ParamGridBuilder()<br/>            .addGrid(gbt.maxIter, maxIter)<br/>            .addGrid(gbt.maxDepth, maxDepth)<br/>            .addGrid(gbt.maxBins, maxBins)<br/>            .build()<br/>        val cv = new CrossValidator()<br/>            .setEstimator(pipeline)<br/>            .setEvaluator(new BinaryClassificationEvaluator())<br/>            .setEstimatorParamMaps(paramGrid)<br/>            .setNumFolds(numFolds)<br/>            .setSeed(123)<br/>        // ************************************************************<br/>        println("Training model with GradientBoostedTrees algorithm")<br/>        // ************************************************************<br/>        // Train model. This also runs the indexers.<br/>        val cvModel = cv.fit(trainingData)<br/>        cvModel.save(rootDir + "cvGBT_22_binary_classes_" +<br/>        System.nanoTime() / 1000000 + ".model")<br/>        println("Evaluating model on train and test data and<br/>        calculating RMSE")<br/>        // **********************************************************************<br/>        // Make a sample prediction<br/>        val predictions = cvModel.transform(testData)<br/><br/>        // Select (prediction, true label) and compute test error.<br/>        val rocEvaluator = new BinaryClassificationEvaluator()<br/>            .setLabelCol("label")<br/>            .setRawPredictionCol("rawPrediction")<br/>            .setMetricName("areaUnderROC")<br/>        val roc = rocEvaluator.evaluate(predictions)<br/>        val prEvaluator = new BinaryClassificationEvaluator()<br/>            .setLabelCol("label")<br/>            .setRawPredictionCol("rawPrediction")<br/>            .setMetricName("areaUnderPR")<br/>        val pr = prEvaluator.evaluate(predictions)<br/>        val gbtModel = cvModel.bestModel.asInstanceOf[PipelineModel]<br/>        gbtModel.save(rootDir + "__cv__gbt_22_binary_classes_" +<br/>        System.nanoTime()/1000000 +".model")<br/><br/>        println("Area under ROC curve = " + roc)<br/>        println("Area under PR curve= " + pr)<br/>        println(predictions.select().show(1))<br/>        spark.stop()<br/>    }<br/>}</pre>
<p>Now let us see how the training went:</p>
<pre>&gt;&gt;&gt; <br/>Area under ROC curve = 0.6045355104779828<br/>Area under PR curve= 0.3823834607704922</pre>
<p>Therefore, we have not received very high accuracy, as the ROC is only 60.50% out of the best GBT model. Nevertheless, if we tune the hyperparameters, we will get better accuracy.</p>
<p>However, as I did not have enough time, I did not iterate the training for long, but you should definitely try.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scala Play web service</h1>
                </header>
            
            <article>
                
<p>As an application framework, Play2 was chosen as an easy-to-configure and robust framework. Compared to Spring (another popular framework), it takes less time to make a small app from scratch. The Play comes with Guice for dependency injection and SBT as the package manager:</p>
<ul>
<li><strong>Spark ML</strong>: The Spark ML library was chosen as it is one of the best-maintained libraries in the Java world. Many algorithms not available in the library itself are implemented by third-party developers and can be trained on top of Spark. A drawback of Spark is that it is quite slow, as by design it is supposed to be distributed; so it uses Hadoop and writes a lot into the filesystem.</li>
<li><strong>Akka</strong>: This allows implementing the actor's pattern—having several instances of independent objects and passing messages to each other concurrently, which increases robustness.</li>
<li><strong>Anorm</strong>: The library to work with SQL on top of JDBC. Slick is another option and it is more powerful, but compatibility issues between libraries required for Akka and Slick made it worth choosing another library.</li>
<li><strong>H2</strong>: A database that is the default for Play and Ruby-on-Rails as an easy-to-start <span>database,</span> with the possibility to store data in a local database file without the need to install a DB server. This gives portability and increases the speed of development. In later stages, it can be replaced with another, as Scala code isn't tied to any particular database; all of it is done on the configuration level.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Concurrency through Akka actors</h1>
                </header>
            
            <article>
                
<p>Concurrency is achieved through utilization of the <kbd>actor</kbd> model using the Akka Scala library. Actors act as independent entities and can pass async messages to other actors. In this project, there are three actors: <kbd>SchedulerActor</kbd>, <kbd>PredictionActor</kbd>, and <kbd>TraderActor</kbd>:</p>
<ul>
<li><kbd>SchedulerActor</kbd>: Requests price data, stores them into DB, sends a message with prices to <kbd>PredictionActor</kbd>, receives an answer, and passes it to <kbd>TraderActor</kbd>.</li>
<li><kbd>PredictionActor</kbd>: After receiving a message with prices, it predicts the next price using the best model available (this has to be chosen in <kbd>application.conf</kbd>; we will see the details later on). It passes a message with the prediction back to <kbd>SchedulerActor</kbd>, uses the rest of the modes from the <kbd>model</kbd> folder to make predictions on previous data, and uses the latest price to evaluate predictions. The results of such <span>predictions </span>are stored in the DB.</li>
<li><kbd>TraderActor</kbd>: After receiving a message about prediction, using <kbd>rules</kbd> (which at this moment <span>are </span>as simple as <em>buy if the price is predicted to grow </em><em>and do nothing otherwise</em>), this writes<span> its decision </span>into logs. It can send an HTTP request to a URL to trigger this decision.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Web service workflow</h1>
                </header>
            
            <article>
                
<p>Now let's take a deeper look into how code works to perform predictions. As shown earlier, every 60 seconds, the app is triggered to fetch data from Cryptocompare, store prices into the database, and run predictions, saving backtrack test results about quality prediction.</p>
<p>In this section, we'll look deeper into which Scala classes play an important role in this project and how they communicate.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">JobModule</h1>
                </header>
            
            <article>
                
<p>When the application is launched, everything starts with <kbd>JobModule</kbd>. It configures the creation of <kbd>Scheduler</kbd>, which sends messages to <kbd>SchedulerActor</kbd> as given in the <kbd>application.conf</kbd> rate:</p>
<pre><strong>class</strong> JobModule extends AbstractModule with AkkaGuiceSupport {<br/>  <strong>  def</strong> configure(): Unit = {<br/>        //configuring launch of price-fetching Actor<br/>        bindActor[SchedulerActor]("scheduler-actor")<br/>        bind(classOf[Scheduler]).asEagerSingleton()<br/>    }<br/>}</pre>
<p>To enable this module, inside <kbd>application.conf</kbd>, the following line is required:</p>
<pre>play.modules.enabled += "modules.jobs.JobModule"</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scheduler</h1>
                </header>
            
            <article>
                
<p><kbd>Scheduler</kbd> takes the frequency constant from the <kbd>application.conf</kbd> and uses the <kbd>Actor</kbd> system to send an <kbd>update</kbd> <span>message</span><span> </span><span>(the content does not matter; </span><kbd>SchedulerActor</kbd> <span>reacts to any message) to</span> <kbd>SchedulerActor</kbd> <span>every X seconds:</span></p>
<pre><strong>class</strong> Scheduler @Inject()<br/>    (val system: ActorSystem, @Named("scheduler-actor") val schedulerActor: ActorRef, configuration:     Configuration)(implicit ec: ExecutionContext) {<br/>    //constants.frequency is set in conf/application.conf file<br/>    val frequency = configuration.getInt("constants.frequency").get<br/>    var actor = system.scheduler.schedule(<br/>    0.microseconds, //initial delay: whether execution starts immediately after app launch<br/>    frequency.seconds, //every X seconds, specified above<br/>    schedulerActor,<br/>    "update")<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">SchedulerActor</h1>
                </header>
            
            <article>
                
<p>The relevant parts of the code are displayed and explained. Now let us see how to obtain price data:</p>
<pre><strong>def</strong> constructUrl(exchange: String): String =<br/>{<br/> "https://min-api.cryptocompare.com/data/histominute?fsym=BTC&amp;tsym=USD&amp;limit=23&amp;aggregate=1&amp;e=" + exchange<br/> }</pre>
<p><kbd>ConstructUrl</kbd> returns a completely formed URL for the request to the Cryptocompare API. More details are given in section related to the API:</p>
<pre><strong>final val</strong> predictionActor = system.actorOf(Props(new PredictionActor(configuration, db)))<br/><strong>final val</strong> traderActor = system.actorOf(Props(new TraderActor(ws)))</pre>
<p>Creates instances of <kbd>PredictionActor</kbd> and <kbd>TraderActor</kbd>:</p>
<pre><strong>override def</strong> receive: Receive = {</pre>
<p>The <kbd>Receive</kbd> method is defined in the <kbd>actor</kbd> trait and has to be implemented. It is triggered when someone passes a message to this <kbd>actor</kbd> (<kbd>Scheduler</kbd> in our case):</p>
<pre><strong>case</strong> _ =&gt;<br/>   <strong> val</strong> futureResponse=restClient.getPayload(constructUrl(exchange))</pre>
<p>In the preceding code, <kbd>case _ =&gt;</kbd> means that we react to any message of any type and content. The first thing that is done is an async call to the Cryptocompare API by the URL specified before. This is done with the help of <kbd>RestClient</kbd>, which returns <kbd>Future</kbd> with the response JSON. After receiving the response (inside <kbd>futureResponse</kbd> on complete callback), <kbd>.json</kbd> is mapped into the custom case class <kbd>CryptoCompareResponse</kbd>:</p>
<pre><strong>case class</strong> CryptoCompareResponse(Response: String, Type: Int, Aggregated: Boolean, Data: List[OHLC],     FirstValueInArray: Boolean, TimeTo: Long,TimeFrom: Long)</pre>
<p>The case class is similar to <strong>POJO</strong> (<strong>Plain Old Java Object</strong>) without the need to write constructors and getters/setters:</p>
<pre><strong>object</strong> CryptoCompareResponse {<br/><strong>        implicit</strong> <strong>val</strong> cryptoCompareResponseReads = Json.reads[CryptoCompareResponse]<br/>            }</pre>
<p>This companion object is required for mapping JSON into this class. The <kbd>CryptocompareResponse</kbd> object stores the output of the API—a list of OHLC data, time range of data and others which that are not relevant to us. The <kbd>OHLC</kbd> class corresponds to actual price data:</p>
<pre><strong>case class</strong> OHLC(time: Long, open: Double, <br/>                high: Double, <br/>                low: Double, <br/>                close: Double, <br/>                volumefrom: Double, <br/>                volumeto: Double)</pre>
<p>After the data is ready, prices are stored in the DB by calling <kbd>storePriceData(cryptoCompareResponse)</kbd>. At first, it does a batch insert (using Anorm's <strong>BatchSQL</strong>) into the <kbd>PRICE_STAGING</kbd> table and re-inserts into <kbd>PRICE</kbd> with deduplication with respect to timestamp, as we are receiving overlapping price data:</p>
<pre><strong>val</strong> batch = <strong>BatchSql</strong>(<br/>        """|INSERT INTO PRICE_STAGING(TIMESTAMP,EXCHANGE,PRICE_OPEN,PRICE_CLOSED,VOLUME_BTC,             <br/>            VOLUME_USD)| VALUES({timestamp}, {exchange}, {priceOpen}, {priceClosed}, {volumeBTC},                   {volumeUSD})""".stripMargin,transformedPriceDta.head,transformedPriceDta.tail:_*)<br/><strong>val</strong> res: Array[Int] = batch.execute() // array of update count<br/><strong>val</strong> reInsert = <strong>SQL</strong>(<br/>        """<br/>          |INSERT INTO PRICE(TIMESTAMP, EXCHANGE, PRICE_OPEN, PRICE_CLOSED, VOLUME_BTC, VOLUME_USD)<br/>          |SELECT  TIMESTAMP, EXCHANGE, PRICE_OPEN, PRICE_CLOSED, VOLUME_BTC, VOLUME_USD<br/>          |FROM PRICE_STAGING AS s<br/>          |WHERE NOT EXISTS (<br/>          |SELECT *<br/>          |FROM PRICE As t<br/>          |WHERE t.TIMESTAMP = s.TIMESTAMP<br/>          |)<br/>        """.stripMargin).execute()<br/>      Logger.debug("reinsert " + reInsert)</pre>
<p>After storing into the DB, <kbd>SchedulerActor</kbd> transforms OHLC data into (timestamp, delta) tuples, where delta is (<kbd>closePrice</kbd>-<kbd>openPrice</kbd>). So the format is suitable for the ML model. The transformed data is passed as a message to <kbd>PredictionActor</kbd> with explicit waiting for a response. This is done by using the <kbd>?</kbd> operator. We ask the prediction <kbd>actor</kbd>:</p>
<pre>(predictionActor ? CryptoCompareDTOToPredictionModelTransformer.tranform(cryptoCompareResponse)).mapTo[CurrentDataWithShortTermPrediction].map {</pre>
<p>Its response is mapped to the <kbd>CurrentDataWithShortTermPrediction</kbd> class and passed to <kbd>TraderActor</kbd> using the <kbd>!</kbd> operator. Unlike <kbd>?</kbd>, the <kbd>!</kbd> operator does not require a response:</p>
<pre>predictedWithCurrent =&gt;<br/>traderActor ! predictedWithCurrent}</pre>
<p>This was basic a walkthrough of <kbd>SchedulerActor</kbd>. We read data from the Cryptocompare API, store it into the database, send to <kbd>PredictionActor</kbd> and wait for its response. Then we forward its response to <kbd>TraderActor</kbd>.</p>
<p>Now let's see what happens inside <kbd>PredictionActor</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">PredictionActor and the prediction step</h1>
                </header>
            
            <article>
                
<p>The Scala web application, which takes the most recent Bitcoin price data on the Bitstamp exchange <span>every minute</span><span> </span><span>from the Cryptocompare API, uses a trained ML classifier to predict the direction of price change for the next minute. It notifies the user about the decision.</span></p>
<p>Now, to launch it, from a directory with project type <kbd>sbt run</kbd> (or <kbd>$ sudo sbt run</kbd> when required). Now let us see the contents of the <kbd>application.conf</kbd> file:</p>
<pre># This is the main configuration file for the application.<br/># Secret key<br/># The secret key is used to secure cryptographics functions.<br/># If you deploy your application to several instances be sure to use the same key!<br/><strong>application.secret="%APPLICATION_SECRET%"</strong><br/># The application languages<br/><strong>application.langs="en"</strong><br/># Global object class<br/># Define the Global object class for this application.<br/># Default to Global in the root package.sb<br/># application.global=Global<br/># Router<br/># Define the Router object to use for this application.<br/># This router will be looked up first when the application is starting up,<br/># so make sure this is the entry point.<br/># Furthermore, it's assumed your route file is named properly.<br/># So for an application router like `my.application.Router`,<br/># you may need to define a router file `conf/my.application.routes`.<br/># Default to Routes in the root package (and conf/routes)<br/># application.router=my.application.Routes<br/># Database configuration<br/># You can declare as many datasources as you want.<br/># By convention, the default datasource is named `default`<br/><strong>rootDir = "&lt;path&gt;/Bitcoin_price_prediction/"</strong><br/><strong>db.default.driver = org.h2.Driver</strong><br/><strong>db.default.url = "jdbc:h2: "&lt;path&gt;/Bitcoin_price_prediction/DataBase"</strong><br/><strong>db.default.user = user</strong><br/><strong>db.default.password = ""</strong><br/><strong>play.evolutions.db.default.autoApply = true</strong><br/># Evolutions<br/># You can disable evolutions if needed<br/># evolutionplugin=disabled<br/># Logger<br/># You can also configure logback (http://logback.qos.ch/),<br/># by providing an application-logger.xml file in the conf directory.<br/># Root logger:<br/><strong>logger.root=ERROR</strong><br/># Logger used by the framework:<br/><strong>logger.play=INFO</strong><br/># Logger provided to your application:<br/><strong>logger.application=DEBUG</strong><br/>#Enable JobModule to run scheduler<br/>play.modules.enabled += "modules.jobs.JobModule"<br/>#Frequency in seconds to run job. Might make sense to put 30 seconds, for recent data<br/><strong>constants.frequency = 30</strong><br/><strong>ml.model_version = "gbt_22_binary_classes_32660767.model"</strong></pre>
<p>Now you can understand that there are also several variables to configure/change based on your platform and choice:</p>
<ul>
<li>Change the <kbd>rootDir</kbd> directory to the one you have used in <kbd>TrainGBT</kbd>:</li>
</ul>
<pre style="padding-left: 60px">rootDir = "&lt;path&gt;/ Bitcoin_price_prediction"</pre>
<ul>
<li>Specify the name for the database file:</li>
</ul>
<pre style="padding-left: 60px">db.default.url = "jdbc:h2: "&lt;path&gt;/Bitcoin_price_prediction/DataBase"</pre>
<ul>
<li>Specify the version of the model that is used for the actual prediction:</li>
</ul>
<pre style="padding-left: 60px">ml.model_version = "gbt_22_binary_classes_32660767.model"</pre>
<div class="packt_infobox">Note that the folder with such a name has to be inside <kbd>rootDir</kbd>. So inside <kbd>rootDir</kbd>, create a folder named <kbd>models</kbd> and copy all the folders of trained models <span>there</span>.</div>
<p>This class also implements the <kbd>actor</kbd> trait and overrides the receive method. The best practice for it is to define types that can be received by the <kbd>actor</kbd> inside the companion object, thus establishing an interface for other classes:</p>
<pre><strong>object</strong> PredictionActor {<br/>    <strong>def</strong> props = Props[PredictionActor]<br/>    <strong>case class</strong> PriceData(timeFrom: Long,<br/>                        timeTo: Long, <br/>                        priceDelta: (Long, Double)*)<br/>        }</pre>
<p>At first, <kbd>PredictionActor</kbd> loads a list of models from the <kbd>models</kbd> folder and loads the <kbd>etalon</kbd> model:</p>
<pre><strong>val</strong> models: List[(Transformer, String)] =<br/>            SubDirectoryRetriever.getListOfSubDirectories(modelFolder)<br/>            .map(modelMap =&gt; (PipelineModel.load(modelMap("path")),modelMap("modelName")))<br/>        .toList</pre>
<p>First, we extract a list of subdirectories inside the <kbd>models</kbd> folder, and from each of them, we load the trained <kbd>PipeLine</kbd> model. In a similar way, the <kbd>etalon</kbd> model is loaded, but we already know its directory. Here's how a message of the <kbd>PriceData</kbd> type is handled inside the <kbd>receive</kbd> method:</p>
<pre><strong>override def</strong> receive: Receive = {<br/>    <strong>case</strong> data: PriceData =&gt;<br/>        <strong>val</strong> priceData = shrinkData(data, 1, 22)<br/>        <strong>val</strong> (predictedLabelForUnknownTimestamp, details) =             <br/>            predictionService.predictPriceDeltaLabel(priceData,productionModel)</pre>
<p>The predicted label (string) and classification details are logged, so is it possible to see the probability distribution for each class? If the <kbd>actor</kbd> receives a message of another type, an error is shown and nothing more is done. Then the results are sent back to <kbd>SchedulerActor</kbd> and sent in the variable <kbd>predictedWithCurrent</kbd>, as was shown in the preceding code:</p>
<pre>sender() ! CurrentDataWithShortTermPrediction(predictedLabelForUnknownTimestamp, data)</pre>
<p>The <kbd>sender</kbd> is an <kbd>ActorRef</kbd> reference to an object that has sent the message we are processing at the moment, so we can pass the message back with the <kbd>!</kbd> operator. Then, for each model we have loaded in the beginning, we predict the label for 1-minute-old data (rows 0-21 out of 23 in total) and get the actual price delta for the latest minute we know:</p>
<pre>models.<strong>foreach</strong> { mlModel =&gt;<br/>    <strong>val</strong> (predictedLabel, details) =predictionService.predictPriceDeltaLabel(shrinkData(data, 0, 21),     mlModel._1)<br/>    <strong>val</strong> actualDeltaPoint = data.priceDelta.toList(22)</pre>
<p>For each model, we store the following in the DB name of the model: the timestamp for each test prediction made, the label that was predicted by the model, and the actual delta. This information is used later to generate reports on the model's performance:</p>
<pre>storeShortTermBinaryPredictionIntoDB( mlModel._2, actualDeltaPoint._1,<br/>predictedLabel, actualDeltaPoint._2)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TraderActor</h1>
                </header>
            
            <article>
                
<p><kbd>TraderActor</kbd> receives the prediction and, based on the label, writes a log message. It can trigger an HTTP request to the specified endpoint:</p>
<pre><strong>override def</strong> receive: Receive = {<br/>    case data: CurrentDataWithShortTermPrediction =&gt;<br/>        Logger.debug("received short-term prediction" + data)<br/>        data.prediction match {<br/>            case "0" =&gt; notifySellShortTerm()<br/>            case "1" =&gt; notifyHoldShortTerm()<br/>    }</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Predicting prices and evaluating the model</h1>
                </header>
            
            <article>
                
<p><kbd>ShortTermPredictionServiceImpl</kbd> is the class that actually performs the prediction with the given model and data. At first, it transforms <kbd>PriceData</kbd> into a Spark DataFrame with the scheme corresponding to the one used for training by calling <kbd>transformPriceData(priceData: PriceData)</kbd><em>.</em> Then, the <kbd>model.transform(dataframe)</kbd> method is called; we extract the variables we need, write into the debugger log and return to the caller:</p>
<pre><strong>override def</strong> predictPriceDeltaLabel(priceData: PriceData, mlModel: org.apache.spark.ml.Transformer): (String, Row) = {<br/>        val df = transformPriceData(priceData)<br/>        val prediction = mlModel.transform(df)<br/>        val predictionData = prediction.select("probability", "prediction", "rawPrediction").head()<br/>        (predictionData.get(1).asInstanceOf[Double].toInt.toString, predictionData)<br/>        }</pre>
<p>While running, the application collects data about the prediction output: predicted label and actual price delta. This information is used to build the root web page, displaying statistics such as <strong>TPR</strong> (<strong>true positive rate</strong>), <strong>FPR</strong> (<strong>false positive rate</strong>), <strong>TNR</strong> (<strong>true negative rate</strong>), and <strong>FNR </strong>(<strong>false negative rate</strong>), which were described earlier.</p>
<p>These statistics are counted on the fly from the <kbd>SHORT_TERM_PREDICTION_BINARY</kbd> table. Basically, by using the <kbd>CASE-WHEN</kbd> construction, we add new columns: TPR, FPR, TNR, and FNR. They are defined as follows:</p>
<ul>
<li>TPR with value <kbd>1</kbd> if the predicted label was 1 and price delta was &gt; <kbd>0</kbd>, and value <kbd>0</kbd> otherwise</li>
<li>FPR with value <kbd>1</kbd> if the predicted label was 1 and price delta was &lt;= <kbd>0</kbd>, and value <kbd>0</kbd> otherwise</li>
<li>TNR with value <kbd>1</kbd> if the predicted label was 0 and price delta was &lt;= <kbd>0</kbd>, and value <kbd>0</kbd> otherwise</li>
<li>FNR with value <kbd>1</kbd> if the predicted label was 0 and price delta was &gt; <kbd>0</kbd>, and value 0 otherwise</li>
</ul>
<p>Then, all records are grouped by model name, and TPR, FPR, TNR, and FNR are summed up, giving us the total numbers for each model. Here is the SQL code responsible for this:</p>
<pre><strong>SELECT</strong> MODEL, SUM(TPR) as TPR, SUM(FPR) as FPR, SUM(TNR) as TNR, <br/>    SUM(FNR) as FNR, COUNT(*) as TOTAL FROM (SELECT *,<br/>    <strong>case</strong> when PREDICTED_LABEL='1' and ACTUAL_PRICE_DELTA &gt; 0<br/>        then 1 else 0 end as TPR,<br/>    <strong>case</strong> when PREDICTED_LABEL='1' and ACTUAL_PRICE_DELTA &lt;=0<br/>        then 1 else 0 end as FPR,<br/>    <strong>case</strong> when PREDICTED_LABEL='0' and ACTUAL_PRICE_DELTA &lt;=0<br/>        then 1 else 0 end as TNR,<br/>    <strong>case</strong> when PREDICTED_LABEL='0' and ACTUAL_PRICE_DELTA &gt; 0<br/>        then 1 else 0 end as FNR<br/><strong>FROM</strong> SHORT_TERM_PREDICTION_BINARY)<br/><strong>GROUP</strong> BY MODEL</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Demo prediction using Scala Play framework</h1>
                </header>
            
            <article>
                
<p>Now that we have seen all the steps for this project, it's time to see a live demo. We will wrap up the whole application as a Scala Play web app. Well, before seeing the demo, let's get our project up and running. However knowing some basic of RESTful architecture using Scala Play would be helpful.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why RESTful architecture?</h1>
                </header>
            
            <article>
                
<p>Well, Play’s architecture is RESTful by default. At its core, Play is based on the Model-View-Controller pattern. Each entry point, paired with an HTTP verb, maps to a Controller function. The controller enables views to be web pages, JSON, XML, or just about anything else.</p>
<p>Play’s stateless architecture enables horizontal scaling, ideal for serving many incoming requests without having to share resources (such as a session) between them. It is at the forefront of the Reactive programming trend, in which servers are event-based and parallel processing is used to cater to the ever-increasing demands of modern websites.</p>
<p>In certain configurations, Play enables fully asynchronous and non-blocking I/O throughout the entire application. The purpose is to reach new heights in terms of scalability on the web through efficient thread management and parallel processing, while avoiding the <strong>callback hell</strong> that JavaScript-based solutions tend to engender.</p>
<p>AngularJs is a JavaScript-based open-source front-end web application framework mainly maintained by Google and by a community of individuals and corporations to address many of the challenges encountered in developing single-page applications.</p>
<p>Now question would be why AngularJS? Well, HTML is great for declaring static documents, but it falters when we try to use it for declaring dynamic views in web-applications. AngularJS lets you extend HTML vocabulary for your application. The resulting environment is extraordinarily expressive, readable, and quick to develop.</p>
<p>Another question would be, are not there any alternatives? Well<strong>,</strong> other frameworks deal with HTML’s shortcomings by either abstracting away HTML, CSS, and/or JavaScript or by providing an imperative way for manipulating the DOM. Neither of these address the root problem that HTML was not designed for dynamic views.</p>
<p>Finally, what is about the extensibility? Well, AngularJS is a toolset for building the framework most suited to your application development. It is fully extensible and works well with other libraries. Every feature can be modified or replaced to suit your unique development workflow and feature needs. Read on to find out how.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Project structure</h1>
                </header>
            
            <article>
                
<p>The wrapped up Scala web ML app has the following directory structure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="171" width="386" class="alignnone size-full wp-image-524 image-border" src="assets/7fa0cf7c-e77e-4a59-aec0-98d57b3b2c98.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 7: Scala ML web app directory structure</div>
<p>In the preceding structure, <kbd>bitcoin_ml</kbd> folder has all the backend and frontend codes. The <kbd>models</kbd> folder has all the trained models. An example-trained model is given in the <kbd>gbt_22_binary_classes_32660767</kbd> folder. Finally, database files and traces are there in the <kbd>DataBase.mv.db</kbd> and <kbd>DataBase.trace.db</kbd> files respectively.</p>
<p>Then let us see the sub-folder structure of the <kbd>bitcoin_ml</kbd> folder that contains the actual codes:</p>
<div class="CDPAlignCenter CDPAlign"><img height="319" width="362" class="alignnone size-full wp-image-525 image-border" src="assets/90451c79-4115-4029-8ed0-e73b57b18799.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Figure 8: The bitcoin_ml directory structure</div>
<p>In the preceding figure, the <kbd>conf</kbd> folder has the Scala web app configuration file, <kbd>application.conf</kbd> containing necessary configurations (as shown already). All the dependencies are defined in the <kbd>build.sbt</kbd> file<em> </em>shown as follows<em>:</em></p>
<pre>libraryDependencies ++= Seq(jdbc, evolutions,<br/> "com.typesafe.play" %% "anorm" % "2.5.1",<br/> cache, ws, specs2 % Test, ws)<br/><br/>unmanagedResourceDirectories in Test &lt;+= baseDirectory(_ / "target/web/public/test")<br/>resolvers += "scalaz-bintray" at "https://dl.bintray.com/scalaz/releases"<br/><br/>resolvers ++= Seq(<br/>     "apache-snapshots" at "http://repository.apache.org/snapshots/")<br/>    routesGenerator := InjectedRoutesGenerator<br/>    val sparkVersion = "2.2.0"<br/>    libraryDependencies += "org.apache.spark" %% "spark-mllib" % sparkVersion<br/>    libraryDependencies += "org.apache.hadoop" % "hadoop-mapreduce-client-core" % "2.7.2"<br/>    libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "2.7.2"<br/>    libraryDependencies += "commons-io" % "commons-io" % "2.4"<br/>    libraryDependencies += "org.codehaus.janino" % "janino" % "3.0.7" //fixing     "java.lang.ClassNotFoundException: de.unkrig.jdisasm.Disassembler" exception<br/><br/>    libraryDependencies ++= Seq(<br/>     "com.typesafe.slick" %% "slick" % "3.1.1",<br/>     "org.slf4j" % "slf4j-nop" % "1.6.4"<br/>)</pre>
<p class="mce-root">To be frank, at the beginning of writing, I did not think of wrapping up this application as a Scala Play web app. Therefore, things went a bit unstructured. However, do not worry to know more about backend as well frontend, refer to the options trading application in <a href="c4a322da-d64b-4c40-a5b8-7ffec8381b41.xhtml" target="_blank">Chapter 7</a>, <em>Options Trading Using Q-Learning and Scala Play Framework</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running the Scala Play web app</h1>
                </header>
            
            <article>
                
<p>To run the application, just follow these steps:</p>
<ol>
<li>Download the historical Bitcoin data from <a href="https://www.kaggle.com/mczielinski/bitcoin-historical-data">https://www.kaggle.com/mczielinski/bitcoin-historical-data</a>. Then unzip and extract the <kbd>.csv</kbd> file.</li>
<li>Open your preferred IDE (for example, Eclipse/IntelliJ) and create the Maven or SBT project.</li>
</ol>
<p> </p>
<ol start="3">
<li>Run the <kbd>Preprocess.scala</kbd> script to convert the historical data into a time series. This script should generate two <kbd>.csv</kbd> files (that is, <kbd>scala_test_x.csv</kbd> and <kbd>scala_test_y.csv</kbd>).</li>
<li>Then train the <kbd>GradientBoostedTree</kbd> model (use <kbd>TrainGBT.scala</kbd> script) using the previously generated files.</li>
<li>Save the best (i.e. cross-validated) <kbd>Pipeline</kbd> model containing all the pipelines' steps.</li>
<li>Then download the Scala Play app and all the files (that is, <kbd>Bitcoin_price_prediction</kbd>) from the Packt repository or GitHub (see in the book).</li>
<li>Then copy the trained model to <kbd>Bitcoin_price_prediction/models/</kbd>.</li>
<li>Then:  <kbd>$ cd Bitcoin_price_prediction/bitcoin_ml/conf/</kbd> and update the parameter values in the <kbd>application.conf</kbd> as shown earlier.</li>
<li>Finally, run the project using the <kbd>$ sudo sbt run</kbd> command.</li>
</ol>
<p>After launching with <kbd><span>$ sudo </span>sbt run</kbd>, the application will read all models from the <kbd>models</kbd> folder, the <kbd>etalon</kbd> model being specified by <kbd>ml.model_version</kbd>. Every 30 seconds (specified in <kbd>constants.frequency = 30</kbd> in <kbd>application.conf</kbd>), the latest price data is retrieved from the Cryptocompare API. A prediction using the <kbd>etalon</kbd> model is made and the results are shown to the user in the form of a log message in the console, with the possibility to trigger an HTTP request to the specified endpoint.</p>
<p>After that, all models from the <kbd>models</kbd> folder are used to make a prediction on the previous 22-minute data and use the latest price data for a current minute as a way to check the quality of predictions. All predictions made by each model are stored in a database file. When a user visits <kbd>http://localhost:9000</kbd>, a table with a summary of predictions is shown to the user:</p>
<ul>
<li class="MsoNormalCxSpMiddle">Model name</li>
<li class="MsoNormalCxSpMiddle">TPR, (not rate actually, in this case, just raw count) - how many times model predicted that price would increase and how many times that was true</li>
<li class="MsoNormalCxSpMiddle">FPR,  how many times model has predicted price increase, but price dropped or stayed the same</li>
<li class="MsoNormalCxSpMiddle">TNR, how many times model predicted non-increase of price and was correct</li>
<li class="MsoNormalCxSpMiddle">FNR, how many times model predicted non-increase of price and was wrong</li>
<li class="MsoNormalCxSpLast">Total count of predictions made by the model</li>
</ul>
<p>Alright, here we go, after launching the app using <kbd>$</kbd> sudo sbt <kbd>run</kbd> (on a terminal):</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a475c8d5-3fd1-4ab5-b796-7350d540ee9f.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 9: Sample signals generated by the model based on historical prices and live data</div>
<p>The preceding figure shows some sample signals generated by our model based on historical prices and live data. Additionally, we can see the raw prediction by the model. When you try to access the app from your browser at <kbd>http://localhost:9000</kbd><span class="MsoHyperlink">, you should see this (the count will increase with time, though):</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5876680a-d23f-4462-b4e1-4cda118482a4.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 10: Model performance using the Scala Play2 framework</div>
<p>In the preceding figure, the performance is not satisfactory, but I would suggest that you train the model with the most suitable hyperparameters and for more iterations, for example, 10,000 times. Additionally, in the next section, I tried to provide some more insights and improvement guidelines.</p>
<p style="margin-top: 6.0pt;text-align: justify" class="NormalPACKT"><span>Finally, if you plan to deploy this application after making some extension (if any), then I would suggest to take a quick look at the last section in <a href="c4a322da-d64b-4c40-a5b8-7ffec8381b41.xhtml" target="_blank">Chapter 7</a>, <em>Options Trading Using Q-Learning and Scala Play Framework</em>, where you will find deployment guideline on server to be exposed as web app.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, a complete ML pipeline was implemented, from collecting historical data, to transforming it into a format suitable for testing hypotheses, training ML models, and running a prediction on <kbd>Live</kbd> data, and with the possibility to evaluate many different models and select the best one.</p>
<p>The test results showed that, as in the original dataset, about 600,000 minutes out of 2.4 million can be classified as <strong>increasing price</strong> (close price was higher than open price); the dataset can be considered imbalanced. Although random forests are usually performed well on an imbalanced dataset, the area under the ROC curve of 0.74 isn't best. As we need to have fewer false positives (fewer times when we trigger <strong>purchase</strong> and the price drops), we might consider a punishing model for such errors in a stricter way.</p>
<p>Although the results achieved by classifiers can't be used for profitable trading, there is a foundation on top of which new approaches can be tested in a relatively rapid way. Here, some possible directions for further development are listed:</p>
<ul>
<li>Implementation of the pipeline discussed in the beginning: Convert your time series data into several clusters and train the regression model/classifier for each of them; then classify recent data into one of the clusters and use the prediction model trained for that cluster. As by definition, ML is <strong>deriving patterns from data</strong>, there might not be only one pattern that fits all of the history of Bitcoin; that's why we need to understand that a market can be in different phases, and each phase has its own pattern.</li>
<li>One of the major challenges with Bitcoin price prediction might be that the training data (historical) doesn't belong to the same distribution as test data during random splits into train-test sets. As the patterns in price changed during 2013 and 2016, they might belong to completely different distributions. It might require a manual inspection of data and some infographics. Probably, someone has already done this research.</li>
<li>One of the main things to try would be to train two <strong>one-versus-all</strong> classifiers: one is trained to predict when the price grows higher than 20$, for example. Another predicts when the price drop by 20$; so it makes sense to take long/short positions, respectively.</li>
<li>Maybe, predicting the delta of the next minute isn't what we need; we'd rather predict the average price. As the Open price can be much higher than last minute's Close price, and the Close price of the next minute can be slightly less than open but still higher than current, it would make it profitable trade. So how to exactly label data is also an open question.</li>
<li>Try with different time-series window size (even 50 minutes might suit) using ARIMA time series prediction model, as it is one of the most widely used algorithms. Then try to predict price change, not for the next minute but for 2-3 following minutes. Additionally, try by incorporating trading volume as well. </li>
<li>Label the data as <strong>price increased</strong> if the price was higher by 20$ <span>during at least one of three following minutes</span> so that we can make a profit from trade.</li>
<li>Currently, <kbd>Scheduler</kbd> isn't synchronized with Cryptocompare minutes. This means we can get data about the minute interval 12:00:00 - 12:00:59 at any point of the following minute - 12:01:00 or 12:01:59. In the latter case, it doesn't make sense to make trade, as we made a prediction based on already <strong>old </strong>data.</li>
<li>Instead of making a prediction every minute on <strong>older</strong> data to accumulate prediction results for <kbd>actor</kbd>, it's better to take maximum available HistoMinute data (seven days), split it into time series data using a Scala script that was used for historical data, and predict for seven days' worth of data. Run this as a scheduled job once a day; it should reduce the load on the DB and <kbd>PredictionActor</kbd>.</li>
<li>Compared to usual datasets, where the order of rows doesn't matter much, in Bitcoin, historical data rows are sorted by ascending order of date, which means that:
<ul>
<li>Latest data might be more relevant to today's price, and less can be more; taking a smaller subset of data might give better performance</li>
<li>The ways of subsampling data can matter (splitting into train-test sets)</li>
<li>Finally try with LSTM network for even better predictive accuracy (see chapter 10 for some clue)</li>
</ul>
</li>
</ul>
<p>The understanding of variations in genome sequences assists us in identifying people who are predisposed to common diseases, solving rare diseases, and finding the corresponding population group of individuals from a larger population group. Although classical ML techniques allow researchers to identify groups (clusters) of related variables, the accuracy and effectiveness of these methods diminish for large and high-dimensional datasets such as the whole human genome. On the other hand, deep neural network architectures (the core of deep learning) can better exploit large-scale datasets to build complex models.</p>
<p>In the next chapter, we will see how to apply the K-means algorithm on large-scale genomic data from the 1,000 Genomes Project aiming at clustering genotypic variants at the population scale. Then we'll train an H2O-based deep learning model for predicting geographic ethnicity. Finally, Spark-based Random Forest will be used to enhance the predictive accuracy.</p>


            </article>

            
        </section>
    </body></html>