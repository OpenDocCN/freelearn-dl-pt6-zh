<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Image Classification</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous chapter, after a quick recap on the concept of convolution, we learned more about the strategies for object recognition and more implementation details through examples in Python (Keras) and Scala (DL4J). This chapter covers the implementation of a full image classification web application or web service. The goal here is to show you how to apply the concepts from the previous chapter to an end-to-end classification system.</p>
<p class="mce-root">The steps to complete this goal are as follows:</p>
<ul>
<li class="mce-root">Pick up a proper Keras (with TensorFlow backend) pre-trained CNN model</li>
<li class="mce-root">Load it and test it in DL4J (and Spark)</li>
<li class="mce-root">Understand how to retrain the Python model on Apache Spark</li>
<li class="mce-root">Implement an image classification web application that uses it</li>
<li class="mce-root">Implement an alternative image classification web service that uses it</li>
</ul>
<p class="mce-root">All of the open source technologies that we have come across in the previous chapters of this book while learning to use DL scenarios are involved in the implementation process that's explained here.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing an end-to-end image classification web application</h1>
                </header>
            
            <article>
                
<p>Using all of the things that we learned about in the previous chapters of this book, we should now be able to implement a real-world web application that allows users to upload an image and then properly classify it.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Picking up a proper Keras model</h1>
                </header>
            
            <article>
                
<p>We are going to use an existing, pre-trained Python Keras CNN model. Keras applications (<a href="https://keras.io/applications/">https://keras.io/applications/</a>) are a set of DL models that are available as part of the framework with pre-trained weights. Among those models is <span class="packt_screen">VGG16</span>, a 16-layer CNN that was implemented by the Visual Geometry Group at the University of Oxford in 2014. This model is compatible with a TensorFlow backend. It has been trained on the ImageNet database (<a href="http://www.image-net.org/">http://www.image-net.org/</a>). The ImageNet dataset is an excellent training set for general image classification, but it isn't suitable for facial recognition model training. Here is the way you can load and use the <span class="packt_screen">VGG16</span> model in Keras. We are using a TensorFlow backend. Let's import the model:</p>
<pre>from keras.applications.vgg16 import VGG16</pre>
<p>Then, we need to import the other necessary dependencies (including NumPy and Pillow):</p>
<pre>from keras.preprocessing import image<br/>from keras.applications.vgg16 import preprocess_input<br/>import numpy as np<br/>from PIL import Image</pre>
<p>Now, we can create an instance of the model:</p>
<pre>model = VGG16(weights='imagenet', include_top=True)</pre>
<p>The pre-trained weights are automatically downloaded the first time we run this application. Successive runs will pick up the weights from the local <kbd>~/.keras/models/</kbd> directory.</p>
<p>Here's the model architecture:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4dec12d6-3628-42c8-9da5-50ed04ec9345.png" style="width:36.42em;height:49.42em;"/></p>
<p>We can test the model by loading an image:</p>
<pre>img_path = 'test_image.jpg'<br/> img = image.load_img(img_path, target_size=(224, 224))</pre>
<p>We can then prepare it to be passed as input to the model (by transforming the image pixels into a NumPy array and preprocessing it):</p>
<pre>x = image.img_to_array(img)<br/> x = np.expand_dims(x, axis=0)<br/> x = preprocess_input(x)</pre>
<p>Then, we can make predictions:</p>
<pre>features = model.predict(x)</pre>
<p>Finally, we save the model configuration (in JSON format):</p>
<pre>model_json = model.to_json()<br/> with open('vgg-16.json', 'w') as json_file:<br/>     json_file.write(model_json)</pre>
<p>We can also save the weights of the model that we want to import into DL4J:</p>
<pre>model.save_weights("vgg-16.h5")</pre>
<p>Then, we pass the following image into the model as input:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0d17d699-4fe3-495d-8af8-24915b71bb35.png" style="width:14.50em;height:15.83em;"/></p>
<p>The image is correctly classified as a tabby cat, with a likelihood of almost 64%.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Importing and testing the model in DL4J</h1>
                </header>
            
            <article>
                
<p>In <a href="1066b0d4-c2f3-44f9-9cc4-d38469d72c3f.xhtml" target="_blank">Chapter 10</a>, <em>Deploying on a Distributed System</em>, we learned how to import a pre-trained Keras model into DL4J. Let's apply the same process here.</p>
<p>The dependencies for the Scala project are DL4J DataVec, NN, model import, zoo, and ND4J plus Apache common math 3.</p>
<p>The first thing we need to do is copy the model configuration (from the <kbd>vgg-16.json</kbd> file) and weights (from the <kbd>vgg-16.h5</kbd> file) into the resource folder of the project. Then, we can load them through the <kbd>importKerasModelAndWeights</kbd> method of the <kbd>KerasModelImport</kbd> class:</p>
<pre>val vgg16Json = new ClassPathResource("vgg-16.json").getFile.getPath<br/> val vgg16 = new ClassPathResource("vgg-16.h5").getFile.getPath<br/> val model = KerasModelImport.importKerasModelAndWeights(vgg16Json, vgg16, false)</pre>
<p>The third argument to pass to the method is a Boolean; if <kbd>false</kbd>, this means that the pre-trained model has been imported to do inference only and won't be re-trained.</p>
<p>Let's test the model using the image in the preceding screenshot. We need to copy it into the resource directory for the application. Then, we can load it and resize it to be the required size (224 × 224 pixels):</p>
<pre>val testImage = new ClassPathResource("test_image.jpg").getFile<br/>     <br/> val height = 224<br/> val width = 224<br/> val channels = 3<br/> val loader = new NativeImageLoader(height, width, channels)</pre>
<p>For this, we are using the <kbd>NativeImageLoader</kbd> class (<a href="https://jar-download.com/javaDoc/org.datavec/datavec-data-image/1.0.0-alpha/org/datavec/image/loader/NativeImageLoader.html">https://jar-download.com/javaDoc/org.datavec/datavec-data-image/1.0.0-alpha/org/datavec/image/loader/NativeImageLoader.html</a>) of the DataVec Image API.</p>
<p>Then, we need to transform the image into an NDArray and preprocess it:</p>
<pre>val image = loader.asMatrix(testImage)<br/> val scaler = new VGG16ImagePreProcessor<br/> scaler.transform(image)</pre>
<p>Afterwards, we need to do inference through the model:</p>
<pre>val output = model.output(image)</pre>
<p>To consume the result in a human readable format, we use the <kbd>org.deeplearning4j.zoo.util.imagenet.ImageNetLabels</kbd> class, which is available in the DL4J zoo library. The input for the <kbd>decodePredictions</kbd> method of this class is the array of the NDArray that's returned from the <kbd>output</kbd> method of the model:</p>
<pre>val imagNetLabels = new ImageNetLabels<br/> val predictions = imagNetLabels.decodePredictions(output(0))<br/> println(predictions)</pre>
<p>The following screenshot shows the output of the preceding code. It presents the prediction results (in descending order) for the uploaded image. According to the model, the highest probability (around 53.3%) is that the main subject in the input picture is a tabby cat (which is the correct one)<span><span>:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/043c9836-e850-4181-b971-7b41f3c87ebc.png" style="width:22.67em;height:9.83em;"/></p>
<p>You should have noticed that, once the model has been imported, the steps to load an image and make an inference through the DL4J API are the same as for the example in Keras that we presented in the previous section.</p>
<p>After the model has been tested, it is a good practice to save it through the <kbd>ModelSerializer</kbd> class:</p>
<pre>val modelSaveLocation = new File("Vgg-16.zip")<br/> ModelSerializer.writeModel(model, modelSaveLocation, true)</pre>
<p>Then, we can load it through the same class because it is less expensive in terms of resource usage than loading from Keras.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Re-training the model in Apache Spark</h1>
                </header>
            
            <article>
                
<p>To improve the accuracy of the Keras VGG16 pre-trained model that we have considered for the use case of this chapter, we could also decide to retrain it and apply all of the best practices we have learned from the previous chapter (running more epochs, image augmentation, and so on). Once the model has been imported into DL4J, its training can be done exactly the same way it was explained in <a href="3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml" target="_blank">Chapter 7</a>, <em>Training Neural Networks with Spark</em> (training with DL4J and Apache Spark). After loading, an instance of <kbd>org.deeplearning4j.nn.graph.ComputationGraph</kbd> is created, so the exact same principles for training multilayer networks apply here.</p>
<p>For completeness of information, you have to know that it is possible to train Keras models in parallel mode on Apache Spark, too. This can be done through the <kbd>dist-keras</kbd> Python framework (<a href="https://github.com/cerndb/dist-keras/">https://github.com/cerndb/dist-keras/</a>), which was created for <span><strong>Distributed Deep Learning</strong> </span>(<strong><span>DDL</span></strong>). The framework can be installed through <kbd>pip</kbd>:</p>
<pre><strong>sudo pip install dist-keras</strong></pre>
<p>It requires TensorFlow (this will be used as a backend) and the following variables to be set:</p>
<pre><strong>export SPARK_HOME=/usr/lib/spark</strong><br/><strong> export PYTHONPATH="$SPARK_HOME/python/:$SPARK_HOME/python/lib/py4j-0.9-src.zip:$PYTHONPATH"</strong></pre>
<p>Let's have a quick look at the typical flow for distributed training with <kbd>dist-keras</kbd>. The following code isn't a complete working example; the goal here is to make you aware of how data parallelism training could be set.</p>
<p>First, we need to import all of the required classes for Keras, PySpark, Spark MLLib, and <kbd>dist-keras</kbd>. We will import Keras first:</p>
<pre>from keras.optimizers import *<br/> from keras.models import Sequential<br/> from keras.layers.core import Dense, Dropout, Activation</pre>
<p>Then, we can import PySpark:</p>
<pre>from pyspark import SparkContext<br/> from pyspark import SparkConf</pre>
<p>Then, we import Spark MLLib:</p>
<pre>from pyspark.ml.feature import StandardScaler<br/> from pyspark.ml.feature import VectorAssembler<br/> from pyspark.ml.feature import StringIndexer<br/> from pyspark.ml.evaluation import MulticlassClassificationEvaluator<br/> from pyspark.mllib.evaluation import BinaryClassificationMetrics</pre>
<p>Finally, we import <kbd>dist-keras</kbd>:</p>
<pre>from distkeras.trainers import *<br/> from distkeras.predictors import *<br/> from distkeras.transformers import *<br/> from distkeras.evaluators import *<br/> from distkeras.utils import *</pre>
<p>We then need to create the Spark configuration, like so:</p>
<pre>conf = SparkConf()<br/> conf.set("spark.app.name", application_name)<br/> conf.set("spark.master", master)<br/> conf.set("spark.executor.cores", num_cores)<br/> conf.set("spark.executor.instances", num_executors)<br/> conf.set("spark.locality.wait", "0")<br/> conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer");</pre>
<p>We can then use this to create a <kbd>SparkSession</kbd>:</p>
<pre>sc = SparkSession.builder.config(conf=conf) \<br/>     .appName(application_name) \<br/>     .getOrCreate()</pre>
<p>The dataset is now as follows:</p>
<pre>raw_dataset = sc.read.format('com.databricks.spark.csv') \<br/>                     .options(header='true', inferSchema='true').load("data/some_data.csv")</pre>
<p>We can use this dataset to perform data preprocessing and normalization using the API provided by the Spark core and Spark MLLib (the strategy depends on the nature of the dataset, so it doesn't make sense to present some code here). Once this phase has been completed, we can define our model using the Keras API.</p>
<p>Here's an example with a simple <kbd>Sequential</kbd> model:</p>
<pre>model = Sequential()<br/> model.add(Dense(500, input_shape=(nb_features,)))<br/> model.add(Activation('relu'))<br/> model.add(Dropout(0.4))<br/> model.add(Dense(500))<br/> model.add(Activation('relu'))<br/> model.add(Dense(nb_classes))<br/> model.add(Activation('softmax'))</pre>
<p>Finally, you can start the training process by choosing one of the multiple optimization algorithms that's available with <kbd>dist-keras</kbd>:</p>
<ul>
<li>Sequential trainer</li>
<li>ADAG</li>
<li>Dynamic SDG</li>
<li>AEASGD</li>
<li>AEAMSGD</li>
<li>DOWNPOUR</li>
<li>Ensemble training</li>
<li>Model averaging</li>
</ul>
<p>While those later in this list are more performant, the first one, <kbd>SingleTrainer</kbd>, which is typically used as a benchmarking <kbd>trainer</kbd>, could be a good <kbd>trainer</kbd> choice in situations where a dataset is too big to fit in memory. Here's a code example of training with <kbd>SingleTrainer</kbd>:</p>
<pre>trainer = SingleTrainer(keras_model=model, worker_optimizer=optimizer,<br/>                         loss=loss, features_col="features_normalized",<br/>                         label_col="label", num_epoch=1, batch_size=32)<br/> trained_model = trainer.train(training_set)</pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the web application</h1>
                </header>
            
            <article>
                
<p>Let's go back to our main task and start implementing a web application that allows users to upload an image, and then use the serialized VGG16 model to make an inference on it. Several frameworks exist for the JVM to implement web applications. In this case, to minimize our efforts, we are going to use SparkJava (<a href="http://sparkjava.com/">http://sparkjava.com/</a>, not to be confused with Apache Spark), a micro framework for JVM programming languages, which has being implemented to keep rapid prototyping in mind. Compared to other web frameworks, it has a minimal boilerplate. SparkJava isn't for web applications only;<span> </span>it is possible to implement the REST API in very few lines of code (it will also be used in the next section to implement our image classification web service).</p>
<p>We have to add SparkJava to the dependencies list of the Java project for the web app:</p>
<pre>groupId: com.sparkjava<br/> artifactId: spark-core<br/> version: 2.7.2</pre>
<p>The reference version for this example is <kbd>2.7.2</kbd> (the latest at the time of writing this book).</p>
<p>In its simplest implementation, a SparkJava web application can be made of a single line of code in the <kbd>main</kbd> method:</p>
<pre>get("/hello", (req, res) -&gt; "Hello VGG16");</pre>
<p>Running the application, the <kbd>hello</kbd> page is accessible from a web browser at the following URL:</p>
<pre>http://localhost:4567/hello</pre>
<p><kbd>4567</kbd> is the default port for SparkJava web apps.</p>
<p>The main building block of a SparkJava application is a route. A route is made up of three pieces: a verb (<kbd>get</kbd>, <kbd>post</kbd>, <kbd>put</kbd>, <kbd>delete</kbd>, <kbd>head</kbd>, <kbd>trace</kbd>, <kbd>connect</kbd>, and <kbd>options</kbd> are the available verbs), a path (<kbd>/hello</kbd> in the preceding code example), and a callback (<kbd>request</kbd> or <kbd>response</kbd>). The SparkJava API also includes classes for sessions, cookies, filters, redirection, and custom errors handling.</p>
<p>Let's start implementing our web application. The other dependencies for the project are DL4J core, DataVec, NN, model import and zoo, and ND4J. We need to add the DL4J serialized model (the <kbd>Vgg-16.zip</kbd> file) to the resources of the project. The model can then be loaded programmatically through the <kbd>ModelSerializer</kbd> class:</p>
<pre>ClassLoader classLoader = getClass().getClassLoader();<br/> File serializedModelFile = new File(classLoader.getResource("Vgg-16.zip").getFile());<br/> ComputationGraph vgg16 = ModelSerializer.restoreComputationGraph(serializedModelFile);</pre>
<p>We need to create a directory where the images from the users will be uploaded:</p>
<pre>File uploadDir = new File("upload");<br/> uploadDir.mkdir();</pre>
<p>The next step is the creation of the form where users can upload an image. In SparkJava, it is possible to use custom styles for web pages. In this example, we are going to add the responsive Foundation 6 framework (<a href="https://foundation.zurb.com/">https://foundation.zurb.com/</a>), CSS. We add the minimal Foundation CSS library (<kbd>foundation-float.min.css</kbd>) in a subdirectory called <kbd>public</kbd> of the resource folder of the project. This way, the web application can access it within its classpath. The registration of the static file's location can be done programmatically:</p>
<pre>staticFiles.location("/public");</pre>
<p>The Foundation CSS and any other static CSS files can be registered in the header of the pages. Here's the method that has been implemented for this example:</p>
<pre>private String buildFoundationHeader() {<br/>     String header = "&lt;head&gt;\n"<br/>           + "&lt;link rel='stylesheet' href='foundation-float.min.css'&gt;\n"<br/>           + "&lt;/head&gt;\n";<br/>         <br/>     return header;<br/> }</pre>
<p>We now implement a method called <kbd>buildUploadForm</kbd> that returns the HTML content for it:</p>
<pre>private String buildUploadForm() {<br/>      String form =<br/>              "&lt;form method='post' action='getPredictions' enctype='multipart/form-data'&gt;\n" +<br/>              " &lt;input type='file' name='uploadedFile'&gt;\n" +<br/>              " &lt;button class='success button'&gt;Upload picture&lt;/button&gt;\n" +<br/>              "&lt;/form&gt;\n";<br/>         <br/>     return form;<br/> }</pre>
<p>We then use this when defining the route to the upload page:</p>
<pre>String header = buildFoundationHeader();<br/> String form = buildUploadForm();<br/> get("Vgg16Predict", (req, res) -&gt; header + form);</pre>
<p>We can now define the <kbd>post</kbd> request:</p>
<pre>post("/doPredictions", (req, res)</pre>
<p>We do this to handle the image upload and classification. In the body of this <kbd>post</kbd> request, we have to take the following actions:</p>
<ol>
<li>Upload the image file to the <kbd>upload</kbd> directory</li>
<li>Convert the image to NDArray</li>
<li>Delete the file (there's no need to keep it in the web server disk after the conversion)</li>
<li>Preprocess the image</li>
<li>Do an inference</li>
<li>Display the results</li>
</ol>
<p>When translated into Java, this is as follows:</p>
<pre>// Upload the image file<br/>Path tempFile = Files.createTempFile(uploadDir.toPath(), "", "");<br/><br/>req.attribute("org.eclipse.jetty.multipartConfig", new MultipartConfigElement("/temp"));<br/><br/>try (InputStream input = req.raw().getPart("uploadedFile").getInputStream()) {<br/>  Files.copy(input, tempFile, StandardCopyOption.REPLACE_EXISTING);<br/>}<br/><br/>// Convert file to INDArray<br/>File file = tempFile.toFile();<br/><br/>NativeImageLoader loader = new NativeImageLoader(224, 224, 3);<br/>INDArray image = loader.asMatrix(file);<br/><br/>// Delete the physical file<br/>file.delete();<br/><br/>// Pre-processing the image to prepare it for the VGG-16 model<br/>DataNormalization scaler = new VGG16ImagePreProcessor();<br/>scaler.transform(image);<br/><br/>// Do inference<br/>INDArray[] output = vgg16.output(false,image);<br/><br/>// Get the predictions<br/>ImageNetLabels imagNetLabels = new ImageNetLabels();<br/>String predictions = imagNetLabels.decodePredictions(output[0]);<br/><br/>// Return the results<br/>return buildFoundationHeader() + "&lt;h4&gt; '" + predictions + "' &lt;/h4&gt;" +<br/>  "Would you like to try another image?" +<br/>  form;</pre>
<p>You will notice that the image preparation and the inference part that's done through DL4J is the exact same as for the standalone application.</p>
<p>After starting the application, it will be accessible at the following URL:</p>
<pre>http://localhost:4567/Vgg16Predict</pre>
<p>It is possible to programmatically set up a different listening port:</p>
<pre>port(8998);</pre>
<p>The following screenshot shows the upload page layout:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/04332b95-57a3-44fc-bfc5-ee17830ec73c.png" style="width:31.92em;height:13.33em;"/></p>
<p>The following screenshot shows us uploading the required image:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9026c652-8ee6-417f-afef-21b458a182b5.png"/></p>
<p>The results for this are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0cf3cf17-6a52-4762-b774-6a55a5974cd0.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing a web service</h1>
                </header>
            
            <article>
                
<p>As we mentioned in the previous section, SparkJava can be used to quickly implement a REST API. The example web application we implemented in the previous section is monolithic, but looking back at its source code, we can notice how easily it would be to separate the frontend from the backend and move this to a REST API.</p>
<p>The frontend client presenting a form for image submission could be implemented with any web frontend framework. The client would then make a call to a REST service that's implemented through SparkJava, which performs the inference with the VGG16 model and finally returns the prediction results in JSON format. Let's see how easy it is to implement this service, starting from the existing code for the web application.</p>
<p>The web service is a Java class with the main method as an entry point. Let's define a custom listening port:</p>
<pre>port(8998);</pre>
<p>Now that we've done this, we need to define the <kbd>upload</kbd> endpoint:</p>
<pre>post("/upload", (req, res) -&gt; uploadFile(req));</pre>
<p>We need to move the code that was part of the original <kbd>post</kbd> body into the <kbd>uploadFile</kbd> method (the only difference is the returned value, which is the prediction content only and not the full HTML content):</p>
<pre>private String uploadFile(Request req) throws IOException, ServletException {<br/>    // Upload the image file<br/>    Path tempFile = Files.createTempFile(uploadDir.toPath(), "", "");<br/><br/>    req.attribute("org.eclipse.jetty.multipartConfig", new MultipartConfigElement("/temp"));<br/><br/>    try (InputStream input = req.raw().getPart("file").getInputStream()) {<br/>      Files.copy(input, tempFile, StandardCopyOption.REPLACE_EXISTING);<br/>    }<br/><br/>    // Convert file to INDArray<br/>    File file = tempFile.toFile();<br/><br/>    NativeImageLoader loader = new NativeImageLoader(224, 224, 3);<br/>    INDArray image = loader.asMatrix(file);<br/><br/>    // Delete the physical file<br/>    file.delete();<br/><br/>    // Pre-processing the image to prepare it for the VGG-16 model<br/>    DataNormalization scaler = new VGG16ImagePreProcessor();<br/>    scaler.transform(image);<br/><br/>    // Do inference<br/>    INDArray[] output = vgg16.output(false,image);<br/><br/>    // Get the predictions<br/>    ImageNetLabels imagNetLabels = new ImageNetLabels();<br/>    String predictions = imagNetLabels.decodePredictions(output[0]);<br/><br/>    // Return the results<br/>    return predictions;<br/>}</pre>
<p>After running the application, you can test it with a simple <kbd>curl</kbd> (<a href="https://curl.haxx.se/">https://curl.haxx.se/</a>) command:</p>
<pre>curl -s -X POST http://localhost:8998/upload -F 'file=@/home/guglielmo/dlws2/vgg16/src/main/resources/test_image-02.jpg'</pre>
<p>The output will be as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/228ec6e2-a872-41d9-8a70-049fdf144063.png"/></p>
<p>If we want to return the output in JSON format, this is the only change to the web service code that we have to perform:</p>
<pre>Gson gson = new Gson();<br/> post("/upload", (req, res) -&gt; uploadFile(req), gson::toJson);</pre>
<p>We just need to create an instance of <kbd>com.google.gson.Gson</kbd> and pass it as the last argument to the <kbd>post</kbd> method. The output from our example will be as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6751468a-ed0b-4424-91ea-12c955d7bdba.png"/></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have implemented our first end-to-end image classification web application by putting together several open source frameworks that we got familiar with throughout the previous chapters of this book. The readers should now have all of the knowledge of the building blocks to start working on their DL models or applications using Scala and/or Python and DL4J and/or Keras or TensorFlow.</p>
<p>This chapter ends the hands-on section of this book. The next and final chapter will discuss the future of DL and AI, with particular focus on DL4J and Apache Spark.</p>


            </article>

            
        </section>
    </body></html>