- en: High Frequency Bitcoin Price Prediction from Historical and Live Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bitcoin is a worldwide cryptocurrency and digital payment system considered
    the **first decentralized digital currency**, since the system works without a
    central repository or single administrator. In recent times, it has gained much
    popularity and attention among people around the world.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will see how to develop a real-life project using Scala,
    Spark ML, Cryptocompare API, and Bitcoin historical (and live) data to predict
    the price for an upcoming week, month, and so on that help us taking automated
    decision for online cryptocurrency. In addition to this, we will see how to generate
    a simple signal for online Bitcoin trading.
  prefs: []
  type: TYPE_NORMAL
- en: 'Briefly, we will learn the following topics throughout this end-to-end project:'
  prefs: []
  type: TYPE_NORMAL
- en: Bitcoin, cryptocurrency, and online trading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Historical and live-price data collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High-level pipeline of the prototype
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient-boosted trees regression for Bitcoin price prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demo prediction and signal generation using the Scala play framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Future outlook—using the same technique for other datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bitcoin, cryptocurrency, and online trading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bitcoin, the first cryptocurrency by date of launch and by market cap (as of
    December2017) has attracted investors and traders because of its ease of starting
    trading, ability to stay pseudo-anonymous, and, of course, dramatic growth during
    its history (see *Table 1* and *Figure 1* for some statistics). This lures long-term
    investors; its high volatility also attracts day traders.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it''s hard predict the value of Bitcoin in the long term, as the value
    behind Bitcoin is less tangible. The price mostly reflects market perception and
    is highly dependent on news, regulations, collaboration of governments and banks,
    technical issues of the platform (such as transactions fee and block size), interest
    of institutional investors in including Bitcoin into their portfolio, and more:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49e8ae72-5c00-48ac-8a1b-ded0004f484f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Bitcoin and its dramatic price increases'
  prefs: []
  type: TYPE_NORMAL
- en: 'Nonetheless, from a short-term perspective, Bitcoin price is a by-product of
    market activity usually happening on a platform, called **exchange** (Bitstamp,
    Coinbase, Kraken, and Bitfinex among the most well-known **exchanges**). Users,
    after registration and after going through **KYC** (**Know Your Customer**) procedures,
    can trade Bitcoin in it for fiat currencies such as dollars and euros, as well
    as for other cryptocurrencies, called **alt-coins** or alternative coins (Ethereum,
    Litecoin, and Dash are well known):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 1 – Bitcoin historical price movement**'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Date** | **USD: 1 BTC** |'
  prefs: []
  type: TYPE_TB
- en: '| Jan 2009 to Mar 2010 | Basically none |'
  prefs: []
  type: TYPE_TB
- en: '| Mar 2010 | $0.003 |'
  prefs: []
  type: TYPE_TB
- en: '| May 2010 | Less than $0.01 |'
  prefs: []
  type: TYPE_TB
- en: '| Jul 2010 | $0.08![](img/90581ab4-0591-4dca-b5ed-443e38e1d7c0.png) |'
  prefs: []
  type: TYPE_TB
- en: '| Feb to Apr 2011 | $1.00![](img/90581ab4-0591-4dca-b5ed-443e38e1d7c0.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 8 Jul 2011 | $31.00![](img/90581ab4-0591-4dca-b5ed-443e38e1d7c0.png) |'
  prefs: []
  type: TYPE_TB
- en: '| Dec 2011 | $2.00![](img/a45968d6-6644-4ea0-b057-04ae1126bf6d.png) |'
  prefs: []
  type: TYPE_TB
- en: '| Dec 2012 | $13.00 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 Apr 2013 | $266![](img/90581ab4-0591-4dca-b5ed-443e38e1d7c0.png) |'
  prefs: []
  type: TYPE_TB
- en: '| May 2013 | $130![](img/a45968d6-6644-4ea0-b057-04ae1126bf6d.png) |'
  prefs: []
  type: TYPE_TB
- en: '| Jun 2013 | $100![](img/a45968d6-6644-4ea0-b057-04ae1126bf6d.png) |'
  prefs: []
  type: TYPE_TB
- en: '| Nov 2013 | $350 to $1,242![](img/b6f31e88-ab89-497f-80de-74f781252588.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Dec 2013 | $600 to $1,000![](img/5a96adb5-2395-4d3f-aafe-9a5611dcf169.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Jan 2014 | $750 to $1,000![](img/32d8f6a6-d273-4f99-8151-9439ab44d07a.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Feb 2014 | $550 to $750![](img/1ca1dfd2-722d-4c6b-96a9-2e0f148bca0b.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Mar 2014 | $450 to $700![](img/5057f2fa-69b0-44ff-9c33-b7a637d18f48.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Apr 2014 | $340 to $530![](img/56ee8714-fe1c-4fc1-9388-9f959cf22b1b.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| May 2014 | $440 to $630![](img/0cb1f1cf-928d-4ec2-8d06-23fad044b50a.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Mar 2015 | $200 to $300![](img/9e3940a9-ff01-4626-be01-e8dbebe89654.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Early Nov 2015 | $395 to $504![](img/d0532b1c-66e1-4127-b5a3-6528ce9a8fe7.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| May to Jun 2016 | $450 to $750![](img/bd67159a-b317-4476-9ad2-d26a9c13b868.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Jul to Sept 2016 | $600 to $630![](img/0610538f-261e-4188-a43d-a0bca9d5be7c.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Oct to Nov 2016 | $600 to $780![](img/45f5cbb4-1622-4d1a-b774-43651f28fa2d.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Jan 2017 | $800 to $1,150![](img/68fe4e41-afe2-4443-970f-12f2f1828000.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 5-12 Jan 2017 | $750 to $920![](img/9fe99d9e-0593-41b5-8e75-2065a34c5e6e.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2-3 Mar 2017 | $1,290+ ![](img/de8a003e-b813-4e3c-9391-aff3af19b018.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Apr 2017 | $1,210 to $1,250![](img/736f9408-d05a-43f0-b2cf-95381b850962.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| May 2017 | $2,000 ![](img/e74ec91a-4184-4ab1-9baf-6381881e32d1.png) |'
  prefs: []
  type: TYPE_TB
- en: '| May to June 2017 | $2,000 to $3,200+![](img/3039f1b3-23ee-4a3f-86af-496ac578b85d.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Aug 2017 | $4,400 ![](img/090e34d8-4208-49d0-931f-2dc81ddd3f91.png) |'
  prefs: []
  type: TYPE_TB
- en: '| Sept 2017 | $5,000![](img/cc407ad7-fc7f-419c-9f30-9f77f5b14b48.png) |'
  prefs: []
  type: TYPE_TB
- en: '| 12 Sept 2017 | $2,900![](img/3b77d917-ab15-4e2a-8fc6-a48fd221c8af.png) |'
  prefs: []
  type: TYPE_TB
- en: '| 13 Oct 2017 | $5,600![](img/3afb31a2-f86b-4345-a3ef-c960f07632b7.png) |'
  prefs: []
  type: TYPE_TB
- en: '| 21 Oct 2017 | $6,180 ![](img/4646b186-f179-4514-abe8-74eef5a3aa0d.png) |'
  prefs: []
  type: TYPE_TB
- en: '| 6 Nov 2017 | $7,300 ![](img/ae5187a3-3bd3-4bcb-a9da-7f08c8ba90db.png) |'
  prefs: []
  type: TYPE_TB
- en: '| 12 Nov 2017 | $5,519 to 6,295 ![](img/b1721a3b-6f03-4de9-8551-2f2f9be53230.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 17-20 Nov 2017 | $7,600 to 8,100 ![](img/2fdfb5d0-b68b-46a9-a6bc-85cb52ed1acb.png)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 15 Dec 2017 | 17,900 ![](img/c5c30f2c-62d2-4287-acbb-18d6e63baebc.png) |'
  prefs: []
  type: TYPE_TB
- en: Exchanges maintain order books—lists of all buy and sell orders, with their
    quantities and prices—and execute when a match is found between somebody buying
    and somebody selling. In addition, exchanges also keep and provide statistics
    about the state of trading, often captured as OCHL and volume for both currencies
    of the trader pai. For this project, we will be using the BTC/USD cryptocurrency
    pair.
  prefs: []
  type: TYPE_NORMAL
- en: This data is presented as aggregated by period, from seconds to days, and even
    months. There are dedicated servers working on collecting Bitcoin data for professional
    traders and institutions. Although one cannot expect to have all orders data available
    free, some of it is accessible to the public and can be used.
  prefs: []
  type: TYPE_NORMAL
- en: State-of-the-art automated trading of Bitcoin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the world of traditional securities, such as a company's stocks, it used
    to be humans who would do the analytics, predict the prices of stocks, and trade.
    Today, the development of **machine learning ** (**ML**) and the growing availability
    of data has almost eliminated humans from high-frequency trading, as a regular
    person can't capture and process all data, and emotions affect one's decisions;
    so it's dominated by automated trading systems by investment institutions.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, the volume of Bitcoin trading is relatively low compared to traditional
    exchanges; financial institutions, being traditionally careful and risk averse,
    haven't got their hands on Bitcoin trading yet (at least, it's not well-known).
    One of the reasons is high fees and uncertainty regarding regulations of cryptocurrencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'So today, mostly individuals buy and sell Bitcoins, with all the consequences
    of irrational behavior connected to that, but some attempts to automate Bitcoin
    trading have been made. The most famous one was stated in a paper by MIT, and
    another one was by Stanford researchers, published in 2014\. Many things have
    changed, and taking into account the massive Bitcoin price increase during these
    three years, anyone who just buys and holds on would be satisfied enough with
    the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f706ebab-7205-4b72-9e9b-b46121b5d0c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Bitcoin buy and sell orders (until November 2017)'
  prefs: []
  type: TYPE_NORMAL
- en: Definitely, some traders use ML for trading, and such applications look promising.
    So far, the best possible approach that was identified from research papers is
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Use order book data, instead of derived OHLC + volume data. Therefore, for
    training and prediction, use data that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the data into a time series of a certain `size` (`size` is a parameter
    to tune).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster the time series data into `K` clusters (`K` is a parameter to tune).
    It's assumed that clusters with some natural trends would appear (sharp drop/rise
    in price and so on).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each cluster, train the regression and classifier to predict the price and
    price change, respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This approach considers the most recent time series with the size of a specific
    window and trains the model. Then it classifies the data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Takes the most recent time series with window size used for training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifies it—which of the clusters does it belong to?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uses the ML model for that cluster to predict the price or price change
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This solution dates back to 2014, but still it gives a certain level of robustness.
    By having many parameters to identify, and not having the order-book historical
    data available easily, in this project, we use a simpler approach and dataset.
  prefs: []
  type: TYPE_NORMAL
- en: High-level data pipeline of the prototype
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of this chapter is to develop a prototype of a system that will predict
    the short-term change of Bitcoin price, using historical data to train the algorithm,
    and real-time data to predict and select algorithms that perform better. In the
    scope of this project, there is no attempt to predict the actual price in dollars,
    but only whether it would increase or not. This is because Bitcoin price, to some
    extent, is not actually about price but about market expectations. This can be
    seen as patterns in a trader's behavior, which, on a higher level, is represented
    by previous price itself.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4031f5ff-0aa6-415e-89a1-8303f1ab3c2c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: High-level data pipeline of the prototype'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there is an objective price associated with Bitcoin; miners are willing
    to sell Bitcoins for profit. So the base price can be estimated by knowing all
    bills that all miners have to pay for the Bitcoins they mine, but that is outside
    the scope of this project.
  prefs: []
  type: TYPE_NORMAL
- en: From this perspective, rather than trying to predict the price in dollars, it
    might make sense to look for trends of the price rising, dropping, or staying
    the same, and act accordingly. The second goal is to build a tool for experiments
    that allows us to try different approaches to predicting prices and evaluate it
    on real-life data easily. The code has to be flexible, robust, and easily extensible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, in summary, there are three main components of the system:'
  prefs: []
  type: TYPE_NORMAL
- en: Scala script for preprocessing of historical data into the required format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scala app to train the ML model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scala web service to predict future prices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Historical and live-price data collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As stated earlier, we will utilize both historical as well live data. We will
    be using the Bitcoin historical price data from Kaggle. For the real-time data,
    Cryptocompare API will be used.
  prefs: []
  type: TYPE_NORMAL
- en: Historical data collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For training the ML algorithm, there is a `Bitcoin Historical Price Data` dataset
    available to the public on Kaggle (version 10). The dataset can be downloaded
    from [https://www.kaggle.com/mczielinski/bitcoin-historical-data/](https://www.kaggle.com/mczielinski/bitcoin-historical-data/).
    It has 1 minute OHLC data for BTC-USD pairs from several exchanges.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the beginning of the project, for most of them, data was available from
    January 1, 2012 to May 31, 2017; but for the Bitstamp exchange, it''s available
    until October 20, 2017 (as well as for Coinbase, but that dataset became available
    later):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3fcf81e6-a1ed-4dc7-b3b5-35d4c0f7ec32.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The Bitcoin historical dataset on Kaggle'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that you need to be a registered user and be logged in in order to download
    the file. The file that we are using is `bitstampUSD_1-min_data_2012-01-01_to_2017-10-20.csv`*.*
    Now, let us get the data we have. It has eight columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Timestamp**: The time elapsed in seconds since January 1, 1970\. It is 1,325,317,920
    for the first row and 1,325,317,920 for the second 1\. (Sanity check! The difference
    is 60 seconds).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open**: The price at the opening of the time interval. It is 4.39 dollars.
    Therefore it is the price of the first trade that happened after **Timestamp**
    (1,325,317,920 in the first row''s case).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Close**: The price at the closing of the time interval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High**: The highest price from all orders executed during the interval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low**: The same as **High** but it is the lowest price.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Volume_(BTC)**: The sum of all Bitcoins that were transferred during the
    time interval. So, take all transactions that happened during the selected interval
    and sum up the BTC values of each of them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Volume_(Currency)**: The sum of all dollars transferred.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weighted_Price**: This is derived from the volumes of BTC and USD. By dividing
    all dollars traded by all bitcoins, we can get the weighted average price of BTC
    during this minute. So `Weighted_Price=Volume_(Currency)/Volume_(BTC)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the most important parts of the data-science pipeline after data collection
    (which is in a sense outsourced; we use data collected by others) is data preprocessing—clearing
    a dataset and transforming it to suit our needs.
  prefs: []
  type: TYPE_NORMAL
- en: Transformation of historical data into a time series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stemming from our goal—predict the direction of price change—we might ask ourselves,
    *does having an actual price in dollars help to achieve this?* Historically, the
    price of Bitcoin was usually rising, so if we try to fit a linear regression,
    it will show further exponential growth (whether in the long run this will be
    true is yet to be seen).
  prefs: []
  type: TYPE_NORMAL
- en: Assumptions and design choices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the assumptions of this project is as follows: whether we are thinking
    about Bitcoin trading in November 2016 with a price of about $700, or trading
    in November 2017 with a price in the $6500-7000 range, patterns in how people
    trade are similar. Now, we have several other assumptions, as described in the
    following points:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Assumption one**: From what has been said previously, we can ignore the actual
    price and rather look at its change. As a measure of this, we can take the delta
    between opening and closing prices. If it is positive, it means the price grew
    during that minute; the price went down if it is negative and stayed the same
    if delta = 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following figure, we can see that Delta was -1.25 for the first minute
    observed, -12.83 for the second one, and -0.23 for the third one. Sometimes, the
    open price can differ significantly from the close price of the previous minute
    (although Delta is negative during all three of the observed minutes, for the
    third minute the shown price was actually higher than close for a second). But
    such things are not very common, and usually the open price doesn't change significantly
    compared to the close price of the previous minute.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Assumption two**: The next need to consider...  is predicting the price change
    in a **black box** environment. We do not use other sources of knowledge such
    as news, Twitter feeds, and others to predict how the market would react to them.
    This is a more advanced topic. The only data we use is price and volume. For simplicity
    of the prototype, we can focus on price only and construct time series data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time series prediction is a prediction of a parameter based on the values of
    this parameter in the past. One of the most common examples is temperature prediction.
    Although there are many supercomputers using satellite and sensor data to predict
    the weather, a simple time series analysis can lead to some valuable results.
    We predict the price at T+60 seconds, for instance, based on the price at T, T-60s,
    T-120s and so on.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Assumption three**: Not all data in the dataset is valuable. The first 600,000
    records are not informative, as price changes are rare and trading volumes are
    small. This can affect the model we are training and thus make end results worse.
    That is why the first 600,000 of rows are eliminated from the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Assumption four**: We need to `Label `our data so that we can use a supervised
    ML algorithm. This is the easiest measure, without concerns about transaction
    fees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Taking into account the goals of data preparation, Scala was chosen as an easy
    and interactive way to manipulate data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/c6304756-3dc9-45f6-af8d-71eef2a2a4c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: A glimpse of the Bitcoin historical price dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '>>>'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we load data from the file downloaded from Kaggle and
    look at what is inside. There are `3045857` rows in the dataset and `8` columns,
    described before. Then we create the `Delta` column, containing the difference
    between closing and opening prices (that is, to consider only that data where
    meaningful trading has started to occur):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code labels our data by assigning 1 to the rows the `Delta` value
    of which was positive; it assigns `0` otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This code transforms the original dataset into time series data. It takes the
    Delta values of `WINDOW_SIZE` rows (`22` in this experiment) and makes a new row
    out of them. In this way, the first row has `Delta` values from `t0` to `t21`,
    and the second one has values from `t1` to `t22`. Then we create the corresponding
    array with labels (`1` or `0`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we save `X` and `Y` into files where `612000` rows were cut off from
    the original dataset; `22` means rolling window size and 2 classes represents
    that labels are binary `0` and `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code segment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Real-time data through the Cryptocompare API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For real-time data, the Cryptocompare API is used ([https://www.cryptocompare.com/api/#](https://www.cryptocompare.com/api/)),
    more specifically HistoMinute ([https://www.cryptocompare.com/api/#-api-data-histominute-](https://www.cryptocompare.com/api/#-api-data-histominute-)),
    which gives us access to OHLC data for the past seven days at most. The details
    of the API will be discussed in a section devoted to implementation, but the API
    response is very similar to our historical dataset, and this data is retrieved
    using a regular HTTP request. For example, a simple JSON response from [https://min-api.cryptocompare.com/data/histominute?fsym=BTC&tsym=USD&limit=23&aggregate=1&e=Bitstamp](https://min-api.cryptocompare.com/data/histominute?fsym=BTC&tsym=USD&limit=23&aggregate=1&e=Bitstamp) has
    the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Through Cryptocompare HistoMinute, we can get `open`, `high`, `low`, `close`,
    `volumefrom`, and `volumeto` from each minute of historical data. This data is
    stored for 7 days only; if you need more, use the hourly or daily path. It uses
    BTC conversion if data is not available because the coin is not being traded in
    the specified currency:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15833859-b3f2-4974-9f9d-3ae6f49d9630.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Open, high, low, close, and volume values through Cryptocompare HistoMinute'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the following method fetches the correctly formed URL of the Cryptocompare
    API ([https://www.cryptocompare.com/api/#-api-data-histominute-](https://www.cryptocompare.com/api/#-api-data-histominute-)),
    which is a fully formed URL with all parameters, such as currency, limit, and
    aggregation specified. It finally returns the future that will have a response
    body parsed into the data model, with the price list to be processed at an upper
    level:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code segment, the `CryptoCompareResponse` class is the model
    of API, which takes the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Response`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Type`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Aggregated`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Data`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FirstValueInArray`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TimeTo`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TimeFrom`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, it has the following signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, the preceding two code segments the **open-high-low-close** (also known
    as **OHLC**), are a model class for mapping with CryptoAPI response `data` array
    internals. It takes these parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Time`: Timestamp in seconds, `1508818680`, for instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Open`: Open price at a given minute interval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`High`: Highest price.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Low`: Lowest price.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Close`: Price at the closing of the interval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Volumefrom`: Trading volume in the `from` currency. It''s BTC in our case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Volumeto`: The trading volume in the `to` currency, USD in our case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dividing `Volumeto` by `Volumefrom` gives us the weighted price of BTC.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, it has the following signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Model training for prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Inside the project, in the package folder `prediction.training`, there is a
    Scala object called `TrainGBT.scala`. Before launching, you have to specify/change
    four things:'
  prefs: []
  type: TYPE_NORMAL
- en: In the code, you need to set up `spark.sql.warehouse.dir` in some actual place
    on your computer that has several gigabytes of free space: `set("spark.sql.warehouse.dir",
    "/home/user/spark")`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `RootDir` is the main folder, where all files and train models will be `stored:rootDir
    = "/home/user/projects/btc-prediction/"`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure that the `x` filename matches the one produced by the Scala script
    in the preceding step: `x = spark.read.format("com.databricks.spark.csv ").schema(xSchema).load(rootDir
    + "scala_test_x.csv")`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure that the `y` filename matches the one produced by Scala script: `y_tmp=spark.read.format("com.databricks.spark.csv").schema(ySchema).load(rootDir
    + "scala_test_y.csv")`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code for training uses the Apache Spark ML library (and libraries required
    for it) to train the classifier, which means they have to be present in your `class`
    path to be able to run it. The easiest way to do that (since the whole project
    uses SBT) is to run it from the project root folder by typing `sbt``run-main prediction.training.TrainGBT`,
    which will resolve all dependencies and launch training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the number of iterations and depth, it can take several hours
    to train the model. Now let us see how training is performed on the example of
    the gradient-boosted trees model. First, we need to create a `SparkSession` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define a schema of data for `x` and `y`. We rename the columns to
    `t0`-`t21,` to indicate that it''s a time series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we read the files we defined for the schema. It was more convenient to
    generate two separate files in Scala for data and labels, so here we have to join
    them into a single DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is required by Spark—we need to vectorize the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We split the data into train and test sets randomly in the proportion of 75%
    to 25%. We set the seed so that the splits would be equal among all times we run
    the training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We then define the model. It tells which columns are features and which are
    labels. It also sets parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `pipeline` of steps—vector assembling of features and running GBT:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Defining evaluator function—how the model knows whether it is doing well or
    not. As we have only two classes that are imbalanced, accuracy is a bad measurement;
    area under the ROC curve is better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'K-fold cross-validation is used to avoid overfitting; it takes out one-fifth
    of the data at each iteration, trains the model on the rest, and then tests on
    this one-fifth:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'After we get the trained model (which can take an hour or more depending on
    the number of iterations and parameters we want to iterate on, specified in `paramGrid`),
    we then compute the predictions on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, evaluate quality of predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The trained model is saved for later usage by the prediction service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In summary, the code for model training is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let us see how the training went:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, we have not received very high accuracy, as the ROC is only 60.50%
    out of the best GBT model. Nevertheless, if we tune the hyperparameters, we will
    get better accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: However, as I did not have enough time, I did not iterate the training for long,
    but you should definitely try.
  prefs: []
  type: TYPE_NORMAL
- en: Scala Play web service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As an application framework, Play2 was chosen as an easy-to-configure and robust
    framework. Compared to Spring (another popular framework), it takes less time
    to make a small app from scratch. The Play comes with Guice for dependency injection
    and SBT as the package manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spark ML**: The Spark ML library was chosen as it is one of the best-maintained
    libraries in the Java world. Many algorithms not available in the library itself
    are implemented by third-party developers and can be trained on top of Spark.
    A drawback of Spark is that it is quite slow, as by design it is supposed to be
    distributed; so it uses Hadoop and writes a lot into the filesystem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Akka**: This allows implementing the actor''s pattern—having several instances
    of independent objects and passing messages to each other concurrently, which
    increases robustness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anorm**: The library to work with SQL on top of JDBC. Slick is another option
    and it is more powerful, but compatibility issues between libraries required for
    Akka and Slick made it worth choosing another library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**H2**: A database that is the default for Play and Ruby-on-Rails as an easy-to-start
    database, with the possibility to store data in a local database file without
    the need to install a DB server. This gives portability and increases the speed
    of development. In later stages, it can be replaced with another, as Scala code
    isn''t tied to any particular database; all of it is done on the configuration
    level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrency through Akka actors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Concurrency is achieved through utilization of the `actor` model using the
    Akka Scala library. Actors act as independent entities and can pass async messages
    to other actors. In this project, there are three actors: `SchedulerActor`, `PredictionActor`,
    and `TraderActor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SchedulerActor`: Requests price data, stores them into DB, sends a message
    with prices to `PredictionActor`, receives an answer, and passes it to `TraderActor`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PredictionActor`: After receiving a message with prices, it predicts the next
    price using the best model available (this has to be chosen in `application.conf`;
    we will see the details later on). It passes a message with the prediction back
    to `SchedulerActor`, uses the rest of the modes from the `model` folder to make
    predictions on previous data, and uses the latest price to evaluate predictions.
    The results of such predictions are stored in the DB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TraderActor`: After receiving a message about prediction, using `rules` (which
    at this moment are as simple as *buy if the price is predicted to grow **and do
    nothing otherwise*), this writes its decision into logs. It can send an HTTP request
    to a URL to trigger this decision.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web service workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's take a deeper look into how code works to perform predictions. As
    shown earlier, every 60 seconds, the app is triggered to fetch data from Cryptocompare,
    store prices into the database, and run predictions, saving backtrack test results
    about quality prediction.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll look deeper into which Scala classes play an important
    role in this project and how they communicate.
  prefs: []
  type: TYPE_NORMAL
- en: JobModule
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When the application is launched, everything starts with `JobModule`. It configures
    the creation of `Scheduler`, which sends messages to `SchedulerActor` as given
    in the `application.conf` rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'To enable this module, inside `application.conf`, the following line is required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Scheduler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Scheduler` takes the frequency constant from the `application.conf` and uses
    the `Actor` system to send an `update` message (the content does not matter; `SchedulerActor`
    reacts to any message) to `SchedulerActor` every X seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: SchedulerActor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The relevant parts of the code are displayed and explained. Now let us see
    how to obtain price data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '`ConstructUrl` returns a completely formed URL for the request to the Cryptocompare
    API. More details are given in section related to the API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Creates instances of `PredictionActor` and `TraderActor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Receive` method is defined in the `actor` trait and has to be implemented.
    It is triggered when someone passes a message to this `actor` (`Scheduler` in
    our case):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, `case _ =>` means that we react to any message of any
    type and content. The first thing that is done is an async call to the Cryptocompare
    API by the URL specified before. This is done with the help of `RestClient`, which
    returns `Future` with the response JSON. After receiving the response (inside
    `futureResponse` on complete callback), `.json` is mapped into the custom case
    class `CryptoCompareResponse`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The case class is similar to **POJO** (**Plain Old Java Object**) without the
    need to write constructors and getters/setters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This companion object is required for mapping JSON into this class. The `CryptocompareResponse`
    object stores the output of the API—a list of OHLC data, time range of data and
    others which that are not relevant to us. The `OHLC` class corresponds to actual
    price data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'After the data is ready, prices are stored in the DB by calling `storePriceData(cryptoCompareResponse)`.
    At first, it does a batch insert (using Anorm''s **BatchSQL**) into the `PRICE_STAGING`
    table and re-inserts into `PRICE` with deduplication with respect to timestamp,
    as we are receiving overlapping price data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'After storing into the DB, `SchedulerActor` transforms OHLC data into (timestamp,
    delta) tuples, where delta is (`closePrice`-`openPrice`). So the format is suitable
    for the ML model. The transformed data is passed as a message to `PredictionActor`
    with explicit waiting for a response. This is done by using the `?` operator.
    We ask the prediction `actor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Its response is mapped to the `CurrentDataWithShortTermPrediction` class and
    passed to `TraderActor` using the `!` operator. Unlike `?`, the `!` operator does
    not require a response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This was basic a walkthrough of `SchedulerActor`. We read data from the Cryptocompare
    API, store it into the database, send to `PredictionActor` and wait for its response.
    Then we forward its response to `TraderActor`.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's see what happens inside `PredictionActor`.
  prefs: []
  type: TYPE_NORMAL
- en: PredictionActor and the prediction step
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Scala web application, which takes the most recent Bitcoin price data on
    the Bitstamp exchange every minute from the Cryptocompare API, uses a trained
    ML classifier to predict the direction of price change for the next minute. It
    notifies the user about the decision.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to launch it, from a directory with project type `sbt run` (or `$ sudo
    sbt run` when required). Now let us see the contents of the `application.conf`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can understand that there are also several variables to configure/change
    based on your platform and choice:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Change the `rootDir` directory to the one you have used in `TrainGBT`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify the name for the database file:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify the version of the model that is used for the actual prediction:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Note that the folder with such a name has to be inside `rootDir`. So inside
    `rootDir`, create a folder named `models` and copy all the folders of trained
    models there.
  prefs: []
  type: TYPE_NORMAL
- en: 'This class also implements the `actor` trait and overrides the receive method.
    The best practice for it is to define types that can be received by the `actor`
    inside the companion object, thus establishing an interface for other classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'At first, `PredictionActor` loads a list of models from the `models` folder
    and loads the `etalon` model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we extract a list of subdirectories inside the `models` folder, and
    from each of them, we load the trained `PipeLine` model. In a similar way, the `etalon` model
    is loaded, but we already know its directory. Here''s how a message of the `PriceData`
    type is handled inside the `receive` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The predicted label (string) and classification details are logged, so is it
    possible to see the probability distribution for each class? If the `actor` receives
    a message of another type, an error is shown and nothing more is done. Then the
    results are sent back to `SchedulerActor` and sent in the variable `predictedWithCurrent`,
    as was shown in the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The `sender` is an `ActorRef` reference to an object that has sent the message
    we are processing at the moment, so we can pass the message back with the `!` operator.
    Then, for each model we have loaded in the beginning, we predict the label for
    1-minute-old data (rows 0-21 out of 23 in total) and get the actual price delta
    for the latest minute we know:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'For each model, we store the following in the DB name of the model: the timestamp
    for each test prediction made, the label that was predicted by the model, and
    the actual delta. This information is used later to generate reports on the model''s
    performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: TraderActor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`TraderActor` receives the prediction and, based on the label, writes a log
    message. It can trigger an HTTP request to the specified endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Predicting prices and evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`ShortTermPredictionServiceImpl` is the class that actually performs the prediction
    with the given model and data. At first, it transforms `PriceData` into a Spark
    DataFrame with the scheme corresponding to the one used for training by calling
    `transformPriceData(priceData: PriceData)`*.* Then, the `model.transform(dataframe)`
    method is called; we extract the variables we need, write into the debugger log
    and return to the caller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'While running, the application collects data about the prediction output: predicted
    label and actual price delta. This information is used to build the root web page,
    displaying statistics such as **TPR** (**true positive rate**), **FPR** (**false
    positive rate**), **TNR** (**true negative rate**), and **FNR **(**false negative
    rate**), which were described earlier.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These statistics are counted on the fly from the `SHORT_TERM_PREDICTION_BINARY`
    table. Basically, by using the `CASE-WHEN` construction, we add new columns: TPR,
    FPR, TNR, and FNR. They are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: TPR with value `1` if the predicted label was 1 and price delta was > `0`, and
    value `0` otherwise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FPR with value `1` if the predicted label was 1 and price delta was <= `0`,
    and value `0` otherwise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TNR with value `1` if the predicted label was 0 and price delta was <= `0`,
    and value `0` otherwise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FNR with value `1` if the predicted label was 0 and price delta was > `0`, and
    value 0 otherwise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, all records are grouped by model name, and TPR, FPR, TNR, and FNR are
    summed up, giving us the total numbers for each model. Here is the SQL code responsible
    for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Demo prediction using Scala Play framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have seen all the steps for this project, it's time to see a live
    demo. We will wrap up the whole application as a Scala Play web app. Well, before
    seeing the demo, let's get our project up and running. However knowing some basic
    of RESTful architecture using Scala Play would be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: Why RESTful architecture?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well, Play’s architecture is RESTful by default. At its core, Play is based
    on the Model-View-Controller pattern. Each entry point, paired with an HTTP verb,
    maps to a Controller function. The controller enables views to be web pages, JSON,
    XML, or just about anything else.
  prefs: []
  type: TYPE_NORMAL
- en: Play’s stateless architecture enables horizontal scaling, ideal for serving
    many incoming requests without having to share resources (such as a session) between
    them. It is at the forefront of the Reactive programming trend, in which servers
    are event-based and parallel processing is used to cater to the ever-increasing
    demands of modern websites.
  prefs: []
  type: TYPE_NORMAL
- en: In certain configurations, Play enables fully asynchronous and non-blocking
    I/O throughout the entire application. The purpose is to reach new heights in
    terms of scalability on the web through efficient thread management and parallel
    processing, while avoiding the **callback hell** that JavaScript-based solutions
    tend to engender.
  prefs: []
  type: TYPE_NORMAL
- en: AngularJs is a JavaScript-based open-source front-end web application framework
    mainly maintained by Google and by a community of individuals and corporations
    to address many of the challenges encountered in developing single-page applications.
  prefs: []
  type: TYPE_NORMAL
- en: Now question would be why AngularJS? Well, HTML is great for declaring static
    documents, but it falters when we try to use it for declaring dynamic views in
    web-applications. AngularJS lets you extend HTML vocabulary for your application.
    The resulting environment is extraordinarily expressive, readable, and quick to
    develop.
  prefs: []
  type: TYPE_NORMAL
- en: Another question would be, are not there any alternatives? Well**,** other frameworks
    deal with HTML’s shortcomings by either abstracting away HTML, CSS, and/or JavaScript
    or by providing an imperative way for manipulating the DOM. Neither of these address
    the root problem that HTML was not designed for dynamic views.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, what is about the extensibility? Well, AngularJS is a toolset for building
    the framework most suited to your application development. It is fully extensible
    and works well with other libraries. Every feature can be modified or replaced
    to suit your unique development workflow and feature needs. Read on to find out
    how.
  prefs: []
  type: TYPE_NORMAL
- en: Project structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The wrapped up Scala web ML app has the following directory structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7fa0cf7c-e77e-4a59-aec0-98d57b3b2c98.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Scala ML web app directory structure'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding structure, `bitcoin_ml` folder has all the backend and frontend
    codes. The `models` folder has all the trained models. An example-trained model
    is given in the `gbt_22_binary_classes_32660767` folder. Finally, database files
    and traces are there in the `DataBase.mv.db` and `DataBase.trace.db` files respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then let us see the sub-folder structure of the `bitcoin_ml` folder that contains
    the actual codes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/90451c79-4115-4029-8ed0-e73b57b18799.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8: The bitcoin_ml directory structure
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, the `conf` folder has the Scala web app configuration
    file, `application.conf` containing necessary configurations (as shown already).
    All the dependencies are defined in the `build.sbt` fileshown as follows*:*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: To be frank, at the beginning of writing, I did not think of wrapping up this
    application as a Scala Play web app. Therefore, things went a bit unstructured.
    However, do not worry to know more about backend as well frontend, refer to the
    options trading application in [Chapter 7](c4a322da-d64b-4c40-a5b8-7ffec8381b41.xhtml),
    *Options Trading Using Q-Learning and Scala Play Framework*.
  prefs: []
  type: TYPE_NORMAL
- en: Running the Scala Play web app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run the application, just follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the historical Bitcoin data from [https://www.kaggle.com/mczielinski/bitcoin-historical-data](https://www.kaggle.com/mczielinski/bitcoin-historical-data).
    Then unzip and extract the `.csv` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open your preferred IDE (for example, Eclipse/IntelliJ) and create the Maven
    or SBT project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the `Preprocess.scala` script to convert the historical data into a time
    series. This script should generate two `.csv` files (that is, `scala_test_x.csv`
    and `scala_test_y.csv`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then train the `GradientBoostedTree` model (use `TrainGBT.scala` script) using
    the previously generated files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the best (i.e. cross-validated) `Pipeline` model containing all the pipelines'
    steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then download the Scala Play app and all the files (that is, `Bitcoin_price_prediction`)
    from the Packt repository or GitHub (see in the book).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then copy the trained model to `Bitcoin_price_prediction/models/`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then:  `$ cd Bitcoin_price_prediction/bitcoin_ml/conf/` and update the parameter
    values in the `application.conf` as shown earlier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, run the project using the `$ sudo sbt run` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After launching with `$ sudo sbt run`, the application will read all models
    from the `models` folder, the `etalon` model being specified by `ml.model_version`.
    Every 30 seconds (specified in `constants.frequency = 30` in `application.conf`),
    the latest price data is retrieved from the Cryptocompare API. A prediction using
    the `etalon` model is made and the results are shown to the user in the form of
    a log message in the console, with the possibility to trigger an HTTP request
    to the specified endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, all models from the `models` folder are used to make a prediction
    on the previous 22-minute data and use the latest price data for a current minute
    as a way to check the quality of predictions. All predictions made by each model
    are stored in a database file. When a user visits `http://localhost:9000`, a table
    with a summary of predictions is shown to the user:'
  prefs: []
  type: TYPE_NORMAL
- en: Model name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TPR, (not rate actually, in this case, just raw count) - how many times model
    predicted that price would increase and how many times that was true
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FPR,  how many times model has predicted price increase, but price dropped or
    stayed the same
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TNR, how many times model predicted non-increase of price and was correct
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FNR, how many times model predicted non-increase of price and was wrong
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total count of predictions made by the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alright, here we go, after launching the app using `$` sudo sbt `run` (on a
    terminal):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a475c8d5-3fd1-4ab5-b796-7350d540ee9f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Sample signals generated by the model based on historical prices
    and live data'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding figure shows some sample signals generated by our model based
    on historical prices and live data. Additionally, we can see the raw prediction
    by the model. When you try to access the app from your browser at `http://localhost:9000`,
    you should see this (the count will increase with time, though):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5876680a-d23f-4462-b4e1-4cda118482a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Model performance using the Scala Play2 framework'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, the performance is not satisfactory, but I would suggest
    that you train the model with the most suitable hyperparameters and for more iterations,
    for example, 10,000 times. Additionally, in the next section, I tried to provide
    some more insights and improvement guidelines.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if you plan to deploy this application after making some extension
    (if any), then I would suggest to take a quick look at the last section in [Chapter
    7](c4a322da-d64b-4c40-a5b8-7ffec8381b41.xhtml), *Options Trading Using Q-Learning
    and Scala Play Framework*, where you will find deployment guideline on server
    to be exposed as web app.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, a complete ML pipeline was implemented, from collecting historical
    data, to transforming it into a format suitable for testing hypotheses, training
    ML models, and running a prediction on `Live` data, and with the possibility to
    evaluate many different models and select the best one.
  prefs: []
  type: TYPE_NORMAL
- en: The test results showed that, as in the original dataset, about 600,000 minutes
    out of 2.4 million can be classified as **increasing price** (close price was
    higher than open price); the dataset can be considered imbalanced. Although random
    forests are usually performed well on an imbalanced dataset, the area under the
    ROC curve of 0.74 isn't best. As we need to have fewer false positives (fewer
    times when we trigger **purchase** and the price drops), we might consider a punishing
    model for such errors in a stricter way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the results achieved by classifiers can''t be used for profitable
    trading, there is a foundation on top of which new approaches can be tested in
    a relatively rapid way. Here, some possible directions for further development
    are listed:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementation of the pipeline discussed in the beginning: Convert your time
    series data into several clusters and train the regression model/classifier for
    each of them; then classify recent data into one of the clusters and use the prediction
    model trained for that cluster. As by definition, ML is **deriving patterns from
    data**, there might not be only one pattern that fits all of the history of Bitcoin;
    that''s why we need to understand that a market can be in different phases, and
    each phase has its own pattern.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the major challenges with Bitcoin price prediction might be that the
    training data (historical) doesn't belong to the same distribution as test data
    during random splits into train-test sets. As the patterns in price changed during
    2013 and 2016, they might belong to completely different distributions. It might
    require a manual inspection of data and some infographics. Probably, someone has
    already done this research.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One of the main things to try would be to train two **one-versus-all** classifiers:
    one is trained to predict when the price grows higher than 20$, for example. Another
    predicts when the price drop by 20$; so it makes sense to take long/short positions,
    respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maybe, predicting the delta of the next minute isn't what we need; we'd rather
    predict the average price. As the Open price can be much higher than last minute's
    Close price, and the Close price of the next minute can be slightly less than
    open but still higher than current, it would make it profitable trade. So how
    to exactly label data is also an open question.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try with different time-series window size (even 50 minutes might suit) using
    ARIMA time series prediction model, as it is one of the most widely used algorithms.
    Then try to predict price change, not for the next minute but for 2-3 following
    minutes. Additionally, try by incorporating trading volume as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Label the data as **price increased** if the price was higher by 20$ during
    at least one of three following minutes so that we can make a profit from trade.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Currently, `Scheduler` isn't synchronized with Cryptocompare minutes. This means
    we can get data about the minute interval 12:00:00 - 12:00:59 at any point of
    the following minute - 12:01:00 or 12:01:59\. In the latter case, it doesn't make
    sense to make trade, as we made a prediction based on already **old **data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of making a prediction every minute on **older** data to accumulate
    prediction results for `actor`, it's better to take maximum available HistoMinute
    data (seven days), split it into time series data using a Scala script that was
    used for historical data, and predict for seven days' worth of data. Run this
    as a scheduled job once a day; it should reduce the load on the DB and `PredictionActor`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Compared to usual datasets, where the order of rows doesn''t matter much, in
    Bitcoin, historical data rows are sorted by ascending order of date, which means
    that:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latest data might be more relevant to today's price, and less can be more; taking
    a smaller subset of data might give better performance
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The ways of subsampling data can matter (splitting into train-test sets)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally try with LSTM network for even better predictive accuracy (see chapter
    10 for some clue)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The understanding of variations in genome sequences assists us in identifying
    people who are predisposed to common diseases, solving rare diseases, and finding
    the corresponding population group of individuals from a larger population group.
    Although classical ML techniques allow researchers to identify groups (clusters)
    of related variables, the accuracy and effectiveness of these methods diminish
    for large and high-dimensional datasets such as the whole human genome. On the
    other hand, deep neural network architectures (the core of deep learning) can
    better exploit large-scale datasets to build complex models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how to apply the K-means algorithm on large-scale
    genomic data from the 1,000 Genomes Project aiming at clustering genotypic variants
    at the population scale. Then we'll train an H2O-based deep learning model for
    predicting geographic ethnicity. Finally, Spark-based Random Forest will be used
    to enhance the predictive accuracy.
  prefs: []
  type: TYPE_NORMAL
