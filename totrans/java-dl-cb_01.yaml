- en: Introduction to Deep Learning in Java
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's discuss various deep learning libraries so as to pick the best for the
    purpose at hand. This is a context-dependent decision and will vary according
    to the situation. In this chapter, we will start with a brief introduction to
    deep learning and explore how DL4J is a good choice for solving deep learning
    puzzles. We will also discuss how to set up DL4J in your workspace.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning intuition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining the right network type to solve deep learning problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining the right activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combating overfitting problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining the right batch size and learning rates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring Maven for DL4J
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring DL4J for a GPU-accelerated environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Troubleshooting installation issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You''ll need the following to get the most out of this cookbook:'
  prefs: []
  type: TYPE_NORMAL
- en: Java SE 7, or higher, installed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic core Java knowledge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DL4J basics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maven basics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic data analytical skills
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning/machine learning basics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OS command basics (Linux/Windows)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IntelliJ IDEA IDE (this is a very easy and hassle-free way of managing code;
    however, you're free to try another IDE, such as Eclipse)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spring Boot basics (to integrate DL4J with Spring Boot for use with web applications)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use DL4J version 1.0.0-beta3 throughout this book except for [Chapter 7](ade34354-ad22-4d3e-b45e-ce112947df49.xhtml),
    *Constructing an LSTM Neural Network for Sequence Classification*, where we used
    the current latest version, 1.0.0-beta4, to avoid bugs.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning intuition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you''re a newbie to deep learning, you may be wondering how exactly it is differs
    from machine learning; or is it the same? Deep learning is a subset of the larger
    domain of machine learning. Let''s think about this in the context of an automobile image classification
    problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aff78d29-b0a5-4b31-984b-c329227ed7da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see in the preceding diagram, we need to perform feature extraction
    ourselves as legacy machine learning algorithms cannot do that on their own. They
    might be super-efficient with accurate results, but they cannot learn signals
    from data. In fact, they don''t learn on their own and still rely on human effort:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78a06015-1bc7-42a6-bf4c-df580e370f0a.png)'
  prefs: []
  type: TYPE_IMG
- en: On the other hand, deep learning algorithms learn to perform tasks on their
    own. Neural networks under the hood are based on the concept of deep learning
    and it trains on their own to optimize the results. However, the final decision
    process is hidden and cannot be tracked. The intent of deep learning is to imitate
    the functioning of a human brain.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The backbone of a neural network is the backpropagation algorithm. Refer to
    the sample neural network structure shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc9dffda-d154-47bc-852c-25b7fca57cb9.png)'
  prefs: []
  type: TYPE_IMG
- en: For any neural network, data flows from the input layer to the output layer
    during the forward pass. Each circle in the diagram represents a neuron. Every
    layer has a number of neurons present. Our data will pass through the neurons
    across layers. The input needs to be in a numerical format to support computational
    operations in neurons. Each neuron in the neural network is assigned a weight
    (matrix) and an activation function. Using the input data, weight matrix, and
    an activation function, a probabilistic value is generated at each neuron. The
    error (that is, a deviation from the actual value) is calculated at the output
    layer using a loss function. We utilize the loss score during the backward pass
    (that is, from the output layer to the input layer ) by reassigning weights to
    the neurons to reduce the loss score. During this stage, some output layer neurons
    will be assigned with high weights and vice versa depending upon the loss score
    results. This process will continue backward as far as the input layer by updating
    the weights of neurons. In a nutshell, we are tracking the rate of change of loss
    with respect to the change in weights across all neurons. This entire cycle (a
    forward and backward pass) is called an epoch. We perform multiple epochs during
    a training session. A neural network will tend to optimize the results after every
    training epoch.
  prefs: []
  type: TYPE_NORMAL
- en: Multilayer Perceptron (MLP)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An MLP is a standard feed-forward neural network with at least three layers:
    an input layer, a hidden layer, and an output layer. Hidden layers come after
    the input layer in the structure. Deep neural networks have two or more hidden
    layers in the structure, while an MLP has only one.'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Network (CNN)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNNs are generally used for image classification problems, but can also be exposed
    in **Natural Language Processing** (**NLP**), in conjunction with word vectors,
    because of their proven results. Unlike a regular neural network, a CNN will have
    additional layers such as convolutional layers and subsampling layers. Convolutional
    layers take input data (such as images) and apply convolution operations on top
    of them. You can think of it as applying a function to the input. Convolutional
    layers act as filters that pass a feature of interest to the upcoming subsampling
    layer. A feature of interest can be anything (for example, a fur, shade and so
    on in the case of an image) that can be used to identify the image. In the subsampling
    layer, the input from convolutional layers is further smoothed. So, we end up
    with a much smaller image resolution and reduced color contrast, preserving only
    the important information. The input is then passed on to fully connected layers.
    Fully connected layers resemble regular feed-forward neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Network (RNN)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An RNN is a neural network that can process sequential data. In a regular feed-forward
    neural network, the current input is considered for neurons in the next layer.
    On the other hand, an RNN can accept previously received inputs as well. It can
    also use memory to memorize previous inputs. So, it is capable of preserving long-term
    dependencies throughout the training session. RNN is a popular choice for NLP
    tasks such as speech recognition. In practice, a slightly variant structure called
    **Long Short-Term Memory** (**LSTM**) is used as a better alternative to RNN.
  prefs: []
  type: TYPE_NORMAL
- en: Why is DL4J important for deep learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following points will help you understand why DL4J is important for deep
    learning:'
  prefs: []
  type: TYPE_NORMAL
- en: DL4J provides commercial support. It is the first commercial-grade, open source,
    deep learning library in Java.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing training code is simple and precise. DL4J supports Plug and Play mode,
    which means switching between hardware (CPU to GPU) is just a matter of changing
    the Maven dependencies and no modifications are needed on the code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DL4J uses ND4J as its backend. ND4J is a computation library that can run twice
    as fast as NumPy (a computation library in Python) in large matrix operations.
    DL4J exhibits faster training times in GPU environments compared to other Python
    counterparts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DL4J supports training on a cluster of machines that are running in CPU/GPU
    using Apache Spark. DL4J brings in automated parallelism in distributed training.
    This means that DL4J bypasses the need for extra libraries by setting up worker
    nodes and connections.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DL4J is a good production-oriented deep learning library. As a JVM-based library,
    DL4J applications can be easily integrated/deployed with existing corporate applications
    that are running in Java/Scala.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining the right network type to solve deep learning problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is crucial to identify the right neural network type to solve a business
    problem efficiently. A standard neural network can be a best fit for most use
    cases and can produce approximate results. However, in some scenarios, the core
    neural network architecture needs to be changed in order to accommodate the features
    (input) and to produce the desired results. In the following recipe, we will walk
    through key steps to decide the best network architecture for a deep learning
    problem with the help of known use cases.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Determine the problem type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine the type of data engaged in the system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To solve use cases effectively, we need to use the right neural network architecture
    by determining the problem type. The following are globally some use cases and
    respective problem types to consider for step 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fraud detection problems**:We want to differentiate between legitimate and
    suspicious transactions so as to separate unusual activities from the entire activity
    list. The intent is to reduce false-positive (that is, incorrectly tagging legitimate
    transactions as fraud) cases. Hence, this is an anomaly detection problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction problems**:Prediction problems can be classification or regression
    problems. For labeled classified data, we can have discrete labels. We need to
    model data against those discrete labels. On the other hand, regression models
    don''t have discrete labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recommendation problems**:You would need to build a recommender system (a
    recommendation engine) to recommend products or content to customers. Recommendation
    engines can also be applied to an agent performing tasks such as gaming, autonomous
    driving, robotic movements, and more. Recommendation engines implement reinforcement learning
    and can be enhanced further by introducing deep learning into it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We also need to know the type of data that is consumed by the neural network.
    Here are some use cases and respective data types for step 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fraud detection problems**:Transactions usually happen over a number of time
    steps. So, we need to continuously collect transaction data over time. This is
    an example of time series data. Each time sequence represents a new transaction
    sequence. These time sequences can be regular or irregular. For instance, if you
    have credit card transaction data to analyze, then you have labeled data. You
    can also have unlabeled data in the case of user metadata from production logs. We
    can have supervised/unsupervised datasets for fraud detection analysis, for example.
    Take a look at the following CSV supervised dataset:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/d882cc72-71ce-454a-9289-1cb7b3261bbf.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding screenshot, features such as `amount`, `oldBalanceOrg`, and
    so on make sense and each record has a label indicating whether the particular
    observation is fraudulent or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, an unsupervised dataset will not give you any clue about
    input features. It doesn''t have any labels either, as shown in the following
    CSV data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be8a3dd1-e5e9-4241-ab1c-f036d08f883a.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the feature labels (top row) follow a numbered naming convention
    without any clue as to its significance for fraud detection outcomes. We can also
    have time series data where transactions are logged over a series of time steps.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prediction problems**:Historical data collected from organizations can be
    used to train neural networks. These are usually simple file types such as a CSV/text
    files. Data can be obtained as records. For a stock market prediction problem,
    the data type would be a time series. A dog breed prediction problem requires
    feeding in dog images for network training. Stock price prediction is an example
    of a regression problem. Stock price datasets usually are time series data where
    stock prices are measured over a series as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/0061cfc1-2139-4c7a-bbd5-612b220c2b28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In most stock price datasets, there are multiple files. Each one of them represents
    a company stock market. And each file will have stock prices recorded over a series
    of time steps, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f553df10-3d7d-4339-9a60-14d4afe47f67.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Recommendation problems**:For a product recommendation system, explicit data
    might be customer reviews posted on a website and implicit data might be the customer
    activity history, such as product search or purchase history. We will use unlabeled data
    to feed the neural network. Recommender systems can also solve games or learn
    a job that requires skills. Agents (trained to perform tasks during reinforcement learning)
    can take real-time data in the form of image frames or any text data (unsupervised)
    to learn what actions to make depending on their states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are possible deep learning solutions to the problem types previously
    discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fraud detection problems**: The optimal solution varies according to the
    data. We previously mentioned two data sources. One was credit card transactions
    and the other was user metadata based on their login/logoff activities. In the
    first case, we have labeled data and have a transaction sequence to analyze.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recurrent networks may be best suited to sequencing data. You can add LSTM ([https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/layers/recurrent/LSTM.html](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/layers/recurrent/LSTM.html)) recurrent
    layers, and DL4J has an implementation for that. For the second case, we have
    unlabeled data and the best choice would be a variational ([https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/layers/variational/VariationalAutoencoder.html](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/layers/variational/VariationalAutoencoder.html)) autoencoder
    to compress unlabeled data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prediction problems**: For classification problems that use CSV records,
    a feed-forward neural network will do. For time series data, the best choice would
    be recurrent networks because of the nature of sequential data. For image classification
    problems, you would need a CNN ([https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/ConvolutionLayer.Builder.html)](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/ConvolutionLayer.Builder.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recommendation problems**: We can employ **Reinforcement Learning** (**RL**)
    to solve recommendation problems. RL is very often used for such use cases and
    might be a better option. RL4J was specifically developed for this purpose. We
    will introduce RL4J in [Chapter 9](9d49be5b-7d29-47f8-848d-b1c5c1b742e9.xhtml),
    *Using RL4J for Reinforcement Learning*, as it would be an advanced topic at this
    point. We can also go for simpler options such as feed-forward networks RNNs)
    with a different approach. We can feed an unlabeled data sequence to recurrent
    or convolutional layers as per the data type (image/text/video). Once the recommended
    content/product is classified, you can apply further logic to pull random products
    from the list based on customer preferences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to choose the right network type, you need to understand the type of
    data and the problem it tries to solve. The most basic neural network that you
    could construct is a feed-forward network or a multilayer perceptron. You can
    create multilayer network architectures using `NeuralNetConfiguration` in DL4J.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following sample neural network configuration in DL4J:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We specify activation functions for every layer in a neural network, and `nIn()`
    and `nOut()` represent the number of connections in/out of the layer of neurons.
    The purpose of the `dropOut()` function is to deal with network performance optimization.
    We mentioned it in [Chapter 3](5cf01186-c9e3-46e7-9190-10cd43933694.xhtml), *Building
    Deep Neural Networks for Binary Classification*. Essentially, we are ignoring
    some neurons at random to avoid blindly memorizing patterns during training. Activation
    functions will be discussed in the *Determining the right activation function *recipe
    in this chapter. Other attributes control how weights are distributed between
    neurons and how to deal with errors calculated across each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s focus on a specific decision-making process: choosing the right network
    type. Sometimes, it is better to use a custom architecture to yield better results.
    For example, you can perform sentence classification using word vectors combined
    with a CNN. DL4J offers the `ComputationGraph` ([https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/graph/ComputationGraph.html](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/graph/ComputationGraph.html))
    implementation to accommodate CNN architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: '`ComputationGraph` allows an arbitrary (custom) neural network architecture.
    Here is how it is defined in DL4J:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Implementing a CNN is just like constructing network layers for a feed-forward
    network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: A CNN has `ConvolutionalLayer` and `SubsamplingLayer` apart from `DenseLayer`
    and `OutputLayer`.
  prefs: []
  type: TYPE_NORMAL
- en: Determining the right activation function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The purpose of an activation function is to introduce non-linearity into a neural
    network. Non-linearity helps a neural network to learn more complex patterns.
    We will discuss some important activation functions, and their respective DL4J
    implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the activation functions that we will consider:'
  prefs: []
  type: TYPE_NORMAL
- en: Tanh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sigmoid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReLU (short for **Rectified Linear Unit**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leaky ReLU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Softmax
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this recipe, we will walk through the key steps to decide the right activation
    functions for a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Choose an activation function according to the network layers**: We need
    to know the activation functions to be used for the input/hidden layers and output
    layers. Use ReLU for input/hidden layers preferably.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Choose the right activation function to handle data impurities**: Inspect
    the data that you feed to the neural network. Do you have inputs with a majority
    of negative values observing dead neurons? Choose the appropriate activation functions
    accordingly. Use Leaky ReLU if dead neurons are observed during training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Choose the right activation function to handle overfitting**: Observe the
    evaluation metrics and their variation for each training period. Understand gradient
    behavior and how well your model performs on new unseen data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Choose the right activation function as per the expected output of your use
    case**: Examine the desired outcome of your network as a first step. For example,
    the SOFTMAX function can be used when you need to measure the probability of the
    occurrence of the output class. It is used in the output layer. For any input/hidden
    layers, ReLU is what you need for most cases. If you''re not sure about what to
    use, just start experimenting with ReLU; if that doesn''t improve your expectations,
    then try other activation functions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For step 1, ReLU is most commonly used because of its non-linear behavior. The
    output layer activation function depends on the expected output behavior. Step
    4 targets this too.
  prefs: []
  type: TYPE_NORMAL
- en: For step 2, Leaky ReLU is an improved version of ReLU and is used to avoid the
    zero gradient problem. However, you might observe a performance drop. We use Leaky
    ReLU if dead neurons are observed during training. Dead neurons are referred to
    as neurons with a zero gradient for all possible inputs, which makes them useless
    for training.
  prefs: []
  type: TYPE_NORMAL
- en: For step 3, the tanh and sigmoid activation functions are similar and are used
    in feed-forward networks. If you use these activation functions, then make sure
    you add regularization to network layers to avoid the vanishing gradient problem.
    These are generally used for classifier problems.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ReLU activation function is non-linear, hence, the backpropagation of errors
    can easily be performed. Backpropagation is the backbone of neural networks. This
    is the learning algorithm that computes gradient descent with respect to weights
    across neurons. The following are ReLU variations currently supported in DL4J:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ReLU`: The standard ReLU activation function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`ReLU6`: ReLU activation, which is capped at 6, where 6 is an arbitrary choice:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`RReLU`: The randomized ReLU activation function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`ThresholdedReLU`: Threshold ReLU:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: There are a few more implementations, such as **SeLU** (short for the **Scaled
    Exponential Linear Unit**), which is similar to the ReLU activation function but
    has a slope for negative values.
  prefs: []
  type: TYPE_NORMAL
- en: Combating overfitting problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we know, overfitting is a major challenge that machine learning developers
    face. It becomes a big challenge when the neural network architecture is complex
    and training data is huge. While mentioning overfitting, we're not ignoring the
    chances of underfitting at all. We will keep overfitting and underfitting in the
    same category. Let's discuss how we can combat overfitting problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are possible reasons for overfitting, including but not limited
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: Too many feature variables compared to the number of data records
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A complex neural network model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-evidently, overfitting reduces the generalization power of the network
    and the network will fit noise instead of a signal when this happens. In this
    recipe, we will walk through key steps to prevent overfitting problems.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Use `KFoldIterator` to perform k-fold cross-validation-based resampling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Construct a simpler neural network architecture.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use enough train data to train the neural network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In step 1, **`k`** is the arbitrary number of choice and `dataSet` is the dataset
    object that represents your training data. We perform k-fold cross-validation
    to optimize the model evaluation process.
  prefs: []
  type: TYPE_NORMAL
- en: Complex neural network architectures can cause the network to tend to memorize
    patterns. Hence, your neural network will have a hard time generalizing unseen
    data. For example, it's better and more efficient to have a few hidden layers
    rather than hundreds of hidden layers. That's the relevance of step 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fairly large training data will encourage the network to learn better and a
    batch-wise evaluation of test data will increase the generalization power of the
    network. That''s the relevance of step 3. Although there are multiple types of
    data iterator and various ways to introduce batch size in an iterator in DL4J,
    the following is a more conventional definition for `RecordReaderDataSetIterator`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you perform k-fold cross-validation, data is divided into *k* number of
    subsets. For every subset, we perform evaluation by keeping one of the subsets
    for testing and the remaining *k-1* subsets for training. We will repeat this *k*
    number of times. Effectively, we use the entire data for training with no data
    loss, as opposed to wasting some of the data on testing.
  prefs: []
  type: TYPE_NORMAL
- en: Underfitting is handled here. However, note that we perform the evaluation *k*
    number of times only.
  prefs: []
  type: TYPE_NORMAL
- en: When you perform batch training, the entire dataset is divided as per the batch
    size. If your dataset has 1,000 records and the batch size is 8, then you have
    125 training batches.
  prefs: []
  type: TYPE_NORMAL
- en: You need to note the training-to-testing ratio as well. According to that ratio,
    every batch will be divided into a training set and testing set. Then the evaluation
    will be performed accordingly. For 8-fold cross-validation, you evaluate the model
    8 times, but for a batch size of 8, you perform 125 model evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: Note the rigorous mode of evaluation here, which will help to improve the generalization
    power while increasing the chances of underfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Determining the right batch size and learning rates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although there is no specific batch size or learning rate that works for all
    models, we can find the best values for them by experimenting with multiple training
    instances. The primary step is to experiment with a set of batch size values and
    learning rates with the model. Observe the efficiency of the model by evaluating
    additional parameters such as `Precision`, `Recall`, and `F1 Score`. Test scores
    alone don't confirm the model's performance. Also, parameters such as `Precision`, `Recall`,
    and `F1 Score` vary according to the use case. You need to analyze your problem
    statement to get an idea about this. In this recipe, we will walk through key
    steps to determine the right batch size and learning rates.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Run the training instance multiple times and track the evaluation metrics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run experiments by increasing the learning rate and track the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consider the following experiments to illustrate step 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following training was performed on 10,000 records with a batch size of
    8 and a learning rate of 0.008:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8669f24e-f834-47fa-9a11-2b6e4e8e85fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the evaluation performed on the same dataset for a batch size
    of 50 and a learning rate of 0.008:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4dc5d658-38f8-4eda-9402-b2531fbcbeab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To perform step 2, we increased the learning rate to 0.6, to observe the results.
    Note that a learning rate beyond a certain limit will not help efficiency in any
    way. Our job is to find that limit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b100a4e-a906-4e13-a870-639b4447828f.png)'
  prefs: []
  type: TYPE_IMG
- en: You can observe that `Accuracy` is reduced to 82.40% and `F1 Score` is reduced
    to 20.7%. This indicates that `F1 Score` might be the evaluation parameter to
    be accounted for in this model. This is not true for all models, and we reach
    this conclusion after experimenting with a couple of batch sizes and learning
    rates. In a nutshell, you have to repeat the same process for your model's training
    and choose arbitrary values that yield the best results.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we increase the batch size, the number of iterations will eventually reduce,
    hence the number of evaluations will also be reduced. This can overfit the data
    for a large batch size. A batch size of 1 is as useless as a batch size based
    on an entire dataset. So, you need to experiment with values starting from a safe
    arbitrary point.
  prefs: []
  type: TYPE_NORMAL
- en: A very small learning rate will lead to a very small convergence rate to the
    target. This can also impact the training time. If the learning rate is very large,
    this will cause divergent behavior in the model. We need to increase the learning
    rate until we observe the evaluation metrics getting better. There is an implementation
    of a cyclic learning rate in the fast.ai and Keras libraries; however, a cyclic
    learning rate is not implemented in DL4J.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Maven for DL4J
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to add DL4J/ND4J Maven dependencies to leverage DL4J capabilities. ND4J
    is a scientific computation library dedicated to DL4J. It is necessary to mention
    the ND4J backend dependency in your `pom.xml` file. In this recipe, we will add
    a CPU-specific Maven configuration in `pom.xml`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s discuss the required Maven dependencies. We assume you have already
    done the following:'
  prefs: []
  type: TYPE_NORMAL
- en: JDK 1.7, or higher, is installed and the `PATH` variable is set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maven is installed and the `PATH` variable is set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A 64-bit JVM is required to run DL4J.
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the `PATH` variable for JDK and Maven:'
  prefs: []
  type: TYPE_NORMAL
- en: '**On Linux**: Use the `export` command to add Maven and JDK to the `PATH` variable:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Replace the version number as per the installation.
  prefs: []
  type: TYPE_NORMAL
- en: '**On Windows**: Set System Environment variables from system Properties:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Replace the JDK version number as per the installation.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Add the DL4J core dependency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the ND4J native dependency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the DataVec dependency to perform ETL (short for **Extract, Transform and
    Load**) operations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Enable logging for debugging:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note that 1.0.0-beta 3 is the current DL4J release version at the time of writing
    this book, and is the official version used in this cookbook. Also, note that
    DL4J relies on an ND4J backend for hardware-specific implementations.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After adding DL4J core dependency and ND4J dependencies, as mentioned in step
    1 and step 2, we are able to create neural networks. In step 2, the ND4J maven
    configuration is mentioned as a necessary backend dependency for Deeplearnign4j.
    ND4J is the scientific computation library for Deeplearning4j.
  prefs: []
  type: TYPE_NORMAL
- en: ND4J is a scientific computing library written for Java, just like NumPy is
    for Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3 is very crucial for the ETL process: that is, data extraction, transformation,
    and loading. So, we definitely need this as well in order to train the neural
    network using data.'
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 is optional but recommended, since logging will reducee the effort involved
    in debugging.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring DL4J for a GPU-accelerated environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For GPU-powered hardware, DL4J comes with a different API implementation. This
    is to ensure the GPU hardware is utilized effectively without wasting hardware
    resources. Resource optimization is a major concern for expensive GPU-powered
    applications in production. In this recipe, we will add a GPU-specific Maven configuration
    to `pom.xml`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will need the following in order to complete this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: JDK version 1.7, or higher, installed and added to the `PATH` variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maven installed and added to the `PATH` variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NVIDIA-compatible hardware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CUDA v9.2+ installed and configured
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cuDNN** (short for **CUDA Deep Neural Network**) installed and configured'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Download and install CUDA v9.2+ from the NVIDIA developer website URL: [https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Configure the CUDA dependencies. For Linux, go to a Terminal and edit the `.bashrc`
    file. Run the following commands and make sure you replace username and the CUDA
    version number as per your downloaded version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Add the `lib64` directory to `PATH` for older DL4J versions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the `nvcc --version` command to verify the CUDA installation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add Maven dependencies for the ND4J CUDA backend:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the DL4J CUDA Maven dependency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Add cuDNN dependencies to use bundled CUDA and cuDNN:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We configured NVIDIA CUDA using steps 1 to 4\. For more detailed OS-specific
    instructions, refer to the official NVIDIA CUDA website at [https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads).
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on your OS, installation instructions will be displayed on the website.
    DL4J version 1.0.0-beta 3 currently supports CUDA installation versions 9.0, 9.2,
    and 10.0\. For instance, if you need to install CUDA v10.0 for Ubuntu 16.04, you
    should navigate the CUDA website as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9472152e-f538-4507-8961-48716ebdd996.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that step 3 is not applicable to newer versions of DL4J. For of 1.0.0-beta
    and later versions, the necessary CUDA libraries are bundled with DL4J. However,
    this is not applicable for step 7.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, before proceeding with steps 5 and 6, make sure that there are
    no redundant dependencies (such as CPU-specific dependencies) present in `pom.xml`.
  prefs: []
  type: TYPE_NORMAL
- en: DL4J supports CUDA, but performance can be further accelerated by adding a cuDNN
    library. cuDNN does not show up as a bundled package in DL4J. Hence, make sure
    you download and install NVIDIA cuDNN from the NVIDIA developer website. Once
    cuDNN is installed and configured, we can follow step 7 to add support for cuDNN
    in the DL4J application.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For multi-GPU systems, you can consume all GPU resources by placing the following
    code in the main method of your application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This is a temporary workaround for initializing the ND4J backend in the case
    of multi-GPU hardware. In this way, we will not be limited to only a few GPU resources
    if more are available.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting installation issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Though the DL4J setup doesn't seem complex, installation issues can still happen
    because of different OSes or applications installed on the system, and so on. CUDA
    installation issues are not within the scope of this book. Maven build issues
    that are due to unresolved dependencies can have multiple causes. If you are working
    for an organization with its own internal repositories and proxies, then you need
    to make relevant changes in the `pom.xml` file. These issues are also outside
    the scope of this book. In this recipe, we will walk through the steps to mitigate
    common installation issues with DL4J.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following checks are mandatory before we proceed:'
  prefs: []
  type: TYPE_NORMAL
- en: Verify Java and Maven are installed and the `PATH` variables are configured.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verify the CUDA and cuDNN installations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verify that the Maven build is successful and the dependencies are downloaded
    at `~/.m2/repository`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Enable logging levels to yield more information on errors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Verify the JDK/Maven installation and configuration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check whether all the required dependencies are added in the `pom.xml` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Remove the contents of the Maven local repository and rebuild Maven to mitigate `NoClassDefFoundError` in
    DL4J. For Linux, this is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Mitigate `ClassNotFoundException` in DL4J. You can try this if step 4 didn't
    help to resolve the issue. DL4J/ND4J/DataVec should have the same version. For
    CUDA-related error stacks, check the installation as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If adding the proper DL4J CUDAversion doesn't fix this, then check your cuDNN
    installation.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To mitigate exceptions such as `ClassNotFoundException`, the primary task is
    to verify we installed the JDK properly (step 2) and whether the environment variables
    we set up point to the right place. Step 3 is also important as the missing dependencies
    result in the same error.
  prefs: []
  type: TYPE_NORMAL
- en: 'In step 4, we are removing redundant dependencies that are present in the local
    repository and are attempting a fresh Maven build. Here is a sample for `NoClassDefFoundError`
    while trying to run a DL4J application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: One possible reason for `NoClassDefFoundError` could be the absence of required
    dependencies in the Maven local repository. So, we remove the repository contents
    and rebuild Maven to download the dependencies again. If any dependencies were
    not downloaded previously due to an interruption, it should happen now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of `ClassNotFoundException` during DL4J training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/128dd62a-9167-4658-9060-b6a46a826555.png)'
  prefs: []
  type: TYPE_IMG
- en: Again, this suggests version issues or redundant dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In addition to the common runtime issues that were discussed previously, Windows
    users may face cuDNN-specific errors while training a CNN. The actual root cause
    could be different and is tagged under `UnsatisfiedLinkError`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the following steps to fix the issue:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the latest dependency walker here: [https://github.com/lucasg/Dependencies/](https://github.com/lucasg/Dependencies/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following code to your DL4J `main()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Replace `<module>` with the name of the JavaCPP preset module that is experiencing
    the problem; for example, `cudnn`. For newer DL4J versions, the necessary CUDA
    libraries are bundled with DL4J. Hence, you should not face this issue.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you feel like you might have found a bug or functional error with DL4J, then
    feel free to create an issue tracker at [https://github.com/eclipse/deeplearning4j](https://github.com/eclipse/deeplearning4j).
  prefs: []
  type: TYPE_NORMAL
- en: You're also welcome to initiate a discussion with the Deeplearning4j community
    here: [https://gitter.im/deeplearning4j/deeplearning4j](https://gitter.im/deeplearning4j/deeplearning4j).
  prefs: []
  type: TYPE_NORMAL
