- en: Chapter 1. Introduction to Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Neural networks are currently capable of providing human level solutions
    to a variety of problems such as image recognition, speech recognition, machine
    translation, natural language processing, and many more.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will look at how neural networks, a biologically-inspired
    architecture has evolved throughout the years. Then we will cover some of the
    important concepts and terminology related to deep learning as a refresher for
    the subsequent chapters. Finally we will understand the intuition behind the creative
    nature of deep networks through a generative model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Evolution of deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stochastic Gradient Descent, ReLU, learning rate, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional network, Recurrent Neural Network and LSTM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Difference between discriminative and generative models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evolution of deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lot of the important work on neural networks happened in the 80's and 90's,
    but back then computers were slow and datasets very tiny. The research didn't
    really find many applications in the real world. As a result, in the first decade
    of the 21st century neural networks have completely disappeared from the world
    of machine learning. It's only in the last few years, first in speech recognition
    around 2009, and then in computer vision around 2012, that neural networks made
    a big comeback (with LeNet, AlexNet, and so on). What changed?
  prefs: []
  type: TYPE_NORMAL
- en: Lots of data (big data) and cheap, fast GPU's. Today, neural networks are everywhere.
    So, if you're doing anything with data, analytics, or prediction, deep learning
    is definitely something that you want to get familiar with.
  prefs: []
  type: TYPE_NORMAL
- en: 'See the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evolution of deep learning](img/B08086_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure-1: Evolution of deep learning'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning is an exciting branch of machine learning that uses data, lots
    of data, to teach computers how to do things only humans were capable of before,
    such as recognizing what's in an image, what people are saying when they are talking
    on their phones, translating a document into another language, and helping robots
    explore the world and interact with it. Deep learning has emerged as a central
    tool to solve perception problems and it's state of the art with computer vision
    and speech recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Today many companies have made deep learning a central part of their machine
    learning toolkit—Facebook, Baidu, Amazon, Microsoft, and Google are all using
    deep learning in their products because deep learning shines wherever there is
    lots of data and complex problems to solve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning is the name we often use for "deep neural networks" composed
    of several layers. Each layer is made of nodes. The computation happens in the
    nodes, where it combines input data with a set of parameters or weights, that
    either amplify or dampen that input. These input-weight products are then summed
    and the sum is passed through the `activation` function, to determine to what
    extent the value should progress through the network to affect the final prediction,
    such as an act of classification. A layer consists of a row of nodes that that
    turn on or off as the input is fed through the network. The input of the first
    layer becomes the input of the second layer and so on. Here''s a diagram of what
    neural a network might look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evolution of deep learning](img/B08086_01_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's get familiarized with some deep neural network concepts and terminology.
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid activation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The sigmoid activation function used in neural networks has an output boundary
    of *(0, 1)*, and *α* is the offset parameter to set the value at which the sigmoid
    evaluates to 0\.
  prefs: []
  type: TYPE_NORMAL
- en: The sigmoid function often works fine for gradient descent as long as the input
    data *x* is kept within a limit. For large values of *x*, *y* is constant. Hence,
    the derivatives *dy/dx* (the gradient) equates to *0*, which is often termed as
    the **vanishing gradient** problem.
  prefs: []
  type: TYPE_NORMAL
- en: This is a problem because when the gradient is 0, multiplying it with the loss
    (actual value - predicted value) also gives us 0 and ultimately networks stop
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Rectified Linear Unit (ReLU)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A neural network can be built by combining some linear classifiers with some
    non-linear functions. The **Rectified Linear Unit** (**ReLU**) has become very
    popular in the last few years. It computes the function *f(x)=max(0,x)*. In other
    words, the activation is simply thresholded at zero. Unfortunately, ReLU units
    can be fragile during training and can die, as a ReLU neuron could cause the weights
    to update in such a way that the neuron will never activate on any datapoint again,
    and so the gradient flowing through the unit will forever be zero from that point
    on.
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome this problem, a leaky `ReLU` function will have a small negative
    slope (of 0.01, or so) instead of zero when *x<0*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Rectified Linear Unit (ReLU)](img/B08086_01_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *αα* is a small constant.
  prefs: []
  type: TYPE_NORMAL
- en: '![Rectified Linear Unit (ReLU)](img/B08086_01_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure-2: Rectified Linear Unit'
  prefs: []
  type: TYPE_NORMAL
- en: Exponential Linear Unit (ELU)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The mean of ReLU activation is not zero and hence sometimes makes learning difficult
    for the network. The **Exponential Linear Unit** (**ELU)** is similar to ReLU
    activation function when the input *x* is positive, but for negative values it
    is a function bounded by a fixed value *-1*, for *α=1* (the hyperparameter *α*
    controls the value to which an ELU saturates for negative inputs). This behavior
    helps to push the mean activation of neurons closer to zero; that helps to learn
    representations that are more robust to noise.
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic Gradient Descent (SGD)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scaling batch gradient descent is cumbersome because it has to compute a lot
    if the dataset is big, and as a rule of thumb, if computing your loss takes *n*
    floating point operations, computing its gradient takes about three times that
    to compute.
  prefs: []
  type: TYPE_NORMAL
- en: But in practice we want to be able to train lots of data because on real problems
    we will always get more gains the more data we use. And because gradient descent
    is iterative and has to do that for many steps, that means that in order to update
    the parameters in a single step, it has to go through all the data samples and
    then do this iteration over the data tens or hundreds of times.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of computing the loss over entire data samples for every step, we can
    compute the average loss for a very small random fraction of the training data.
    Think between 1 and 1000 training samples each time. This technique is called
    **Stochastic Gradient Descent** (**SGD**) and is at the core of deep learning.
    That's because SGD scales well with both data and model size.
  prefs: []
  type: TYPE_NORMAL
- en: SGD gets its reputation for being black magic as it has lots of hyper-parameters
    to play and tune such as initialization parameters, learning rate parameters,
    decay, and momentum, and you have to get them right.
  prefs: []
  type: TYPE_NORMAL
- en: 'AdaGrad is a simple modification of SGD, which implicitly does momentum and
    learning rate decay by itself. Using AdaGrad often makes learning less sensitive
    to hyper-parameters. But it often tends to be a little worse than precisely tuned
    SDG with momentum. It''s still a very good option though, if you''re just trying
    to get things to work:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stochastic Gradient Descent (SGD)](img/B08086_01_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure-4a: Loss computation in batch gradient descent and SGD'
  prefs: []
  type: TYPE_NORMAL
- en: '*Source*: [https://www.coursera.org/learn/machine-learning/lecture/DoRHJ/stochasticgradient-
    descent](https://www.coursera.org/learn/machine-learning/lecture/DoRHJ/stochasticgradient-
    descent)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stochastic Gradient Descent (SGD)](img/B08086_01_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4b: Stochastic Gradient Descent and AdaGrad'
  prefs: []
  type: TYPE_NORMAL
- en: You can notice from *Figure 4a* that in case of batch gradient descent the `loss`/`optimization`
    function is well minimized, whereas SGD calculates the loss by taking a random
    fraction of the data in each step and often oscillates around that point. In practice,
    it's not that bad and SGD often converges faster.
  prefs: []
  type: TYPE_NORMAL
- en: Learning rate tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `loss` function of the neural network can be related to a surface, where
    the weights of the network represent each direction you can move in. Gradient
    descent provides the steps in the current direction of the slope, and the learning
    rate gives the length of each step you take. The learning rate helps the network
    to abandons old beliefs for new ones.
  prefs: []
  type: TYPE_NORMAL
- en: Learning rate tuning can be very strange. For example, you might think that
    using a higher learning rate means you learn more or that you learn faster. That's
    just not true. In fact, you can often take a model, lower the learning rate, and
    get to a better model faster.
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning rate tuning](img/B08086_01_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure-3: Learning rate'
  prefs: []
  type: TYPE_NORMAL
- en: You might be tempted to look at the learning curve that shows the loss over
    time to see how quickly the network learns. Here the higher learning rate starts
    faster, but then it plateaus, whereas the lower learning rate keeps on going and
    gets better. It is a very familiar picture for anyone who has trained neural networks.
    *Never trust how quickly you learn*.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first way to prevent over fitting is by looking at the performance under
    validation set, and stopping to train as soon as it stops improving. It's called
    early termination, and it's one way to prevent a neural network from over-optimizing
    on the training set. Another way is to apply regularization. Regularizing means
    applying artificial constraints on the network that implicitly reduce the number
    of free parameters while not making it more difficult to optimize.
  prefs: []
  type: TYPE_NORMAL
- en: '![Regularization](img/B08086_01_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6a: Early termination'
  prefs: []
  type: TYPE_NORMAL
- en: In the skinny jeans analogy as shown in *Figure 6b*, think stretch pants. They
    fit just as well, but because they're flexible, they don't make things harder
    to fit in. The stretch pants of deep learning are sometime called **L2 regularization**.
    The idea is to add another term to the loss, which penalizes large weights.
  prefs: []
  type: TYPE_NORMAL
- en: '![Regularization](img/B08086_01_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6b: Stretch pant analogy of deep learning'
  prefs: []
  type: TYPE_NORMAL
- en: '![Regularization](img/B08086_01_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6c: L2 tegularization'
  prefs: []
  type: TYPE_NORMAL
- en: Currently, in deep learning practice, the widely used approach for preventing
    overfitting is to feed lots of data into the deep network.
  prefs: []
  type: TYPE_NORMAL
- en: Shared weights and pooling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let say an image has a cat in it and it doesn't really matter where the cat
    is in the image, as it's still an image with a cat. If the network has to learn
    about cats in the left corner and about cats in the right corner independently,
    that's a lot of work that it has to do. But objects and images are largely the
    same whether they're on the left or on the right of the picture. That's what's
    called **translation invariance**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The way of achieving this in networks is called **weight sharing**. When networks
    know that two inputs can contain the same kind of information, then it can share
    the weights and train the weights jointly for those inputs. It is a very important
    idea. Statistical invariants are things that don''t change on average across time
    or space, and are everywhere. For images, the idea of weight sharing will get
    us to study convolutional networks. For text and sequences in general, it will
    lead us to recurrent neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Shared weights and pooling](img/B08086_01_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7a: Translation variance'
  prefs: []
  type: TYPE_NORMAL
- en: '![Shared weights and pooling](img/B08086_01_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7b: Weight sharing'
  prefs: []
  type: TYPE_NORMAL
- en: To reduce the spatial extent of the feature maps in the convolutional pyramid,
    a very small stride could run and take all the convolutions in a neighborhood
    and combine them somehow. This is known as **pooling**.
  prefs: []
  type: TYPE_NORMAL
- en: In max-pooling as shown in *Figure 7d*, at every point in the feature map, look
    at a small neighborhood around that point and compute the maximum of all the responses
    around it. There are some advantages to using max pooling. First, it doesn't add
    to your number of parameters. So, you don't risk an increasing over fitting. Second,
    it simply often yields more accurate models. However, since the convolutions that
    run below run at a lower stride, the model then becomes a lot more expensive to
    compute. Max-pooling extracts the most important feature, whereas average pooling
    sometimes can't extract good features because it takes all into account and results
    in an average value that may/may not be important for object detection-type tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Shared weights and pooling](img/B08086_01_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7c: Pooling'
  prefs: []
  type: TYPE_NORMAL
- en: '![Shared weights and pooling](img/B08086_01_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7d: Max and average pooling'
  prefs: []
  type: TYPE_NORMAL
- en: Local receptive field
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A simple way to encode the local structure is to connect a submatrix of adjacent
    input neurons into one single hidden neuron belonging to the next layer. That
    single hidden neuron represents one local receptive field. Let''s consider CIFAR-10
    images that have an input feature of size [32 x 32 x 3]. If the receptive field
    (or the filter size) is 4 x 4, then each neuron in the convolution layer will
    have weights to a [4 x 4 x 3] region in the input feature, for a total of 4*4*3
    = 48 weights (and +1 bias parameter). The extent of the connectivity along the
    depth axis must be 3, since this is the depth (or number of channel: RGB) of the
    input feature.'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional network (ConvNet)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Convolutional Networks** (**ConvNets**) are neural networks that share their
    parameters/weights across space. An image can be represented as a flat pancake
    that has width, height, and depth or number of channel (for RGB: having red, green,
    and blue channel the depth is 3, whereas for grayscale the depth is 1).'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's slide a tiny neural network with *K* outputs across the image without
    changing the weights.
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolutional network (ConvNet)](img/B08086_01_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8a: Weight sharing across space'
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolutional network (ConvNet)](img/B08086_01_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8b: Convolutional pyramid with layers of convolution'
  prefs: []
  type: TYPE_NORMAL
- en: On the output, a different image will be drawn with different width, different
    height, and different depth (from just R, G, B color channels to *K* number of
    channels). This operation is known as convolution.
  prefs: []
  type: TYPE_NORMAL
- en: A ConvNet is going to basically be a deep network with layers of convolutions
    that stack together to form a pyramid like structure. You can see from the preceding
    figure that the network takes an image as an input of dimension (width x height
    x depth) and then applys convolutions progressively over it to reduce the spatial
    dimension while increasing the depth, which is roughly equivalent to its semantic
    complexity. Let's understand some of the common terminology in convent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each layer or depth in the image stack is called a feature map and patches
    or kernels are used for mapping three feature maps to *K* feature maps. A stride
    is the number of pixels that is shifted each time you move your filter. Depending
    on the type of padding a stride of 1 makes the output roughly the same size as
    the input. A stride of 2 makes it about half the size. In the case of valid padding,
    a sliding filter don''t cross the edge of the image, whereas in same-padding it
    goes off the edge and is padded with zeros to make the output map size exactly
    the same size as the input map:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolutional network (ConvNet)](img/B08086_01_15.jpg)![Convolutional network
    (ConvNet)](img/B08086_01_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8c: Different terminology related to convolutional network'
  prefs: []
  type: TYPE_NORMAL
- en: Deconvolution or transpose convolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the case of a computer vision application where the resolution of final output
    is required to be larger than the input, deconvolution/transposed convolution
    is the de-facto standard. This layer is used in very popular applications such
    as GAN, image super-resolution, surface depth estimation from image, optical flow
    estimation, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: CNN in general performs down-sampling, that is, they produce output of a lower
    resolution than the input, whereas in deconvolution the layer up-samples the image
    to get the same resolution as the input image. Note since a naive up-sampling
    inadvertently loses details, a better option is to have a trainable up-sampling
    convolutional layer whose parameters will change during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tensorflow method: `tf.nn.conv2d_transpose(value, filter, output_shape, strides,
    padding, name)`'
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks and LSTM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The key idea behind **Recurrent Neural Networks** (**RNN**) is to share parameters
    over time. Imagine that you have a sequence of events, and at each point in time
    you want to make a decision about what's happened so far in this sequence. If
    the sequence is reasonably stationary, you can use the same classifier at each
    point in time. That simplifies things a lot already. But since this is a sequence,
    you also want to take into account the past-everything that happened before that
    point.
  prefs: []
  type: TYPE_NORMAL
- en: 'RNN is going to have a single model responsible for summarizing the past and
    providing that information to your classifier. It basically ends up with a network
    that has a relatively simple repeating pattern, with part of the classifier connecting
    to the input at each time step and another part called the recurrent connection
    connecting you to the past at each step, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recurrent Neural Networks and LSTM](img/B08086_01_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9a: Recurrent neural network'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recurrent Neural Networks and LSTM](img/B08086_01_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure-9b: Long short-term memory (LSTM)'
  prefs: []
  type: TYPE_NORMAL
- en: '**LSTM** stands for **long short-term memory**. Now, conceptually, a recurrent
    neural network consists of a repetition of simple little units like this, which
    take as an input the past, a new input, and produce a new prediction and connect
    to the future. Now, what''s in the middle of that is typically a simple set of
    layers with some weights and linearities.'
  prefs: []
  type: TYPE_NORMAL
- en: In LSTM as shown in *Figure 9b*, the gating values for each gate get controlled
    by a tiny logistic regression on the input parameters. Each of them has its own
    set of shared parameters. And there's an additional hyperbolic tension sprinkled
    to keep the outputs between -1 and 1\. Also it's differentiable all the way, which
    means it can optimize the parameters very easily. All these little gates help
    the model keep its memory for longer when it needs to, and ignore things when
    it should.
  prefs: []
  type: TYPE_NORMAL
- en: Deep neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The central idea of deep learning is to add more layers and make your model
    deeper. There are lots of good reasons to do that. One is parameter efficiency.
    You can typically get much more performance with fewer parameters by going deeper
    rather than wider.
  prefs: []
  type: TYPE_NORMAL
- en: Another one is that a lot of the natural phenomena that you might be interested
    in, tend to have a hierarchical structure, which deep models naturally capture.
    If you poke at a model for images, for example, and visualize what the model learns,
    you'll often find very simple things at the lowest layers, such as lines or edges.
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep neural networks](img/B08086_01_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10a: Deep neural networks'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep neural networks](img/B08086_01_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10b: Network layers capturing hierarchical structure of image'
  prefs: []
  type: TYPE_NORMAL
- en: A very typical architecture for a ConvNet is a few layers alternating convolutions
    and max pooling, followed by a few fully connected layers at the top. The first
    famous model to use this architecture was LeNet-5 designed by Yann Lecun for character
    recognition back in 1998.
  prefs: []
  type: TYPE_NORMAL
- en: Modern convolutional networks such as AlexNet, which famously won the competitive
    ImageNet object recognition challenge in 2012, use a very similar architecture
    with a few wrinkles. Another notable form of pooling is average pooling. Instead
    of taking the max, just take an average over the window of pixels around a specific
    location.
  prefs: []
  type: TYPE_NORMAL
- en: Discriminative versus generative models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A discriminative model learns the conditional probability distribution *p(y|x)*
    which could be interpreted as the *probability of y given x*. A discriminative
    classifier learns by observing data. It makes fewer assumptions on the distributions,
    but depends heavily on the quality of the data. The distribution *p(y|x)* simply
    classifies a given example x directly into a label *y*. For example, in logistic
    regression all we have to do is to learn weights and bias that would minimize
    the squared loss.
  prefs: []
  type: TYPE_NORMAL
- en: Whereas a generative model learns the joint probability distribution *p(x,y)*,
    where *x* is the input data and *y* is the label that you want to classify. A
    generative model can generate more samples by itself artificially, based on assumptions
    about the distribution of data. For example, in the Naive Bayes' model, we can
    learn *p(x)* from data, also *p(y)*, the prior class probabilities, and we can
    also learn *p(x|y)* from the data using say maximum likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have *p(x)*, *p(y)* and *p(x|y)*, *p(x, y)* is not difficult to find
    out. Now using Bayes'' rule, we can replace the *p(y|x)* with *(p(x|y)p(y))/p(x)*.
    And since we are just interested in the *arg max*, the denominator can be removed,
    as that will be the same for every *y*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Discriminative versus generative models](img/B08086_01_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is the equation we use in generative models, as *p(x, y) = p(x | y) p(y)*,
    which explicitly models the actual distribution of each class.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the discriminative models generally outperform generative models
    in classification tasks, but the generative model shines over discriminative models
    in creativity/generation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far you have refreshed various concepts related to deep learning and also
    learned how deep networks have evolved from the arena of supervised tasks of classifying
    an image, recognizing voice, text, and so on, towards the creative power through
    generative model. In the next chapter we will see how deep learning can be used
    for performing wonderful creativity tasks in the unsupervised domain using **Generative
    Adversarial Networks** (**GANs**).
  prefs: []
  type: TYPE_NORMAL
