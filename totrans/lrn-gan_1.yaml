- en: Chapter 1. Introduction to Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章：深度学习简介
- en: Deep Neural networks are currently capable of providing human level solutions
    to a variety of problems such as image recognition, speech recognition, machine
    translation, natural language processing, and many more.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络目前能够为许多问题提供接近人类水平的解决方案，例如图像识别、语音识别、机器翻译、自然语言处理等等。
- en: In this chapter, we will look at how neural networks, a biologically-inspired
    architecture has evolved throughout the years. Then we will cover some of the
    important concepts and terminology related to deep learning as a refresher for
    the subsequent chapters. Finally we will understand the intuition behind the creative
    nature of deep networks through a generative model.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将探讨神经网络这一生物启发式架构是如何在这些年中发展的。接下来，我们将回顾一些与深度学习相关的重要概念和术语，为后续章节做准备。最后，我们将通过生成模型理解深度网络的创造性本质背后的直觉。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论以下主题：
- en: Evolution of deep learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习的演变
- en: Stochastic Gradient Descent, ReLU, learning rate, and so on
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机梯度下降、ReLU、学习率等
- en: Convolutional network, Recurrent Neural Network and LSTM
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积网络、递归神经网络和长短期记忆网络（LSTM）
- en: Difference between discriminative and generative models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判别模型与生成模型的区别
- en: Evolution of deep learning
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的演变
- en: A lot of the important work on neural networks happened in the 80's and 90's,
    but back then computers were slow and datasets very tiny. The research didn't
    really find many applications in the real world. As a result, in the first decade
    of the 21st century neural networks have completely disappeared from the world
    of machine learning. It's only in the last few years, first in speech recognition
    around 2009, and then in computer vision around 2012, that neural networks made
    a big comeback (with LeNet, AlexNet, and so on). What changed?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 许多关于神经网络的重要研究发生在80年代和90年代，但那时计算机速度很慢，数据集非常小。这些研究在现实世界中并没有找到很多应用。因此，在21世纪的第一个十年，神经网络完全从机器学习的世界中消失了。直到最近几年，首先是在2009年左右的语音识别，然后是在2012年左右的计算机视觉领域，神经网络才迎来了一次大的复兴（比如LeNet、AlexNet等）。发生了什么变化？
- en: Lots of data (big data) and cheap, fast GPU's. Today, neural networks are everywhere.
    So, if you're doing anything with data, analytics, or prediction, deep learning
    is definitely something that you want to get familiar with.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大量数据（大数据）和廉价、高速的GPU。如今，神经网络无处不在。因此，如果你从事任何与数据、分析或预测相关的工作，深度学习绝对是你需要熟悉的领域。
- en: 'See the following figure:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见以下图示：
- en: '![Evolution of deep learning](img/B08086_01_01.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习的演变](img/B08086_01_01.jpg)'
- en: 'Figure-1: Evolution of deep learning'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：深度学习的演变
- en: Deep learning is an exciting branch of machine learning that uses data, lots
    of data, to teach computers how to do things only humans were capable of before,
    such as recognizing what's in an image, what people are saying when they are talking
    on their phones, translating a document into another language, and helping robots
    explore the world and interact with it. Deep learning has emerged as a central
    tool to solve perception problems and it's state of the art with computer vision
    and speech recognition.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一个令人兴奋的分支，它利用大量的数据来教会计算机执行以前只有人类能够完成的任务，比如识别图像中的内容、理解人们在打电话时说的话、将文档翻译成另一种语言，以及帮助机器人探索世界并与之互动。深度学习已成为解决感知问题的核心工具，并且在计算机视觉和语音识别领域处于最前沿。
- en: Today many companies have made deep learning a central part of their machine
    learning toolkit—Facebook, Baidu, Amazon, Microsoft, and Google are all using
    deep learning in their products because deep learning shines wherever there is
    lots of data and complex problems to solve.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，许多公司已经将深度学习作为其机器学习工具包的核心部分——Facebook、百度、亚马逊、微软和谷歌都在其产品中使用深度学习，因为深度学习在数据量大且问题复杂的场景中具有显著优势。
- en: 'Deep learning is the name we often use for "deep neural networks" composed
    of several layers. Each layer is made of nodes. The computation happens in the
    nodes, where it combines input data with a set of parameters or weights, that
    either amplify or dampen that input. These input-weight products are then summed
    and the sum is passed through the `activation` function, to determine to what
    extent the value should progress through the network to affect the final prediction,
    such as an act of classification. A layer consists of a row of nodes that that
    turn on or off as the input is fed through the network. The input of the first
    layer becomes the input of the second layer and so on. Here''s a diagram of what
    neural a network might look like:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![Evolution of deep learning](img/B08086_01_22.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Let's get familiarized with some deep neural network concepts and terminology.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid activation
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The sigmoid activation function used in neural networks has an output boundary
    of *(0, 1)*, and *α* is the offset parameter to set the value at which the sigmoid
    evaluates to 0\.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: The sigmoid function often works fine for gradient descent as long as the input
    data *x* is kept within a limit. For large values of *x*, *y* is constant. Hence,
    the derivatives *dy/dx* (the gradient) equates to *0*, which is often termed as
    the **vanishing gradient** problem.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: This is a problem because when the gradient is 0, multiplying it with the loss
    (actual value - predicted value) also gives us 0 and ultimately networks stop
    learning.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Rectified Linear Unit (ReLU)
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A neural network can be built by combining some linear classifiers with some
    non-linear functions. The **Rectified Linear Unit** (**ReLU**) has become very
    popular in the last few years. It computes the function *f(x)=max(0,x)*. In other
    words, the activation is simply thresholded at zero. Unfortunately, ReLU units
    can be fragile during training and can die, as a ReLU neuron could cause the weights
    to update in such a way that the neuron will never activate on any datapoint again,
    and so the gradient flowing through the unit will forever be zero from that point
    on.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome this problem, a leaky `ReLU` function will have a small negative
    slope (of 0.01, or so) instead of zero when *x<0*:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![Rectified Linear Unit (ReLU)](img/B08086_01_24.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: where *αα* is a small constant.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![Rectified Linear Unit (ReLU)](img/B08086_01_02.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: 'Figure-2: Rectified Linear Unit'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Exponential Linear Unit (ELU)
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The mean of ReLU activation is not zero and hence sometimes makes learning difficult
    for the network. The **Exponential Linear Unit** (**ELU)** is similar to ReLU
    activation function when the input *x* is positive, but for negative values it
    is a function bounded by a fixed value *-1*, for *α=1* (the hyperparameter *α*
    controls the value to which an ELU saturates for negative inputs). This behavior
    helps to push the mean activation of neurons closer to zero; that helps to learn
    representations that are more robust to noise.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic Gradient Descent (SGD)
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机梯度下降（SGD）
- en: Scaling batch gradient descent is cumbersome because it has to compute a lot
    if the dataset is big, and as a rule of thumb, if computing your loss takes *n*
    floating point operations, computing its gradient takes about three times that
    to compute.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 批量梯度下降的扩展比较繁琐，因为当数据集较大时，需要进行大量的计算。作为经验法则，如果计算损失需要 *n* 次浮点运算，那么计算梯度大约需要三倍的运算量。
- en: But in practice we want to be able to train lots of data because on real problems
    we will always get more gains the more data we use. And because gradient descent
    is iterative and has to do that for many steps, that means that in order to update
    the parameters in a single step, it has to go through all the data samples and
    then do this iteration over the data tens or hundreds of times.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但在实际操作中，我们希望能够训练大量的数据，因为在真实问题中，我们总是能从使用更多数据中获得更多的收益。而且因为梯度下降是迭代的，并且需要进行许多步更新，这意味着为了在单步中更新参数，必须遍历所有数据样本，然后对数据进行数十次或数百次的迭代。
- en: Instead of computing the loss over entire data samples for every step, we can
    compute the average loss for a very small random fraction of the training data.
    Think between 1 and 1000 training samples each time. This technique is called
    **Stochastic Gradient Descent** (**SGD**) and is at the core of deep learning.
    That's because SGD scales well with both data and model size.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 与其在每一步都计算整个数据集的损失，我们可以计算一个非常小的随机训练数据子集的平均损失。每次选择的样本数在 1 到 1000 之间。这种技术叫做**随机梯度下降**（**SGD**），是深度学习的核心。这是因为
    SGD 在数据和模型大小上都能很好地扩展。
- en: SGD gets its reputation for being black magic as it has lots of hyper-parameters
    to play and tune such as initialization parameters, learning rate parameters,
    decay, and momentum, and you have to get them right.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: SGD 因其有很多超参数需要调整而被认为是黑魔法，例如初始化参数、学习率参数、衰减和动量，你必须正确调整这些参数。
- en: 'AdaGrad is a simple modification of SGD, which implicitly does momentum and
    learning rate decay by itself. Using AdaGrad often makes learning less sensitive
    to hyper-parameters. But it often tends to be a little worse than precisely tuned
    SDG with momentum. It''s still a very good option though, if you''re just trying
    to get things to work:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: AdaGrad 是 SGD 的一个简单修改，它隐式地实现了动量和学习率衰减。使用 AdaGrad 通常使学习对超参数的敏感度降低。但它往往比精确调优的带动量的
    SGD 要稍微差一点。不过，如果你只是想让模型运行起来，它仍然是一个非常好的选择：
- en: '![Stochastic Gradient Descent (SGD)](img/B08086_01_04.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![随机梯度下降（SGD）](img/B08086_01_04.jpg)'
- en: 'Figure-4a: Loss computation in batch gradient descent and SGD'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图4a：批量梯度下降和SGD中的损失计算
- en: '*Source*: [https://www.coursera.org/learn/machine-learning/lecture/DoRHJ/stochasticgradient-
    descent](https://www.coursera.org/learn/machine-learning/lecture/DoRHJ/stochasticgradient-
    descent)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*来源*：[https://www.coursera.org/learn/machine-learning/lecture/DoRHJ/stochasticgradient-descent](https://www.coursera.org/learn/machine-learning/lecture/DoRHJ/stochasticgradient-descent)'
- en: '![Stochastic Gradient Descent (SGD)](img/B08086_01_05.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![随机梯度下降（SGD）](img/B08086_01_05.jpg)'
- en: 'Figure 4b: Stochastic Gradient Descent and AdaGrad'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图4b：随机梯度下降和AdaGrad
- en: You can notice from *Figure 4a* that in case of batch gradient descent the `loss`/`optimization`
    function is well minimized, whereas SGD calculates the loss by taking a random
    fraction of the data in each step and often oscillates around that point. In practice,
    it's not that bad and SGD often converges faster.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *图4a* 中可以看出，批量梯度下降的 `loss`/`优化` 函数已经得到很好的最小化，而 SGD 在每一步随机选择数据子集来计算损失，往往在该点附近震荡。在实际应用中，这并不算太糟，SGD
    往往会更快地收敛。
- en: Learning rate tuning
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习率调优
- en: The `loss` function of the neural network can be related to a surface, where
    the weights of the network represent each direction you can move in. Gradient
    descent provides the steps in the current direction of the slope, and the learning
    rate gives the length of each step you take. The learning rate helps the network
    to abandons old beliefs for new ones.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的 `loss` 函数可以与一个表面相关联，其中网络的权重表示可以移动的每个方向。梯度下降提供了当前坡度方向上的步长，而学习率则决定了每一步的长度。学习率帮助网络抛弃旧的信念，接受新的信念。
- en: Learning rate tuning can be very strange. For example, you might think that
    using a higher learning rate means you learn more or that you learn faster. That's
    just not true. In fact, you can often take a model, lower the learning rate, and
    get to a better model faster.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率调优可能非常奇怪。例如，你可能认为使用较高的学习率意味着学习更多，或者学习更快。实际上并非如此。事实上，你常常可以通过降低学习率，快速得到更好的模型。
- en: '![Learning rate tuning](img/B08086_01_03.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: 'Figure-3: Learning rate'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: You might be tempted to look at the learning curve that shows the loss over
    time to see how quickly the network learns. Here the higher learning rate starts
    faster, but then it plateaus, whereas the lower learning rate keeps on going and
    gets better. It is a very familiar picture for anyone who has trained neural networks.
    *Never trust how quickly you learn*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first way to prevent over fitting is by looking at the performance under
    validation set, and stopping to train as soon as it stops improving. It's called
    early termination, and it's one way to prevent a neural network from over-optimizing
    on the training set. Another way is to apply regularization. Regularizing means
    applying artificial constraints on the network that implicitly reduce the number
    of free parameters while not making it more difficult to optimize.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![Regularization](img/B08086_01_06.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6a: Early termination'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: In the skinny jeans analogy as shown in *Figure 6b*, think stretch pants. They
    fit just as well, but because they're flexible, they don't make things harder
    to fit in. The stretch pants of deep learning are sometime called **L2 regularization**.
    The idea is to add another term to the loss, which penalizes large weights.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![Regularization](img/B08086_01_07.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6b: Stretch pant analogy of deep learning'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![Regularization](img/B08086_01_08.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6c: L2 tegularization'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Currently, in deep learning practice, the widely used approach for preventing
    overfitting is to feed lots of data into the deep network.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Shared weights and pooling
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let say an image has a cat in it and it doesn't really matter where the cat
    is in the image, as it's still an image with a cat. If the network has to learn
    about cats in the left corner and about cats in the right corner independently,
    that's a lot of work that it has to do. But objects and images are largely the
    same whether they're on the left or on the right of the picture. That's what's
    called **translation invariance**.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'The way of achieving this in networks is called **weight sharing**. When networks
    know that two inputs can contain the same kind of information, then it can share
    the weights and train the weights jointly for those inputs. It is a very important
    idea. Statistical invariants are things that don''t change on average across time
    or space, and are everywhere. For images, the idea of weight sharing will get
    us to study convolutional networks. For text and sequences in general, it will
    lead us to recurrent neural networks:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![Shared weights and pooling](img/B08086_01_09.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7a: Translation variance'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![Shared weights and pooling](img/B08086_01_10.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7b: Weight sharing'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: To reduce the spatial extent of the feature maps in the convolutional pyramid,
    a very small stride could run and take all the convolutions in a neighborhood
    and combine them somehow. This is known as **pooling**.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: In max-pooling as shown in *Figure 7d*, at every point in the feature map, look
    at a small neighborhood around that point and compute the maximum of all the responses
    around it. There are some advantages to using max pooling. First, it doesn't add
    to your number of parameters. So, you don't risk an increasing over fitting. Second,
    it simply often yields more accurate models. However, since the convolutions that
    run below run at a lower stride, the model then becomes a lot more expensive to
    compute. Max-pooling extracts the most important feature, whereas average pooling
    sometimes can't extract good features because it takes all into account and results
    in an average value that may/may not be important for object detection-type tasks.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![Shared weights and pooling](img/B08086_01_11.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7c: Pooling'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '![Shared weights and pooling](img/B08086_01_12.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7d: Max and average pooling'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Local receptive field
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A simple way to encode the local structure is to connect a submatrix of adjacent
    input neurons into one single hidden neuron belonging to the next layer. That
    single hidden neuron represents one local receptive field. Let''s consider CIFAR-10
    images that have an input feature of size [32 x 32 x 3]. If the receptive field
    (or the filter size) is 4 x 4, then each neuron in the convolution layer will
    have weights to a [4 x 4 x 3] region in the input feature, for a total of 4*4*3
    = 48 weights (and +1 bias parameter). The extent of the connectivity along the
    depth axis must be 3, since this is the depth (or number of channel: RGB) of the
    input feature.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional network (ConvNet)
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Convolutional Networks** (**ConvNets**) are neural networks that share their
    parameters/weights across space. An image can be represented as a flat pancake
    that has width, height, and depth or number of channel (for RGB: having red, green,
    and blue channel the depth is 3, whereas for grayscale the depth is 1).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Now let's slide a tiny neural network with *K* outputs across the image without
    changing the weights.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolutional network (ConvNet)](img/B08086_01_13.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8a: Weight sharing across space'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolutional network (ConvNet)](img/B08086_01_14.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8b: Convolutional pyramid with layers of convolution'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: On the output, a different image will be drawn with different width, different
    height, and different depth (from just R, G, B color channels to *K* number of
    channels). This operation is known as convolution.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: A ConvNet is going to basically be a deep network with layers of convolutions
    that stack together to form a pyramid like structure. You can see from the preceding
    figure that the network takes an image as an input of dimension (width x height
    x depth) and then applys convolutions progressively over it to reduce the spatial
    dimension while increasing the depth, which is roughly equivalent to its semantic
    complexity. Let's understand some of the common terminology in convent.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Each layer or depth in the image stack is called a feature map and patches
    or kernels are used for mapping three feature maps to *K* feature maps. A stride
    is the number of pixels that is shifted each time you move your filter. Depending
    on the type of padding a stride of 1 makes the output roughly the same size as
    the input. A stride of 2 makes it about half the size. In the case of valid padding,
    a sliding filter don''t cross the edge of the image, whereas in same-padding it
    goes off the edge and is padded with zeros to make the output map size exactly
    the same size as the input map:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图像堆栈中的每一层或深度叫做特征图（feature map），并且使用补丁或卷积核（kernels）将三个特征图映射到*K*个特征图。步幅（stride）是指每次移动滤波器时，像素的移动数量。根据填充方式，步幅为1时，输出的大小大致与输入相同；步幅为2时，输出约为输入的一半。对于有效填充（valid
    padding），滑动滤波器不会越过图像的边缘，而在同样填充（same-padding）情况下，滤波器会越过边缘，并且用零填充，使得输出图的大小与输入图的大小完全相同：
- en: '![Convolutional network (ConvNet)](img/B08086_01_15.jpg)![Convolutional network
    (ConvNet)](img/B08086_01_16.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![卷积网络（ConvNet）](img/B08086_01_15.jpg)![卷积网络（ConvNet）](img/B08086_01_16.jpg)'
- en: 'Figure 8c: Different terminology related to convolutional network'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图8c：与卷积网络相关的不同术语
- en: Deconvolution or transpose convolution
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反卷积或转置卷积
- en: In the case of a computer vision application where the resolution of final output
    is required to be larger than the input, deconvolution/transposed convolution
    is the de-facto standard. This layer is used in very popular applications such
    as GAN, image super-resolution, surface depth estimation from image, optical flow
    estimation, and so on.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要最终输出分辨率大于输入的计算机视觉应用，反卷积/转置卷积是事实上的标准。这一层被广泛应用于如GAN、图像超分辨率、从图像估计表面深度、光流估计等热门应用中。
- en: CNN in general performs down-sampling, that is, they produce output of a lower
    resolution than the input, whereas in deconvolution the layer up-samples the image
    to get the same resolution as the input image. Note since a naive up-sampling
    inadvertently loses details, a better option is to have a trainable up-sampling
    convolutional layer whose parameters will change during training.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: CNN通常执行下采样，即输出分辨率低于输入，而在反卷积中，层会对图像进行上采样，使其获得与输入图像相同的分辨率。需要注意的是，由于简单的上采样不可避免地会丢失细节，因此一个更好的选择是使用可训练的上采样卷积层，其参数会在训练过程中发生变化。
- en: 'Tensorflow method: `tf.nn.conv2d_transpose(value, filter, output_shape, strides,
    padding, name)`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Tensorflow方法：`tf.nn.conv2d_transpose(value, filter, output_shape, strides, padding,
    name)`
- en: Recurrent Neural Networks and LSTM
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 递归神经网络和LSTM
- en: The key idea behind **Recurrent Neural Networks** (**RNN**) is to share parameters
    over time. Imagine that you have a sequence of events, and at each point in time
    you want to make a decision about what's happened so far in this sequence. If
    the sequence is reasonably stationary, you can use the same classifier at each
    point in time. That simplifies things a lot already. But since this is a sequence,
    you also want to take into account the past-everything that happened before that
    point.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**递归神经网络**（**RNN**）的关键思想是共享参数。假设你有一系列事件，在每个时刻你需要根据到目前为止发生的事件做出决策。如果序列是相对稳定的，你可以在每个时刻使用相同的分类器，这样就简化了很多。但由于这是一个序列，你还需要考虑过去——即在某个时刻之前发生的所有事情。'
- en: 'RNN is going to have a single model responsible for summarizing the past and
    providing that information to your classifier. It basically ends up with a network
    that has a relatively simple repeating pattern, with part of the classifier connecting
    to the input at each time step and another part called the recurrent connection
    connecting you to the past at each step, as shown in the following figure:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: RNN将拥有一个单一模型，负责总结过去的内容并将这些信息提供给分类器。它基本上形成了一个具有相对简单重复模式的网络，其中分类器的一部分在每个时间步连接到输入，而另一部分叫做递归连接，在每一步将你与过去的内容连接，如下图所示：
- en: '![Recurrent Neural Networks and LSTM](img/B08086_01_18.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![递归神经网络和LSTM](img/B08086_01_18.jpg)'
- en: 'Figure 9a: Recurrent neural network'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图9a：递归神经网络
- en: '![Recurrent Neural Networks and LSTM](img/B08086_01_19.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![递归神经网络和LSTM](img/B08086_01_19.jpg)'
- en: 'Figure-9b: Long short-term memory (LSTM)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图-9b：长短期记忆（LSTM）
- en: '**LSTM** stands for **long short-term memory**. Now, conceptually, a recurrent
    neural network consists of a repetition of simple little units like this, which
    take as an input the past, a new input, and produce a new prediction and connect
    to the future. Now, what''s in the middle of that is typically a simple set of
    layers with some weights and linearities.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**LSTM**代表**长短期记忆**。从概念上讲，循环神经网络由简单的重复单元组成，这些单元以过去的信息、一个新的输入为输入，产生新的预测并与未来连接。其中心通常是一组简单的层，带有一些权重和线性激活。'
- en: In LSTM as shown in *Figure 9b*, the gating values for each gate get controlled
    by a tiny logistic regression on the input parameters. Each of them has its own
    set of shared parameters. And there's an additional hyperbolic tension sprinkled
    to keep the outputs between -1 and 1\. Also it's differentiable all the way, which
    means it can optimize the parameters very easily. All these little gates help
    the model keep its memory for longer when it needs to, and ignore things when
    it should.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在LSTM中，如*图9b*所示，每个门的门控值都由输入参数上的一个小型逻辑回归控制。每个门都有自己的一组共享参数。此外，还额外加入了双曲正切函数，以保持输出值在-1和1之间。并且它是可微分的，这意味着它可以很容易地优化参数。所有这些小门帮助模型在需要时保持更长时间的记忆，并在应该时忽略不重要的内容。
- en: Deep neural networks
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度神经网络
- en: The central idea of deep learning is to add more layers and make your model
    deeper. There are lots of good reasons to do that. One is parameter efficiency.
    You can typically get much more performance with fewer parameters by going deeper
    rather than wider.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的核心思想是增加更多的层次，使模型更深。这样做有很多好的理由。其中之一就是参数效率。通常，通过增加网络深度而非宽度，你可以用更少的参数获得更好的性能。
- en: Another one is that a lot of the natural phenomena that you might be interested
    in, tend to have a hierarchical structure, which deep models naturally capture.
    If you poke at a model for images, for example, and visualize what the model learns,
    you'll often find very simple things at the lowest layers, such as lines or edges.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个原因是，许多你可能感兴趣的自然现象，通常具有层次结构，而深度模型自然能够捕捉这一点。例如，如果你探究一个图像模型，并可视化模型学习到的内容，你通常会发现在最底层学到的是非常简单的东西，比如线条或边缘。
- en: '![Deep neural networks](img/B08086_01_20.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![深度神经网络](img/B08086_01_20.jpg)'
- en: 'Figure 10a: Deep neural networks'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图10a：深度神经网络
- en: '![Deep neural networks](img/B08086_01_21.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![深度神经网络](img/B08086_01_21.jpg)'
- en: 'Figure 10b: Network layers capturing hierarchical structure of image'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图10b：网络层捕捉图像的层次结构
- en: A very typical architecture for a ConvNet is a few layers alternating convolutions
    and max pooling, followed by a few fully connected layers at the top. The first
    famous model to use this architecture was LeNet-5 designed by Yann Lecun for character
    recognition back in 1998.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（ConvNet）的一种典型架构是几层交替进行卷积和最大池化，最后顶部接几层全连接层。第一个使用这种架构的著名模型是1998年Yann Lecun为字符识别设计的LeNet-5。
- en: Modern convolutional networks such as AlexNet, which famously won the competitive
    ImageNet object recognition challenge in 2012, use a very similar architecture
    with a few wrinkles. Another notable form of pooling is average pooling. Instead
    of taking the max, just take an average over the window of pixels around a specific
    location.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现代卷积神经网络，如AlexNet，在2012年著名赢得了ImageNet物体识别挑战赛，采用了与之非常相似的架构，并进行了一些调整。另一种值得注意的池化方式是平均池化。与其选择最大值，不如在特定位置周围的像素窗口上取平均值。
- en: Discriminative versus generative models
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 判别式模型与生成式模型
- en: A discriminative model learns the conditional probability distribution *p(y|x)*
    which could be interpreted as the *probability of y given x*. A discriminative
    classifier learns by observing data. It makes fewer assumptions on the distributions,
    but depends heavily on the quality of the data. The distribution *p(y|x)* simply
    classifies a given example x directly into a label *y*. For example, in logistic
    regression all we have to do is to learn weights and bias that would minimize
    the squared loss.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 判别式模型学习条件概率分布*p(y|x)*，这可以解释为*x给定y的概率*。判别式分类器通过观察数据来学习。它对分布的假设较少，但严重依赖数据的质量。分布*p(y|x)*简单地将给定的样本x直接分类到标签*y*中。例如，在逻辑回归中，我们所要做的就是学习能够最小化平方损失的权重和偏差。
- en: Whereas a generative model learns the joint probability distribution *p(x,y)*,
    where *x* is the input data and *y* is the label that you want to classify. A
    generative model can generate more samples by itself artificially, based on assumptions
    about the distribution of data. For example, in the Naive Bayes' model, we can
    learn *p(x)* from data, also *p(y)*, the prior class probabilities, and we can
    also learn *p(x|y)* from the data using say maximum likelihood.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 而生成模型学习的是联合概率分布*p(x,y)*，其中*x*是输入数据，*y*是你希望分类的标签。生成模型可以根据对数据分布的假设，自行生成更多样本。例如，在朴素贝叶斯模型中，我们可以从数据中学习*p(x)*，还可以学习*p(y)*，即先验类别概率，我们也可以使用最大似然估计从数据中学习*p(x|y)*。
- en: 'Once we have *p(x)*, *p(y)* and *p(x|y)*, *p(x, y)* is not difficult to find
    out. Now using Bayes'' rule, we can replace the *p(y|x)* with *(p(x|y)p(y))/p(x)*.
    And since we are just interested in the *arg max*, the denominator can be removed,
    as that will be the same for every *y*:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了*p(x)*，*p(y)*和*p(x|y)*，那么*p(x, y)*就不难找出来了。现在使用贝叶斯定理，我们可以将*p(y|x)*替换为*(p(x|y)p(y))/p(x)*。由于我们只关心*arg
    max*，分母可以被去掉，因为对于每个*y*来说它都是相同的：
- en: '![Discriminative versus generative models](img/B08086_01_25.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![判别模型与生成模型](img/B08086_01_25.jpg)'
- en: This is the equation we use in generative models, as *p(x, y) = p(x | y) p(y)*,
    which explicitly models the actual distribution of each class.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在生成模型中使用的方程，*p(x, y) = p(x | y) p(y)*，它明确地建模了每个类别的实际分布。
- en: In practice, the discriminative models generally outperform generative models
    in classification tasks, but the generative model shines over discriminative models
    in creativity/generation tasks.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，判别模型通常在分类任务中优于生成模型，但在创作/生成任务中，生成模型则胜过判别模型。
- en: Summary
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: So far you have refreshed various concepts related to deep learning and also
    learned how deep networks have evolved from the arena of supervised tasks of classifying
    an image, recognizing voice, text, and so on, towards the creative power through
    generative model. In the next chapter we will see how deep learning can be used
    for performing wonderful creativity tasks in the unsupervised domain using **Generative
    Adversarial Networks** (**GANs**).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经刷新了与深度学习相关的各种概念，并且学习了深度网络如何从监督任务的领域（例如分类图像、语音识别、文本等）演变到通过生成模型展现创作能力。在下一章中，我们将看到深度学习如何在无监督领域中利用**生成对抗网络**（**GANs**）执行精彩的创作任务。
