- en: Cancer Types Prediction Using Recurrent Type Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用递归类型网络进行癌症类型预测
- en: Large-scale cancer genomics data often comes in multiplatform and heterogeneous
    forms. These datasets impose great challenges in terms of the bioinformatics approach
    and computational algorithms. Numerous researchers have proposed to utilize this
    data to overcome several challenges, using classical machine learning algorithms
    as either the primary subject or a supporting element for cancer diagnosis and
    prognosis.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模的癌症基因组学数据通常以多平台和异构形式存在。这些数据集在生物信息学方法和计算算法方面带来了巨大的挑战。许多研究人员提出利用这些数据克服多个挑战，将经典的机器学习算法作为癌症诊断和预后的主要方法或辅助元素。
- en: In this chapter, we will use some deep learning architectures for cancer type
    classification from a very-high-dimensional dataset curated from The Cancer Genome
    Atlas (TCGA). First, we will describe the dataset and perform some preprocessing
    such that the dataset can be fed to our networks. We will then see how to prepare
    our programming environment, before moving on to coding with an open source, deep
    learning library called **Deeplearning4j** (**DL4J**). First, we will revisit
    the Titanic survival prediction problem again using a **Multilayer Perceptron**
    (**MLP**) implementation from DL4J.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将使用一些深度学习架构来进行癌症类型分类，数据来自The Cancer Genome Atlas（TCGA）中整理的高维数据集。首先，我们将描述该数据集并进行一些预处理，使得数据集可以输入到我们的网络中。然后，我们将学习如何准备编程环境，接下来使用一个开源深度学习库**Deeplearning4j**（**DL4J**）进行编码。首先，我们将再次回顾Titanic生存预测问题，并使用DL4J中的**多层感知器**（**MLP**）实现。
- en: Then we will use an improved architecture of **Recurrent Neural Networks** (**RNN**)
    called **Long Short-Term Memory** (**LSTM**) for cancer type prediction. Finally,
    we will see some frequent questions related to this project and DL4J hyperparameters/nets
    tuning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用一种改进的**递归神经网络**（**RNN**）架构，称为**长短期记忆**（**LSTM**），进行癌症类型预测。最后，我们将了解一些与此项目及DL4J超参数/网络调优相关的常见问题。
- en: 'In a nutshell, we will be learning the following topics in the chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，本章将学习以下内容：
- en: Deep learning in cancer genomics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 癌症基因组学中的深度学习
- en: Cancer genomics dataset description
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 癌症基因组学数据集描述
- en: Getting started with Deeplearning4j
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用Deeplearning4j
- en: Developing a cancer type predictive model using LSTM-RNN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LSTM-RNN开发癌症类型预测模型
- en: Frequently asked questions
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见问题
- en: Deep learning in cancer genomics
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 癌症基因组学中的深度学习
- en: Biomedical informatics includes all techniques regarding the development of
    data analytics, mathematical modeling, and computational simulation for the study
    of biological systems. In recent years, we've witnessed huge leaps in biological
    computing that has resulted in large, information-rich resources being at our
    disposal. These cover domains such as anatomy, modeling (3D printers), genomics,
    and pharmacology, among others.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 生物医学信息学包括与生物系统研究相关的数据分析、数学建模和计算仿真技术的开发。近年来，我们见证了生物计算的巨大飞跃，结果是大量信息丰富的资源已可供我们使用。这些资源涵盖了诸如解剖学、建模（3D打印机）、基因组学和药理学等多个领域。
- en: One of the most famous success stories of biomedical informatics is from the
    domain of genomics. The **Human Genome Project** (**HGP**) was an international
    research project with the objective of determining the full sequence of human
    DNA. This project has been one of the most important landmarks in computational
    biology and has been used as a base for other projects, including the Human Brain
    Project, which is determined to sequence the human brain. The data that was used
    in this thesis is also the indirect result of the HGP.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 生物医学信息学最著名的成功案例之一来自基因组学领域。**人类基因组计划**（**HGP**）是一个国际研究项目，旨在确定人类DNA的完整序列。这个项目是计算生物学中最重要的里程碑之一，并为其他项目提供了基础，包括致力于对人类大脑进行基因组测序的人类大脑计划。本文所使用的数据也是HGP的间接成果。
- en: The era of big data starts from the last decade or so, which was marked by an
    overflow of digital information in comparison to its analog counterpart. Just
    in the year 2016, 16.1 zettabytes of digital data were generated, and it is predicted
    to reach 163 ZB/year by 2025\. As good a piece of news as this is, there are some
    problems lingering, especially of data storage and analysis. For the latter, simple
    machine learning methods that were used in normal-size data analysis won't be
    effective anymore and should be substituted by deep neural network learning methods.
    Deep learning is generally known to deal very well with these types of large and
    complex datasets.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据时代大约从过去十年开始，标志着数字信息的激增，相比其模拟对手。仅在2016年，16.1泽字节的数字数据被生成，预计到2025年将达到每年163泽字节。虽然这是一则好消息，但仍然存在一些问题，尤其是在数据存储和分析方面。对于后者，简单的机器学习方法在常规数据分析中的应用已不再有效，应被深度神经网络学习方法所取代。深度学习通常被认为能非常有效地处理这些类型的大型复杂数据集。
- en: Along with other crucial areas, the biomedical area has also been exposed to
    these big data phenomena. One of the main largest data sources is omics data such
    as genomics, metabolomics, and proteomics. Innovations in biomedical techniques
    and equipment, such as DNA sequencing and mass spectrometry, have led to a massive
    accumulation of -omics data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他重要领域一样，生物医学领域也受到了大数据现象的影响。最主要的大型数据来源之一是诸如基因组学、代谢组学和蛋白质组学等 omics 数据。生物医学技术和设备的创新，如DNA测序和质谱分析，导致了
    -omics 数据的巨大积累。
- en: Typically -omics data is full of veracity, variability and high dimensionality.
    These datasets are sourced from multiple, and even sometimes incompatible, data
    platforms. These properties make these types of data suitable for applying DL
    approaches. Deep learning analysis of -omics data is one of the main tasks in
    the biomedical sector as it has a chance to be the leader in personalized medicine.
    By acquiring information about a person's omics data, diseases can be dealt with
    better and treatment can be focused on preventive measures.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，-omics 数据充满了真实性、变异性和高维度性。这些数据集来源于多个，甚至有时是不兼容的数据平台。这些特性使得这些类型的数据适合应用深度学习方法。对
    -omics 数据的深度学习分析是生物医学领域的主要任务之一，因为它有可能成为个性化医疗的领导者。通过获取一个人 omics 数据的信息，可以更好地应对疾病，治疗可以集中于预防措施。
- en: 'Cancer is generally known to be one of the deadliest diseases in the world,
    which is mostly due to its complexity of diagnosis and treatment. It is a genetic
    disease that involves multiple gene mutations. As the importance of genetic knowledge
    in cancer treatment is increasingly addressed, several projects to document the
    genetic data of cancer patients has emerged recently. One of the most well known
    is **The Cancer Genome Atlas** (**TCGA**) project, which is available on the TCGA
    research network: [http://cancergenome.nih.gov/](http://cancergenome.nih.gov/).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 癌症通常被认为是世界上最致命的疾病之一，主要是由于其诊断和治疗的复杂性。它是一种涉及多种基因突变的遗传性疾病。随着癌症治疗中遗传学知识重要性的逐渐受到重视，最近出现了多个记录癌症患者遗传数据的项目。其中最著名的项目之一是**癌症基因组图谱**（**TCGA**）项目，该项目可在TCGA研究网络上找到：[http://cancergenome.nih.gov/](http://cancergenome.nih.gov/)。
- en: As mentioned before, there have been a number of deep learning implementations
    in the biomedical sector, including cancer research. For cancer research, most
    researchers usually use -omics or medical imaging data as inputs. Several research
    works have focused on cancer analysis. Some of them use either a histopathology
    image or a PET image as a source. Most of that research focuses on classification
    based on that image data with **convolutional neural networks** (**CNNs**).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，生物医学领域，包括癌症研究，已经有许多深度学习应用。在癌症研究中，大多数研究者通常使用 -omics 或医学影像数据作为输入。多个研究工作聚焦于癌症分析。其中一些使用组织病理图像或PET图像作为数据来源。大多数研究集中于基于这些图像数据的分类，采用**卷积神经网络**（**CNNs**）。
- en: However, many of them use -omics data as their source. Fakoor et al. classified
    the various types of cancer using patients' gene expression data. Due to the different
    dimensionality of each data from each cancer type, they used **principal component
    analysis** (**PCA**) first to reduce the dimensionality of microarray gene expression
    data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，许多研究使用-omics 数据作为其数据来源。Fakoor 等人使用患者的基因表达数据对各种类型的癌症进行了分类。由于每种癌症类型的数据维度不同，他们首先使用**主成分分析**（**PCA**）来减少微阵列基因表达数据的维度。
- en: PCA is a statistical technique used to emphasize variation and extract the most
    significant patterns from a dataset; principal components are the simplest of
    the true eigenvector-based multivariate analyses. PCA is frequently used for making
    data exploration easy to visualize. Consequently, PCA is one of the most used
    algorithms in exploratory data analysis and for making predictive models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）是一种统计技术，用于强调数据的变化并提取数据集中最显著的模式；主成分是基于真实特征向量的最简单的多元分析方法。PCA通常用于使数据探索更易于可视化。因此，PCA是数据探索分析和构建预测模型中最常用的算法之一。
- en: Then they applied sparse and stacked autoencoders to classify various cancers,
    including acute myeloid leukemia, breast cancer, and ovarian cancer.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，他们应用稀疏和堆叠自编码器对多种癌症进行分类，包括急性髓性白血病、乳腺癌和卵巢癌。
- en: For detailed information, refer to the following publication, entitled *Using
    deep learning to enhance cancer diagnosis and classification* by R. Fakoor et
    al. in proceedings of the International Conference on Machine Learning, 2013.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有关详细信息，请参阅以下文献：《使用深度学习增强癌症诊断与分类》，作者：R. Fakoor 等人，发表于2013年国际机器学习会议论文集中。
- en: Ibrahim et al. , on the other hand, used miRNA expression data from six types
    of cancer genes/miRNA feature selection. They proposed a novel multilevel feature
    selection approach named **MLFS** (short for **Multilevel gene**/**miRNA feature
    selection**), which was based on **Deep Belief Networks (DBN)** and unsupervised
    active learning.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Ibrahim等人使用了来自六种癌症的基因/miRNA特征选择的miRNA表达数据。他们提出了一种新的多级特征选择方法，名为**MLFS**（**多级基因/miRNA特征选择**的简称），该方法基于**深度置信网络（DBN）**和无监督主动学习。
- en: You can read more in the publication titled *Multilevel gene/miRNA feature selection
    using deep belief nets and active learning* (R. Ibrahim, et al.) in Proceedings
    36th annual International Conference Eng. Med. Biol. Soc. (EMBC), pp. 3957-3960,
    IEEE, 2014.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下文献中阅读更多内容：《使用深度置信网络和主动学习的多级基因/miRNA特征选择》，作者：R. Ibrahim 等人，发表于2014年36届国际工程医学生物学学会年会（EMBC）论文集，页3957-3960，IEEE，2014。
- en: Finally, Liang et al. clustered ovarian and breast cancer patients using multiplatform
    genomics and clinical data. The ovarian cancer dataset contained gene expression,
    DNA methylation, and miRNA expression data across 385 patients, which were downloaded
    from **The Cancer Genome Atlas (TCGA)**.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Liang等人使用多平台基因组学和临床数据对卵巢癌和乳腺癌患者进行了聚类。卵巢癌数据集包含385名患者的基因表达、DNA甲基化和miRNA表达数据，这些数据从**癌症基因组图谱（TCGA）**下载。
- en: You can read more more in the following publication entitled *Integrative data
    analysis of multi-platform cancer data with a multimodal deep learning approach*
    (by M. Liang et al.) in Molecular Pharmaceutics, vol. 12, pp. 928{937, IEEE/ACM
    Transaction Computational Biology and Bioinformatics, 2015.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下文献中阅读更多内容：《多平台癌症数据的集成数据分析与多模态深度学习方法》，作者：M. Liang 等人，发表于《分子药学》期刊，卷12，页928-937，IEEE/ACM
    计算生物学与生物信息学学报，2015。
- en: The breast cancer dataset included GE data and corresponding clinical information,
    such as survival time and time to recurrence data, which was collected by the
    Netherlands Cancer Institute. To deal with this multiplatform data, they used
    **multimodal Deep Belief Networks** (**mDBN**).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 乳腺癌数据集包括基因表达数据和相应的临床信息，如生存时间和复发时间数据，这些数据由荷兰癌症研究所收集。为了处理这些多平台数据，他们使用了**多模态深度置信网络**（**mDBN**）。
- en: First, they implemented a DBN for each of those data to get their latent features.
    Then, another DBN used to perform the clustering is implemented using those latent
    features as the input. Apart from these researchers, much research work is going
    on to give cancer genomics, identification, and treatment a significant boost.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，他们为每种数据实现了一个深度置信网络（DBN）以获取其潜在特征。然后，另一个用于执行聚类的深度置信网络使用这些潜在特征作为输入。除了这些研究人员外，还有大量研究正在进行，旨在为癌症基因组学、识别和治疗提供重要推动。
- en: Cancer genomics dataset description
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 癌症基因组学数据集描述
- en: Genomics data covers all data related to DNA on living things. Although in this
    thesis we will also use other types of data like transcriptomic data (RNA and
    miRNA), for convenience purposes, all data will be termed as genomics data. Research
    on human genetics found a huge breakthrough in recent years due to the success
    of the HGP (1984-2000) on sequencing the full sequence of human DNA.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 基因组学数据涵盖与生物体DNA相关的所有数据。尽管在本论文中我们还将使用其他类型的数据，如转录组数据（RNA和miRNA），但为了方便起见，所有数据将统称为基因组数据。人类基因组学的研究在最近几年取得了巨大的突破，这得益于HGP（1984-2000）在测序人类DNA全序列方面的成功。
- en: 'One of the areas that have been helped a lot due to this is the research of
    all diseases related to genetics, including cancer. Due to various biomedical
    analyses done on DNA, there exist various types of -omics or genomics data. Here
    are some types of -omics data that were crucial to cancer analysis:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 受此影响最大的领域之一是与遗传学相关的所有疾病的研究，包括癌症。通过对DNA进行各种生物医学分析，出现了各种类型的-组学或基因组数据。以下是一些对癌症分析至关重要的-组学数据类型：
- en: '**Raw sequencing data:** This corresponds to the DNA coding of whole chromosomes.
    In general, every human has 24 types of chromosomes in each cell of their body,
    and each chromosome consists of 4.6-247 million base pairs. Each base pair can
    be coded in four different types, which are **adenine** (**A**), **cytosine**
    (**C**), **guanine** (**G**), and **thymine** (**T**). Therefore, raw sequencing
    data consists of billions of base pair data, with each coded in one of these four
    different types.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**原始测序数据**：这对应于整个染色体的DNA编码。一般来说，每个人体内的每个细胞都有24种染色体，每条染色体由4.6亿至2.47亿个碱基对组成。每个碱基对可以用四种不同的类型进行编码，分别是**腺嘌呤**（**A**）、**胞嘧啶**（**C**）、**鸟嘌呤**（**G**）和**胸腺嘧啶**（**T**）。因此，原始测序数据由数十亿个碱基对数据组成，每个碱基对都用这四种类型之一进行编码。'
- en: '**Single-Nucleotide Polymorphism** (**SNP**) data: Each human has a different
    raw sequence, which causes genetic mutation. Genetic mutation can cause an actual
    disease, or just a difference in physical appearance (such as hair color), or
    nothing at all. When this mutation happens only on a single base pair instead
    of a sequence of base pairs, it is called **Single-Nucleotide Polymorphism** (**SNP**).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单核苷酸多态性**（**SNP**）数据：每个人都有不同的原始序列，这会导致基因突变。基因突变可能导致实际的疾病，或者仅仅是外貌上的差异（如发色），也可能什么都不发生。当这种突变仅发生在单个碱基对上，而不是一段碱基对序列时，这被称为**单核苷酸多态性**（**SNP**）。'
- en: '**Copy Number Variation** (**CNV**) data: This corresponds to a genetic mutation
    that happens in a sequence of base pairs. Several types of mutation can happen,
    including deletion of a sequence of base pairs, multiplication of a sequence of
    base pairs, and relocation of a sequence of base pairs into other parts of the
    chromosome.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拷贝数变异**（**CNV**）数据：这对应于发生在碱基对序列中的基因突变。突变可以有多种类型，包括碱基对序列的缺失、碱基对序列的倍增以及碱基对序列在染色体其他部位的重排。'
- en: '**DNA methylation data**: Which corresponds to the amount of methylation (methyl
    group connected to base pair) that happens to areas in the chromosome. A large
    amount of methylation in promoter regions of a gene can cause gene repression.
    DNA methylation is the reason each of our organs acts differently even though
    all of them have the same DNA sequence. In cancer, this DNA methylation is disrupted.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DNA甲基化数据**：这对应于染色体上某些区域发生的甲基化量（甲基基团连接到碱基对上）。基因启动子区域的甲基化量过多可能会导致基因沉默。DNA甲基化是我们每个器官表现出不同功能的原因，尽管它们的DNA序列是相同的。在癌症中，这种DNA甲基化被破坏。'
- en: '**Gene expression data**: This corresponds to the number of proteins that were
    expressed from a gene at a given time. Cancer happens either because of high expression
    of an oncogene (that is, a gene that causes a tumor), low expression of a tumor
    suppressor gene (a gene that prevents a tumor), or both. Therefore, the analysis
    of gene expression data can help discover protein biomarkers in cancer. We will
    use this in this project.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基因表达数据**：这对应于某一时刻从基因中表达的蛋白质数量。癌症的发生通常是由于致癌基因（即引发肿瘤的基因）表达过高、抑癌基因（即防止肿瘤的基因）表达过低，或两者兼有。因此，基因表达数据的分析有助于发现癌症中的蛋白质生物标志物。我们将在本项目中使用这种数据。'
- en: '**miRNA expression data**: Corresponds to the amount of microRNA that was expressed
    at a given time. miRNA plays a role in protein silencing at the mRNA stage. Therefore,
    an analysis of gene expression data can help discover miRNA biomarkers in cancer.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**miRNA表达数据**：对应于在特定时间内表达的微小RNA的数量。miRNA在mRNA阶段起到蛋白质沉默的作用。因此，基因表达数据的分析有助于发现癌症中的miRNA生物标志物。'
- en: 'There are several databases of genomics datasets, where the aforementioned
    data can be found. Some of them focus on the genomics data of cancer patients.
    These databases include:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有多个基因组数据集的数据库，其中可以找到上述数据。它们中的一些专注于癌症患者的基因组数据。这些数据库包括：
- en: '**The Cancer Genome Atlas** (**TCGA**): **[https://cancergenome.nih.gov/](https://cancergenome.nih.gov/)**'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**癌症基因组图谱**（**TCGA**）：**[https://cancergenome.nih.gov/](https://cancergenome.nih.gov/)**'
- en: '**International Cancer Genome Consortium** (**ICGC**): **[https://icgc.org/](https://icgc.org/)**'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**国际癌症基因组联盟**（**ICGC**）：**[https://icgc.org/](https://icgc.org/)**'
- en: '**Catalog of Somatic Mutations in Cancer** (**COSMIC**): **[https://cancer.sanger.ac.uk/cosmic](https://cancer.sanger.ac.uk/cosmic)**'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**癌症体细胞突变目录**（**COSMIC**）：**[https://cancer.sanger.ac.uk/cosmic](https://cancer.sanger.ac.uk/cosmic)**'
- en: This genomics data is usually accompanied by clinical data of the patient. This
    clinical data can comprise general clinical information (for example, age or gender)
    and their cancer status (for example, cancer location or cancer stage). All of
    this genomics data itself has a characteristic of high dimensions. For example,
    the gene expression data for each patient is structured based on the gene ID,
    which reaches around 60,000 types.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基因组数据通常伴随着患者的临床数据。临床数据可以包括一般的临床信息（例如，年龄或性别）以及他们的癌症状态（例如，癌症的位置或癌症的分期）。所有这些基因组数据本身具有高维度的特点。例如，每个患者的基因表达数据是基于基因ID构建的，达到约60,000种类型。
- en: Moreover, some of the data itself comes from more than one format. For example,
    70% of the DNA methylation data is collected from breast cancer patients and the
    remaining 30% are curated from different platforms. Therefore, there are two different
    structures on in this dataset. Therefore, to analyze genomics data by dealing
    with the heterogeneity, researchers have often used powerful machine learning
    techniques or even deep neural networks.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些数据本身来自多个格式。例如，70%的DNA甲基化数据来自乳腺癌患者，剩余30%则是来自不同平台的整理数据。因此，这个数据集有两种不同的结构。因此，为了分析基因组数据并处理其异质性，研究人员通常采用强大的机器学习技术，甚至是深度神经网络。
- en: Now let's see what a real-life dataset looks like that can be used for our purpose.
    We will be using the gene expression cancer RNA-Seq dataset downloaded from the
    UCI machine learning repository (see [https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq#](https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq)
    for more information).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看一个可以用于我们目的的实际数据集。我们将使用从UCI机器学习库下载的基因表达癌症RNA-Seq数据集（有关更多信息，请参见[https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq#](https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq)）。
- en: '![](img/918516cc-0429-4221-a5e6-b4b6a07981f9.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/918516cc-0429-4221-a5e6-b4b6a07981f9.png)'
- en: 'The data collection pipeline for the pan-cancer analysis project (source: "Weinstein,
    John N., et al. ''The cancer genome atlas pan-cancer analysis project.'' Nature
    Genetics 45.10 (2013): 1113-1120")'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '泛癌症分析项目的数据收集流程（来源：“Weinstein, John N., et al. ''The cancer genome atlas pan-cancer
    analysis project.'' Nature Genetics 45.10 (2013): 1113-1120”）'
- en: 'This dataset is a random subset of another dataset reported in the following
    paper: Weinstein, John N., et al. *The cancer genome atlas pan-cancer analysis
    project*. *Nature Genetics 45.10 (2013): 1113-1120*. The preceding diagram shows
    the data collection pipeline for the pan-cancer analysis project.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '这个数据集是从以下论文中报告的另一个数据集的随机子集：Weinstein, John N., et al. *The cancer genome atlas
    pan-cancer analysis project*. *Nature Genetics 45.10 (2013): 1113-1120*。前面的图示展示了泛癌症分析项目的数据收集流程。'
- en: 'The name of the project is The Pan-Cancer analysis project. It assembled data
    from thousands of patients with primary tumors occurring in different sites of
    the body. It covered 12 tumor types (see the upper-left panel in the preceding
    figure) including:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 项目的名称是泛癌症分析项目。该项目汇集了来自不同部位发生原发肿瘤的数千名患者的数据。它涵盖了12种肿瘤类型（见前面图示的左上角面板），包括：
- en: '**Glioblastoma Multiform** (**GBM**)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**胶质母细胞瘤**（**GBM**）'
- en: '**Lymphoblastic acute myeloid leukemia** (**AML**)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**急性淋巴细胞白血病**（**AML**）'
- en: '**Head and Neck Squamous Carcinoma** (**HNSC**)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**头颈部鳞状细胞癌**（**HNSC**）'
- en: '**Lung Adenocarcinoma** (**LUAD**)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**肺腺癌**（**LUAD**）'
- en: '**lung Squamous Carcinoma** (**LUSC**)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**肺鳞状细胞癌** (**LUSC**)'
- en: '**Breast Carcinoma** (**BRCA**)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**乳腺癌** (**BRCA**)'
- en: '**kidney Renal Clear Cell Carcinoma** (**KIRC**)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**肾脏透明细胞癌** (**KIRC**)'
- en: '**ovarian Carcinoma** (**OV**)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卵巢癌** (**OV**)'
- en: '**Bladder Carcinoma** (**BLCA**)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**膀胱癌** (**BLCA**)'
- en: '**Colon Adenocarcinoma** (**COAD**)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结肠腺癌** (**COAD**)'
- en: '**Uterine Cervical and Endometrial Carcinoma** (**UCEC**)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**子宫颈和子宫内膜癌** (**UCEC**)'
- en: '**Rectal Adenocarcinoma** (**READ**)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**直肠腺癌** (**READ**)'
- en: 'This collection of data is part of the RNA-Seq (HiSeq) PANCAN dataset. It is
    a random extraction of gene expressions of patients having different types of
    tumors: BRCA, KIRC, COAD, LUAD, and PRAD.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这组数据是RNA-Seq (HiSeq) PANCAN数据集的一部分。它是从患有不同类型肿瘤的患者（如BRCA、KIRC、COAD、LUAD和PRAD）中随机提取的基因表达数据。
- en: This dataset is a random collection of cancer patients from 801 patients, each
    having 20,531 attributes. Samples (instances) are stored row-wise. Variables (attributes)
    of each sample are RNA-Seq gene expression levels measured by the illumina HiSeq
    platform. A dummy name (`gene_XX`) is given to each attribute. The attributes
    are ordered consistently with the original submission. For example, `gene_1` on
    `sample_0` is significantly and differentially expressed with a a value of `2.01720929003`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集是从801名癌症患者中随机收集的，每名患者有20,531个属性。样本（实例）按行存储。每个样本的变量（属性）是通过illumina HiSeq平台测量的RNA-Seq基因表达水平。每个属性都被赋予一个虚拟名称（`gene_XX`）。属性的顺序与原始提交一致。例如，`sample_0`上的`gene_1`的基因表达水平显著且有差异，数值为`2.01720929003`。
- en: 'When you download the dataset, you will see there are two CSV files:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当你下载数据集时，你会看到有两个CSV文件：
- en: '`data.csv`**:** Contains the gene expression data of each sample'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data.csv`**：** 包含每个样本的基因表达数据'
- en: '`labels.csv`**:** The labels associated with each sample'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels.csv`**：** 与每个样本相关的标签'
- en: 'Let''s take a look at the processed dataset. Note we will see only a few selected
    features considering the high dimensionality in the following screenshot, where
    the first column represents sample IDs (that is, anonymous patient IDs). The rest
    of the columns represent how a certain gene expression occurs in the tumor samples
    of the patients:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下处理过的数据集。请注意，由于高维度性，我们只会看到一些选择的特征，以下截图中第一列表示样本ID（即匿名患者ID）。其余列表示某些基因在患者肿瘤样本中的表达情况：
- en: '![](img/52a7de3c-2b87-45db-992b-18d3fa4cf1bc.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52a7de3c-2b87-45db-992b-18d3fa4cf1bc.png)'
- en: Sample gene expression dataset
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 样本基因表达数据集
- en: 'Now look at the labels in *Figure 3*. Here, `id` contains the sample ids and
    `Class` represents the cancer labels:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看一下*图3*中的标签。在这里，`id`包含样本ID，`Class`表示癌症标签：
- en: '![](img/daf22379-e09a-4d38-bfde-7ef397fa9d83.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/daf22379-e09a-4d38-bfde-7ef397fa9d83.png)'
- en: Samples are classified into different cancer types
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 样本被分类为不同的癌症类型
- en: Now you can imagine why I have chosen this dataset. Well, although we will not
    have so many samples, the dataset is still very high dimensional. In addition,
    this type of high-dimensional dataset is very suitable for applying a deep learning
    algorithm.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以理解为什么我选择了这个数据集。尽管我们没有太多样本，但这个数据集仍然是非常高维的。此外，这种高维数据集非常适合应用深度学习算法。
- en: Alright. Therefore, if the features and labels are given, can we classify these
    samples based on features and the ground truth. Why not? We will try to solve
    the problem with the DL4J library. First, we have to configure our programming
    environment so that we can start writing our codes.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。那么，如果给定了特征和标签，我们能否根据特征和真实标签对这些样本进行分类呢？为什么不呢？我们将尝试使用DL4J库解决这个问题。首先，我们需要配置我们的编程环境，以便开始编写代码。
- en: Preparing programming environment
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备编程环境
- en: 'In this section, we will discuss how to configure DL4J, ND4s, Spark, and ND4J
    before getting started with the coding. The following are prerequisites when working
    with DL4J:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何在开始编写代码之前配置DL4J、ND4s、Spark和ND4J。使用DL4J时需要的前提条件如下：
- en: Java 1.8+ (64-bit only)
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java 1.8+（仅限64位）
- en: Apache Maven for automated build and dependency manager
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于自动构建和依赖管理的Apache Maven
- en: IntelliJ IDEA or Eclipse IDE
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IntelliJ IDEA或Eclipse IDE
- en: Git for version control and CI/CD
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于版本控制和CI/CD的Git
- en: 'The following libraries can be integrated with DJ4J to enhance your JVM experience
    while developing your ML applications:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下库可以与DJ4J集成，以增强你在开发机器学习应用时的JVM体验：
- en: '**DL4J**: The core neural network framework, which comes up with many DL architectures
    and underlying functionalities.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DL4J**：核心神经网络框架，提供了许多深度学习架构和底层功能。'
- en: '**ND4J**: Can be considered as the NumPy of the JVM. It comes with some basic
    operations of linear algebra. Examples are matrix creation, addition, and multiplication.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ND4J**：可以被认为是JVM的NumPy。它包括一些基本的线性代数操作，例如矩阵创建、加法和乘法。'
- en: '**DataVec**: This library enables ETL operation while performing feature engineering.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataVec**：这个库在执行特征工程的同时，支持ETL操作。'
- en: '**JavaCPP**: This library acts as the bridge between Java and Native C++.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JavaCPP**：这个库充当Java与本地C++之间的桥梁。'
- en: '**Arbiter**: This library provides basic evaluation functionalities for the
    DL algorithms.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Arbiter**：这个库为深度学习算法提供基本的评估功能。'
- en: '**RL4J**: Deep reinforcement learning for the JVM.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RL4J**：为JVM提供深度强化学习。'
- en: '**ND4S**: This is a scientific computing library, and it also supports n-dimensional
    arrays for JVM-based languages.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ND4S**：这是一个科学计算库，并且它也支持JVM语言中的n维数组。'
- en: 'If you are using Maven on your preferred IDE, let''s define the project properties
    to mention the versions in the `pom.xml` file:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在你喜欢的IDE中使用Maven，我们可以在`pom.xml`文件中定义项目属性来指定版本：
- en: '[PRE0]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then use the following dependencies required for DL4J, ND4S, ND4J, and so on:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用以下依赖项，这些依赖项是DL4J、ND4S、ND4J等所需要的：
- en: '[PRE1]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'By the way, DL4J comes with Spark 2.1.0\. Additionally, if a native system
    BLAS is not configured on your machine, ND4J''s performance will be reduced. You
    will experience the following warning once you execute simple code written in
    Scala:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下，DL4J随Spark 2.1.0一起提供。此外，如果你的机器上没有配置本地系统BLAS，ND4J的性能会降低。当你执行Scala编写的简单代码时，你将看到以下警告：
- en: '[PRE2]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'However, installing and configuring BLAS such as `OpenBLAS` or `IntelMKL` is
    not that difficult; you can invest some time and do it. Refer to the following
    URL for further details: [http://nd4j.org/getstarted.html#open](http://nd4j.org/getstarted.html#open).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，安装和配置BLAS（如`OpenBLAS`或`IntelMKL`）并不难；你可以花些时间去完成它。更多细节可以参考以下网址：[http://nd4j.org/getstarted.html#open](http://nd4j.org/getstarted.html#open)。
- en: Well done! Our programming environment is ready for simple deep learning application
    development. Now it's time to get your hands dirty with some sample code.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！我们的编程环境已经准备好用于简单的深度学习应用开发。现在是时候动手写一些示例代码了。
- en: Titanic survival revisited with DL4J
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DL4J重新审视泰坦尼克号生存预测
- en: In the preceding chapter, we solved the Titanic survival prediction problem
    using Spark-based MLP. We also saw that by using Spark-based MLP, the user has
    very little transparency of using the layering structure. Moreover, it was not
    explicit to define hyperparameters and so on.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们使用基于Spark的MLP解决了泰坦尼克号生存预测问题。我们还看到，通过使用基于Spark的MLP，用户几乎无法了解层次结构的使用情况。此外，超参数等的定义也不够明确。
- en: 'Therefore, what I have done is used the training dataset and then performed
    some preprocessing and feature engineering. Then I randomly split the pre-processed
    dataset into training and testing (to be precise, 70% for training and 30% for
    testing). First, we create the Spark session as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我所做的是使用训练数据集，并进行了预处理和特征工程。然后，我将预处理后的数据集随机分为训练集和测试集（具体来说，70%用于训练，30%用于测试）。首先，我们按照如下方式创建Spark会话：
- en: '[PRE3]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In this chapter, we have seen that there are two CSV files. However, `test.csv`
    one does not provide any ground truth. Therefore, I decided to use only the `training.csv`
    one, so that we can compare the model''s performance. So let''s read the training
    dataset using the spark `read()` API:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到有两个CSV文件。然而，`test.csv`没有提供任何实际的标签。因此，我决定只使用`training.csv`文件，以便我们可以比较模型的性能。所以我们通过Spark的`read()`
    API读取训练数据集：
- en: '[PRE4]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We have seen in [Chapter 1](fba163f0-88c0-4278-a4a1-df5389cc3a10.xhtml), *Getting
    Started with Deep Learning* that the `Age` and `Fare` columns have many null values.
    So, instead of writing `UDF` for each column, here I just replace the missing
    values of the age and fare columns by their mean:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第一章](fba163f0-88c0-4278-a4a1-df5389cc3a10.xhtml)《深度学习入门》中看到，`Age`和`Fare`列有许多空值。因此，在这里，我直接用这些列的均值来替换缺失值，而不是为每一列编写`UDF`：
- en: '[PRE5]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To get more detailed insights into handling missing/null values and machine
    learning, interested readers can take a look at Boyan Angelov's blog at [https://towardsdatascience.com/working-with-missing-data-in-machine-learning-9c0a430df4ce](https://towardsdatascience.com/working-with-missing-data-in-machine-learning-9c0a430df4ce).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解如何处理缺失/空值和机器学习，感兴趣的读者可以阅读Boyan Angelov的博客，链接如下：[https://towardsdatascience.com/working-with-missing-data-in-machine-learning-9c0a430df4ce](https://towardsdatascience.com/working-with-missing-data-in-machine-learning-9c0a430df4ce)。
- en: 'For simplicity, we can drop a few more columns too, such as `"PassengerId"`,
    `"Name"`, `"Ticket"`, and `"Cabin"`:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们还可以删除一些列，例如“`PassengerId`”、“`Name`”、“`Ticket`”和“`Cabin`”：
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, here comes the tricky part. Similar to Spark ML-based estimators, DL4J-based
    networks also need training data in numeric form. Therefore, we now have to convert
    the categorical features into numerics. For that, we can use a `StringIndexer()`
    transformer. What we will do is we will create two that is, `StringIndexer` for
    the `"Sex"` and `"Embarked"` columns:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，进入难点了。类似于基于Spark ML的估计器，基于DL4J的网络也需要数字形式的训练数据。因此，我们现在必须将类别特征转换为数值。为此，我们可以使用`StringIndexer()`转换器。我们要做的是为“`Sex`”和“`Embarked`”列创建两个`StringIndexer`：
- en: '[PRE7]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then we will chain them into a single pipeline. Next, we will perform the transformation
    operation:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将它们串联成一个管道。接下来，我们将执行转换操作：
- en: '[PRE8]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then we will fit the pipeline, transform, and drop both the `"Sex"` and `"Embarked"`
    columns to get the transformed dataset:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们将拟合管道，转换数据，并删除“`Sex`”和“`Embarked`”列，以获取转换后的数据集：
- en: '[PRE9]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then our final pre-processed dataset will have only the numerical features.
    Note that DL4J considers the last column as the label column. That means DL4J
    will consider `"Pclass"`, `"Age"`, `"SibSp"`, `"Parch"`, `"Fare"`, `"sexIndex"`,
    and `"embarkedIndex"` as features. Therefore, I placed the `"Survived"` column
    as the last column:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们的最终预处理数据集将只包含数值特征。请注意，DL4J将最后一列视为标签列。这意味着DL4J会将“`Pclass`”、“`Age`”、“`SibSp`”、“`Parch`”、“`Fare`”、“`sexIndex`”和“`embarkedIndex`”视为特征。因此，我将“`Survived`”列放在了最后：
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then we randomly split the dataset into training and testing as 70% and 30%,
    respectively. That is, we used 70% for training and the rest to evaluate the model:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将数据集随机拆分为70%训练集和30%测试集。即，我们使用70%数据进行训练，剩余的30%用于评估模型：
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, we have both the DataFrames as separate CSV files to be used by DL4J:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将两个DataFrame分别保存为CSV文件，供DL4J使用：
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Additionally, DL4J does not support the header info in the training set, so
    I intentionally skipped writing the header.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，DL4J不支持训练集中的头信息，因此我故意跳过了写入头信息。
- en: Multilayer perceptron network construction
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知器网络构建
- en: As I informed you in the preceding chapter, DL4J-based neural networks are made
    of multiple layers. Everything starts with a `MultiLayerConfiguration`, which
    organizes those layers and their hyperparameters.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在前一章中提到的，基于DL4J的神经网络由多个层组成。一切从`MultiLayerConfiguration`开始，它组织这些层及其超参数。
- en: Hyperparameters are a set of variables that determine how a neural network would
    learn. There are many parameters, for example, how many times and how often to
    update the weights of the model (called an **epoch**), how to initialize network
    weights, which activation function to be used, which updater and optimization
    algorithms to be used, the learning rate (that is, how fast the model should learn),
    how many hidden layers are there, how many neurons are there in each layer, and
    so on.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数是一组决定神经网络学习方式的变量。有很多参数，例如：更新模型权重的次数和频率（称为**epoch**），如何初始化网络权重，使用哪种激活函数，使用哪种更新器和优化算法，学习率（即模型学习的速度），隐藏层有多少层，每层有多少神经元等等。
- en: 'We now create the network. First, let us create the layers. Similar to the
    MLP we created in [Chapter 1](fba163f0-88c0-4278-a4a1-df5389cc3a10.xhtml), *Getting
    Started with Deep Learning*, our MLP will have four layers:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来创建网络。首先，创建层。类似于我们在[第1章](fba163f0-88c0-4278-a4a1-df5389cc3a10.xhtml)中创建的MLP，*深度学习入门*，我们的MLP将有四层：
- en: '**Layer 0**: Input layer'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第0层**：输入层'
- en: '**Lauer 1**: Hidden layer 1'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第1层**：隐藏层1'
- en: '**Layer 2**: Hidden layer 2'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第2层**：隐藏层2'
- en: '**Layer 3**: Output layer'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第3层**：输出层'
- en: 'More technically, the first layer is the input layer, and then two layers are
    placed as hidden layers. For the first three layers, we initialized the weights
    using Xavier and the activation function is ReLU. Finally, the output layer is
    placed. This setting is shown in the following figure:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 更技术性地讲，第一层是输入层，然后将两层作为隐藏层放置。对于前三层，我们使用Xavier初始化权重，激活函数为ReLU。最后，输出层放置在最后。这一设置如下图所示：
- en: '![](img/afcd3570-cc02-41ad-b098-30f8d1fb93f5.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/afcd3570-cc02-41ad-b098-30f8d1fb93f5.png)'
- en: Multilayer perceptron for Titanic survival prediction input layer
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 泰坦尼克号生存预测的多层感知器输入层
- en: 'We have specified the neurons (that is, nodes), which are an equal number of
    inputs, and an arbitrary number of neurons as output. We set a smaller value considering
    very few inputs and features:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Hidden layer 1
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The number of input neurons is equal to the output of the input layer. Then
    the number of outputs is an arbitrary value. We set a smaller value considering
    very few inputs and features:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Hidden layer 2
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The number of input neurons is equal to the output of hidden layer 1\. Then
    the number of outputs is an arbitrary value. Again we set a smaller value considering
    very few inputs and features:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Output layer
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The number of input neurons is equal to the output of the hidden layer 1\. Then
    the number of outputs is equal to the number of predicted labels. We set a smaller
    value yet again, considering a very few inputs and features.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we used the Softmax activation function, which gives us a probability
    distribution over classes (the outputs sum to 1.0), and the losses function as
    cross-entropy for binary classification (XNET) since we want to convert the output
    (probability) to a discrete class, that is, zero or one:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: XNET is used for binary classification with logistic regression. Check out more
    about this in `LossFunctions.java` class in DL4J.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we create a `MultiLayerConfiguration` by specifying `NeuralNetConfiguration`
    before conducting the training. With DL4J, we can add a layer by calling `layer`
    on the `NeuralNetConfiguration.Builder()`, specifying its place in the order of
    layers (the zero-indexed layer in the following code is the input layer):'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Apart from these, we also specify how to set the network's weights. For example,
    as discussed, we use Xavier as the weight initialization and **Stochastic Gradient
    Descent** (**SGD**) optimization algorithm with Adam as the updater. Finally,
    we also specify that we do not need to do any pre-training (which is typically
    needed in DBN or stacked autoencoders). Nevertheless, since MLP is a feedforward
    network, we set backpropagation as true.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Network training
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we create a `MultiLayerNetwork` using the preceding `MultiLayerConfiguration`.
    Then we initialize the network and start the training on the training set:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the preceding code block, we start training the model by invoking the `model.fit()`
    on the training set (`trainingDataIt` in our case). Now we will discuss how we
    prepared the training and test set. Well, for reading the training set or test
    set that are in an inappropriate format (features are numeric and labels are integers),
    I have created a method called `readCSVDataset()`:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If you see the previous code block, you can realize that it is basically a
    wrapper that reads the data in CSV format, and then the `RecordReaderDataSetIterator()`
    method converts the record reader as a dataset iterator. Technically, `RecordReaderDataSetIterator()`
    is the main constructor for classification. It takes the following parameters:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '`RecordReader`: This is the `RecordReader` that provides the source of the
    data'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batchSize`: Batch size (that is, number of examples) for the output `DataSet`
    objects'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labelIndex`: The index of the label writable (usually an `IntWritable`) as
    obtained by `recordReader.next()`'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numPossibleLabels`: The number of classes (possible labels) for classification'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This will then convert the input class index (at position `labelIndex`, with
    integer values `0` to `numPossibleLabels-1`, inclusive) to the appropriate one-hot
    output/labels representation. So let''s see how to proceed. First, we show the
    path of training and test sets:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now let''s prepare the data we want to use for training:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, let''s prepare the data we want to classify:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Fantastic! We have managed to prepare the training and test `DataSetIterator`.
    Remember, we will be following nearly the same approach to prepare the training
    and test sets for other problems too.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the training has been completed, the next task would be evaluating the
    model. We will evaluate the model''s performance on the test set. For the evaluation,
    we will be using `Evaluation()`; it creates an evaluation object with two possible
    classes (survived or not survived). More technically, the Evaluation class computes
    the evaluation metrics such as precision, recall, F1, accuracy, and Matthews''
    correlation coefficient. The last one is used to evaluate a binary classifier.
    Now let''s take a brief overview on these metrics:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy** is the ratio of correctly predicted samples to total samples:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76057702-d43e-4861-ac12-6a036a553a83.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: '**Precision** is the ratio of correctly predicted positive samples to the total
    predicted positive samples:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ebcd216-45df-45d1-a389-681ce465fa30.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
- en: '**Recall** is the ratio of correctly predicted positive samples to all samples
    in the actual class—yes:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/557e92fb-f546-40eb-8e6f-f24718421d69.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: '**F1 score** is the weighted average (harmonic mean) of Precision and Recall::'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a931ec3c-90b7-41a3-9800-5114b46a9dc2.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: '**Matthews Correlation Coefficient** (**MCC**) is a measure of the quality
    of binary (two-class) classifications. MCC can be calculated directly from the
    confusion matrix as follows (given that TP, FP, TN, and FN are already available):'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69ca053d-0119-46ca-b39e-722ed60ced3c.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
- en: Unlike the Apache Spark-based classification evaluator, when solving a binary
    classification problem using the DL4J-based evaluator, special care should be
    taken for binary classification metrics such as F1, precision, recall, and so
    on.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, we will see these later on. First, let''s iterate the evaluation over
    every test sample and get the network''s prediction from the trained model. Finally,
    the `eval()` method checks the prediction against the true classes:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Oops! Unfortunately, we have not managed to achieve very high classification
    accuracy for class 1 (that is, 65%). Now, we compute another metric called MCC
    for this binary classification problem.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now let''s try to interpret this result based on the Matthews paper (see more
    at [www.sciencedirect.com/science/article/pii/0005279575901099](http://www.sciencedirect.com/science/article/pii/0005279575901099)),
    which describes the following properties: A correlation of C = 1 indicates perfect
    agreement, C = 0 is expected for a prediction no better than random, and C = -1
    indicates total disagreement between prediction and observation.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Following this, our result shows a weak positive relationship. Alright! Although
    we have not achieved good accuracy, you guys can still try by tuning hyperparameters
    or even by changing other networks such as LSTM, which we are going to discuss
    in the next section. But we'll do so for solving our cancer prediction problem,
    which is the main goal of this chapter. So stay with me!
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Cancer type prediction using an LSTM network
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we have seen what our data (that is, features and labels)
    looks like. Now in this section, we try to classify those samples according to
    their labels. However, as we have seen, DL4J needs the data in a well-defined
    format so that it can be used to train the model. So let us perform the necessary
    preprocessing and feature engineering.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Dataset preparation for training
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since we do not have any unlabeled data, I would like to select some samples
    randomly for test. Well, one more thing is that features and labels come in two
    separate files. Therefore, we can perform the necessary preprocessing and then
    join them together so that our pre-processed data will have features and labels
    together.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'Then the rest will be used for training. Finally, we''ll save the training
    and testing set in a separate CSV file to be used later on. First, let''s load
    the samples and see the statistics. By the way, we use the `read()` method of
    Spark but specify the necessary options and format too:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then we see some related statistics such as number of features and number of
    samples:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Therefore, there are `801` samples from `801` distinct patients and the dataset
    is too high in dimensions, having `20532` features. In addition, in *Figure 2*,
    we have seen that the `id` column represents only the patient''s anonymous ID,
    so we can simply drop it:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then we load the labels using the `read()` method of Spark and also specify
    the necessary options and format:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![](img/9d270be1-d0f0-4c68-b1c2-36f293e6f48b.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: We have already seen how the labels dataframe looks. We will skip the `id`.
    However, the `Class` column is categorical. Now, as I said, DL4J does not support
    categorical labels to be predicted. Therefore, we have to convert it to numeric
    (integer, to be more specific); for that I would use `StringIndexer()` from Spark.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create a `StringIndexer()`; we apply the index operation to the `Class`
    column and rename it as `label`. Additionally, we skip null entries:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then we perform the indexing operation by calling the `fit()` and `transform()`
    operations as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now let''s take a look at the indexed DataFrame:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![](img/3af96ada-3e47-48e5-a11b-9d22419e50a8.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: 'Fantastic! Now all of our columns (including features and labels) are numeric.
    Thus, we can join both features and labels into a single DataFrame. For that,
    we can use the `join()` method from Spark as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now we can generate both the training and test sets by randomly splitting the
    `combindedDF`, as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now let''s see the count of samples in each set:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Thus, our training set has `561` samples and the test set has `240` samples.
    Finally, save these two sets as separate CSV files to be used later on:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Now that we have the training and test sets, we can now train the network with
    the training set and evaluate the model with the test set. Considering the high
    dimensionality, I would rather try a better network such as LSTM, which is an
    improved variant of RNN. At this point, some contextual information about LSTM
    would be helpful to grasp the idea.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent and LSTM networks
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As discussed in [Chapter 1](fba163f0-88c0-4278-a4a1-df5389cc3a10.xhtml), *Getting
    Started with Deep Learning*, RNNs make use of information from the past; they
    can make predictions in data with high temporal dependencies. A more explicit
    architecture can be found in following diagram where the temporally shared weights
    **w2** (for the hidden layer) must be learned in addition to **w1** (for the input
    layer) and **w3** (for the output layer). From a computational point of view,
    an RNN takes many input vectors to process and generate output vectors. Imagine
    that each rectangle in the following diagram has a vectorial depth and other special
    hidden quirks:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2bc80381-93a3-4b60-90ce-4b00ea30e9a5.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: An RNN architecture where all weights in all layers have to be learned with
    time
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we often need to look at only recent information to perform the present
    task, rather than stored information or information that arrived a long time ago.
    This happens frequently in NLP for language modeling. Let''s see a common example:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cbde902a-d013-4124-b453-f9ba290d0684.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: If the gap between the relevant information is small, RNNs can learn to use
    past information
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we want to develop a DL-based NLP model to predict the next word based
    on the previous words. As a human being, if we try to predict the last word in
    *Berlin is the capital of...,* without further context, the next word is most
    likely *Germany*. In such cases, the gap between the relevant information and
    the position is small. Thus, RNNs can learn to use past information easily.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'However, consider another example that is a bit longer: *Reza grew up in Bangladesh.
    He studied in Korea. He speaks fluent...* Now to predict the last word, we would
    need a little bit more context. In this sentence, the most recent information
    advises the network that the next word would probably be the name of a language.
    However, if we narrow down to language level, the context of Bangladesh (from
    the previous words) would be needed.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de957c74-8ffe-4f84-a18a-14e11aa94883.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: If the gap between the relevant information and the place that is needed is
    bigger, RNNs can't learn to use past information
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Here, the gap is larger than in the previous example, so an RNN is unable to
    learn to map the information. Nevertheless, gradients for deeper layers are calculated
    by multiplication (that is, the product) of many gradients coming from activation
    functions in the multilayer network. If those gradients are very small or close
    to zero, gradients will easily vanish. On the other hand, when they are bigger
    than 1, it will possibly explode. So, it becomes very hard to calculate and update.
    Let's explain them in more detail.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: These two issues of RNN are jointly called a **vanishing-exploding gradient**
    problem, which directly affects performance. In fact, the backpropagation time
    rolls out the RNN, creating *a very deep* feedforward neural network. The impossibility
    of getting a long-term context from the RNN is precisely due to this phenomenon;
    if the gradient vanishes or explodes within a few layers, the network will not
    be able to learn high-temporal-distance relationships between the data.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the inability of handling long-term dependency, gradient exploding
    and vanishing problems is a serious drawback of RNNs. Here comes LSTM as the savior.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'As the name signifies, short-term patterns are not forgotten in the long term.
    An LSTM network is composed of cells (LSTM blocks) linked to each other. Each
    LSTM block contains three types of gates: an input gate, an output gate, and a
    forget gate. They implement the functions of writing, reading, and reset on the
    cell memory, respectively. These gates are not binary but analog (generally managed
    by a sigmoidal activation function mapped in the range *[0, 1]*, where zero indicates
    total inhibition and one shows total activation).'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'We can consider an LSTM cell very much like a basic cell, but still the training
    will converge more quickly and it will detect long-term dependencies in the data.
    Now the question would be: how does an LSTM cell work? The architecture of a basic
    LSTM cell is shown in the following diagram:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1fc7d7b2-006a-4aff-99c1-89e49f6facb1.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: Block diagram of an LSTM cell
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see the mathematical notation behind this architecture. If we don''t
    look at what''s inside the LSTM box, the LSTM cell itself looks exactly like a
    regular memory cell, except that its state is split into two vectors, *h(t)* and
    *c(t)*:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '*c* is a cell'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*h(t)* is the short-term state'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*c(t)* is the long-term state'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s open the box! The key idea is that the network can learn the following:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: What to store in the long-term state
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What to throw away
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What to read
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In more simplified words, in an STM, all hidden units of the original RNN are
    replaced by memory blocks, where each memory block contains a memory cell to store
    input history information and three gates to define how to update the information.
    These gates are an input gate, a forget gate, and an output gate.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: The presence of these gates allows LSTM cells to remember information for an
    indefinite period. In fact, if the input gate is below the activation threshold,
    the cell will retain the previous state, and if the current state is enabled,
    it will be combined with the input value. As the name suggests, the forget gate
    resets the current state of the cell (when its value is cleared to zero), and
    the output gate decides whether the value of the cell must be carried out or not.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the long-term state is copied and passed through the tanh function,
    internally in an LSTM cell, incorporation between two activation functions is
    needed. For example, in the following diagram, tanh decides which values to add
    to the state, with the help of the sigmoid gate:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21bcb1c3-96e4-41ff-b57f-6281f0e45715.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
- en: The internal organization of the LSTM cell structure
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Now, since this book is not meant to teach theory, I would like to stop the
    discussion here, but interested readers can find more details on the DL4J website
    at [https://deeplearning4j.org/lstm.html](https://deeplearning4j.org/lstm.html).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Dataset preparation
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we prepared the training and test sets. However, we
    need to put some extra efforts into making them consumable by DL4J. To be more
    specific, DL4J expects the training data as numeric and the last column to be
    the label column, and the remaining are features.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now try to prepare our training and test sets like that. First, we
    show the files where we saved the training and test sets:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then, we define the required parameters, such as the number of features, number
    of classes, and batch size. Here, I use `128` as the `batchSize` but adjust it
    accordingly:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This dataset is used for training:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This is the data we want to classify:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: If you see the preceding two lines, you can realize that `readCSVDataset()`
    is basically a wrapper that reads the data in CSV format, and then the `RecordReaderDataSetIterator()`
    method converts the record reader as a dataset iterator. For more details, refer
    to the *Titanic survival revisited with DL4J* section.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: LSTM network construction
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in the Titanic survival prediction section, again everything starts
    with `MultiLayerConfiguration`, which organizes those layers and their hyperparameters.
    Our LSTM network consists of five layers. The input layer is followed by three
    LSTM layers. Then the last layer is an RNN layer, which is also the output layer.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: More technically, the first layer is the input layer, and then three layers
    are placed as LSTM layers. For the LSTM layers, we initialized the weights using
    Xavier. We use SGD as the optimization algorithm with Adam updater and the activation
    function is tanh.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the RNN output layer has a softmax activation function, which gives
    us a probability distribution over classes (that is, outputs sum to *1.0*), and
    MCXENT, which is the Multiclass cross-entropy loss function. This setting is shown
    in the following figure:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2a036774-db9c-4e74-bbec-5e29b818ca01.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
- en: Multilayer perceptron for Titanic survival prediction. It takes 20,531 features
    and fixed bias (that is, 1) and generates multi-class outputs.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: For creating LSTM layers, DL4J provides both LSTM and GravesLSTM classes. The
    latter is an LSTM recurrent net, based on *Supervised Sequence Labelling with
    Recurrent Neural Networks* (see more at [http://www.cs.toronto.edu/~graves/phd.pdf](http://www.cs.toronto.edu/~graves/phd.pdf)).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: GravesLSTM is not compatible with CUDA. Thus, using LSTM is recommended while
    performing the training on GPU. Otherwise, GravesLSTM is faster than LSTM.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'Now before, we start creating the network, let''s define required hyperparameters
    such as the number of input/hidden/output nodes (neurons):'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We now create a network configuration and conduct network training. With DL4J,
    you add a layer by calling `layer` on the `NeuralNetConfiguration.Builder()`,
    specifying its place in the order of layers (the zero-indexed layer in the following
    code is the input layer):'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Finally, we also specify that we do not need to do any pre-training (which is
    typically needed in DBN or stacked autoencoders).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Network training
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we create a `MultiLayerNetwork` using the preceding `MultiLayerConfiguration`.
    Then we initialize the network and start the training on the training set:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Typically, this type of network has so many hyperparameters. Let''s print the
    number of parameters in the network (and for each layer):'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: As I said, our network has 910 million parameters, which is huge. This also
    poses a great challenge while tuning hyperparameters. However, we will see some
    tricks in the FAQs section.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the training has been completed, the next task would be evaluating the
    model. We will evaluate the model''s performance on the test set. For the evaluation,
    we will be using `Evaluation(). This method` creates an evaluation object with
    five possible classes. First, let''s iterate the evaluation over every test sample
    and get the network''s prediction from the trained model. Finally, the `eval()`
    method checks the prediction against the true class:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Wow! Unbelievable! Our LSTM network has accurately classified the samples so
    accurately. Finally, let''s see how the classifier predicts across each class:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The predictive accuracy for cancer type prediction using LSTM is suspiciously
    higher. Did our network underfit? Is there any way to observe how the training
    went? In other words, the question would be why our LSTM neural net shows 100%
    accuracy. We will try to answer these questions in the next section. So stay with
    me!
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Frequently asked questions (FAQs)
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have solved the Titanic survival prediction problem with an acceptable
    level of accuracy, there are other practical aspects of this problem and overall
    deep learning phenomena that need to be considered too. In this section, we will
    see some frequently asked questions that might be already in your mind. Answers
    to these questions can be found in *Appendix A*.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Can't we use MLP to solve the cancer type prediction by handling this too high-dimensional
    data?
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which activation and loss function can be used with RNN type nets?
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the best way of recurrent net weight initialization?
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which updater and optimization algorithm should be used?
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the Titanic survival prediction problem, we did not experience good accuracy.
    What could be possible reasons and how can we improve the accuracy?
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The predictive accuracy for cancer type prediction using LSTM is suspiciously
    higher. Did our network underfit? Is there any way to observe how the training
    went?
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which type RNN variants should I use, that is, LSTM or GravesLSTM?
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is my neural net throwing nan score values?
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to configure/change the DL4J UI port?
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how to classify cancer patients on the basis of tumor
    types from a very-high-dimensional gene expression dataset curated from TCGA.
    Our LSTM architecture managed to achieve 100% accuracy, which is outstanding.
    Nevertheless, we discussed many aspects of DL4J, which will be helpful in upcoming
    chapters. Finally, we saw answers to some frequent questions related to this project,
    LSTM network, and DL4J hyperparameters/nets tuning.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how to develop an end-to-end project for handling
    a multilabel (each entity can belong to multiple classes) image classification
    problem using CNN based on Scala and the DL4J framework on real Yelp image datasets.
    We will also discuss some theoretical aspects of CNNs before getting started.
    Nevertheless, we will discuss how to tune hyperparameters for better classification
    results.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Answers to questions
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Answer** **to question 1**: The answer is yes, but not very comfortably.
    That means a very deep feedforward network such as deep MLP or DBN can classify
    them with too many iterations.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'However, also to speak frankly, MLP is the weakest deep architecture and is
    not ideal for very high dimensions like this. Moreover, DL4J has deprecated DBN
    since the DL4J 1.0.0-alpha release. Finally, I would still like to show an MLP
    network config just in case you want to try it:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Then, just change the line from `MultiLayerNetwork model = new MultiLayerNetwork(LSTMconf);`
    to `**MultiLayerNetwork** model = **new** **MultiLayerNetwork**(MLPconf);`. Readers
    can see the full source in the `CancerPreddictionMLP.java` file.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer** **to question 2**: There are two aspects to be aware of with regard
    to the choice of activation function.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '**Activation** **function for hidden layers:** Usually, ReLU or leakyrelu activations
    are good choices. Some other activation functions (tanh, sigmoid, and so on) are
    more prone to vanishing gradient problems. However, for LSTM layers, the tanh
    activation function is still commonly used.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'A note here: The reason some people do not want to use Rectified Linear Unit
    (ReLU) is that it seems not to perform very well relative to smoother nonlinearity,
    such as sigmoid in the case of RNNs (see more at [https://arxiv.org/pdf/1312.4569.pdf](https://arxiv.org/pdf/1312.4569.pdf)).
    Even tanh works much better with LSTM. Therefore, I used tanh as the activation
    function in the LSTM layers.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '**The activation** **function for the output layer:** For classification problems,
    using the Softmax activation function combined with the negative log-likelihood
    / MCXENT is recommended. However, for a regression problem, the "IDENTITY" activation
    function is a good choice, with MSE as the loss function. In short, the choice
    is really application-specific.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer** **to question 3:** Well, we need to make sure that the network weights
    are neither too big nor too small. I will not recommend using random or zero;
    rather, Xavier weight initialization is usually a good choice for this.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer** **to question 4:** Unless SGD converges well, momentum/rmsprop/adagrad
    optimizers are a good choice. However, I''ve often used Adam as the updater and
    observed good performance too.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer** **to question 5:** Well, there is no concrete answer to this question.
    In fact, there could be several reasons. For example, probably we have not chosen
    the appropriate hyperparameters. Secondly, we may not have enough data. Thirdly,
    we could be using another network such as LSTM. Fourthly, we did not normalize
    our data.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, for the third one, you can of course try using an LSTM network similarly;
    I did it for cancer type prediction. For the fourth one, of course normalized
    data always gives better classification accuracy. Now the question would be: what
    is the distribution of your data? Are you scaling it properly? Well, continuous
    values have to be in the range of -1 to 1, 0 to 1, or distributed normally with
    mean 0 and standard deviation 1.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, I would like to give you a concrete example of data normalization
    in the Titanic example. For that, we can use DL4J''s `NormalizerMinMaxScaler()`.
    Once we created the training dataset iterator, we can instantiate a `NormalizerMinMaxScaler()`
    object and then normalize the data by invoking the `fit()` method. Finally, we
    perform the transformation using the `setPreProcessor()` method, as follows:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now, for the test dataset iterator, we apply the same normalization for better
    results but without invoking the `fit()` method:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'More elaborately, `NormalizerMinMaxScaler ()` acts as the pre-processor for
    datasets that normalizes feature values (and optionally label values) to lie between
    a minimum and maximum value (by default, between 0 and 1). Readers can see the
    full source in the `CancerPreddictionMLP.java` file. After this normalization,
    I experienced slightly better result for class 1, as follows (you could try the
    same for class 0 too):'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '**Answer to question 6:** In the real world, it''s a rare case that a neural
    net would achieve 100% accuracy. However, if the data is linearly separable, then
    yes, it''s possible! Take a look at the following scatter plots, which show that
    the black line clearly separates the red points and the dark blue points:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/184293ff-a744-4941-b1c5-8084d6786233.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
- en: Very clearly and linearly separable data points
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: More technically, since a neuron's output (before it passes through an activation
    function) is a linear combination of its inputs, a network consisting of a single
    neuron can learn this pattern. That means if our neural net got the line right,
    it is possible to achieve 100% accuracy.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to answer the second part: probably no. To prove this, we can observe
    the training loss, score, and so on on the DL4J UI, which is the interface used
    to visualize the current network status and progress of training in real time
    on your browser.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: The UI is typically used to help with tuning neural networks, that is, the selection
    of hyperparameters to obtain good performance for a network. These are already
    in the `CancerPreddictionLSTM.java` file, so do not worry but just keep going.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Adding the dependency for DL4J to your project**'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following dependency tag, `_2.11 suffix` is used to specify which Scala
    version should be used for the Scala play framework. You should setting accordingly:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '**Step 2: Enabling UI in your project**'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'This is relatively straightforward. First, you have to initialize the user
    interface backend as follows:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'You then configure where the network information is to be stored. Then the
    StatsListener can be added to collect this information:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Finally, we attach the StatsStorage instance to the UI:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '**Step 3: Start collecting information by invoking the fit() method** Information
    will then be collected and routed to the UI when you call the `fit` method on
    your network.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4: Accessing the UI** Once it is configured, the UI can be accessed
    at [http://localhost:9000/train](http://localhost:9000/train). Now to answer "Did
    our network under fit? Is there any way to observe how the training went?" We
    can observe the **Model Score versus Iteration Chart** on the overview page**.**
    As suggested in the model tuning section at [https://deeplearning4j.org/visualization](https://deeplearning4j.org/visualization)**,**
    we have the following observation**:**'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: The overall score versus iteration should go down over time
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The score does not increase consistently but decreases drastically when the
    iteration moves on
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The issue might be that there is no noise in the line chart, which is ideally
    expected (that is, the line will go up and down within a small range).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Now to deal with this, again we can normalize the data and perform the training
    again to see how the performance differs. Well, I would like to leave this up
    to you, folks. One more clue would be following the same data normalization that
    we discussed in question 5.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2cbc83a2-105b-4027-a354-c9bddef68089.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
- en: LSTM model score over iterations
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, another observation would be worth mentioning too. For example, the gradients
    did not vanish until the end, which becomes clearer from this figure:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49b98b00-61ba-44dc-bb37-9622cb4545c3.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
- en: The LSTM network's gradients across different iterations
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the activation functions performed their role consistently, which
    becomes clearer from the following figure:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c7ef990-ef40-45d1-948f-343bfa31c34f.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
- en: The LSTM network's activation functions performed their role consistently across
    different layers
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: The thing is that there are many more factors to be considered too. However,
    in reality, tuning a neural network is often more an art than a science, and we
    have not considered many aspects as I said. Yet, do not worry; we will see them
    in upcoming projects. So hang on and let's move on to the next question.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer** **to question 7:** LSTM allows both GPU/CUDA support, but GravesLSTM
    supports only CUDA, hence no support for CuDNN yet. Nonetheless, if you want faster
    training and convergence, using LSTM type is recommended.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer** **to question 8:** While training a neural network, the backpropagation
    involves multiplications across very small gradients. This happens due to limited
    precision when representing real numbers; values very close to zero cannot be
    represented.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: It introduces an arithmetic underflow issue, which often happens in a deeper
    network such as DBN, MLP, or CNN. Moreover, if your network throws NaN, then you'll
    need to retune your network to avoid very small gradients.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer** **to question 9:** You can set the port by using the org.deeplearning4j.ui.port
    system property. To be more specific, to use port `9001` for example, pass the
    following to the JVM on launch:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '`-Dorg.deeplearning4j.ui.port=9001`'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
