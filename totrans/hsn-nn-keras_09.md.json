["```py\n ! pip install gym \n```", "```py\nimport numpy as np\nimport gym\nfrom gym import envs\n\n# This will print allavailable environemnts\n# print(envs.registry.all())\n\n```", "```py\n# running env.reset() returns the initial state of the environment\n\nprint('Initial state of environment:' , env.reset())\nenv.render()\n```", "```py\nenv.observation_space.n\n\n500\n```", "```py\nenv.action_space.n\n\n6\n```", "```py\n#render current position\nenv.render()\n\n#move down\nenv.step(0)\n\n#render new position\nenv.render()\n```", "```py\n# env.step(i) will return four variables\n# They are defiend as (in order) the state, reward, done, info\nenv.step(1)\n\n(204, -1, False, {'prob': 1.0})\n```", "```py\n# Overriding current state, used for reproducibility\nstate = env.env.s = 114\n\n# counter tracks number of moves made\ncounter = 0\n\n#No reward to begin with\nreward = None\n\ndropoffs = 0\n\n#loop through random actions until successful dropoff (20 points)\n\nwhile reward != 20:\n    state, reward, done, info = env.step(env.action_space.sample())\n    counter += 1\n    print(counter)\n    env.render()\n\nprint(counter, dropoffs)\n```", "```py\n#Q-table, functions as agent's memory of state action pairs\nQ = np.zeros([env.observation_space.n, env.action_space.n])\n\n#track reward\nR = 0\n\n#discount factor\ngamma = 0.85\n\n# Track successful dropoffs\ndropoffs_done = 0\n\n#Run for 1000 episodes\nfor episode in range(1,1001):\n    done = False\n\n    #Initialize reward\n    R, reward = 0,0\n\n    #Initialize state\n    state = env.reset()\n\n    counter=0\n\n    while done != True:\n\n            counter+=1\n\n            #Pick action with highest Q value\n            action = np.argmax(Q[state]) \n\n            #Stores future state for compairason\n            new_state, reward, done, info = env.step(action)\n\n            #Update state action pair using reward and max Q-value for the new state\n            Q[state,action] += gamma * (reward + np.max(Q[new_state]) - Q[state,action]) \n\n            #Update reward\n            R += reward  \n\n            #Update state\n            state = new_state\n\n            #Check how many times agent completes task\n            if reward == 20:\n                dropoffs_done +=1\n\n    #Print reward every 50 episodes        \n    if episode % 50 == 0:\n        print('Episode {}   Total Reward: {}   Dropoffs done: {}  Time-Steps taken {}'\n              .format(episode,R, dropoffs_done, counter))\n\nEpisode 50 Total Reward: -30 Dropoffs done: 19 Time-Steps taken 51\nEpisode 100 Total Reward: 14 Dropoffs done: 66 Time-Steps taken 7\nEpisode 150 Total Reward: -5 Dropoffs done: 116 Time-Steps taken 26\nEpisode 200 Total Reward: 14 Dropoffs done: 166 Time-Steps taken 7\nEpisode 250 Total Reward: 12 Dropoffs done: 216 Time-Steps taken 9\nEpisode 300 Total Reward: 5 Dropoffs done: 266 Time-Steps taken 16\n```", "```py\n ! pip install keras-rl\n```", "```py\n! pip install --no-index -f https://github.com/Kojoley/atari- \n  py/releases atari_py\n```", "```py\nfrom PIL import Image\nimport numpy as np\nimport gym\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\nfrom keras.optimizers import Adam\nimport keras.backend as K\n\nfrom rl.agents.dqn import DQNAgent\nfrom rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\nfrom rl.memory import SequentialMemory\nfrom rl.core import Processor\nfrom rl.callbacks import FileLogger, ModelIntervalCheckpoint\n```", "```py\nclass AtariProcessor(Processor):\n\n    def process_observation(self, observation):\n        # Assert dimension (height, width, channel) \n        assert observation.ndim == 3 \n        # Retrieve image from array\n        img = Image.fromarray(observation) \n        # Resize and convert to grayscale\n        img = img.resize(INPUT_SHAPE).convert('L') \n        # Convert back to array\n        processed_observation = np.array(img) \n        # Assert input shape\n        assert processed_observation.shape == INPUT_SHAPE \n        # Save processed observation in experience memory (8bit)\n        return processed_observation.astype('uint8')\n   def process_state_batch(self, batch):\n      #Convert the batches of images to float32 datatype\n       processed_batch = batch.astype('float32') / 255.\n       return processed_batch\n   def process_reward(self, reward):\n       return np.clip(reward, -1., 1.) # Clip reward\n```", "```py\nenv = gym.make('SpaceInvaders-v0')\nnp.random.seed(123)\nenv.seed(123)\nnb_actions = env.action_space.n\n```", "```py\ninput_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n# Build Conv2D model\nmodel = Sequential()\nmodel.add(Permute((2, 3, 1), input_shape=input_shape))\nmodel.add(Convolution2D(32, (8, 8), strides=(4, 4), activation='relu'))\nmodel.add(Convolution2D(64, (4, 4), strides=(2, 2), activation='relu'))\nmodel.add(Convolution2D(64, (3, 3), strides=(1, 1), activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\n# Last layer: no. of neurons corresponds to action space\n# Linear activation\nmodel.add(Dense(nb_actions, activation='linear'))  \nprint(model.summary())\n\n_____________________________________________________________\nLayer (type) Output Shape Param # \n=============================================================\npermute_2 (Permute) (None, 84, 84, 4) 0 \n_____________________________________________________________\nconv2d_4 (Conv2D) (None, 20, 20, 32) 8224 \n_____________________________________________________________\nconv2d_5 (Conv2D) (None, 9, 9, 64) 32832 \n_____________________________________________________________\nconv2d_6 (Conv2D) (None, 7, 7, 64) 36928 \n_____________________________________________________________\nflatten_2 (Flatten) (None, 3136) 0 \n_____________________________________________________________\ndense_3 (Dense) (None, 512) 1606144 \n_____________________________________________________________\ndense_4 (Dense) (None, 6) 3078 \n=============================================================\nTotal params: 1,687,206\nTrainable params: 1,687,206\nNon-trainable params: 0\n______________________________________________________________\nNone\n\n```", "```py\nmemory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n```", "```py\npolicy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n                              attr='eps',\n                              value_max=1.,\n                              value_min=.1,\n                              value_test=.05,\n                              nb_steps=1000000)\n```", "```py\n#Initialize the atari_processor() class\n\nprocessor = AtariProcessor()\n\n# Initialize the DQN agent \ndqn = DQNAgent(model=model,             #Compiled neural network model\n               nb_actions=nb_actions,   #Action space\n               policy=policy,   #Policy chosen (Try Boltzman Q policy)\n               memory=memory,   #Replay memory (Try Episode Parameter  \n                                                memory)\n               processor=processor,     #Atari processor class\n#Warmup steps to ignore initially (due to random initial weights)\n               nb_steps_warmup=50000,   \n               gamma=.99,                #Discount factor\n               train_interval=4,         #Training intervals\n               delta_clip=1.,            #Reward clipping\n              )\n```", "```py\ndqn.compile(optimizer=Adam(lr=.00025), metrics=['mae'])\n```", "```py\ndqn.fit(env, nb_steps=1750000)   #visualize=True\n\nTraining for 1750000 steps ...\nInterval 1 (0 steps performed)\n 2697/10000 [=======>....................] - ETA: 26s - reward: 0.0126\n```", "```py\ndqn.test(env, nb_episodes=10, visualize=True)\n\nTesting for 10 episodes ...\nEpisode 1: reward: 3.000, steps: 654\nEpisode 2: reward: 11.000, steps: 807\nEpisode 3: reward: 8.000, steps: 812\nEpisode 4: reward: 3.000, steps: 475\nEpisode 5: reward: 4.000, steps: 625\nEpisode 6: reward: 9.000, steps: 688\nEpisode 7: reward: 5.000, steps: 652\nEpisode 8: reward: 12.000, steps: 826\nEpisode 9: reward: 2.000, steps: 632\nEpisode 10: reward: 3.000, steps: 643\n\n<keras.callbacks.History at 0x24280aadc50>\n```", "```py\ninitialize replay memory\ninitialize Q-Value function with random weights\nsample initial state from environment\nKeep repeating:\n\n     choose an action to perform:\n            with probability ε select a random action\n            otherwise select action with argmax a Q(s, a')\n     execute chosen action\n     collect reward and next state\n     save experience <s, a, r, s'> in replay memory\n\n     sample random transitions <s, a, r, s'> from replay memory\n     compute target variable for each mini-batch transition:\n\n             if s' is terminal state then target = r\n             otherwise t = r + γ max a'Q(s', a')\n     train the network with loss (target - Q(s,a)`^2)\n     s = s'\n\nuntil done\n```", "```py\ndouble_dqn = DQNAgent(model=model,\n               nb_actions=nb_actions,\n               policy=policy,\n               memory=memory,\n               processor=processor,\n               nb_steps_warmup=50000,\n               gamma=.99, \n               target_model_update=1e-2,\n               train_interval=4,\n               delta_clip=1.,\n               enable_double_dqn=True,\n              )\n```", "```py\ndueling_dqn = DQNAgent(model=model,\n               nb_actions=nb_actions,\n               policy=policy,\n               memory=memory,\n               processor=processor,\n               nb_steps_warmup=50000,\n               gamma=.99, \n               target_model_update=10000,\n               train_interval=4,\n               delta_clip=1.,\n               enable_dueling_network=True,\n               dueling_type='avg'\n              )\n```"]