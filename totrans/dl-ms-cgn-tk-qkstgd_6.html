<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Working with Time Series Data</h1>
                </header>
            
            <article>
                
<p>Classifying images with a neural network is one of the most iconic jobs in deep learning. But it certainly isn't the only job that neural networks excel at. Another area where there's a lot of research happening is recurrent neural networks.</p>
<p>In this chapter, we'll dive into recurrent neural networks, and how they can be used in scenarios where you have to deal with time series data; for example, in an IoT solution where you need to predict temperatures or other important values.</p>
<p>The following topics are covered in this chapter:</p>
<ul>
<li>What are recurrent neural networks?</li>
<li>Usage scenarios for recurrent neural networks</li>
<li>How do recurrent neural networks work</li>
<li>Building recurrent neural networks with CNTK</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>We assume that you have a recent version of Anaconda installed on your computer, and have followed the steps in <a href="9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml">Chapter 1</a>, <em>Getting Started with CNTK</em>, to install CNTK on your computer. The sample code for this chapter can be found in our<span> </span>GitHub<span> </span>repository at<span> <a href="https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch6">https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch6</a></span>.</p>
<p>In this<span> </span>chapter<span>, </span>we'll work on an example stored in Jupyter notebooks. To access the sample code, run the following commands inside an Anaconda prompt in the directory where you've downloaded the code:</p>
<pre><strong>cd ch6</strong><br/><strong>jupyter notebook</strong></pre>
<p>The sample code is stored in the <kbd>Training recurrent neural networks.ipynb</kbd> notebook. Please be aware that running the sample code for this chapter will take a long time if you don't have a machine with a GPU that can be used by CNTK.</p>
<p>Check out the following video to see the Code in Action:<br/>
<a href="http://bit.ly/2TAdtyr">http://bit.ly/2TAdtyr</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What are recurrent neural networks?</h1>
                </header>
            
            <article>
                
<p>Recurrent neural networks are a special breed of neural networks that are capable of reasoning over time. They are primarily used in scenarios where you have to deal with values that change over time.</p>
<p>In a regular neural network, you can provide only one input, which results in one prediction. This limits what you can do with a regular neural network. For example, regular neural networks are not good at translating text, while there have been quite a few successful experiments with recurrent neural networks in translation tasks.</p>
<p>In a recurrent neural network, it is possible to provide a sequence of samples that result in a single prediction. You can also use a recurrent neural network to predict an output sequence based on a single input sample. Finally, you can predict an output sequence based on an input sequence.</p>
<p>As with the other types of neural networks, you can use recurrent neural networks in classification jobs as well as regression tasks, although it may be harder to recognize the kind of job performed with a recurrent network based on the output of the network.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recurrent neural networks variations</h1>
                </header>
            
            <article>
                
<p>Recurrent neural networks can be used in a variety of ways. In this section we'll take a look at the different variations of recurrent neural networks and how they can be used to solve specific types of problems. Specifically we'll look at the following variations:</p>
<ul>
<li>Predicting a single output based on an input sequence</li>
<li>Predicting a sequence based on a single input value</li>
<li>Predicting sequences based on other sequences</li>
</ul>
<p>Finally we'll also explore stacking multiple recurrent neural networks together and how that helps get better performance in a scenario like processing text.</p>
<p>Let's take a look at the scenarios in which recurrent networks can be used, as there are several ways in which you can use the unique properties of recurrent neural networks. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Predicting a single output based on a sequence</h1>
                </header>
            
            <article>
                
<p>A recurrent neural network contains a loopback connection to the input. When we feed a sequence of values it will process each element in the sequence as time steps. Because of the loopback connection it can combine output generated when processing one element in the sequence with input for the next element in the sequence. By combining the output of previous time steps with the input of the next time steps it will build a memory over the whole sequence which can be used to make a prediction. Schematically, a basic recurrent neural network looks like this:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-679 image-border" src="assets/ca594c5b-ff8b-4a04-8a89-c364e1a1216b.png" style=""/></div>
<p>This recurrent behavior becomes clearer when we unroll a recurrent neural network into its individual steps, as demonstrated in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-680 image-border" src="assets/b5319a3f-4a49-4e89-8920-812f00ec7cf7.png" style=""/></div>
<p>To make a prediction with this recurrent neural network we'll perform the following steps:</p>
<ol>
<li>First, we feed the first element of the input sequence to create an initial hidden state.</li>
<li>Then, we take the initial hidden state and combine it with the second element in the input sequence to produce an updated hidden state.</li>
<li>Finally, we take the third element in the input sequence to produce the final hidden state and predict the output for the recurrent neural network.</li>
</ol>
<p>Because of this loopback connection, you can teach a recurrent neural network to recognize patterns that happen over time. For example, when you want to predict tomorrow's temperature, you will need to look at the weather from the past few days to discover a pattern that can be used to determine the temperature for tomorrow.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Predicting a sequence based on a single sample</h1>
                </header>
            
            <article>
                
<p>The basic model for a recurrent neural network can be extended to other use cases as well. For example, you can use the same network architecture to predict a sequence of values based on a single input as is shown in the next diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-681 image-border" src="assets/60c27a51-468f-4049-8e34-2f12ee8acda0.png" style=""/></div>
<p>In this scenario we have three time steps, each time step will predict one step in the output sequence based on the input we provided.</p>
<ol>
<li>First, we feed an input sample into the neural network to produce the initial hidden state and predict the first element in the output sequence</li>
<li>Then, we combine the initial hidden state with the same sample to produce an updated hidden state and output for the second element in the output sequence</li>
<li>Finally, we feed the sample another time to update the hidden state one more time and predict the final element in the output sequence</li>
</ol>
<p>Generating a sequence from one sample is very different from our previous sample where we collected information about all time steps in the input sequence to get a single prediction. In this scenario we generate output at each time step. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>There's one more variation on the recurrent neural network that takes concepts of the setup we just discussed with the setup that we discussed in the previous section to predict a sequence of values based on a sequence of values. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Predicting sequences based on sequences</h1>
                </header>
            
            <article>
                
<p>Now that we've seen how to predict a single value based on a sequence and predicting a sequence based on a single value, let's take a look at predicting sequences for sequences. In this scenario, you perform the same steps as in the previous scenario, where we predicted a sequence based on a single sample, as is demonstrated in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-682 image-border" src="assets/d06546de-4155-4584-bed2-a8117e94046a.png" style=""/></div>
<p class="CDPAlignLeft CDPAlign">In this scenario we have three time steps that take in elements from the input sequence and predict a corresponding element in the output sequence that we want to predict. Let's go over the scenario step-by-step:</p>
<ol>
<li>First, we take the first element in the input sequence and create an initial hidden state and predict the first element in the output sequence.</li>
<li>Next, we take the initial hidden state and the second element from the input sequence to update the hidden state and predict the second element in the output sequence.</li>
<li>Finally, we take the updated hidden state and the final element in the input sequence to predict the final element in the output sequence.</li>
</ol>
<p>So Instead of repeating the same input sample <span>for each step, like we did in the previous section, we feed in the input sequence one element at a time, and keep the generated prediction of each step as the output sequence of the model.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stacking multiple recurrent layers</h1>
                </header>
            
            <article>
                
<p>Recurrent neural networks can have multiple recurrent layers. This makes the memory capacity of the recurrent network bigger, enabling the model to learn more complex relations. </p>
<p>For example, when you want to translate text, you need to stack together at least two recurrent layers, one to encode the input text to an intermediate form, and another one to decode it in to the language into which you want to translate the text. Google has an interesting paper that demonstrates how to use this technique to translate from one language to another that is available at <a href="https://arxiv.org/abs/1409.3215">https://arxiv.org/abs/1409.3215</a>.</p>
<p><span>Because you can use a recurrent neural network in so many ways, it is quite versatile in making predictions with </span>time series<span> data. In the next </span>section<span>, we'll dive into more detail of how a recurrent network works internally, to get a good grasp of how the hidden state works.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How do recurrent neural networks work?</h1>
                </header>
            
            <article>
                
<p>In order to understand how recurrent neural networks work, we need to take a closer look at how recurrent layers in these networks work. There are several different types of recurrent layers you can use in a recurrent neural network. Before we dive into the more advanced versions of recurrent units, let's first discuss how to predict output with a standard recurrent layer, and how to train a neural network that contains recurrent layers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Making predictions with a recurrent neural network</h1>
                </header>
            
            <article>
                
<p>A basic recurrent layer is quite different from a regular layer in a neural network. Recurrent layers, in general, feature a hidden state that serves as the memory for the layer. There's a loopback connection from the output of the layer back to the input of the layer, as demonstrated in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-683 image-border" src="assets/562450bb-333b-4dcd-aa44-7374466c7d98.png" style=""/></div>
<p>Now that we've seen what a basic recurrent layer looks like, let's go over how this layer type works step-by-step, using a sequence of three elements. Each step in the sequence is called a time step. To predict output with a recurrent layer, we need to initialize the layer with an initial hidden state. This is usually done using all zeros. The hidden state has the same size as the number of features in a single time step in the input sequence. </p>
<p><span>Next, we will need to update the hidden state for the first time step in the sequence. To update the hidden state for the first time step we'll use the following formula:</span></p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/d98b7208-2516-4f5e-a56e-5c283bbb58c4.png" style="width:18.25em;height:1.58em;"/></div>
<p>In this formula we calculate the new hidden state by calculating the dot product (that is, the element-wise product) between the <span>initial </span>hidden state (initialized with zeros) and a set of weights. We'll add to this the dot product between another set of weights and the input for the layer. The result of the sum of both dot products is passed through an activation function, just like in a regular neural network layer. This gives us the hidden state for the current time step.</p>
<p>The hidden state for the current time step is used as the initial hidden state for the next time step in the sequence. We'll repeat the calculations performed in the first time step to update the hidden state for the second time step. The formula for the second time step is shown below:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/def3ec5c-8b66-4c14-b6a6-a4fbfad94c75.png" style="width:19.42em;height:1.67em;"/></div>
<p><span>We'll calculate the dot product between the weights for the hidden state, and the hidden state from step 1, and add to this the dot product between the input and the weights for the input. Note that we're reusing the weights from the previous time step. </span></p>
<p>We'll repeat the process of updating the hidden state for the third and final step in the sequence, as shown in the following formula:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/b5096965-de91-411a-bc1b-d3a5014b15c4.png" style="width:21.75em;height:1.67em;"/></div>
<p>When we've processed all the steps in the sequence we can calculate the output using a third set of weights and the hidden state from the final time step, <span>as shown in the following formula</span>:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/459719df-c4d9-4ce5-8fd8-cccb2865b739.png" style="width:8.08em;height:1.67em;"/></div>
<p>When you're using a recurrent network to predict an output sequence, you will need to perform this final calculation at every time step, instead of just the final time step in the sequence.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a recurrent neural network</h1>
                </header>
            
            <article>
                
<p>As with regular neural network layers, you can train a recurrent layer using backpropagation. This time, we're going to apply a trick to the regular backpropagation algorithm.</p>
<p>In a regular neural network, you'd calculate the gradients based on the <kbd>loss</kbd> function, the input, and the expected output of the model. But this won't work for a recurrent neural network. The loss of a recurrent layer can't be calculated using a single sample, the target value, and the <kbd>loss</kbd> function. Because the predicted output is based on all time steps in the input of the network, you also need to calculate the loss using all time steps of the input sequence. So, instead of a single set of gradients, you get a sequence of gradients that results in the final loss when summed up.</p>
<p>Backpropagation over time is harder than regular backpropagation. To reach the global optimum for a <kbd>loss</kbd> function, we need to work harder to descend down the gradients. The hillside for our gradient descent algorithm to walk down is much higher than with a regular neural network. Aside from higher losses, it also takes longer because we need to process each time step in the sequence to calculate and optimize the loss for a single input sequence.</p>
<p>To make things worse, there's a bigger chance we will see exploding gradients during backpropagation, because of the addition of gradients over multiple time steps. You can resolve the problem with exploding gradients by using a bounded activation, such as the <strong>hyperbolic tangent function</strong> (<strong>tanh</strong>) or <kbd>sigmoid</kbd>. These activation functions limit the output value of the recurrent layer to values between -1 and 1 for the <kbd>tanh</kbd> function, and 0 and 1 for the <kbd>sigmoid</kbd> function. The <kbd>ReLU</kbd> activation function is less useful in a recurrent neural network, because the gradients are not limited, which will definitely lead to exploding gradients at some point.</p>
<p>Limiting the values produced by the activation function can cause another problem. Remember from <a href="4c9da7a9-6873-4de9-99a9-43de693d65f8.xhtml">Chapter 2</a>, <em>Building Neural Networks with CNTK</em>, that the <kbd>sigmoid</kbd> has a specific curve where the gradients quickly decrease to zero at both ends of the curve. The <kbd>tanh</kbd> function that we've been using in the sample in this section has the same type of curve, as demonstrated in the following diagram:</p>
<p class="mce-root"/>
<p class="mce-root"/>
<div class="CDPAlignCenter CDPAlign"><img src="assets/73f475dd-54a0-48ea-8e9e-cd17f2598706.png" style=""/></div>
<p>Input values between -2 and +2 have a reasonably well-defined gradient. This means that we can effectively use gradient descent to optimize the weights in the neural network. However, when the output of the recurrent layer gets lower than -2 or higher than +2, the gradient gets shallower. This can get extremely low, to a point where the CPU or GPU starts to round the gradients to zero. This means that we are no longer learning.</p>
<p>Recurrent layers suffer <span>more </span>from the vanishing gradient or saturation problem than regular neural network layers because of the multiple time steps involved. You can't do much about it when using a regular recurrent layer. There are, however, other recurrent layer types that have a more advanced setup that can solve this problem to some extent.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using other recurrent layer types</h1>
                </header>
            
            <article>
                
<p>Because of the vanishing gradient problem, the basic recurrent layer is not very good at learning long-term correlations. In other words, it does not handle long sequences very well. You run in into this problem when you try to process sentences or longer sequences of text and try to classify what they mean. In English and other languages, there's quite a long distance between two related words in a sentence that give the sentence meaning. When you only use a basic recurrent layer in your model, you will quickly discover that your model won't be very good at classifying sequences of text.</p>
<p>There are, however, other recurrent layer types that are much more suited for working with longer sequences. Also, they tend to combine long and short-term correlations better.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with gated recurrent units</h1>
                </header>
            
            <article>
                
<p>One alternative to the basic recurrent layer is the <strong>Gated Recurrent Unit</strong> (<strong>GRU</strong>). This layer type has two gates that help it handle long-distance correlations in sequences, as demonstrated in the following diagram:</p>
<p>The GRU is much more complex in shape than the regular recurrent layer. There are a lot more lines connecting different inputs to the output. Let's go over the diagram and take a look at what the general idea is behind this layer type.</p>
<p class="CDPAlignLeft CDPAlign">Unlike the regular recurrent layer, the GRU layer has an <strong>update gate</strong> and <strong>reset gate</strong>. The reset and update gates are the valves that control how much of the memory of previous time steps is kept, and how much of the new data is used for producing the new memory. </p>
<p>Predicting output is very similar to predicting with a regular recurrent layer. When we feed data into the layer, the previous hidden state is used to calculate the value for the new hidden state. When all elements in the sequence have been processed, the output is calculated using one additional set of weights, just as we did in the regular recurrent layer.</p>
<p><span>Calculating the hidden state over multiple time steps is a lot more complicated in a GRU. There are a few steps needed to update the hidden state of the GRU. First, we need to calculate the value for the update gate as follows:</span></p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/1205b70b-a516-409d-9e38-6151b5526369.png" style="width:15.25em;height:1.50em;"/></div>
<p>The update gate is controlled using two sets of weights, one for the hidden state from the previous time step, and one for the current input provided to the layer. The value produced by the update gate controls how much of the past time steps is kept in the hidden state.</p>
<p>The second step is to update the reset gate. This is done using the following formula:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/105e2de9-9e05-4295-aff9-759ca037a53d.png" style="width:16.08em;height:1.58em;"/></div>
<p>The reset gate is also controlled using two sets of weights; one for the input value for the current time step and another set of weights for the hidden state. The reset gate controls how much of the hidden state is removed. This becomes clear when we calculate an initial version of the new hidden state as follows:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/e85f1b8b-4519-401f-8c02-01ee1e254c35.png" style="width:14.25em;height:1.33em;"/></div>
<p class="mce-root"/>
<p>First, we multiply the input with its corresponding weights. Then, we multiply the previous hidden state with its corresponding weights. We then calculate the element-wise or Hadamard product between the reset gate and the weighted hidden state. Finally, we add this to the weighted input, and use a <kbd>tanh</kbd> activation over this to calculate the remembered hidden state.</p>
<p>The reset gate in this formula controls how much is forgotten of the previous hidden state. A reset gate with a low value will remove a lot of data from the previous time step. A higher value will help the layer remember a lot from the previous time step.</p>
<p>We're not done yet though—once we have the information coming from the previous timestamp increased by the update gate and tempered by the reset gate, this produces the remembered information from the previous time step. We can now calculate the final hidden state value based on this remembered information as follows:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/57708565-093b-49c9-98c2-dda656dad36a.png" style="width:15.92em;height:1.50em;"/></div>
<p>First, we take the element-wise product between the previous hidden state and the update gate to determine how much information from the previous state should be kept. Then we add to that the element-wise product between the update gate and the remembered information from the previous state. Note that the update gate is used to feed a percentage of new information and a percentage of old information. That's why we're using a <em>1-</em> operation in the second part of the formula.</p>
<p>The GRU is a large step up from the recurrent layer in terms of the calculations involved and its capability to remember long-term and short-term relationships. However, it cannot do both at the same time. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with long short-term memory units</h1>
                </header>
            
            <article>
                
<p>Another alternative to working with basic recurrent layers is to use a <strong>Long Short-Term Memory </strong>(<strong>LSTM</strong>) unit. This recurrent layer works with gates just like the GRU that we discussed in the previous section, except the LSTM has a lot more gates.</p>
<p class="mce-root"/>
<p>The following diagram outlines the structure of the LSTM layer:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-684 image-border" src="assets/5b72f45d-f78f-47ee-8dfa-1840fcfa61c0.png" style=""/></div>
<p>The LSTM unit has a cell state that is central to how this layer type works. The cell state is kept over long periods of time and doesn't change much. The LSTM layer also has a hidden state, but this state serves a different role in the layer. </p>
<p>In short, the LSTM has a long-term memory modeled as the cell state, and a short-term memory modeled as the hidden state. The access to the long-term memory is guarded using several gates. There are two gates that control the long-term memory access in the LSTM layer:</p>
<ul>
<li>The forget gate, which controls what will be forgotten from the cell state</li>
<li>The input gate, which controls what will be stored from the hidden state and input in the cell state</li>
</ul>
<p>There's one final gate in the LSTM layer that controls what to take from the cell state into the new hidden state. Essentially, we're using the output gate to control what to take from long-term memory into short-term memory. <span>Let's go over how the layer works step by step.</span></p>
<p class="mce-root"/>
<p>First, we'll take a look at the forget gate. The forget gate is the first gate that will get updated when you make a prediction with an LSTM layer:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-685 image-border" src="assets/dfb67acd-f099-4a82-9feb-7c3afa72470e.png" style=""/></div>
<p>The forget gate controls how much of the cell state should be forgotten. It is updated with the following formula:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/c773dd4c-f829-4e0f-9273-a61a20e28fde.png" style="width:16.25em;height:1.33em;"/></div>
<p>When you take a closer look at this formula, you will notice that it is essentially a dense layer with a <kbd>sigmoid</kbd> activation function. The forget gate generates a vector with values between zero and one to control how much of the elements in the cell state are forgotten. A value of one on the forget gate means that the value in the cell state is kept. A value of zero on the forget gate makes the cell state forget its value.</p>
<p>We're concatenating the hidden state from the previous step and the new input into one matrix along the column axis. The cell state will essentially store long-term information about the input provided to the layer with the hidden state that was stored in the layer.</p>
<p>The second gate in the LSTM layer is the input gate. The input gate controls how much new data is stored in the cell state. The new data is a combination of the hidden state from the previous step and the input for the current time step, as is demonstrated in the following diagram: </p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-686 image-border" src="assets/851cfd14-6e8d-4b29-a10b-1980d63593eb.png" style=""/></div>
<p>We'll use the following formula to determine the value for the update gate:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/b2d16767-7b3a-4143-9d07-19af88809941.png" style="width:18.83em;height:1.67em;"/></div>
<p>Just like the forget gate, the input gate is modeled as a nested dense layer within the LSTM layer. You can see the input gate as the left branch within the highlighted section of the previous diagram. The input gate is used in the following formula to determine the new value to put into the cell state:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/e5e5aa98-5c77-4939-9761-de781c1fe5dc.png" style="width:16.42em;height:1.58em;"/></div>
<p class="mce-root"/>
<p>To update the cell state, we need one more step, which is highlighted in the next diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-687 image-border" src="assets/2098dad2-762f-4ec9-b3cf-8e59ce28d043.png" style=""/></div>
<p>Once we know the values for the forget gate and input gate, we can calculate the updated cell state using the following formula: </p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/10386cb0-d388-4c08-ad54-b5a68b6f2ea1.png" style="width:14.50em;height:1.67em;"/></div>
<p>First, we'll multiply the forget gate with the previous cell state to forget old information. We then multiply the update gate with the new values for the cell state to learn new information. We sum both values up to produce the final cell state for the current time step.</p>
<p class="mce-root"/>
<p>The final gate in the LSTM layer is the output gate. This gate controls how much information from the cell state is used in the output of the layer and the hidden state for the next time step, as demonstrated in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-688 image-border" src="assets/428c9e0e-cf19-4656-bea2-3cadc8624716.png" style=""/></div>
<p>The output gate is calculated using the following formula:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/e4613046-8448-455f-8353-2d003fe7e0d2.png" style="width:21.25em;height:1.83em;"/></div>
<p>The output gate is, just like the input gate and forget gate, a dense layer that controls how much of the cell state is copied to the output. We can now calculate the new hidden state, or output, of the layer using the following formula:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/32364dca-d4db-48a4-9d2f-364ba600154e.png" style="width:11.92em;height:1.83em;"/></div>
<p>You can use this new hidden state to calculate the next time step, or return it as the output for the layer. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">When to use other recurrent layer types</h1>
                </header>
            
            <article>
                
<p>The GRU and LSTM layers are definitely more complex than regular recurrent layers. They have a lot more parameters that need to be trained. This will make it harder to debug the model when you run into problems.</p>
<p>The regular recurrent layer doesn't hold up well when you work with longer sequences of data, because it gets saturated quickly. You can use both the LSTM and GRU to resolve this problem. The GRU layer works without an additional memory state. The LSTM uses a cell state to model long-term memory.</p>
<p>Because the GRU has fewer gates and no memory, it takes less time to train it. So, if you're looking to process longer sequences and need a network that can be trained relatively quickly, use the GRU layer.</p>
<p>The LSTM layer has more power to express relationships in the sequences you feed it. This means that it will perform better if you have enough data to train it. In the end, it comes down to experimentation to determine which layer type works best for your solution.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building recurrent neural networks with CNTK</h1>
                </header>
            
            <article>
                
<p>Now that we've explored the theory behind recurrent neural networks, it's time to build one with CNTK. There are several building blocks that CNTK offers for building recurrent neural networks. We're going to explore how to build a recurrent neural network using a sample dataset containing power measurements from a solar panel. </p>
<p>The power output of a solar panel changes during the day, so it's hard to predict how much power is generated for a typical house. This makes it hard for a local energy company to predict how much additional power they should generate to keep up with demand. </p>
<p>Luckily, many energy companies offer software that allows customers to keep track of the power output of their solar panels. This will allow them to train a model based on this historical data, so we can predict what the total power output will be per day.</p>
<p>We're going to train a power output prediction model using recurrent neural networks based on a dataset offered by Microsoft as part of the CNTK documentation.</p>
<p>The dataset contains multiple measurements per day, and contains the current power output at a timestamp, and the total amount of power produced up to that timestamp. It's our goal to predict the total power produced for a day, based on the measurements collected during the day.</p>
<p>You can use a regular neural network, but that would mean that we would have to turn each collected measurement into a feature for the input. Doing so assumes that there's no correlation between the measurements. But, in practice, there is. Each future measurement depends on a measurement that came before. So, a recurrent model that can reason over time is much more practical for this case.</p>
<p>In the next three sections we'll explore how to build a recurrent neural network in CNTK. After that we'll explore how to train the recurrent neural network using data from the solar panel dataset. Finally, we'll take a look how to predict output with a recurrent neural network.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the neural network structure</h1>
                </header>
            
            <article>
                
<p>Before we can start to make predictions about the output of a solar panel we need to construct a recurrent neural network. A recurrent neural network is built in the same fashion as a regular neural network. Here's how to build one:</p>
<pre>features = sequence.input_variable(1)<br/><br/>with default_options(initial_state = 0.1):<br/>    model = Sequential([<br/>        Fold(LSTM(15)),<br/>        Dense(1)<br/>    ])(features)<br/>    <br/>target = input_variable(1, dynamic_axes=model.dynamic_axes)</pre>
<p><span>Follow the given steps:</span></p>
<ol>
<li>First, create a new input variable to store the input sequence. </li>
<li>Then, initialize the default_options for the neural network and provide the initial_state setting with a value of 0.1.</li>
<li>Next, Create a Sequential layer set for the neural network.</li>
<li>In the Sequential layer set, provide a LSTM recurrent layer with 15 neurons wrapped in a Fold layer.</li>
<li>Finally, add a Dense layer with one neuron.</li>
</ol>
<p>There are two ways in which you can model recurrent neural networks in CNTK. If you're only interested in the final output of a recurrent layer, you can use the <kbd>Fold</kbd> layer combined with a recurrent layer, such as GRU, LSTM, or even RNNStep. The <kbd>Fold</kbd> layer collects the final hidden state of the recurrent layer, and returns that as the output to be used by the next layer.</p>
<p>As an alternative to the <kbd>Fold</kbd> layer, you can also use the <kbd>Recurrence</kbd> block. This wrapper returns the full sequence generated by the recurrent layer you wrap in it. This is useful if you want to generate sequential output with your recurrent neural network.</p>
<p>A recurrent neural network works with sequential input, this is why we're using the <kbd>sequence.input_variable</kbd> function instead of a regular <kbd>input_variable</kbd> function.</p>
<p>The regular <kbd>input_variable</kbd> function supports only fixed dimensions for the input. This means that we have to know the number of features that we want to feed into the network for each sample. This applies to both regular models and models that process images. In image classification models, we typically use one dimension for color channels, and another two dimensions for the width and height of the input image. We know all these dimensions upfront. The only dimension that is dynamic in the regular <kbd>input_variable</kbd> function is the batch dimension. This dimension gets calculated when you train the model with a certain minibatch size setting, which results in a fixed value for the batch dimension.</p>
<p>In recurrent neural networks, we don't know how long each sequence will be. We only know the shape of each piece of data stored in the sequence as a time step. The <kbd>sequence.input_variable</kbd> function allows us to provide the dimensions for each time step, and keep the dimension that models the sequence length dynamic. As with the regular <kbd>input_variable</kbd> function, the batch dimension is also dynamic. We configure this dimension when we start training with a particular minibatch size setting.</p>
<p>CNTK is unique in the way it handles sequential data. In frameworks such as TensorFlow, you have to specify the dimensions for both the sequence length and batch upfront, before you can start training. Because you have to use fixed size sequences, you will need to add padding to sequences that are shorter than the maximum sequence length supported by your model. Also, if you have longer sequences, you need to truncate them. This leads to lower quality models, because you ask the model to learn information from empty time steps in your sequence. CNTK handles dynamic sequences quite well, so you don't have to use padding when working with sequences in CNTK.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stacking multiple recurrent layers</h1>
                </header>
            
            <article>
                
<p>In the previous section, we only talked about using a single recurrent layer. You can, however, stack multiple recurrent layers in CNTK. For example, when we want to stack two recurrent layers, we need to use the following combination of layers:</p>
<pre>from cntk import sequence, default_options, input_variable<br/>from cntk.layers import Recurrence, LSTM, Dropout, Dense, Sequential, Fold, Recurrence<br/><br/>features = sequence.input_variable(1)<br/><br/>with default_options(initial_state = 0.1):<br/>    model = Sequential([<br/>        Recurrence(LSTM(15)),<br/>        Fold(LSTM(15)),<br/>        Dense(1)<br/>    ])(features)</pre>
<p><span>Follow the given steps:</span></p>
<ol>
<li>First, import the <kbd>sequence</kbd> module, <kbd>default_options</kbd> function and <kbd>input_variable</kbd> function from the <kbd>cntk</kbd> package</li>
<li>Next, import the layers for the recurrent neural network</li>
<li>Then, create a new <kbd>LSTM</kbd> layer with 15 neurons and wrap it in a <kbd>Recurrence</kbd> layer so the layer returns a sequence instead of a single output</li>
<li>Now, create the second <kbd>LSTM</kbd> layer with 15 neurons, but this time wrap it in a <kbd>Fold</kbd> layer to return only the final time step as output</li>
<li>Finally, invoke the created <kbd>Sequential</kbd> layer stack with the features variable to complete the neural network</li>
</ol>
<p>This technique can be extended beyond two layers as well; just wrap the layers before the last recurrent layer in <kbd>Recurrence</kbd> layers and wrap the final layer in a <kbd>Fold</kbd> layer.</p>
<p>For the sample in this chapter we'll limit ourselves to using one recurrent layer as we've constructed it in the previous section, <em>Building the neural network structure</em>. In the next section we'll talk about training the recurrent neural network that we've created.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the neural network with time series data</h1>
                </header>
            
            <article>
                
<p>Now that we have a model, let's take a look at how to train a recurrent neural network in CNTK.</p>
<p>First, we need to define what loss function we want to optimize. Since we're predicting a continuous variable—power output—we need to use a mean squared error loss. We'll combine the loss with a mean square error metric to measure the performance of our model. Remember, from <a href="e39df191-73e4-414f-b44b-efca6f0ad4cd.xhtml">Chapter 4</a>, <em>Validating Model Performance</em>, that we can combine the loss and metric in a single function object using <kbd>@Function</kbd>:</p>
<pre>@Function<br/>def criterion_factory(z, t):<br/>    loss = squared_error(z, t)<br/>    metric = squared_error(z, t) <br/>    <br/>    return loss, metric<br/><br/>loss = criterion_factory(model, target)<br/>learner = adam(model.parameters, lr=0.005, momentum=0.9)</pre>
<p>We're going to use the <kbd><span><span>adam</span></span></kbd> learner to optimize the model. This learner is an extension of the <strong>Stochastic Gradient Descent</strong> (<strong>SGD</strong>) algorithm. While SGD uses a fixed learning rate, Adam changes the learning rate over time. In the beginning, it will use a high learning rate to get results fast. Once it has run for a while, it will start to lower the learning rate to increase accuracy. The <kbd>adam</kbd> optimizer is a lot faster than SGD in optimizing the <kbd>loss</kbd> function.</p>
<p>Now we have a loss, metric, we can use both in-memory and out-of-memory data to train the recurrent neural network.</p>
<p>The data for a recurrent neural network needs to be modeled as sequences. In our case, the input data is a sequence of power measurements for each day, stored in a <strong>CNTK Text Format </strong><span>(</span><strong>CTF</strong><span>) file</span>.Follow the given steps:</p>
<p>In <a href="f7cd9148-99e8-427c-acf4-d74c3e52df58.xhtml">Chapter 3</a>, <em>Getting Data into Your Neural Network</em>, we discussed how you can store data for training in CNTK in CTF format. The CTF file format not only supports storing basic samples, but also supports storing sequences. A CTF file for sequences has the following layout:</p>
<pre>&lt;sequence_id&gt; |&lt;input_name&gt; &lt;values&gt; |&lt;input_name&gt; &lt;values&gt;</pre>
<p>Each line is prefixed with a unique number to identify the sequence. CNTK will consider lines with the same sequence identifier to be one sequence. So, you can store one sequence over multiple lines. Each line can contain one time step in the sequence. </p>
<p>There's one important detail you have to keep in mind when storing sequences over multiple lines in a CTF file. One of the lines storing the sequence should also contain the expected output for the sequence. Let's take a look at what that looks like in practice:</p>
<pre>0 |target 0.837696335078534 |features 0.756544502617801<br/>0 |features 0.7931937172774869<br/>0 |features 0.8167539267015707<br/>0 |features 0.8324607329842932<br/>0 |features 0.837696335078534<br/>0 |features 0.837696335078534<br/>0 |features 0.837696335078534<br/>1 |target 0.4239092495636999 |features 0.24554973821989529<br/>1 |features 0.24554973821989529<br/>1 |features 0.00017225130534296885<br/>1 |features 0.0014886562154347149<br/>1 |features 0.005673647442829338<br/>1 |features 0.01481675392670157</pre>
<p>The first line for a sequence contains the <kbd>target</kbd> variable, as well as the data for the first time step in the sequence. The <kbd>target</kbd> variable is used to store the expected power output for a particular sequence. The other lines for the same sequence only contain the <kbd>features</kbd> variable. You can't use the input file if you do put the <kbd>target</kbd> variable on a separate line. The minibatch source will fail to load.</p>
<p>You can load the sequence data into your training code, like so:</p>
<pre>def create_datasource(filename, sweeps=INFINITELY_REPEAT):<br/>    target_stream = StreamDef(field='target', shape=1, is_sparse=False)<br/>    features_stream = StreamDef(field='features', shape=1, is_sparse=False)<br/><br/>    deserializer = CTFDeserializer(filename, StreamDefs(features=features_stream, target=target_stream))<br/>    datasource = MinibatchSource(deserializer, randomize=True, max_sweeps=sweeps) <br/>    <br/>    return datasource<br/><br/>train_datasource = create_datasource('solar_train.ctf')<br/>test_datasource = create_datasource('solar_val.ctf', sweeps=1)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p><span>Follow the given steps:</span></p>
<ol>
<li class="mce-root">First, create a new function create_datasource with two parameters: filename, and sweeps which has a default of INFINITELY_REPEAT so we can iterate over the same dataset multiple times.</li>
<li class="mce-root">In the <kbd>create_datasource</kbd> function, define two streams for the minibatch source, one for the input features and one for the expected output of our model.</li>
<li class="mce-root">Then use <kbd>CTFDeserializer</kbd> to read the input file.</li>
<li class="mce-root">Finally, return a new <kbd>MinibatchSource</kbd> for the input file <span>provided</span>.</li>
</ol>
<p>To train the model, we need to iterate over the same data multiple times to train for multiple epochs. That's why you should use an infinity setting for the <kbd>max_sweeps</kbd> of the minibatch source. Testing is done by iterating over a set of validation samples so that we configure the minibatch source with just one sweep.</p>
<p>Let's train the neural network with the data sources <span>provided</span>, as follows:</p>
<pre>progress_writer = ProgressPrinter(0)<br/>test_config = TestConfig(test_datasource)<br/><br/>input_map = {<br/>    features: train_datasource.streams.features,<br/>    target: train_datasource.streams.target<br/>}<br/><br/>history = loss.train(<br/>    train_datasource, <br/>    epoch_size=EPOCH_SIZE,<br/>    parameter_learners=[learner], <br/>    model_inputs_to_streams=input_map,<br/>    callbacks=[progress_writer, test_config],<br/>    minibatch_size=BATCH_SIZE,<br/>    max_epochs=EPOCHS)</pre>
<p><span>Follow the given steps:</span></p>
<ol>
<li>First, initialize a <kbd>ProgressPrinter</kbd> to log the output of the training process.</li>
<li>Then, create a new test configuration to validate the neural network using data from the <kbd>test_datasource</kbd>.</li>
<li>Next, Create a mapping between the input variables of the neural network and the streams from the training datasource.</li>
<li>Finally, invoke the train method on the loss function to start the training process. Provide it the <kbd>train_datasource</kbd>, settings, the learner, <kbd>input_map</kbd> and the callbacks for logging and testing.</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The model needs to train for quite a long time, so grab yourself a coffee or two when you plan to run the sample code on your machine.</p>
<p>The <kbd>train</kbd> method will output the metrics and loss values on screen, because we used <kbd>ProgressPrinter</kbd> as a callback for the <kbd>train</kbd> method. The output will look similar to this:</p>
<pre><strong>average      since    average      since      examples
    loss       last     metric       last              
 ------------------------------------------------------
Learning rate per minibatch: 0.005
     0.66       0.66       0.66       0.66            19
    0.637      0.626      0.637      0.626            59
    0.699      0.752      0.699      0.752           129
    0.676      0.656      0.676      0.656           275
    0.622      0.573      0.622      0.573           580
    0.577      0.531      0.577      0.531          1150</strong></pre>
<p>As good practice, you want to validate your model against a separate test set. That's why we created the <kbd>test_datasource</kbd> function earlier. To use this data to validate your model, you can use a <kbd>TestConfig</kbd> object as a callback for the <kbd>train</kbd> method. The testing logic will be called automatically when the training process is completed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Predicting output </h1>
                </header>
            
            <article>
                
<p>When the model is finally done training, you can test it using a few sample sequences that can be found in the sample code for this chapter. Remember, a CNTK model is a function, so you can invoke it with a numpy array representing the sequence for which you want to predict the total output, as follows:</p>
<pre>import pickle<br/><br/>NORMALIZE = 19100<br/><br/>with open('test_samples.pkl', 'rb') as test_file:<br/>    test_samples = pickle.load(test_file)<br/>    <br/>model(test_samples) * NORMALIZE</pre>
<p><span>Follow the given steps:</span></p>
<ol>
<li>First, import the pickle package</li>
<li>Next, define the settings to normalize the data</li>
<li>After that, open the test_samples.pkl file for reading.</li>
</ol>
<p> </p>
<ol start="4">
<li>Once the file is opened, load its contents using the pickle.load function.</li>
<li>Finally, run the samples through the network and multiply them with the NORMALIZE constant to obtain the predicted output for the solar panel.</li>
</ol>
<p>The output produced by the model is between zero and one, because that's what we stored in the original dataset. The values represent a normalized version of the power output of the solar panel. We need to multiply them by the normalization value that we used to normalize the original measurements to get the actual power output of the solar panel.</p>
<p>The final denormalized output for the model looks like this:</p>
<pre>array([[ 8161.595],
       [16710.596],
       [13220.489],
       ...,
       [10979.5  ],
       [15410.741],
       [16656.523]], dtype=float32)</pre>
<p>Predicting with a recurrent neural network is pretty similar to making predictions with any other CNTK model, except for the fact that you need to provide sequences rather than single samples.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we've looked at how to use recurrent neural networks to make predictions based on time series data. Recurrent neural networks are useful in scenarios where you have to deal with financial data, IoT data, or any other information that is collected over time.</p>
<p>One important building block for recurrent neural networks is the <kbd>Fold</kbd> and the <kbd>Recurrence</kbd> layer types, which you can combine with any of the recurrent layer types, such as RNNStep, GRU, or LSTM, to build a recurrent layer set. Depending on whether you want to predict a sequence or single value, you can use the <kbd>Recurrence</kbd> or <kbd>Fold</kbd> layer types to wrap the recurrent layers.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>When you're training a recurrent neural network, you can make use of the sequence data stored in the CTF file format to make it easier to train the model. But, you can just as easily use sequences stored as numpy arrays, as long as you use the correct combination of sequence input variables with recurrent layers.</p>
<p>Making predictions with a recurrent neural network is just as easy as it is for regular neural networks. The only difference is the input data format, which is, just as for training, a sequence.</p>
<p>In the next chapter, we'll look at one last topic for this book: <em>Deploying Models to Production</em>. We'll explore how to use CNTK models you've built in C# or Java, and how to properly manage your experiments using tools such as the Azure Machine Learning service.</p>


            </article>

            
        </section>
    </body></html>