<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Sentiment Analysis Using Word2Vec and LSTM Network</h1>
                </header>
            
            <article>
                
<p>Sentiment analysis is a systematic way to identify, extract, quantify, and study effective states and subjective information. This is widely used in <strong>natural language processing</strong> (<strong>NLP</strong>), text analytics, and computational linguistics. This chapter demonstrates how to implement and deploy a hands-on deep learning project that classifies review texts as either positive or negative based on the words they contain. A large-scale movie review dataset that contains 50k reviews (training plus testing) will be used.</p>
<p>A combined approach using Word2Vec (that is, a widely used word embedding technique in NLP) and the <strong>Long Short-Term Memory</strong> (<strong>LSTM</strong>) network for modeling will be applied: the pre-trained Google news vector model will be used as the neural word embeddings. Then, the training vectors, along with the labels, will be fed into the LSTM network to classify them as negative or positive sentiments. Finally, it evaluates the trained model on the test set.</p>
<p>Additionally, it shows how to apply text preprocessing techniques such as tokenizer, stop words removal, and <strong>term frequency-inverse document frequency</strong> (<strong>TF-IDF</strong>), and word-embedding operations in <strong>Deeplearning4j</strong> (<strong>DL4J</strong>).</p>
<p>Nevertheless, it also shows how to save the trained DL4J model. Later on, the saved model will be restored from disk to make sentiment prediction on other small-scale review texts from Amazon cell, Yelp, and IMDb. Finally, it has answers to some frequently asked questions related to the projects and possible outlook.</p>
<p>The following topics will be covered throughout this end-to-end project:</p>
<ul>
<li>Sentiment analysis in NLP</li>
<li>Using Word2Vec for neural word embeddings</li>
<li>Dataset collection and description</li>
<li>Saving and restoring pre-trained models with DL4J</li>
<li>Developing a sentiment-analyzing model using Word2Vec and LSTM</li>
<li>Frequently asked questions (FAQs)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sentiment analysis is a challenging task</h1>
                </header>
            
            <article>
                
<p>Text analytics in NLP is all about processing and analyzing large-scale structured and unstructured text to discover hidden patterns and themes and derive contextual meaning and relationships. Text analytics has so many potential use cases, such as sentiment analysis, topic modeling, TF-IDF, named entity recognition, and event extraction.</p>
<p>Sentiment analysis includes many example use cases, such as analyzing the political opinions of people on Facebook, Twitter, and other social media. Similarly, analyzing the reviews of restaurants on Yelp is also another great example of Sentiment Analysis. NLP frameworks and libraries such as OpenNLP and Stanford NLP are typically used to implement sentiment analysis.</p>
<p>However, for analyzing sentiments using text, particularly unstructured texts, we must find a robust and efficient way of feature engineering to convert the text into numbers. However, several stages of transformation of data are possible before a model is trained, and then subsequently deployed and finally performing predictive analytics. Moreover, we should expect the refinement of the features and model attributes. We could even explore a completely different algorithm, repeating the entire sequence of tasks as part of a new workflow.</p>
<p>When you look at a line of text, we see sentences, phrases, words, nouns, verbs, punctuation, and so on, which, when put together, have a meaning and purpose. Humans are very good at understanding sentences, words, slang, annotations, and context extremely well. This comes from years of practice and learning how to read/write proper grammar, punctuation, exclamations, and so on.</p>
<p>For example, the two sentences: <em>DL4J makes predictive analytics easy</em>, and <em>Predictive analytics makes <span>DL4J</span> easy</em>, might result in the same sentence vector having the same length equal to the size of our vocabulary that we pick. The second drawback is that the words "is" and "<span>DL4J" </span>have the same numerical index value of one, but our intuition says that the word "is" isn't important compared to "<span>DL4J"</span>. Let's take a look at the second example: when your search string is <em>hotels in Berlin</em> in Google, we want results pertaining to <em>bnb</em>, <em>motel</em>, <em>lodging</em>, and <em>accommodation</em> in Berlin, too.</p>
<p>When layman terms come to the party, natural language learning becomes more complicated. Take the word bank as an example. This has several connections with a financial institution and land alongside a body of water. Now, if a natural sentence contains the term "bank" in conjunction with words such as finance, money, treasury, and interest rates, we can understand that its intended meaning is the former. However, if the neighboring words are water, shore, river, and lake, and so on, the case is the latter. Now, the question would be: can we exploit this concept to deal with polysemy and synonyms and make our model learn better?</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-653 image-border" src="assets/f620eb24-b4e5-4a8d-9a95-ef5685415cd2.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Classical machine learning versus deep learning-based NPL</div>
<p>Nevertheless, natural language sentences also contain vague words, slang, trivial words, and special characters, and all of these make the overall understanding and machine learning troublesome.</p>
<p>We have already seen how to use one-hot encoding or StringIndexer techniques to convert categorical variables (or even words) into numeric form. However, this kind of program often fails to interpret the semantics in a complex sentence, especially for lengthy sentences or even words. Consequently, human words have no natural notion of similarity. Thus, we naturally won't try to replicate this kind of capability, right?</p>
<p>How can we build a simple, scalable, faster way to deal with the regular texts or sentences and derive relations between a word and its contextual words, and then embed them in billions of words that will produce exceedingly good word representations into numeric vector space so that the machine learning models can consume them? Let's look at the Word2Vec model to find the answer to this.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using Word2Vec for neural word embeddings</h1>
                </header>
            
            <article>
                
<p>Word2Vec is a two-layer neural network that processes texts and turns them into numerical features. This way, the output of the Word2Vec is a vocabulary in which each word is embedded in vector space. The resulting vector can then be fed into a neural network for better understanding of natural languages. The novelist EL Doctorow has expressed this idea quite poetically in his book <em>Billy Bathgate</em>:</p>
<div class="packt_quote">"It's like numbers are language, like all the letters in the language are turned into numbers, and so it's something that everyone understands the same way. You lose the sounds of the letters and whether they click or pop or touch the palate, or go ooh or aah, and anything that can be misread or con you with its music or the pictures it puts in your mind, all of that is gone, along with the accent, and you have a new understanding entirely, a language of numbers, and everything becomes as clear to everyone as the writing on the wall. So as I say there comes a certain time for the reading of the numbers."</div>
<p>While using BOW and TF-IDF, all words are projected into the same position and their vectors are averaged: we address the word importance without considering the importance of word order in a collection of documents or in a single document.</p>
<p>As the order of words in the history does not influence the projection, both BOW and TF-IDF have no such features that can take care of this issue. Word2Vec encodes each word into a vector by using either context to predict a target word using a <strong>continuous bag-of-words</strong> (<strong>CBOW</strong>) or using a word to predict a target context, which is called <strong>continuous skip-gram</strong>.</p>
<ul>
<li><strong>N-gram versus skip-gram</strong>: Words are read into vectors one at a time and scanned back and forth within a certain range</li>
<li><strong>CBOW</strong>: The CBOW technique uses a continuously distributed representation of the context</li>
<li><strong>Continuous skip-gram</strong>: Unlike CBOW, this method tries to maximize classification of a word based on another word in the same sentence</li>
</ul>
<p>I have experienced that an increase in the range improves the quality of the resulting word vectors, but it also increases the computational complexity. Since more distant words are usually less related to the current word than those close to it are, we give less weight to the distant words by sampling less from those words in our training examples. Because of the model building and prediction, times also increase.</p>
<p>A comparative analysis from the architecture's point of view can be seen in the following diagram, where the architecture predicts the current word based on the context, and the skip-gram predicts the surrounding words given the current word:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-654 image-border" src="assets/bba3319f-913c-4a4d-af80-de53f74b9cfd.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">CBOW versus skip-gram (source: Tomas Mikolov et al., Efficient Estimation of Word Representations in Vector Space, <span class="URLPACKT">https://arxiv.org/pdf/1301.3781.pdf</span>)</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Datasets and pre-trained model description</h1>
                </header>
            
            <article>
                
<p>We are going to use the Large Movie Review dataset for training and testing the mode. Additionally, we will be using the <span class="heading0">Sentiment labeled Sentences dataset</span> for making a single prediction on reviews on products, movies, and restaurants.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Large Movie Review dataset for training and testing</h1>
                </header>
            
            <article>
                
<p>The former one is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. The dataset can be downloaded from <a href="http://ai.stanford.edu/~amaas/data/sentiment/">http://ai.stanford.edu/~amaas/data/sentiment/</a>. Alternatively, I have utilized a Java method that comes from DL4J examples that also downloads and extracts this dataset.</p>
<p>I would like to acknowledge the following publications: Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011), <em>Learning Word Vectors for Sentiment Analysis</em>, The 49<sup>th</sup> Annual Meeting of the Association for Computational Linguistics (ACL 2011).</p>
<p>This dataset contains 50,000 movie reviews along with their associated binary sentiment polarity labels. The reviews are split evenly into 25,000 for both train and test sets. The overall distribution of labels is balanced (25,000 positive and 25,000 negative). We also include an additional 50 thousand unlabeled documents for unsupervised learning. In the labeled train/test sets, if a reviews has a score &lt;= 4 out of 10, it is treated as a negative review, but having a score &gt;= 7 out of 10 is treated as a positive review. Nevertheless, reviews with more neutral ratings are not included in the datasets.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Folder structure of the dataset</h1>
                </header>
            
            <article>
                
<p>There are two folders, namely <kbd>train</kbd> and <kbd>test</kbd> for the training and test sets, respectively. Each folder has two separate subfolders called <kbd>pos</kbd> and <kbd>neg</kbd>, which contain reviews with binary labels (pos, neg). Reviews are stored in text files having the name <kbd>id_rating.txt</kbd>, where <kbd>id</kbd> is a unique ID and <kbd>rating</kbd> is the star rating on a 1-10 scale. Take a look at the following diagram to get a clearer view on the directory's structure:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-655 image-border" src="assets/3c758053-b424-4acf-b7d1-76fa4dcc7225.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Folder structure in Large Movie Review Dataset</div>
<p>For example, the <kbd>test/pos/200_8.txt</kbd> file is the text for a positive-labeled test set example with a unique ID of 200 and a star rating of 8/10 from IMDb. The <kbd>train/unsup/</kbd> directory has zero for all ratings because the ratings are omitted for this portion of the dataset. <span class="heading0">Let's look at a sample positive review from IMDb:</span></p>
<div class="packt_figref packt_quote">"Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as "Teachers". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is "Teachers". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far-fetched. What a pity that it isn't!"<span class="heading0"><br/></span></div>
<p><span class="heading0">Therefore, from the preceding review text, we can understand that the respective audience gave Bromwell High (</span>a British-Canadian adult animated series about a British high school in South London, which you can <span class="heading0">see more of at</span> <a href="https://en.wikipedia.org/wiki/Bromwell_High">https://en.wikipedia.org/wiki/Bromwell_High</a><span class="heading0">) a positive review, that is, a positive sentiment.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Description of the sentiment labeled dataset</h1>
                </header>
            
            <article>
                
<p>The s<span class="heading0">entiment labeled sentences dataset was downloaded from the UCI machine learning repository at <a href="http://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences" target="_blank">http://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences</a>. This dataset was a research outcome by Kotzias and is used in the following publication: <em>From Group to Individual Labels using Deep Features</em>, Kotzias et. al, KDD' 2015.</span></p>
<p><span class="heading0">The dataset contains sentences labeled with a positive or negative sentiment, extracted from reviews of products, movies, and restaurants. The review is a tab-delimited review having review sentences and a score of either 1 (for positive) or 0 (for negative). Let's look at a sample review from Yelp with an associated label:</span></p>
<p><span class="heading0"><em>"I was disgusted because I was pretty sure that was human hair."</em><br/></span></p>
<p><span class="heading0">In the preceding review text, the score is 0, so it is a negative review and it expresses a negative sentiment on the part of the customer. On the other hand, there are 500 positive and 500 negative sentences.</span></p>
<p><span class="heading0">Those were selected at random for larger datasets of reviews. The author has attempted to select sentences that have a clearly positive or negative connotation; the goal was for no neutral sentences to be selected. The review sentences are collected from three different websites/fields, which are as follows:</span></p>
<ul>
<li><a href="https://www.imdb.com/">https://www.imdb.com/</a></li>
<li><a href="https://www.amazon.com/">https://www.amazon.com/</a></li>
<li><a href="https://www.yelp.com/">https://www.yelp.com/</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Word2Vec pre-trained model</h1>
                </header>
            
            <article>
                
<p>Instead of generating a new Word2Vec model from scratch, Google's pre-trained news word vector model can be used, which provides an efficient implementation of the CBOW and skip-gram architectures for computing vector representations of words. These representations can subsequently be used in many NLP applications and further research.</p>
<p>The model can be downloaded from <a href="https://code.google.com/p/word2vec/">https://code.google.com/p/word2vec/</a> manually. The Word2Vec model takes a text corpus as input and produces the word vectors as output. It first constructs a vocabulary from the training text data and then learns vector representation of words.</p>
<div class="packt_infobox">There are two ways to achieve a Word2Vec model: by using continuous bag-of-words and continuous skip-gram. Skip-gram is slower, but better for infrequent words, although CBOW is faster.</div>
<p>The resulting word vector file can be used as a feature in many natural language processing and machine learning applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sentiment analysis using Word2Vec and LSTM</h1>
                </header>
            
            <article>
                
<p>First, let's define the problem. Given a movie review (raw text), we have to classify that movie review as either positive or negative based on the words it contains, that is, sentiment. We do this by combining the Word2Vec model and LSTM: each word in a review is vectorized using the Word2Vec model and fed into an LSTM net. As stated earlier, we will train data in the Large Movie Review dataset. Now, here is the workflow of the overall project:</p>
<ul>
<li>First, we download the movie/product reviews dataset</li>
<li>Then we create or reuse an existing Word2Vec model (for example, Google News word vectors)</li>
<li>Then we load each review text and convert words to vectors and reviews to sequences of vectors</li>
<li>Then we create and train the LSTM network</li>
<li>Then we save the trained model</li>
<li>Then we evaluate the model on the test set</li>
<li>Then we restore the trained model and evaluate a sample review text from the s<span class="heading0">entiment labeled dataset<br/></span></li>
</ul>
<p>Now, let's take a look at what the <kbd>main()</kbd> method would look like if we go with the preceding workflow:</p>
<pre><strong>public static void main</strong>(String[] args) throws Exception {<br/>       Nd4j.getMemoryManager().setAutoGcWindow(10000);// see more in the FAQ section<br/>       wordVectors = WordVectorSerializer.loadStaticModel(new File(WORD_VECTORS_PATH)); // Word2vec path   <br/>       downloadAndExtractData(); // download and extract the dataset<br/>       networkTrainAndSaver(); // create net, train and save the model<br/>       networkEvaluator(); // evaluate the model on test set<br/>       sampleEvaluator(); // evaluate a simple review from text/file.<br/>}</pre>
<p>Let's break the preceding steps down into smaller steps. We'll start with dataset preparation using the Word2Vec model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the train and test set using the Word2Vec model</h1>
                </header>
            
            <article>
                
<p>Now, to prepare the dataset for training and testing, first, we have to download three files, which are outlined as follows:</p>
<ul>
<li>A Google-trained Word2Vec model</li>
<li>A large Movie Review dataset</li>
<li><span class="heading0">A sentiment labeled dataset</span></li>
</ul>
<p>The pre-trained Word2Vec is downloaded from <a href="https://code.google.com/p/word2vec/">https://code.google.com/p/word2vec/</a> <span class="MsoHyperlink">and then we can set the l</span>ocation for the Google News vectors manually:</p>
<pre><strong>public static final</strong> <strong>String</strong> WORD_VECTORS_PATH = "/Downloads/GoogleNews-vectors-negative300.bin.gz";</pre>
<p><span class="MsoHyperlink">Then, we will download and extract the L</span>arge Movie Review dataset from the following URL.</p>
<pre><strong>public static final String</strong> DATA_URL = "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz";</pre>
<p>Now, let's set the location to save and extract the training/testing data:</p>
<pre><strong>public static final String</strong> DATA_PATH = FilenameUtils.concat(System.getProperty("java.io.tmpdir"), "dl4j_w2vSentiment/");</pre>
<p>Now, we can either manually download or extract the dataset in our preferred location or, alternatively, the following method does it in an automated way. Note that I have slightly modified the original <span>DL4J </span>implementation:</p>
<pre><strong>public static void</strong> downloadAndExtractData() <strong>throws</strong> <strong>Exception</strong> {<br/>  //Create directory if required<br/>  <strong>File</strong> directory = new File(DATA_PATH);<br/><br/>  <strong>if</strong>(!directory.exists()) directory.mkdir();<br/>  //Download file:<br/>  <strong>String</strong> archizePath = DATA_PATH + "aclImdb_v1.tar.gz";<br/>  <strong>File</strong> archiveFile = new File(archizePath);<br/>  <strong>String</strong> extractedPath = DATA_PATH + "aclImdb";<br/>  <strong>File</strong> extractedFile = new File(extractedPath);<br/><br/>  <strong>if</strong>( !archiveFile.exists() ){<br/>    System.out.println("Starting data download (80MB)...");<br/>    <strong>FileUtils</strong>.copyURLToFile(new URL(DATA_URL), archiveFile);<br/>    System.out.println("Data (.tar.gz file) downloaded to " + archiveFile.getAbsolutePath());<br/><br/>    //Extract tar.gz file to output directory<br/>    <strong>DataUtilities</strong>.extractTarGz(archizePath, DATA_PATH);<br/>  } <strong>else</strong> {<br/>    //Assume if archive (.tar.gz) exists, then data has already been extracted<br/>    System.out.println("Data (.tar.gz file) already exists at " + archiveFile.getAbsolutePath());<br/><br/>    <strong>if</strong>( !extractedFile.exists()){<br/>    //Extract tar.gz file to output directory<br/>      <strong>DataUtilities</strong>.extractTarGz(archizePath, DATA_PATH);<br/>    } <strong>else</strong> {<br/>      System.out.println("Data (extracted) already exists at " + extractedFile.getAbsolutePath());<br/>    }<br/>  }<br/>}</pre>
<p>In the preceding method, download the dataset from the URL I mentioned using HTTP protocol. Then, extract the dataset to the location we mentioned. For this, I have used the <kbd>TarArchiveEntry</kbd>, <kbd>TarArchiveInputStream</kbd>, and <kbd>GzipCompressorInputStream</kbd> utilities from Apache Commons. Interested readers can find more details at <a href="http://commons.apache.org/">http://commons.apache.org/</a>.</p>
<p>In short, I have provided a class named <kbd>DataUtilities.java</kbd> that has two methods, <kbd>downloadFile()</kbd> and <kbd>extractTarGz()</kbd>, that are used for downloading and extracting the dataset.</p>
<p>First, the <kbd>downloadFile()</kbd> method takes the remote URL (that is, the URL of the remote file) and the local path (that is, where to download the file) as parameters and downloads a remote file if it doesn't exist. Now, let's see what the signature looks like:</p>
<pre><strong>public static boolean</strong> downloadFile(<strong>String</strong> remoteUrl, <strong>String</strong> localPath) <strong>throws IOException</strong> {<br/>  <strong>boolean</strong> downloaded = false;<br/><br/>  <strong>if</strong> (remoteUrl == null || localPath == null)<br/>       <strong>return</strong> downloaded;<br/><br/>  <strong>File</strong> file = new File(localPath);<br/>  <strong>if</strong> (!file.exists()) {<br/>    file.getParentFile().mkdirs();<br/>    <strong>HttpClientBuilder</strong> builder = HttpClientBuilder.create();<br/>    <strong>CloseableHttpClient</strong> client = builder.build();<br/>    <strong>try</strong> (CloseableHttpResponse response = client.execute(new HttpGet(remoteUrl))) {<br/>      <strong>HttpEntity</strong> entity = response.getEntity();<br/>      <strong>if</strong> (entity != null) {<br/>        <strong>try</strong> (<strong>FileOutputStream</strong> outstream = <strong>new FileOutputStream</strong>(file)) {<br/>          entity.writeTo(outstream);<br/>          outstream.flush();<br/>          outstream.close();<br/>        }<br/>      }<br/>    }<br/>    downloaded = true;<br/>  }<br/>  <strong>if</strong> (!file.exists())<br/>  <strong>throw new IOException</strong>("File doesn't exist: " + localPath);<br/>  <strong>return</strong> downloaded;<br/>}</pre>
<p>Second, the <kbd>extractTarGz()</kbd> method takes an input path (the <kbd>ism</kbd> input file path) and the output path (that is, the output directory path) as parameters and extracts the <kbd>tar.gz</kbd> file to a local folder. Now, let's see what the signature looks like:</p>
<pre><strong>public static void</strong> extractTarGz(<strong>String</strong> inputPath, <strong>String</strong> outputPath) throws IOException {<br/>  <strong>if</strong> (inputPath == null || outputPath == null)<br/>       return;<br/><br/>  <strong>final int</strong> bufferSize = 4096;<br/>  <strong>if</strong> (!outputPath.endsWith("" + File.separatorChar))<br/>      outputPath = outputPath + File.separatorChar;<br/><br/>  <strong>try</strong> (<strong>TarArchiveInputStream</strong> tais = new <strong>TarArchiveInputStream</strong>(<strong><br/>        new GzipCompressorInputStream</strong>(<strong>new BufferedInputStream</strong>(<br/>                                      <strong>new FileInputStream</strong>(inputPath))))) {<br/>    <strong>TarArchiveEntry</strong> entry;<br/>    <strong>while</strong> ((entry = (<strong>TarArchiveEntry</strong>) tais.getNextEntry()) != null) {<br/>      <strong>if</strong> (entry.isDirectory()) {<br/>        <strong>new File</strong>(outputPath + entry.getName()).mkdirs();<br/>      } <strong>else</strong> {<br/>        <strong>int</strong> count;<br/>        <strong>byte</strong> data[] = newbyte[bufferSize];<br/>        <strong>FileOutputStream</strong> fos = new FileOutputStream(outputPath + entry.getName());<br/>        <strong>BufferedOutputStream</strong> dest = new BufferedOutputStream(fos, bufferSize);<br/>        <strong>while</strong> ((count = tais.read(data, 0, bufferSize)) != -1) {<br/>              dest.write(data, 0, count);<br/>        }<br/>        dest.close();<br/>      }<br/>    }<br/>  }<br/>}</pre>
<p>Now, to utilize the preceding methods, you have to import the following packages:</p>
<pre><strong>import</strong> org.apache.commons.compress.archivers.tar.TarArchiveEntry;<br/><strong>import</strong> org.apache.commons.compress.archivers.tar.TarArchiveInputStream;<br/><strong>import</strong> org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream;</pre>
<div class="packt_infobox">By the way, Apache Commons is an Apache project focused on all aspects of reusable Java components. See more at <a href="https://commons.apache.org/">https://commons.apache.org/</a>.</div>
<p>Finally, the s<span class="heading0">entiment labeled dataset can be downloaded from</span> <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/00331/">https://archive.ics.uci.edu/ml/machine-learning-databases/00331/</a><span class="heading0">.</span> <span class="heading0">Once you have finished these steps, the next task will be to prepare the training and testing set.</span> For this, I have written a class named <kbd>SentimentDatasetIterator</kbd>, which is a <kbd>DataSetIterator</kbd> that is specialized for the IMDb review dataset used in our project. However, this can also be applied to any text dataset for text analytics in NLP. This class is a slight extension of the <kbd>SentimentExampleIterator.java</kbd> class, which is provided by the DL4J example. Thanks to the DL4J folks for making our life easier.</p>
<p>The <kbd>SentimentDatasetIterator</kbd> class takes either the train or test set from the s<span class="heading0">entiment labeled dataset and</span> Google pre-trained Word2Vec and generates training data sets. On the other hand, a single class (negative or positive) is used as the label to predict the final time step of each review. Additionally, since we are dealing with reviews of different lengths and only one output at the final time step, we use padding arrays. In short, our training dataset should contain the following items, that is, the 4D object:</p>
<ul>
<li>Features from each review text</li>
<li>Labels in either 1 or 0 (that is, for positive and negative, respectively)</li>
<li>Feature masks</li>
<li>Label masks</li>
</ul>
<p>So, let's get started with the following constructor, which is used for the following purposes:</p>
<pre><strong>private final</strong> WordVectors wordVectors;<br/><strong>private final int</strong> batchSize;<br/><strong>private final int</strong> vectorSize;<br/><strong>private final int</strong> truncateLength;<br/><strong>private int</strong> cursor = 0;<br/><strong>private final</strong> File[] positiveFiles;<br/><strong>private final</strong> File[] negativeFiles;<br/><strong>private final</strong> TokenizerFactory tokenizerFactory;<br/><br/><strong>public</strong> SentimentDatasetIterator(<strong>String</strong> dataDirectory, <strong>WordVectors</strong> wordVectors, <br/>                                 <strong>int</strong> batchSize, <strong>int</strong> truncateLength, <strong>boolean</strong> train) <strong>throws IOException</strong> {<br/>  <strong>this</strong>.batchSize = batchSize;<br/>  <strong>this</strong>.vectorSize = wordVectors.getWordVector(wordVectors.vocab().wordAtIndex(0)).length;<br/>  <strong>File</strong> p = <strong>new File</strong>(<strong>FilenameUtils</strong>.concat(dataDirectory, "aclImdb/" + (train ? "train" : "test") <br/>                                         + "/pos/") + "/");<br/>  <strong>File</strong> n = <strong>new File</strong>(<strong>FilenameUtils</strong>.concat(dataDirectory, "aclImdb/" + (train ? "train" : "test")<br/>                                         + "/neg/") + "/");<br/>  positiveFiles = p.listFiles();<br/>  negativeFiles = n.listFiles();<br/><br/>  <strong>this</strong>.wordVectors = wordVectors;<br/>  <strong>this</strong>.truncateLength = truncateLength;<br/>  tokenizerFactory = new DefaultTokenizerFactory();<br/>  tokenizerFactory.setTokenPreProcessor(new CommonPreprocessor());<br/>}</pre>
<p>In the preceding signature of the constructor, we used the following purposes:</p>
<ul>
<li>To keep track of the positive and negative review files in the directory of the IMDb review data set</li>
<li>To tokenize the review texts to words with stop words and unknown words removed</li>
<li>If the longest review exceeds <kbd>truncateLength</kbd>, only take the first <kbd>truncateLength</kbd> words</li>
<li>Word2Vec object</li>
<li>The batch size, which is the size of each minibatch for training</li>
</ul>
<p>Once initialization is complete, we load each review test as a string. Then, we alternate between positive and negative reviews:</p>
<pre><strong>List&lt;String&gt;</strong> reviews = new ArrayList&lt;&gt;(num);<br/><strong>boolean[]</strong> positive = newboolean[num];<br/><br/><strong>for</strong>(<strong>int</strong> i=0; i&lt;num &amp;&amp; cursor&lt;totalExamples(); i++ ){<br/>  <strong>if</strong>(cursor % 2 == 0){<br/>    //Load positive review<br/>    <strong>int</strong> posReviewNumber = cursor / 2;<br/>    <strong>String</strong> review = <strong>FileUtils</strong>.readFileToString(positiveFiles[posReviewNumber]);<br/>    reviews.add(review);<br/>    positive[i] = true;<br/>  } <strong>else</strong> {<br/>    //Load negative review<br/>    <strong>int</strong> negReviewNumber = cursor / 2;<br/>    <strong>String</strong> review = <strong>FileUtils</strong>.readFileToString(negativeFiles[negReviewNumber]);<br/>    reviews.add(review);<br/>    positive[i] = false;<br/>  }<br/>  cursor++;<br/>}</pre>
<p>Then, we tokenize the reviews and filter out unknown words (that is, words that are not included in the pre-trained Word2Vec model, for example, stop words):</p>
<pre><strong>List&lt;List&lt;String&gt;&gt;</strong> allTokens = <strong>new</strong> <strong>ArrayList</strong>&lt;&gt;(reviews.size());<br/><strong>int</strong> maxLength = 0;<br/><br/><strong>for</strong>(String s : reviews){<br/>  <strong>List&lt;String&gt;</strong> tokens = tokenizerFactory.create(s).getTokens();<br/>  <strong>List&lt;String&gt;</strong> tokensFiltered = <strong>new</strong> <strong>ArrayList</strong>&lt;&gt;();<br/><strong>  for</strong>(String t : tokens ){<br/><strong>    if</strong>(wordVectors.hasWord(t)) tokensFiltered.add(t);<br/>  }<br/>  allTokens.add(tokensFiltered);<br/>  maxLength = Math.<em>max</em>(maxLength,tokensFiltered.size());<br/>}</pre>
<p>Then, if the longest review exceeds the threshold <kbd>truncateLength</kbd>, we only take the first <kbd>truncateLength</kbd> words:</p>
<pre><strong>if</strong>(maxLength &gt; truncateLength) <br/>    maxLength = truncateLength;</pre>
<p>Then, we create data for training. Here, we have <kbd>reviews.size()</kbd> examples of varying lengths since we have two labels, positive or negative:</p>
<pre><strong>INDArray</strong> features = Nd4j.create(newint[]{reviews.size(), vectorSize, maxLength}, 'f');<br/><strong>INDArray</strong> labels = Nd4j.create(newint[]{reviews.size(), 2, maxLength}, 'f');</pre>
<p>Now that we are dealing with reviews of different lengths and only one output at the final time step, we use padding arrays, where the mask arrays contain 1 if data is present at that time step for that example, or 0 if the data is just padding:</p>
<pre><strong>INDArray</strong> featuresMask = Nd4j.<em>zeros</em>(reviews.size(), maxLength);<br/><strong>INDArray</strong> labelsMask = Nd4j.<em>zeros</em>(reviews.size(), maxLength);</pre>
<p>It is to be noted that creating the mask arrays for the features and labels are optional and may be null too. Then, we get the truncated sequence length of the <em>i</em><sup>th</sup> document, obtain all the word vectors for the current document, and transpose them to fit the second and third feature shape.</p>
<p>Once we have the word vectors ready, we put them into the features array at three indices, which is equal to <kbd>NDArrayIndex.interval(0, vectorSize)</kbd> having all elements between 0 and the length of the current sequence. Then, we assign 1 to each position where a feature is present, that is, at the interval of 0 and the sequence length.</p>
<p>Now, when it comes to label encoding, we set [0, 1] for a negative review text and [1, 0] for a positive review text. Finally, we specify that an output exists at the final time step for this example:</p>
<pre><strong>for</strong>( int i=0; i&lt;reviews.size(); i++ ){<br/>  <strong>List&lt;String&gt;</strong> tokens = allTokens.get(i);<br/>  <strong>int</strong> seqLength = Math.min(tokens.size(), maxLength);<br/>  <strong>final INDArray</strong> vectors = wordVectors.getWordVectors(tokens.subList(0, seqLength)).transpose();<br/>  features.put(<strong>new INDArrayIndex</strong>[] {<br/>      <strong>NDArrayIndex</strong>.point(i), <strong>NDArrayIndex</strong>.all(), <strong>NDArrayIndex</strong>.interval(0, seqLength)<br/>    }, vectors);<br/><br/>  featuresMask.get(new INDArrayIndex[] {<strong>NDArrayIndex</strong>.point(i), NDArrayIndex.interval(0,      <br/>                   seqLength)}).assign(1);<br/>  <strong>int</strong> idx = (positive[i] ? 0 : 1);<br/>  <strong>int</strong> lastIdx = Math.min(tokens.size(),maxLength);<br/><br/>  labels.putScalar(newint[]{i,idx,lastIdx-1},1.0);<br/>  labelsMask.putScalar(newint[]{i,lastIdx-1},1.0);<br/>}</pre>
<p>Note that the main problem hindering dropout in NLP has been that it could not be applied to recurrent connections, as the aggregating dropout masks would effectively zero out embeddings over time—hence, feature masking has been used in the preceding code block.</p>
<p>Now, up to this point, all the required elements are prepared so finally, we return the dataset as an <kbd>NDArray</kbd> (that is, 4D) containing the features, labels, <kbd>featuresMask</kbd>, and <kbd>labelsMask</kbd>:</p>
<pre><strong>return new DataSet</strong>(features,labels,featuresMask,labelsMask);</pre>
<p>More elaborately, using <kbd>DataSet</kbd>, we will create a dataset with the specified input <kbd>INDArray</kbd> and labels (output) <kbd>INDArray</kbd>, and (optionally) mask arrays for the features and labels.</p>
<p>Finally, our training set will be at hand using the following invocation:</p>
<pre><strong>SentimentDatasetIterator</strong> train = <strong>new SentimentDatasetIterator</strong>(DATA_PATH, wordVectors, <br/>                                                              batchSize, truncateReviewsToLength, true);</pre>
<p>Fantastic! Now we can create our neural networks by specifying the layers and hyperparameters in the next step.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Network construction, training, and saving the model</h1>
                </header>
            
            <article>
                
<p>As discussed in the Titanic survival prediction section, again, everything starts with <kbd>MultiLayerConfiguration</kbd>, which organizes those layers and their hyperparameters. Our LSTM network consists of five layers. The input layer is followed by three LSTM layers. Then, the last layer is an RNN layer, which is also the output layer.</p>
<p>More technically, the first layer is the input layer, and then three layers are placed as LSTM layers. For the LSTM layers, we initialize the weights using Xavier, we use SGD as the optimization algorithm with the Adam updater, and we use Tanh as the activation function. Finally, the RNN output layer has a softmax activation function that gives us a probability distribution over classes (that is, it outputs the sum to 1.0) and MCXENT, which is the multiclass cross entropy loss function.</p>
<p><span class="uiqtextrenderedqtext">For creating LSTM layers, DL4J provides both LSTM and <kbd>GravesLSTM</kbd> classes. The latter is</span> an LSTM recurrent net, which is based on Graves, but comes up without CUDA support: supervised sequence labelling with RNN (see more at <a href="http://www.cs.toronto.edu/~graves/phd.pdf">http://www.cs.toronto.edu/~graves/phd.pdf</a>). <span class="uiqtextrenderedqtext">Now, before we start creating the network, first let's define the required hyperparameters such as the number of input/hidden/output nodes (that is, neurons):</span></p>
<pre>// Network hyperparameters: Truncate reviews with length greater than this<br/><strong>static int</strong> truncateReviewsToLength = 30;<br/><strong>static int</strong> numEpochs = 10; // number of training epochs<br/><strong>static int</strong> batchSize = 64; //Number of examples in each minibatch<br/><strong>static int</strong> vectorSize = 300; //Size of word vectors in Google Word2Vec<br/><strong>static int</strong> seed = 12345; //Seed for reproducibility<br/><strong>static int</strong> numClasses = 2; // number of classes to be predicted<br/><strong>static int</strong> numHiddenNodes = 256;</pre>
<p>We will now create a network configuration and conduct network training. With DL4J, you add a layer by calling a layer on the <kbd>NeuralNetConfiguration.Builder()</kbd>, specifying its place in the order of layers (the zero-indexed layer in the following code is the input layer):</p>
<pre><strong>MultiLayerConfiguration</strong> LSTMconf = new <strong>NeuralNetConfiguration</strong>.Builder()<br/>     .seed(seed)<br/>     .updater(new Adam(1e-8)) // Gradient updater with Adam<br/>     .l2(1e-5) // L2 regularization coefficient for weights<br/>     .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)<br/>     .weightInit(WeightInit.XAVIER)<br/>     .gradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue)<br/>     .gradientNormalizationThreshold(1.0)     <br/>     .trainingWorkspaceMode(WorkspaceMode.SEPARATE).inferenceWorkspaceMode(WorkspaceMode.SEPARATE)<br/>     .list()<br/>     .layer(0, new LSTM.Builder()<br/>           .nIn(vectorSize)<br/>           .nOut(numHiddenNodes)<br/>           .activation(Activation.TANH)<br/>           .build())<br/>     .layer(1, new LSTM.Builder()<br/>           .nIn(numHiddenNodes)<br/>           .nOut(numHiddenNodes)<br/>           .activation(Activation.TANH)<br/>           .build())<br/>     .layer(2, new RnnOutputLayer.Builder()<br/>          .activation(Activation.SOFTMAX)<br/>          .lossFunction(LossFunction.XENT)<br/>          .nIn(numHiddenNodes)<br/>          .nOut(numClasses)<br/>          .build())<br/>    .pretrain(false).backprop(true).build();</pre>
<p>Finally, we also specify that we do not need to do any pre-training (which is typically needed in a deep belief network or stacked auto-encoders). Then, we initialize the network and start the training on the training set:</p>
<pre><strong>MultiLayerNetwork</strong> model = <strong>new</strong> <strong>MultiLayerNetwork</strong>(LSTMconf);<br/>model.init();</pre>
<p>Typically, this type of network has lots of hyperparameters. Let's print the number of parameters in the network (and for each layer):</p>
<pre><strong>Layer</strong>[] layers = model.getLayers();<br/><strong>int</strong> totalNumParams = 0;<br/><strong>for</strong>(<strong>int</strong> i=0; i&lt;layers.length; i++ ){<br/>  <strong>int</strong> nParams = layers[i].numParams();<br/>  System.out.println("Number of parameters in layer " + i + ": " + nParams);<br/>  totalNumParams += nParams;<br/>}<br/>System.out.println("Total number of network parameters: " + totalNumParams);<br/><br/><q>&gt;&gt;</q><br/> <span class="packt_screen">Number of parameters in layer 0: 570,368<br/> Number of parameters in layer 1: 525,312<br/> Number of parameters in layer 2: 514<br/> Total number of network parameters: 1,096,194</span></pre>
<p>As I said, our network has 1 million parameters, which is huge. This also imposes a great challenge while tuning hyperparameters. However, we will see some tricks in the FAQ section.</p>
<pre><strong>MultiLayerNetwork</strong> net = <strong>new MultiLayerNetwork</strong>(LSTMconf);<br/>net.init();<br/>net.setListeners(new ScoreIterationListener(1));<br/><strong>for</strong> (<strong>int</strong> i = 0; i &lt; numEpochs; i++) {<br/>  net.fit(train);<br/>  train.reset();<br/>  System.out.println("Epoch " + (i+1) + " finished ...");<br/>}<br/>System.out.println("Training has been completed");</pre>
<p>Once the training has been completed, we can save the trained model for model persistence and subsequent reuse. For that, DL4J provides support for the trained model sterilization using the <kbd>writeModel()</kbd> method from the <kbd>ModelSerializer</kbd> class. Additionally, it provides the functionality for restoring the saved model using the <kbd>restoreMultiLayerNetwork()</kbd> method.</p>
<p>We will see more in the following step. Nevertheless, we can also save the network updater too, that is, the state for momentum, RMSProp, Adagrad, and so on:</p>
<pre><strong>File</strong> locationToSave = <strong>new File</strong>(modelPath); //location and file format<br/><strong>boolean</strong> saveUpdater = true; // we save the network updater too<br/><strong>ModelSerializer</strong>.writeModel(net, locationToSave, saveUpdater);</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Restoring the trained model and evaluating it on the test set</h1>
                </header>
            
            <article>
                
<p>Once the training has been completed, the next task will be to evaluate the model. We will evaluate the model's performance on the test set. For the evaluation, we will be using <kbd>Evaluation()</kbd>, which creates an evaluation object with two possible classes.</p>
<p>First, let's iterate the evaluation on every test sample and get the network's prediction from the trained model. Finally, the <kbd>eval()</kbd> method checks the prediction against the true class:</p>
<pre><strong>public static void</strong> networkEvaluator() <strong>throws Exception</strong> {<br/>      System.out.println("Starting the evaluation ...");<br/>      <strong>boolean</strong> saveUpdater = true;<br/><br/>      //Load the model<br/>      <strong>MultiLayerNetwork</strong> restoredModel = <strong>ModelSerializer</strong>.restoreMultiLayerNetwork(modelPath, saveUpdater);<br/>      //WordVectors wordVectors = getWord2Vec();<br/>      <strong>SentimentDatasetIterator</strong> test = <strong>new SentimentDatasetIterator</strong>(DATA_PATH, wordVectors, batchSize,   <br/>                                                                   truncateReviewsToLength, false);<br/>      <strong>Evaluation</strong> evaluation = restoredModel.evaluate(test);<br/>      System.out.println(evaluation.stats());<br/>      System.out.println("----- Evaluation completed! -----");<br/>}<br/><br/><q>&gt;&gt;&gt;</q><br/> <q>==========================Scores========================================</q><br/> <q># of classes: 2</q><br/> <q>Accuracy: 0.8632</q><br/> <q>Precision: 0.8632</q><br/> <q>Recall: 0.8632</q><br/> <q>F1 Score: 0.8634</q><br/> <q>Precision, recall, and F1: Reported for positive class (class 1 -"negative") only</q><br/> <q>========================================================================</q></pre>
<p><br/>
The predictive accuracy for the sentiment analysis using LSTM is about 87%, which is good considering that we have not focused on hyperparameter tuning! Now, let's see how the classifier predicts across each class:</p>
<pre><span class="packt_screen">Predictions labeled as positive classified by model as positive: 10,777 times<br/> Predictions labeled as positive classified by model as negative: 1,723 times<br/> Predictions labeled as negative classified by model as positive: 1,696 times<br/> Predictions labeled as negative classified by model as negative: 10,804 times</span></pre>
<p>Similar to <a href="e27fb252-7892-4659-81e2-2289de8ce570.xhtml" target="_blank">Chapter 2</a>, <em>Cancer Types Prediction Using Recurrent Type Networks</em>, we will now compute another metric called Matthews's correlation coefficient for this binary classification problem:</p>
<pre>// Compute Matthews correlation coefficient<br/><strong>EvaluationAveraging</strong> averaging = <strong>EvaluationAveraging</strong>.<strong><em>Macro</em></strong>;<br/><strong>double</strong> MCC = eval.matthewsCorrelation(averaging);<br/>System.<strong><em>out</em></strong>.println("Matthews correlation coefficient: "+ MCC);<br/><br/><span class="packt_screen"><q>&gt;&gt;</q><br/> Matthews's correlation coefficient: 0.22308172619187497</span></pre>
<p>This shows a weakly positive relationship, showing that our model performs quite well. Up next, we will use the trained model for inferencing, that is, we will perform predictions on sample review texts.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Making predictions on sample review texts</h1>
                </header>
            
            <article>
                
<p>Now, let's take a look at how our trained model generalizes, that is, how it performs on unseen review texts from the sentiment labeled sentences dataset. First, we need to restore the trained model from the disk:</p>
<pre>System.<strong><em>out</em></strong>.println("Starting the evaluation on sample texts ...");<br/><strong>boolean</strong> saveUpdater = <strong>true</strong>;<br/><br/><strong>MultiLayerNetwork</strong> restoredModel = ModelSerializer.<em>restoreMultiLayerNetwork</em>(<em>modelPath</em>, saveUpdater);<br/><strong>SentimentDatasetIterator</strong> test = <strong>new</strong> SentimentDatasetIterator(<strong><em>DATA_PATH</em></strong>, <em>wordvectors</em>, <em>batchSize</em>, <br/>                                                             <em>truncateReviewsToLength</em>, <strong>false</strong>);</pre>
<p>Now, we can randomly extract two review texts from IMDb, Amazon, and Yelp, where the first one expresses a positive sentiment, and the second one expresses a negative sentiment (according to the known labels). Then, we can create a HashMap containing both the review strings and labels:</p>
<pre><strong>String</strong> IMDb_PositiveReview = "Not only did it only confirm that the film would be unfunny and generic, but <br/>                              it also managed to give away the ENTIRE movie; and I'm not exaggerating - <br/>                              every moment, every plot point, every joke is told in the trailer";<br/><br/><strong>String</strong> IMDb_NegativeReview = "One character is totally annoying with a voice that gives me the feeling of <br/>                              fingernails on a chalkboard.";<br/><br/><strong>String</strong> Amazon_PositiveReview = "This phone is very fast with sending any kind of messages and web browsing <br/>                                is significantly faster than previous phones i have used";<br/><br/><strong>String</strong> Amazon_NegativeReview = "The one big drawback of the MP3 player is that the buttons on the phone's <br/>                             front cover that let you pause and skip songs lock out after a few seconds.";<br/><br/><strong>String</strong> Yelp_PositiveReview = "My side Greek salad with the Greek dressing was so tasty, and the pita and <br/>                              hummus was very refreshing.";<br/><br/><strong>String</strong> Yelp_NegativeReview = "Hard to judge whether these sides were good because we were grossed out by <br/>                              the melted styrofoam and didn't want to eat it for fear of getting sick.";</pre>
<p>Then, we create an array of the preceding strings:</p>
<pre><strong>String</strong>[] reviews = {IMDb_PositiveReview, IMDb_NegativeReview, Amazon_PositiveReview, <br/>                    Amazon_NegativeReview, Yelp_PositiveReview, Yelp_NegativeReview};<br/><br/><strong>String</strong>[] sentiments = {"Positive", "Negative", "Positive", "Negative", "Positive", "Negative"};<br/><strong>Map&lt;String, String&gt;</strong> reviewMap = <strong>new</strong> HashMap&lt;String, String&gt;();<br/><br/>reviewMap.put(reviews[0], sentiments[0]);<br/>reviewMap.put(reviews[1], sentiments[1]);<br/>reviewMap.put(reviews[2], sentiments[2]);<br/>reviewMap.put(reviews[3], sentiments[3]);</pre>
<p>Then, we iterate over the map and do the sample evaluation using the pre-trained model as follows:</p>
<pre>System.out.println("Starting the evaluation on sample texts ...");         <br/><strong>for</strong> (<strong>Map.Entry&lt;String, String&gt;</strong> entry : reviewMap.entrySet()) {<br/>            <strong>String</strong> text = entry.getKey();<br/>            <strong>String</strong> label = entry.getValue();<br/>            <br/>            <strong>INDArray</strong> features = test.loadFeaturesFromString(text, truncateReviewsToLength);<br/>            <strong>INDArray</strong> networkOutput = restoredModel.output(features);<br/>            <br/>            <strong>int</strong> timeSeriesLength = networkOutput.size(2);<br/>            <strong>INDArray</strong> probabilitiesAtLastWord = networkOutput.get(NDArrayIndex.point(0), <br/>                              NDArrayIndex.all(), NDArrayIndex.point(timeSeriesLength - 1));<br/><br/>            System.out.println("-------------------------------");<br/>            System.out.println("\n\nProbabilities at last time step: ");<br/>            System.out.println("p(positive): " + probabilitiesAtLastWord.getDouble(0));<br/>            System.out.println("p(negative): " + probabilitiesAtLastWord.getDouble(1));<br/><br/>            <strong>Boolean</strong> flag = false;<br/>            <strong>if</strong>(probabilitiesAtLastWord.getDouble(0) &gt; probabilitiesAtLastWord.getDouble(1))<br/>                flag = true;<br/>            <strong>else</strong><br/>                flag = false;<br/>            <strong>if</strong> (flag == true) {<br/>                System.out.println("The text express a positive sentiment, actually it is " + label);<br/>            } <strong>else</strong> {<br/>                System.out.println("The text express a negative sentiment, actually it is " + label);<br/>            }<br/>        }<br/>    System.out.println("----- Sample evaluation completed! -----");<br/>    }</pre>
<p>If you look at the preceding code block carefully, you can see that we converted each review text as a time series by extracting features. Then, we computed the network output (that is, probability). Then, we compare the probability, that is, if the probability is that of it being a positive sentiment, we set a flag as true, or false otherwise. This way, we then take a decision on the final class prediction.</p>
<p>We have also utilized the <kbd>loadFeaturesFromString()</kbd> method in the preceding code block, which converts a review string to features in <kbd>INDArray</kbd> format. It takes two parameters, <kbd>reviewContents</kbd>, which is the content of the review to vectorize, and <kbd>maxLength</kbd>, which is the maximum length of the review text. Finally, it returns a <kbd>features</kbd> array for the given input string:</p>
<pre><strong>public INDArray</strong> loadFeaturesFromString(String reviewContents, int maxLength){<br/>        <strong>List&lt;String&gt;</strong> tokens = tokenizerFactory.create(reviewContents).getTokens();<br/>        <strong>List&lt;String&gt;</strong> tokensFiltered = new ArrayList&lt;&gt;();<br/>        <strong>for</strong>(<strong>String</strong> t : tokens ){<br/>            <strong>if</strong>(wordVectors.hasWord(t)) tokensFiltered.add(t);<br/>        }<br/>        <strong>int</strong> outputLength = Math.max(maxLength,tokensFiltered.size());<br/>        <strong>INDArray</strong> features = Nd4j.create(1, vectorSize, outputLength);<br/><br/>        <strong>for</strong>(<strong>int</strong> j=0; j&lt;tokens.size() &amp;&amp; j&lt;maxLength; j++ ){<br/>            <strong>String</strong> token = tokens.get(j);<br/>            <strong>INDArray</strong> vector = wordVectors.getWordVectorMatrix(token);<br/>            features.put(new INDArrayIndex[]{NDArrayIndex.point(0), <br/>                          NDArrayIndex.all(), NDArrayIndex.point(j)}, vector);<br/>        }<br/>        <strong>return</strong> features;<br/>    }</pre>
<div class="packt_tip">If you don't want to truncate, simply use <kbd>Integer.MAX_VALUE</kbd>.</div>
<p>Now, let's go back to our original discussion. Hilariously, we made it more human, that is, without utilizing an activation function. Finally, we print the result for each review text and its associated label:</p>
<pre><q>&gt;<br/></q>Probabilities at last time step:<br/> p(positive): 0.003569001331925392<br/> p(negative): 0.9964309930801392<br/> The text express a negative sentiment, actually, it is Positive<br/><br/>p(positive): 0.003569058608263731<br/> p(negative): 0.9964308738708496<br/> The text express a negative sentiment, actually, it is Negative<br/> -------------------------------<br/> Probabilities at last time step:<br/> p(positive): 0.003569077467545867<br/> p(negative): 0.9964308738708496<br/> The text express a negative sentiment, actually, it is Negative<br/><br/>p(positive): 0.003569045104086399<br/> p(negative): 0.9964308738708496<br/> The text express a negative sentiment, actually, it is Positive<br/> -------------------------------<br/> Probabilities at last time step:<br/> p(positive): 0.003570008557289839<br/> p(negative): 0.996429979801178<br/> The text express a negative sentiment, actually, it is Positive<br/><br/>p(positive): 0.0035690285731106997<br/> p(negative): 0.9964309930801392<br/> The text express a negative sentiment, actually, it is Negative<br/><br/>----- Sample evaluation completed! -----</pre>
<p>So, our trained model has made 50% incorrect predictions, especially since it always predicts a positive review as a negative. In short, it is not that good at generalizing to unknown texts, which can be seen with an accuracy of 50%.</p>
<p>Now, a stupid question might come to mind. Did our network underfit? Is there any way to observe how the training went? In other words, the question would be: Why didn't our LSTM net neural show higher accuracy? We will try to answer these questions in the next section. So stay with me!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Frequently asked questions (FAQs)</h1>
                </header>
            
            <article>
                
<p>Now that we have solved the sentiment analysis problem with an acceptable level of accuracy, there are other practical aspects of this problem and overall deep learning phenomena that need to be considered too. In this section, we will see some frequently asked questions that might already be on your mind. Answers to these questions can be found in Appendix A:</p>
<ol>
<li>I understand that the predictive accuracy of sentiment analysis using LSTM is still reasonable. However, it does not perform well on the Sentiment labeled dataset. Did our network overfit? Is there any way to observe how the training went?</li>
<li>Considering a huge number of review texts, can we perform the training on the GPU?</li>
<li>In relation to question 2, can we even undertake the whole process using Spark?</li>
<li>Where can I get more training datasets for sentiment analysis?</li>
<li>Instead of downloading the training data in <kbd>.zip</kbd> format manually, can we use the <kbd>extractTarGz()</kbd> method?</li>
<li>My machine has limited memory. Give me a clue as to how memory management and garbage collection work in DL4J.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have seen how to implement and deploy a hands-on deep learning project that classifies review texts as either positive or negative based on the words they contain. We have used a large-scale movie review dataset that contains 50,000 reviews (training plus testing). A combined approach using Word2Vec (that is, a widely used word embedding technique in NLP) and the LSTM network for modeling was applied: the pre-trained Google news vector model was used as the neural word embeddings.</p>
<p>Then, the training vectors, along with the labels, were fed into the LSTM network, which successfully classified them as negative or positive sentiments. Then, it evaluated the trained model on the test set. Additionally, we have also seen how to apply text-based preprocessing techniques such as tokenizer, stop words removal and TF-IDF, as well as word-embedding operations in DL4J.</p>
<p>In the next chapter, we will see a complete example of how to develop a deep learning project to classify images using the DL4J transfer learning API. Through this application, users will be able to modify the architecture of an existing model, fine-tune learning configurations of an existing model, and hold parameters of a specified layer constantly during training, which is also referred to as frozen.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Answers to questions</h1>
                </header>
            
            <article>
                
<p><strong>Answer to question 1</strong>: We have seen that our trained model performs pretty well on the test set with an accuracy of 87%. Now, if we see the model versus iteration score and other parameters from the following graph, then we can see that our model was not overfitted:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-656 image-border" src="assets/45849f1d-1af8-415e-ba55-2557cb8acf40.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Model versus iteration score and other parameters of the LSTM sentiment analyzer</div>
<p>Now, for the sentiment labeled sentences, the trained model did not perform well. There could be several reasons for that. For example, our model is trained with only the movie review dataset, but here, we try to force our model to perform on different types of datasets too, for example, Amazon and Yelp. Nevertheless, we have not tuned the hyperparameters carefully.</p>
<p><strong>Answer to question 2</strong>: Yes, in fact, this will be very helpful. For this, we have to make sure that our programming environment is ready. In other words, first, we have to configure CUDA and cuDNN on our machine.</p>
<p>However, make sure that your machine has a NVIDIA GPU installed and configured with sufficient memory and CUDA compute capability. If you do not know how to configure such prerequisites, refer to this URL at <a href="https://docs.nvidia.com/deeplearning/sdk/cudnn-install/">https://docs.nvidia.com/deeplearning/sdk/cudnn-install/</a>. Once your machine has CUDA/cuDNN installed, in the <kbd>pom.xml</kbd> file, you have to add two entries:</p>
<ul>
<li>Backend in the project properties</li>
<li>CUDA as the platform dependency</li>
</ul>
<p>For step 1, the properties should now look as follows:</p>
<pre><strong>&lt;properties&gt;</strong><br/>        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;<br/>        &lt;java.version&gt;1.8&lt;/java.version&gt;<br/>        &lt;nd4j.backend&gt;nd4j-cuda-9.0-platform&lt;/nd4j.backend&gt;<br/>        &lt;nd4j.version&gt;1.0.0-alpha&lt;/nd4j.version&gt;<br/>        &lt;dl4j.version&gt;1.0.0-alpha&lt;/dl4j.version&gt;<br/>        &lt;datavec.version&gt;1.0.0-alpha&lt;/datavec.version&gt;<br/>        &lt;arbiter.version&gt;1.0.0-alpha&lt;/arbiter.version&gt;<br/>        &lt;logback.version&gt;1.2.3&lt;/logback.version&gt;<br/><strong> &lt;/properties&gt;</strong></pre>
<p>Now, for step 2, add the following dependency in the <kbd>pop.xml</kbd> file (that is, inside the dependencies tag):</p>
<pre><strong>&lt;dependency&gt;</strong><br/>         &lt;groupId&gt;org.nd4j&lt;/groupId&gt;<br/>         &lt;artifactId&gt;nd4j-cuda-9.0-platform&lt;/artifactId&gt;<br/>         &lt;version&gt;${nd4j.version}&lt;/version&gt;<br/><strong>&lt;/dependency&gt;</strong></pre>
<p>Then, update the Maven project, and the required dependencies will be downloaded automatically. Now, unless we perform the training on multiple GPUs, we do not need to make any changes. However, just run the same script again to perform the training. Then, you will experience the following logs on the console:</p>
<pre><strong>17:03:55.317 [main] INFO org.nd4j.linalg.factory.Nd4jBackend - Loaded [JCublasBackend] backend</strong><br/><strong> 17:03:55.360 [main] WARN org.reflections.Reflections - given scan urls are empty. set urls in the configuration</strong><br/><strong> 17:04:06.410 [main] INFO org.nd4j.nativeblas.NativeOpsHolder - Number of threads used for NativeOps: 32</strong><br/><strong> 17:04:08.118 [main] DEBUG org.nd4j.jita.concurrency.CudaAffinityManager - Manually mapping thread [18] to device [0], out of [1] devices...</strong><br/><strong> 17:04:08.119 [main] DEBUG org.nd4j.jita.concurrency.CudaAffinityManager - Manually mapping thread [19] to device [0], out of [1] devices...</strong><br/><strong> 17:04:08.119 [main] DEBUG org.nd4j.jita.concurrency.CudaAffinityManager - Manually mapping thread [20] to device [0], out of [1] devices...</strong><br/><strong> 17:04:08.119 [main] DEBUG org.nd4j.jita.concurrency.CudaAffinityManager - Manually mapping thread [21] to device [0], out of [1] devices...</strong><br/><strong> 17:04:08.119 [main] DEBUG org.nd4j.jita.concurrency.CudaAffinityManager - Manually mapping thread [22] to device [0], out of [1] devices...</strong><br/><strong> 17:04:08.119 [main] DEBUG org.nd4j.jita.concurrency.CudaAffinityManager - Manually mapping thread [23] to device [0], out of [1] devices...</strong><br/><strong> 17:04:08.123 [main] INFO org.nd4j.nativeblas.Nd4jBlas - Number of threads used for BLAS: 0</strong><br/><strong> 17:04:08.127 [main] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Backend used: [CUDA]; OS: [Windows 10]</strong><br/><strong> 17:04:08.127 [main] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Cores: [8]; Memory: [7.0GB];</strong><br/><strong> 17:04:08.127 [main] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Blas vendor: [CUBLAS]</strong><br/><strong> 17:04:08.127 [main] INFO org.nd4j.linalg.jcublas.ops.executioner.CudaExecutioner - Device opName: [GeForce GTX 1050]; CC: [6.1]; Total/free memory: [4294967296]</strong></pre>
<p>Nevertheless, in <a href="a59fb1f2-b585-44f5-a467-903b8c25867b.xhtml" target="_blank">Chapter 8</a>, <em>Distributed Deep Learning – Video Classification Using Convolutional LSTM Networks</em>, we will see how to make everything faster and scalable overall on multiple GPUs.</p>
<p><strong>Aswer</strong> <strong>to question 3</strong>: Yes, in fact, this will be very helpful. For this, we have to make sure that our programming environment is ready. In other words, first, we have to configure Spark on our machine. Once your machine has CUDA/cuDNN installed, we just want to configure Spark. In the <kbd>pom.xml</kbd> file, you have to add two entries:</p>
<ul>
<li>Backend in the project properties</li>
<li>Spark dependency</li>
</ul>
<p>For step 1, the properties should now look as follows:</p>
<pre><strong>&lt;properties&gt;</strong><br/>        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;<br/>        &lt;java.version&gt;1.8&lt;/java.version&gt;<br/>        &lt;nd4j.backend&gt;nd4j-cuda-9.0-platform&lt;/nd4j.backend&gt;<br/>        &lt;nd4j.version&gt;1.0.0-alpha&lt;/nd4j.version&gt;<br/>        &lt;dl4j.version&gt;1.0.0-alpha&lt;/dl4j.version&gt;<br/>        &lt;datavec.version&gt;1.0.0-alpha&lt;/datavec.version&gt;<br/>        &lt;arbiter.version&gt;1.0.0-alpha&lt;/arbiter.version&gt;<br/>        &lt;dl4j.spark.version&gt;1.0.0-alpha_spark_2&lt;/dl4j.spark.version&gt;<br/>        &lt;logback.version&gt;1.2.3&lt;/logback.version&gt;<br/><strong> &lt;/properties&gt;</strong></pre>
<p>Now, for step 2, add the following dependency in the <kbd>pop.xml</kbd> file (that is, inside the dependencies tag):</p>
<pre><strong>&lt;dependency&gt;</strong><br/>      &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;<br/>      &lt;artifactId&gt;dl4j-spark_2.11&lt;/artifactId&gt;<br/>      &lt;version&gt;1.0.0-alpha_spark_2&lt;/version&gt;<br/><strong>&lt;/dependency&gt;</strong></pre>
<p>Then, update the Maven project, and the required dependencies will be downloaded automatically. Now, unless we perform the training on multiple GPUs, we do not need to make any changes. However, we need to convert the training/testing dataset into a Spark-compatible JavaRDD.</p>
<p>I have written all of the steps in the <kbd>SentimentAnalyzerSparkGPU.java</kbd> file that can be used to see how the overall steps work. A general warning is that if you perform the training on Spark, the DL4J UI will not work because of cross-dependencies on the Jackson library. For that, we must first create the <kbd>JavaSparkContext</kbd> using the <kbd>sparkSession()</kbd> method as follows:</p>
<pre><strong>public </strong><strong>static</strong> JavaSparkContext <em>spark</em>;<br/><strong>static </strong><strong>int </strong><em>batchSizePerWorker</em> = 16;<br/><br/><strong>public </strong><strong>static</strong> JavaSparkContext getJavaSparkContext () {<br/>                    SparkConf sparkConf = <strong>new</strong> SparkConf();<br/>                    sparkConf.set("spark.locality.wait", "0");<br/>                    sparkConf.setMaster("local[*]").setAppName("DL4J Spark");<br/><em>                    spak</em> = <strong>new</strong> JavaSparkContext(sparkConf);<br/><strong>                    return </strong><em>spark</em>;<br/>}</pre>
<p>Then, we have to convert the sentiment training dataset iterator to JavaRDD of the dataset. First, we create a list of datasets and then add each training sample to the list as follows:</p>
<pre><strong>List&lt;DataSet&gt;</strong> trainDataList = <strong>new</strong> <strong>ArrayList</strong>&lt;&gt;();<br/><strong>while</strong>(train.hasNext()) {<br/>       trainDataList.add(train.next());<br/>    }</pre>
<p>Then, we create a <kbd>JavaSparkContext</kbd> by invoking the <kbd>sparkSession()</kbd> method as follows:</p>
<pre>spark = createJavaSparkContext();</pre>
<p>Finally, we utilize the <kbd>parallelize()</kbd> method of Spark to create the JavaRDD of the dataset, which can then be used to perform the training using Spark:</p>
<pre><strong>JavaRDD&lt;DataSet&gt;</strong> trainData = <em>spark</em>.parallelize(trainDataList);</pre>
<p>Then, the Spark <kbd>TrainingMaster</kbd> uses the <kbd>ParameterAveragingTrainingMaster</kbd>, which helps perform the training using Spark. Please refer to <a href="a59fb1f2-b585-44f5-a467-903b8c25867b.xhtml" target="_blank">Chapter 8</a>, <em>Distributed Deep Learning – Video Classification Using Convolutional LSTM Networks</em>, for more details:</p>
<pre><strong>TrainingMaster&lt;?, ?&gt;</strong> tm = (TrainingMaster&lt;?, ?&gt;) <strong>new</strong> ParameterAveragingTrainingMaster<br/>               .Builder(<em>batchSizePerWorker</em>)             <br/>               .averagingFrequency(5).workerPrefetchNumBatches(2)<br/>               .batchSizePerWorker(<em>batchSizePerWorker</em>).build();</pre>
<p>Then, we create the <kbd>SparkDl4jMultiLayer</kbd> instead of just the <kbd>MultilayerNetwork</kbd> as we did previously:</p>
<pre><strong>SparkDl4jMultiLayer</strong> sparkNet = <strong>new</strong> <strong>SparkDl4jMultiLayer</strong>(<em>spark</em>, LSTMconf, tm);</pre>
<p>Then, we create a training listener that records the score of each iteration as follows:</p>
<pre>sparkNet.setListeners(Collections.&lt;IterationListener&gt;<em>singletonList</em>(<strong>new</strong> ScoreIterationListener(1)));<br/>sparkNet.setListeners(<strong>new</strong> ScoreIterationListener(1));</pre>
<p>Finally, we start the training as follows:</p>
<pre><strong>for</strong> (<strong>int</strong> i = 0; i &lt; <em>numEpochs</em>; i++) {<br/>         sparkNet.fit(trainData);<br/>         System.<strong><em>out</em></strong>.println("Epoch " + (i+1) + " has been finished ...");<br/>       }</pre>
<p>However, using this approach, there is a drawback, that is, we cannot save the trained model directly like this but first, we have to fit the network using the training data and collect the output as the <kbd>MultiLayerNetwork</kbd> as follows:</p>
<pre><strong>MultiLayerNetwork</strong> outputNetwork = sparkNet.fit(trainData);<br/><br/>//Save the model<br/><strong>File</strong> locationToSave = <strong>new</strong> <strong>File</strong>(<em>modelPath</em>);<br/><br/><strong>boolean</strong> saveUpdater = <strong>true</strong>;<br/><strong>ModelSerializer</strong>.<em>writeModel</em>(outputNetwork, locationToSave, saveUpdater);</pre>
<p><strong>Answer to question 4</strong>: There are many sources where you can get a sentiment analysis dataset. A few of them are listed here:</p>
<ul>
<li>The huge n-grams dataset from Google: <a href="http://storage.googleapis.com/books/ngrams/books/datasetsv2.html">storage.googleapis.com/books/ngrams/books/datasetsv2.html</a></li>
<li>Twitter sentiment: <a href="http://www.sananalytics.com/lab/twitter-sentiment/">http://www.sananalytics.com/lab/twitter-sentiment/</a></li>
<li>UMICH SI650—sentiment classification dataset on Kaggle: <a href="http://inclass.kaggle.com/c/si650winter11/data">http://inclass.kaggle.com/c/si650winter11/data</a></li>
<li>Multi-domain sentiment dataset: <a href="http://www.cs.jhu.edu/~mdredze/datasets/sentiment/">http://www.cs.jhu.edu/~mdredze/datasets/sentiment/</a></li>
</ul>
<p><strong>Answer to question 5</strong>: The answer is no, but with a little effort we can make it work. For that, we can use the <kbd>ZipArchiveInputStream</kbd> and <kbd>GzipCompressorInputStream</kbd> classes from Apache commons as follows:</p>
<pre><strong>public </strong><strong>static </strong><strong>void</strong> extractZipFile(String inputPath, String outputPath) <br/>               <strong>throws</strong> IOException {<strong>                    <br/>               if</strong> (inputPath == <strong>null</strong> || outputPath == <strong>null</strong>)<br/><strong>                             return</strong>;<br/><strong>               final </strong><strong>int</strong> bufferSize = 4096;<br/><strong>               if</strong> (!outputPath.endsWith("" + File.<strong><em>separatorChar</em></strong>))<br/>                            outputPath = outputPath + File.<strong><em>separatorChar</em></strong>; <br/><strong>               try</strong> (ZipArchiveInputStream tais = <strong>new</strong> ZipArchiveInputStream(<strong>new</strong> <br/>                         GzipCompressorInputStream(<br/>                             <strong>new</strong> BufferedInputStream(<strong>new</strong> FileInputStream(inputPath))))) {<br/>                             ZipArchiveEntry entry;<br/><strong>                              while</strong> ((entry = (ZipArchiveEntry) tais.getNextEntry()) != <strong>null</strong>) {<br/><strong>                              if</strong> (entry.isDirectory()) {<br/><strong>                                        new</strong> File(outputPath + entry.getName()).mkdirs();<br/>                               } <strong>else</strong> {<br/>                                <strong>int</strong> count; <br/>                                <strong>byte</strong> data[] = <strong>new</strong><strong>byte</strong>[bufferSize];<br/>                                FileOutputStream fos = <strong>new</strong> FileOutputStream(outputPath + entry.getName());<br/>                                BufferedOutputStream dest = <strong>new</strong> BufferedOutputStream(fos, bufferSize);<br/>                                <strong>while</strong> ((count = tais.read(data, 0, bufferSize)) != -1) {<br/>                                       dest.write(data, 0, count);<br/>                                       }<br/>                            dest.close();<br/>                       }<br/>                 }<br/>           }<br/>}</pre>
<p><strong>Answer to question 6</strong>: Well, this is really only a concern if your machine doesn't have enough memory. For this application, I did not face any OOP type issue while running this project as my laptop has 32 GB of RAM.</p>
<p>Apart from this step, we can also choose DL4J garbage collection, especially because memory is constrained on your end. DL4J provides a method called <kbd>getMemoryManager()</kbd> that returns a backend-specific <kbd>MemoryManager</kbd> implementation for low-level memory management. Additionally, we have to enable the periodic <kbd>System.gc()</kbd> calls with the windowsills minimal time in milliseconds between calls. Let's see an example:</p>
<pre>Nd4j.getMemoryManager().setAutoGcWindow(10000); // min 10s between calls</pre>
<p>However, simply set <kbd>windowMillis</kbd> to <kbd>0</kbd> to disable this option.</p>


            </article>

            
        </section>
    </body></html>