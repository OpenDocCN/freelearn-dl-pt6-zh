["```py\n# Create the model helper for the train model\ntrain_model = model_helper.ModelHelper(name=\"mnist_lenet_train_model\")\n```", "```py\n# Specify the input is from the train lmdb\ndata, label = add_model_inputs(\n    train_model,\n    batch_size=64,\n    db=os.path.join(data_folder, \"mnist-train-nchw-lmdb\"),\n    db_type=\"lmdb\",\n)\n```", "```py\n# Load data from DB\ninput_images_uint8, input_labels = brew.db_input(\n    model,\n    blobs_out=[\"input_images_uint8\", \"input_labels\"],\n    batch_size=batch_size,\n    db=db,\n    db_type=db_type,\n)\n```", "```py\n# Cast grayscale pixel values to float\n# Scale pixel values to [0, 1]\ninput_images = model.Cast(input_images_uint8, \"input_images\", to=core.DataType.FLOAT)\ninput_images = model.Scale(input_images, input_images, scale=float(1./256))\n```", "```py\n# We do not need gradient for backward pass\n# This op stops gradient computation through it\ninput_images = model.StopGradient(input_images, input_images)\n```", "```py\n# Build the LeNet network\nsoftmax_layer = build_mnist_lenet(train_model, data)\n```", "```py\n# Convolution layer that operates on the input MNIST image\n# Input is grayscale image of size 28x28 pixels\n# After convolution by 20 kernels each of size 5x5,\n# output is 20 channels, each of size 24x24\nlayer_1_input_dims = 1   # Input to layer is grayscale, so 1 channel\nlayer_1_output_dims = 20 # Output from this layer has 20 channels\nlayer_1_kernel_dims = 5  # Each kernel is of size 1x5x5\nlayer_1_conv = brew.conv(\n    model,\n    input_blob_name,\n    \"layer_1_conv\",\n    dim_in=layer_1_input_dims,\n    dim_out=layer_1_output_dims,\n    kernel=layer_1_kernel_dims,\n)\n```", "```py\n# Max-pooling layer that operates on output from previous convolution layer\n# Input is 20 channels, each of size 24x24\n# After pooling by 2x2 windows and stride of 2, the output of this layer\n# is 20 channels, each of size 12x12\nlayer_2_kernel_dims = 2 # Max-pool over 2x2 windows\nlayer_2_stride = 2      # Stride by 2 pixels between each pool\nlayer_2_pool = brew.max_pool(\n    model,\n    layer_1_conv,\n    \"layer_2_pool\",\n    kernel=layer_2_kernel_dims,\n    stride=layer_2_stride,\n)\n```", "```py\n# Convolution layer that operates on output from previous pooling layer.\n# Input is 20 channels, each of size 12x12\n# After convolution by 50 kernels, each of size 20x5x5,\n# the output is 50 channels, each of size 8x8\nlayer_3_input_dims = 20  # Number of input channels\nlayer_3_output_dims = 50 # Number of output channels\nlayer_3_kernel_dims = 5  # Each kernel is of size 50x5x5\nlayer_3_conv = brew.conv(\n    model,\n    layer_2_pool,\n    \"layer_3_conv\",\n    dim_in=layer_3_input_dims,\n    dim_out=layer_3_output_dims,\n    kernel=layer_3_kernel_dims,\n)\n\n# Max-pooling layer that operates on output from previous convolution layer\n# Input is 50 channels, each of size 8x8\n# Apply pooling by 2x2 windows and stride of 2\n# Output is 50 channels, each of size 4x4\nlayer_4_kernel_dims = 2 # Max-pool over 2x2 windows\nlayer_4_stride = 2      # Stride by 2 pixels between each pool\nlayer_4_pool = brew.max_pool(\n    model,\n    layer_3_conv,\n    \"layer_4_pool\",\n    kernel=layer_4_kernel_dims,\n    stride=layer_4_stride,\n)\n```", "```py\n# Fully-connected layer that operates on output from previous pooling layer\n# Input is 50 channels, each of size 4x4\n# Output is vector of size 500\nlayer_5_input_dims = 50 * 4 * 4\nlayer_5_output_dims = 500\nlayer_5_fc = brew.fc(\n    model,\n    layer_4_pool,\n    \"layer_5_fc\",\n    dim_in=layer_5_input_dims,\n    dim_out=layer_5_output_dims,\n)\n\n# ReLU layer that operates on output from previous fully-connected layer\n# Input and output are both of size 500\nlayer_6_relu = brew.relu(\n    model,\n    layer_5_fc,\n    \"layer_6_relu\",\n)\n```", "```py\n# Fully-connected layer that operates on output from previous ReLU layer\n# Input is of size 500\n# Output is of size 10, the number of classes in MNIST dataset\nlayer_7_input_dims = 500\nlayer_7_output_dims = 10\nlayer_7_fc = brew.fc(\n    model,\n    layer_6_relu,\n    \"layer_7_fc\",\n    dim_in=layer_7_input_dims,\n    dim_out=layer_7_output_dims,\n)\n```", "```py\n# Softmax layer that operates on output from previous fully-connected layer\n# Input and output are both of size 10\n# Each output (0 to 9) is a probability score on that digit\nlayer_8_softmax = brew.softmax(\n    model,\n    layer_7_fc,\n    \"softmax\",\n)\n```", "```py\n# Compute cross entropy between softmax scores and labels\ncross_entropy = train_model.LabelCrossEntropy([softmax_layer, label], \"cross_entropy\")\n\n# Compute the expected loss\nloss = train_model.AveragedLoss(cross_entropy, \"loss\")\n```", "```py\n# Use the average loss we just computed to add gradient operators to the model\ntrain_model.AddGradientOperators([loss])\n```", "```py\n# Specify the optimization algorithm\noptimizer.build_sgd(\n    train_model,\n    base_learning_rate=0.1,\n    policy=\"step\",\n    stepsize=1,\n    gamma=0.999,\n)\n```", "```py\nbrew.accuracy(model, [softmax_layer, label], \"accuracy\")\n```", "```py\n# The parameter initialization network only needs to be run once.\nworkspace.RunNetOnce(train_model.param_init_net)\n```", "```py\n# Creating an actual network as a C++ object in memory.\n# We need this as the object is going to be used a lot\n# so we avoid creating an object every single time it is used.\nworkspace.CreateNet(train_model.net, overwrite=True)\n```", "```py\n# Run the network for some iterations and track its loss and accuracy\ntotal_iters = 100\naccuracy = np.zeros(total_iters)\nloss = np.zeros(total_iters)\nfor i in range(total_iters):\n    workspace.RunNet(train_model.net)\n    accuracy[i] = workspace.blobs[\"accuracy\"]\n    loss[i] = workspace.blobs[\"loss\"]\n    print(\"Iteration: {}, Loss: {}, Accuracy: {}\".format(i, loss[i], \n    accuracy[i]))\n\n```"]