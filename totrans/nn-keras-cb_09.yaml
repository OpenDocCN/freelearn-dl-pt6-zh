- en: Encoding Inputs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Need for encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoding an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoding for recommender systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A typical image is comprised thousands of pixels; text is also comprised thousands
    of unique words, and the number of distinct customers of a company could be in
    the millions. Given this, all three—user, text, and images—would have to be represented
    as a vector in thousands of dimensional planes. The drawback of representing a
    vector in such a high dimensional space is that we will not able to calculate
    the similarity of vectors efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Representing an image, text, or user in a lower dimension helps us in grouping
    entities that are very similar. Encoding is a way to perform unsupervised learning
    to represent an input in a lower dimension with minimal loss of information while
    retaining the information about images that are similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be learning about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Encoding an image to a much a lower dimension
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vanilla autoencoder
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Multilayer autoencoder
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional autoencoder
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing encodings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoding users and items in recommender systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating the similarity between encoded entities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Need for encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Encoding is typically used where the number of dimensions in a vector is huge.
    Encoding helps turn a large vector into a vector that has far fewer dimensions
    without losing much information from the original vector. In the following sections,
    let's explore the need for encoding images, text, and recommender systems.
  prefs: []
  type: TYPE_NORMAL
- en: Need for encoding in text analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand the need for encoding in text analysis, let''s consider the following
    scenario. Let''s go through the following two sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/295eeeca-8cb4-4c90-b9a8-c303c76c4fdf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In traditional text analysis, the preceding two sentences are one-hot encoded,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26bdf572-bf24-4ad1-9c8d-30c1cc2c3814.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that there are five unique words in the two sentences.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding one-hot encoded versions of the words result in an encoded version
    of sentences as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87b77716-a1e5-4266-b56f-37c4292347a9.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding scenario, we can see that the Euclidian distance between the
    two sentences is greater than zero, as the encodings of **like** and **enjoy**
    are different. However, intuitively, we know that the words enjoy and like are
    very similar to each other. Further, the distance between (**I**, **Chess**) is
    the same as (**like**, **enjoy**).
  prefs: []
  type: TYPE_NORMAL
- en: Note that, given that there are five unique words across the two sentences,
    we represent each word in a five-dimensional space. In an encoded version, we
    represent a word in a lower dimension (let's say, three-dimensions) in such a
    way that words that are similar will have less distance between them when compared
    to words that are not similar.
  prefs: []
  type: TYPE_NORMAL
- en: Need for encoding in image analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand the need for encoding in image analysis, let''s consider the
    scenario where we group images; however, the labels of images are not present.
    For further clarification, let''s consider the following images of the same label
    in the MNIST dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a63ab262-0216-4f6e-82c4-5a3d9254d6f0.png)'
  prefs: []
  type: TYPE_IMG
- en: Intuitively, we know that both the preceding images correspond to the same label.
    However, when we take the Euclidian distance between the preceding two images,
    the distance is greater than zero, as different pixels are highlighted in the
    preceding two images.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should notice the following issue in storing the information of an image:'
  prefs: []
  type: TYPE_NORMAL
- en: While the image comprises a total of 28 x 28 = 784 pixels, the majority of the
    columns are black and thus no information is composed in them, resulting in them
    occupying more space while storing information than is needed.
  prefs: []
  type: TYPE_NORMAL
- en: Using autoencoders, we represent the preceding two images in a lower dimension
    in such a way that the distance between the two encoded versions is now much smaller
    and at the same time ensuring that the encoded version does not lose much information
    from the original image.
  prefs: []
  type: TYPE_NORMAL
- en: Need for encoding in recommender systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand the need for encoding in recommender systems, let's consider the
    scenario of movie recommendations for customers. Similar to text analysis, if
    we were to one-hot encode each movie/customer, we would end up with multiple thousand-dimensional
    vectors for each movie (as there are thousands of movies). Encoding users in a
    much lower dimension based on the viewing habits of customers, which results in
    grouping movies based on the similarity of movies, could help us map movies that
    a user is more likely to watch.
  prefs: []
  type: TYPE_NORMAL
- en: A similar concept can also be applied to e-commerce recommendation engines,
    as well as recommending products to a customer in a supermarket.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding an image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image encoding can be performed in multiple ways. In the following sections,
    we will contrast the performance of vanilla autoencoders, multilayer autoencoders,
    and convolutional autoencoders. The term auto-encoding refers to encoding in such
    a way that the original input is recreated with a far fewer number of dimensions
    in an image.
  prefs: []
  type: TYPE_NORMAL
- en: An autoencoder takes an image as input and encodes the input image into a lower
    dimension in such a way that we can reconstruct the original image by using only
    the encoded version of the input image. Essentially, you can think of the encoded
    version of similar images as having similar encoded values.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we define our strategy, let''s get a feel for how autoencoders work:'
  prefs: []
  type: TYPE_NORMAL
- en: We'll define a toy dataset that has one vector with 11 values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We''ll represent the 11 values in a lower dimension (two-dimensions):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The information present in input data is preserved as much as possible while
    lowering the dimensions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The vector in low dimensional space is called an **embedding**/**encoded** **vector**,
    **bottleneck** **feature**/**vector**, or a **compressed representation**
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The 11 values are converted into two values by performing a matrix multiplication
    of input values with a random weight matrix that is 11 x 2 in dimensions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The lower dimension vector represents bottleneck features. Bottleneck features
    are features that are required to reconstruct the original image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We''ll reconstruct the lower dimension bottleneck feature vector to obtain
    the output vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The two-dimension feature vector is multiplied by a matrix that is 2 x 11 in
    shape to obtain an output that is 1 x 11 in shape. Matrix multiplication of 1
    x 2 with 2 x 11 vectors gives an output that is 1 x 11 in shape.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll calculate the sum of squared difference between the input vector and the
    output vector
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We vary the randomly initialized weight vectors to minimize the sum of squared
    difference between the input and output vectors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The resulting encoded vector would be a lower dimensional vector that represents
    an 11-dimensional vector in two-dimensional space
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While leveraging neural networks, you can consider the encoded vector as a hidden
    layer that connects the input and output layer.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, for the neural network, the input and output layer values are
    exactly the same and the hidden layer has a lower dimension than the input layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we''ll learn about multiple autoencoders:'
  prefs: []
  type: TYPE_NORMAL
- en: Vanilla autoencoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multilayer autoencoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional autoencoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following sections, we will implement multiple variations of autoencoders
    in Python (the code file is available as `Auto_encoder.ipynb` in GitHub).
  prefs: []
  type: TYPE_NORMAL
- en: Vanilla autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A vanilla autoencoder looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d67047e4-6d9e-4cd9-bccf-fdd5f74f2ea5.png)'
  prefs: []
  type: TYPE_IMG
- en: As displayed in the preceding diagram, a Vanilla autoencoder reconstructs the
    input with a minimal number of hidden layers and hidden units in its network.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how a vanilla autoencoder works, let''s go through the following
    recipe, where we reconstruct MNIST images using a lower-dimensional encoded version
    of the original image (the code file is available as `Auto_encoder.ipynb` in GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Reshape and scale the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Construct the network architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'A summary of model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20ebaedb-619c-4ea0-a329-6281c3dc9837.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding code, we are representing a 784-dimensional input in a 32-dimensional
    encoded version.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compile and fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are using the mean squared error loss function, as the pixel values
    are continuous. Additionally, the input and output arrays are just the same—`X_train`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Print a reconstruction of the first four input images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The reconstructured MNIST digits are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f536303-198e-479f-a4d6-b3d57a3b67a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To understand how well the autoencoder worked, let''s compare the preceding
    predictions with the original input images:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The original MNIST digits are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17100fd3-9b8d-42bd-8eab-da2d28d86814.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding images, we can see that the reconstructed images are blurred
    when compared to the original input image.
  prefs: []
  type: TYPE_NORMAL
- en: To get around the issue of blurring, let's build multilayer autoencoders that
    are deep (thereby resulting in more parameters) and thus potentially a better
    representation of the original image.
  prefs: []
  type: TYPE_NORMAL
- en: Multilayer autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A multilayer autoencoder looks as follows, where there are more number of hidden
    layers connecting the input layer to output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f3ee687-42fa-453a-b0a0-93c0bf13c546.png)'
  prefs: []
  type: TYPE_IMG
- en: Essentially, a multilayer autoencoder reconstructs the input with more hidden
    layers in its network.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build a multilayer autoencoder, we will repeat the same steps that we had
    in the previous section, up until *step 3*. However, *step 4*, where the network
    architecture is defined, will be modified to include multilayers, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'A summary of model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2314656c-b2fc-4c71-a031-206d5f870de8.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding network, our first hidden layer has 100 units, the second hidden
    layer (which is the embedded version of the image) is 32-dimensional, and the
    third hidden layer is 100-dimensional in shape.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the network architecture is defined, we compile and run it, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The predictions of the preceding model are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff4c5081-bf2f-4346-9439-33d59b9a2bd4.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the preceding predictions are still a little blurred compared to the
    original images.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have explored vanilla and multilayer autoencoders. In this section,
    we will see how convolutional autoencoders work in reconstructing the original
    images from a lower-dimensional vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Convolutional autoencoders look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3cd826e0-e037-416a-992e-7ab29296eb34.png)'
  prefs: []
  type: TYPE_IMG
- en: Essentially, a convolutional autoencoder reconstructs the input with more hidden
    layers in its network where the hidden layers consist of convolution, pooling,
    and upsampling the downsampled image.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to a multilayer autoencoder, a convolutional autoencoder differs from
    other types of autoencoder in its model architecture. In the following code, we
    will define the model architecture for the convolutional autoencoder while every
    other step remains similar to the vanilla autoencoder up until *step 3*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The only differences between the `X_train` and `X_test` shapes are defined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that, in the preceding step, we are reshaping the image so that it can
    be passed to a `conv2D` method:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the model architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we have defined a convolutional architecture where we
    reshaped the input image so that it has a 32-dimensional embedded version in the
    middle of its architecture and finally upsample it so that we are able to reconstruct
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'A summary of the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/008c615f-9a99-4eb9-9a39-141aa90aeda8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Compile and fit the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we make predictions on the first four test data points, the reconstructed
    images look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ede7d605-daf3-4166-a7ae-c42a38c8a464.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the reconstruction is now slightly better than the previous two reconstructions
    (using Vanilla and multilayer autoencoders) of the test images.
  prefs: []
  type: TYPE_NORMAL
- en: Grouping similar images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we represented each image in a much lower dimension
    with the intuition that images that are similar will have similar embeddings and
    images that are not similar will have dissimilar embeddings. However, we have
    not yet looked at the similarity measure or examined embeddings in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will try and plot embeddings in a 2D space. We can reduce
    the 32-dimensional vector to a two-dimensional space by using a technique called
    **t-SNE**. (More about t-SNE can be found here: [http://www.jmlr.org/papers/v9/vandermaaten08a.html](http://www.jmlr.org/papers/v9/vandermaaten08a.html).)'
  prefs: []
  type: TYPE_NORMAL
- en: This way, our feeling that similar images will have similar embeddings can be
    proved, as similar images should be clustered together in the two-dimensional
    plane.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we will represent embeddings of all the test images
    in a two-dimensional plane:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the 32-dimensional vector of each of the 10,000 images in the test:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform t-SNE to generate a two-dimensional vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the visualization of the t-SNE dimensions for the test image embeddings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'A visualization of embeddings in two dimensional space is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f76bf7a-b13e-4f60-85bd-a035f6b14dd5.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that, in the preceding chart, we see that, more often than not, clusters
    are formed among images that correspond to the same label.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding for recommender systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, in the previous sections, we have encoded an image. In this section,
    we will encode users and movies in a movie-related dataset. The reason for this
    is that there could be millions of users as customers and thousands of movies
    in a catalog. Thus, we are not in a position to one-hot encode such data straight
    away. Encoding comes in handy in such a scenario. One of the most popular techniques
    that's used in encoding for recommender systems is matrix factorization. In the
    next section, we'll understand how it works and generate embeddings for users
    and movies.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The thinking behind encoding users and movies is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If two users are similar in terms of liking certain movies, the vectors that
    represent the two users should be similar. In the same manner, if two movies are
    similar (potentially, they belong to the same genre or have the same cast), they
    should have similar vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The strategy that we''ll adopt to encode movies, so that we recommend a new
    set of movies based on the historical set of movies watched by a user, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the dataset that contains information of the users and the rating they
    gave to different movies that they watched
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign IDs to both users and movies
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert users and movies into 32-dimensional vectors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the functional API in Keras to perform the dot product of the 32-dimensional
    vectors of movies and users:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If there are 100,000 users and 1,000 movies, the movie matrix will be 1,000
    x 32 dimensions and the user matrix will be 100,000 x 32 dimensions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The dot product of the two will be 100,000 x 1,000 in dimension
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Flatten the output and pass it through a dense layer, before connecting to the
    output layer, which has a linear activation and has output values ranging from
    1 to 5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the embedding weights of movies
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the embedding weights of users
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Movies that are similar to a given movie of interest can be found by calculating
    the pairwise similarity of the movie of interest with every other movie in the
    dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following code, we will come up with with a vector for a user and a
    movie in a typical recommender system (The code file is available as `Recommender_systems.ipynb` in
    GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: Import the dataset. The recommended dataset is available in code in GitHub.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/922112db-3673-419c-ae17-295a587c0051.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Convert the user and movies into a categorical variable. In the following code,
    we create two new variables—`User2` and `Movies2`—which are categorical:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Assign a unique ID to each user and movie:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the unique IDs as new columns to our original table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Define embeddings for each user ID and unique ID:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we are extracting the total number of unique users and
    unique movies in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we are defining a function that takes an ID as input
    and converts it into an embedding vector that is `n_out` in dimensions for the
    total of `n_in` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are extracting 100 dimensions for each unique user
    and also for each unique movie.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'A summary of model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f135011c-a121-4c89-868a-a4d1f89a3baa.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the vectors of each user or movie:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As we thought earlier, movies that are similar should have similar vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, while identifying the similarity between embeddings, we use a measure
    named cosine similarity (there's more information on how cosine similarity is
    calculated in the next chapter).
  prefs: []
  type: TYPE_NORMAL
- en: 'For a randomly selected movie that is located in the 574^(th) position, cosine
    similarity is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding code, we can calculate the ID that is most similar to the
    movie located in the 574^(th) location of the categorical movie column.
  prefs: []
  type: TYPE_NORMAL
- en: Once we look into the movie ID list, we should see that the most similar movies
    to the given movie, indeed happen to be similar, intuitively.
  prefs: []
  type: TYPE_NORMAL
