- en: Chapter 5. Improved GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the introduction of the **Generative Adversarial Networks** (**GAN**s)
    in 2014[1], its popularity has rapidly increased. GANs have proved to be a useful
    generative model that can synthesize new data that look real. Many of the research
    papers in deep learning that followed, proposed measures to address the difficulties
    and limitations of the original GAN.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed in previous chapters, GANs can be notoriously difficult to train
    and prone to mode collapse. Mode collapse is a situation where the generator is
    producing outputs that look the same even though the loss functions are already
    optimized. In the context of MNIST digits, with mode collapse, the generator may only
    be producing digits 4 and 9 since they look similar. **Wasserstein GAN** (**WGAN**)[2]
    addressed these problems by arguing that stable training and mode collapse can
    be avoided by simply replacing the GAN loss function based on Wasserstein 1 or
    **Earth-Mover distance** (**EMD**).
  prefs: []
  type: TYPE_NORMAL
- en: However, the issue of stability is not the only problem of GANs. There is also
    the increasing need to improve the perceptive quality of the generated images.
    **Least Squares GAN** (**LSGAN**)[3] proposed to address both these problems simultaneously.
    The basic premise is that sigmoid cross entropy loss leads to a vanishing gradient
    during training. This results in poor image quality. Least squares loss does not
    induce vanishing gradients. The resulting generated images are of higher perceptive
    quality when compared to vanilla GAN generated images.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, CGAN introduced a method for conditioning the output
    of the generator. For example, if we wanted to get digit 8, we would include the
    conditioning label in the input to the generator. Inspired by CGAN, the **Auxiliary
    Classifier GAN** (**ACGAN**)[4] proposed a modified conditional algorithm that
    results in better perceptive quality and diversity of the outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the goal of this chapter is to introduce these improved GANs and
    to present:'
  prefs: []
  type: TYPE_NORMAL
- en: The theoretical formulation of the WGAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An understanding of the principles of LSGAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An understanding of the principles of ACGAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge of how to implement improved GANs - WGAN, LSGAN, and ACGAN using Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wasserstein GAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we've mentioned before, GANs are notoriously hard to train. The opposing
    objectives of the two networks, the discriminator and the generator, can easily
    cause training instability. The discriminator attempts to correctly classify the
    fake data from the real data. Meanwhile, the generator tries its best to trick
    the discriminator. If the discriminator learns faster than the generator, the
    generator parameters will fail to optimize. On the other hand, if the discriminator
    learns more slowly, then the gradients may vanish before reaching the generator.
    In the worst case, if the discriminator is unable to converge, the generator is
    not going to be able to get any useful feedback.
  prefs: []
  type: TYPE_NORMAL
- en: Distance functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The stability in training a GAN can be understood by examining its loss functions. To better
    understand the GAN loss functions, we're going to review the common distance or
    divergence functions between two probability distributions. Our concern is the
    distance between *p*[data] for true data distribution and *p*[g] for generator
    data distribution. The goal of GANs is to make *p* [g] → *p*[data]. *Table 5.1.1*
    shows the divergence functions.
  prefs: []
  type: TYPE_NORMAL
- en: In most maximum likelihood tasks, we'll use **Kullback-Leibler** (**KL**) divergence
    or *D*[KL] in the loss function as a measure of how far our neural network model
    prediction is from the true distribution function. As shown in *Equation* *5.1.1*,
    *D*[KL] is not symmetric since ![Distance functions](img/B08956_05_001.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: '**Jensen-Shannon** (**JS**) or *D*[JS] is a divergence that is based on *D*[KL].
    However, unlike *D*[KL], *D*[JS] is symmetrical and will be finite. In this section,
    we''ll show that optimizing the GAN loss functions is equivalent to optimizing
    *D*[JS].'
  prefs: []
  type: TYPE_NORMAL
- en: '| Divergence | Expression |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Kullback-Leibler (KL)5.1.1 | ![Distance functions](img/B08956_05_002.jpg)![Distance
    functions](img/B08956_05_03.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| Jensen-Shannon (JS)5.1.2 | ![Distance functions](img/B08956_05_004.jpg) |'
  prefs: []
  type: TYPE_TB
- en: '| Earth-Mover Distance (EMD) or Wasserstein 15.1.3 | ![Distance functions](img/B08956_05_005.jpg)where![Distance
    functions](img/B08956_05_006.jpg) is the set of all joint distributions *y(x,y)*
    whose marginal are *p*[data] and *p*[g]. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5.1.1: The divergence functions between two probability distribution
    functions *p*[data] and *p*[g]'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distance functions](img/B08956_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1.1: The EMD is the weighted amount of mass from **x** to be transported
    in order to match the target distribution, **y**'
  prefs: []
  type: TYPE_NORMAL
- en: The intuition behind EMD is that it is a measure of how much mass ![Distance
    functions](img/B08956_05_008.jpg) should be transported by *d* = ||*x* - *y*||
    for the probability distribution *p*[data] in order to match the probability distribution
    *p*[g]. ![Distance functions](img/B08956_05_009.jpg)is a joint distribution in
    the space of all possible joint distributions ![Distance functions](img/B08956_05_010.jpg).
    ![Distance functions](img/B08956_05_011.jpg) is also known as a transport plan
    to reflect the strategy for transporting masses to match the two probability distributions.
    There are many possible transport plans given the two probability distributions.
    Roughly speaking, *inf* indicates a transport plan with the minimum cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, *Figure 5.1.1* shows us two simple discrete distributions ![Distance
    functions](img/B08956_05_012.jpg) and ![Distance functions](img/B08956_05_013.jpg).
    ![Distance functions](img/B08956_05_014.jpg) has masses *m* *i* *for* i = 1, 2,
    3 and 4 at locations *x* *i* * for* i = 1, 2, 3 and 4\. Meanwhile ![Distance functions](img/B08956_05_015.jpg)
    has masses *m* *i* * for* i =1 and 2 at locations *y* *i* * for* i = 1 and 2\.
    To match the distribution ![Distance functions](img/B08956_05_016.jpg), the arrows
    show the minimum transport plan to move each mass *x* *i* by *d* *i*. The EMD
    is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distance functions](img/B08956_05_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: (Equation 5.1.4)
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 5.1.1*, the EMD can be interpreted as the least amount of work needed
    to move the pile of dirt ![Distance functions](img/B08956_05_018.jpg) to fill
    the hole ![Distance functions](img/B08956_05_019.jpg). While in this example,
    the *inf* can also be deduced from the figure, in most cases especially in continuous
    distributions, it is intractable to exhaust all possible transport plans. We will
    come back to this problem later on in this chapter. In the meantime, we'll show
    how the GAN loss functions are, in fact, minimizing the **Jensen-Shannon** (**JS**)
    divergence.
  prefs: []
  type: TYPE_NORMAL
- en: Distance function in GANs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''re now going to compute the optimal discriminator given any generator from
    the loss function in the previous chapter. We''ll recall the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distance function in GANs](img/B08956_05_020.jpg) (Equation 4.1.1)'
  prefs: []
  type: TYPE_IMG
- en: 'Instead of sampling from the noise distribution, the preceding equation can
    also be expressed as sampling from the generator distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distance function in GANs](img/B08956_05_021.jpg) (Equation 5.1.5)'
  prefs: []
  type: TYPE_IMG
- en: 'To find the minimum ![Distance function in GANs](img/B08956_05_022.jpg):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distance function in GANs](img/B08956_05_023.jpg) (Equation 5.1.6)'
  prefs: []
  type: TYPE_IMG
- en: '![Distance function in GANs](img/B08956_05_024.jpg) (Equation 5.1.7)'
  prefs: []
  type: TYPE_IMG
- en: 'The term inside the integral is in the form of *y* → *a* log *y* + *b* log(1
    - *y*) which has a known maximum value at ![Distance function in GANs](img/B08956_05_025.jpg)
    for ![Distance function in GANs](img/B08956_05_026.jpg), for any ![Distance function
    in GANs](img/B08956_05_027.jpg) not including {0,0}. Since the integral does not
    change the location of the maximum value (or the minimum value of ![Distance function
    in GANs](img/B08956_05_028.jpg)) for this expression, the optimal discriminator
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distance function in GANs](img/B08956_05_029.jpg) (Equation 5.1.8)'
  prefs: []
  type: TYPE_IMG
- en: 'Consequently, the loss function is given the optimal discriminator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distance function in GANs](img/B08956_05_030.jpg) (Equation 5.1.9)'
  prefs: []
  type: TYPE_IMG
- en: '![Distance function in GANs](img/B08956_05_031.jpg) (Equation 5.1.10)'
  prefs: []
  type: TYPE_IMG
- en: '![Distance function in GANs](img/B08956_05_032.jpg) (Equation 5.1.11)'
  prefs: []
  type: TYPE_IMG
- en: '![Distance function in GANs](img/B08956_05_033.jpg) (Equation 5.1.12)'
  prefs: []
  type: TYPE_IMG
- en: We can observe from *Equation 5.1.12* that the loss function of the optimal
    discriminator is a constant minus twice the Jensen-Shannon divergence between
    the true distribution, *p*[data], and any generator distribution, *p*[g]. Minimizing
    ![Distance function in GANs](img/B08956_05_034.jpg) implies maximizing ![Distance
    function in GANs](img/B08956_05_035.jpg) or the discriminator must correctly classify
    fake from real data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Meanwhile, we can safely argue that the optimal generator is when the generator
    distribution is equal to the true data distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distance function in GANs](img/B08956_05_036.jpg) (Equation 5.1.13)'
  prefs: []
  type: TYPE_IMG
- en: This makes sense since the objective of the generator is to fool the discriminator
    by learning the true data distribution. Effectively, we can arrive at the optimal
    generator by minimizing *D*[JS]*,* or by making *p*[g] → *p*[data]. Given an optimal
    generator, the optimal discriminator is ![Distance function in GANs](img/B08956_05_037.jpg)
    with ![Distance function in GANs](img/B08956_05_038.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: '![Distance function in GANs](img/B08956_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1.2: An example of two distributions with no overlap. ![Distance function
    in GANs](img/B08956_05_039.jpg) for *p*[g]'
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem is that when the two distributions have no overlap, there''s no
    smooth function that will help to close the gap between them. Training the GANs
    will not converge by gradient descent. For example, let''s suppose:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p*[data] =(*x*, *y*) where ![Distance function in GANs](img/B08956_05_040.jpg)
    (Equation 5.1.14)'
  prefs: []
  type: TYPE_NORMAL
- en: '*p*[g] = (*x*, *y*) where ![Distance function in GANs](img/B08956_05_041.jpg)
    (Equation 5.1.15)'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 5.1.2*. *U*(0,1) is the uniform distribution. The divergence
    for each distance function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distance function in GANs](img/B08956_05_42.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![Distance function in GANs](img/B08956_05_043.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![Distance function in GANs](img/B08956_05_44.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![Distance function in GANs](img/B08956_05_045.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: Since *D*[JS] is a constant, the GAN will not have a sufficient gradient to
    drive *p*[g] → *p*[data]. We'll also find that *D*[KL] or reverse *D*[KL] is not
    helpful either. However, with *W*(*p*[data],*p*[g]) we can have a smooth function
    in order to attain *p*[g] → *p*[data] by gradient descent. EMD or Wasserstein
    1 seems to be a more logical loss function in order to optimize GANs since *D*[JS]
    fails in situations when two distributions have minimal to no overlap.
  prefs: []
  type: TYPE_NORMAL
- en: For further understanding, an excellent discussion on distance functions can
    be found at [https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-W](https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-W)
    [GAN.html](http://GAN.html).
  prefs: []
  type: TYPE_NORMAL
- en: Use of Wasserstein loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before using EMD or Wasserstein 1, there is one more problem to overcome. It
    is intractable to exhaust the space of ![Use of Wasserstein loss](img/B08956_05_046.jpg)
    to find ![Use of Wasserstein loss](img/B08956_05_047.jpg). The proposed solution
    is to use its Kantorovich-Rubinstein dual:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Use of Wasserstein loss](img/B08956_05_048.jpg)'
  prefs: []
  type: TYPE_IMG
- en: (Equation 5.1.16)
  prefs: []
  type: TYPE_NORMAL
- en: 'Equivalently, EMD, ![Use of Wasserstein loss](img/B08956_05_049.jpg), is the
    supremum (roughly, maximum value) over all the *K*-Lipschitz functions: ![Use
    of Wasserstein loss](img/B08956_05_050.jpg). *K*-Lipschitz functions satisfy the
    constraint:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Use of Wasserstein loss](img/B08956_05_51.jpg) (Equation 5.1.17)'
  prefs: []
  type: TYPE_IMG
- en: For all ![Use of Wasserstein loss](img/B08956_05_052.jpg), the *K*-Lipschitz
    functions have bounded derivatives and almost always continuously differentiable
    (for example, *f*(*x*), = |*x*| has bounded derivatives and continuous but not
    differentiable at *x* = 0).
  prefs: []
  type: TYPE_NORMAL
- en: '*Equation 5.1.16* can be solved by finding a family of *K*-Lipschitz functions
    ![Use of Wasserstein loss](img/B08956_05_053.jpg):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Use of Wasserstein loss](img/B08956_05_54.jpg) (Equation 5.1.18)'
  prefs: []
  type: TYPE_IMG
- en: 'In the context of GANs, *Equation* *5.1.18* can be rewritten by sampling from
    *z*-noise distribution and replacing *f*[w] by the discriminator function, *D*[w]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Use of Wasserstein loss](img/B08956_05_55.jpg) (Equation 5.1.19)'
  prefs: []
  type: TYPE_IMG
- en: 'We use the bold letter to highlight the generality to multi-dimensional samples.
    The final problem we face is how to find the family of functions ![Use of Wasserstein
    loss](img/B08956_05_056.jpg). The proposed solution we''re going to go over is
    that at every gradient update, the weights of the discriminator, *w*, are clipped
    between lower and upper bounds, (for example, -0.0,1 and 0.01):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Use of Wasserstein loss](img/B08956_05_57.jpg) (Equation 5.1.20)'
  prefs: []
  type: TYPE_IMG
- en: The small values of *w* constrains the discriminator to a compact parameter
    space thus ensuring Lipschitz continuity.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use *Equation 5.1.19* as the basis of our new GAN loss functions. EMD
    or Wasserstein 1 is the loss function that the generator aims to minimize, and
    the cost function that the discriminator tries to maximize (or minimize -*W*(*p*[data],*p*[g])):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Use of Wasserstein loss](img/B08956_05_058.jpg)'
  prefs: []
  type: TYPE_IMG
- en: (Equation 5.1.21)
  prefs: []
  type: TYPE_NORMAL
- en: '![Use of Wasserstein loss](img/B08956_05_59.jpg)'
  prefs: []
  type: TYPE_IMG
- en: (Equation 5.1.22)
  prefs: []
  type: TYPE_NORMAL
- en: In the generator loss function, the first term disappears since it is not directly
    optimizing with respect to the real data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following table shows the difference between the loss functions of GAN and
    WGAN. For conciseness, we''ve simplified the notation for ![Use of Wasserstein
    loss](img/B08956_05_060.jpg), and ![Use of Wasserstein loss](img/B08956_05_061.jpg).
    These loss functions are used in training the WGAN as shown in *Algorithm* *5.1.1*.
    *Figure 5.1.3* illustrates that the WGAN model is practically the same as the
    DCGAN model except for the fake/true data labels and loss functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Network | Loss Functions | Equation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GAN | ![Use of Wasserstein loss](img/B08956_05_062.jpg)![Use of Wasserstein
    loss](img/B08956_05_063.jpg) | 4.1.14.1.5 |'
  prefs: []
  type: TYPE_TB
- en: '| WGAN | ![Use of Wasserstein loss](img/B08956_05_064.jpg)![Use of Wasserstein
    loss](img/B08956_05_065.jpg)![Use of Wasserstein loss](img/B08956_05_066.jpg)
    | 5.1.215.1.225.1.20 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5.1.1: A comparison between the loss functions of GAN and WGAN'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Algorithm 5.1.1 WGAN**'
  prefs: []
  type: TYPE_NORMAL
- en: The values of the parameters are ![Use of Wasserstein loss](img/B08956_05_067.jpg),
    *c* = 0.01 *m* = 64, and *n*[critic] = 5.
  prefs: []
  type: TYPE_NORMAL
- en: '*Require*: ![Use of Wasserstein loss](img/B08956_05_068.jpg), the learning
    rate. *c*, the clipping parameter. *m*, the batch size. *n*[critic], the number
    of the critic (discriminator) iterations per generator iteration.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Require*: *w*[0], initial critic (discriminator) parameters. ![Use of Wasserstein
    loss](img/B08956_05_069.jpg), initial generator parameters'
  prefs: []
  type: TYPE_NORMAL
- en: '`while`![Use of Wasserstein loss](img/B08956_05_070.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: has not converged `do`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`for` *t* = 1, …, *n*[critic]`do`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample a batch![Use of Wasserstein loss](img/B08956_05_071.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: from the real data
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Sample a batch![Use of Wasserstein loss](img/B08956_05_72.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: from the uniform noise distribution
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Use of Wasserstein loss](img/B08956_05_073.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_IMG
- en: ', compute the discriminator gradients'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Use of Wasserstein loss](img/B08956_05_074.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_IMG
- en: ', update the discriminator parameters'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Use of Wasserstein loss](img/B08956_05_075.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_IMG
- en: ', clip discriminator weights'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`end for`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample a batch![Use of Wasserstein loss](img/B08956_05_076.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: from the uniform noise distribution
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Use of Wasserstein loss](img/B08956_05_077.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_IMG
- en: ', compute the generator gradients'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Use of Wasserstein loss](img/B08956_05_078.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_IMG
- en: ', update generator parameters'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`end while`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Use of Wasserstein loss](img/B08956_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1.3: Top: Training the WGAN discriminator requires fake data from
    the generator and real data from the true distribution. Bottom: Training the WGAN
    generator requires fake data from the generator pretending to be real.'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to GANs, WGAN alternately trains the discriminator and generator (through adversarial).
    However, in WGAN, the discriminator (also called the critic) trains *n*[critic]
    iterations (Lines 2 to 8) before training the generator for one iteration (Lines
    9 to 11). This in contrast to GANs with an equal number of training iteration
    for both discriminator and generator. Training the discriminator means learning
    the parameters (weights and biases) of the discriminator. This requires sampling
    a batch from the real data (Line 3) and a batch from the fake data (Line 4) and
    computing the gradient of discriminator parameters (Line 5) after feeding the
    sampled data to the discriminator network. The discriminator parameters are optimized
    using RMSProp (Line 6). Both lines 5 and 6 are the optimization of *Equation 5.1.21*.
    Adam was found to be unstable in WGAN.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the Lipschitz constraint in the EM distance optimization is imposed
    by clipping the discriminator parameters (Line 7). Line 7 is the implementation
    of *Equation 5.1.20*. After *n*[critic] iterations of discriminator training,
    the discriminator parameters are frozen. The generator training starts by sampling
    a batch of fake data (Line 9). The sampled data is labeled as real (1.0) trying
    to fool the discriminator network. The generator gradients are computed in Line
    10 and optimized using the RMSProp in Line 11\. Lines 10 and 11 perform gradients
    update to optimize *Equation 5.1.22*.
  prefs: []
  type: TYPE_NORMAL
- en: After training the generator, the discriminator parameters are unfrozen, and
    another *n*[critic] discriminator training iterations start. We should take note
    that there is no need to freeze the generator parameters during discriminator
    training as the generator is only involved in the fabrication of data. Similar
    to GANs, the discriminator can be trained as a separate network. However, training
    the generator always requires the participation of the discriminator through the
    adversarial network since the loss is computed from the output of the generator
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike GAN, in WGAN real data are labeled 1.0 while fake data are labeled -1.0
    as a workaround in computing the gradient in Line 5\. Lines 5-6 and 10-11 perform
    gradient update to optimize *Equations* *5.1.21* and *5.1.22* respectively. Each
    term in Lines 5 and 10 is modelled as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Use of Wasserstein loss](img/B08956_05_079.jpg)'
  prefs: []
  type: TYPE_IMG
- en: (Equation 5.1.23)
  prefs: []
  type: TYPE_NORMAL
- en: 'Where *y*[label] = 1.0 for the real data and *y*[label] = -1.0 for the fake
    data. We removed the superscript (i) for simplicity of the notation. For discriminator,
    WGAN increases ![Use of Wasserstein loss](img/B08956_05_80.jpg) to minimize the
    loss function when training using the real data. When training using fake data,
    WGAN decreases ![Use of Wasserstein loss](img/B08956_05_081.jpg) to minimize the
    loss function. For the generator, WGAN increases ![Use of Wasserstein loss](img/B08956_05_082.jpg)
    as to minimize the loss function when the fake data is labeled as real during
    training. Note that *y*[label] has no direct contribution in the loss function
    other than its sign. In Keras, *Equation 5.1.23* is implemented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: WGAN implementation using Keras
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To implement WGAN within Keras, we can reuse the DCGAN implementation of GANs,
    something we introduced in the previous chapter. The DCGAN builder and utility
    functions are implemented in `gan.py` in `lib` folder as a module.
  prefs: []
  type: TYPE_NORMAL
- en: 'The functions include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`generator()`: A generator model builder'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`discriminator()`: Discriminator model builder'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train()`: DCGAN trainer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`plot_images()`: Generic generator outputs plotter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test_generator()`: Generic generator test utility'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As shown in *Listing* *5.1.1*, we can build a discriminator by simply calling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'WGAN uses linear output activation. For the generator, we execute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The overall network model in Keras is similar to the one seen in *Figure 4.2.1*
    for DCGAN.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing* *5.1.1* highlights the use of the RMSprop optimizer and Wasserstein
    loss function. The hyper-parameters in *Algorithm* *5.1.1* are used during training.
    *Listing* *5.1.2* is the training function that closely follows the *algorithm*.
    However, there is a minor tweak in the training of the discriminator. Instead
    of training the weights in a single combined batch of both real and fake data,
    we''ll train with one batch of real data first and then a batch of fake data.
    This tweak will prevent the gradient from vanishing because of the opposite sign
    in the label of real and fake data and the small magnitude of weights due to clipping.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The complete code is available on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5.1.4* shows the evolution of the WGAN outputs on MNIST dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 5.1.1, `wgan-mnist-5.1.2.py`. The WGAN model instantiation and training.
    Both discriminator and generator use Wassertein 1 loss, `wasserstein_loss()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5.1.2, `wgan-mnist-5.1.2.py`. The training procedure for WGAN closely
    follows *Algorithm* *5.1.1*. The discriminator is trained *n*[critic] iterations
    per 1 generator training iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![WGAN implementation using Keras](img/B08956_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1.4: The sample outputs of WGAN vs. training steps. WGAN does not
    suffer mode collapse in all the outputs during training and testing.'
  prefs: []
  type: TYPE_NORMAL
- en: WGAN is stable even under network configuration changes. For example, DCGAN is known
    to be unstable when batch normalization is inserted before the ReLU in the discriminator
    network. The same configuration is stable in WGAN.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following figure shows us the outputs of both DCGAN and WGAN with batch normalization
    on the discriminator network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![WGAN implementation using Keras](img/B08956_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1.5: A comparison of the output of the DCGAN (Left) and WGAN (Right)
    when batch normalization is inserted before the ReLU activation in the discriminator
    network'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the GAN training in the previous chapter, the trained model is saved
    on a file after 40,000 train steps. I would encourage you to run the trained generator
    model to see new synthesized MNIST digits images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Least-squares GAN (LSGAN)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in the previous section, the original GAN is difficult to train.
    The problem arises when the GAN optimizes its loss function; it's actually optimizing
    the *Jensen-Shannon* divergence, *D*[JS]. It is difficult to optimize *D*[JS]
    when there is little to no overlap between two distribution functions.
  prefs: []
  type: TYPE_NORMAL
- en: WGAN proposed to address the problem by using the EMD or Wasserstein 1 loss function
    which has a smooth differentiable function even when there is little or no overlap
    between the two distributions. However, WGAN is not concerned with the generated
    image quality. Apart from stability issues, there are still areas of improvement
    in terms of perceptive quality in the generated images of the original GAN. LSGAN
    theorizes that the twin problems can be solved simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: LSGAN proposes the least squares loss. *Figure 5.2.1* demonstrates why the use
    of a sigmoid cross entropy loss in the GAN results in poorly generated data quality.
    Ideally, the fake samples distribution should be as close as possible to the true
    samples' distribution. However, for GANs, once the fake samples are already on the correct
    side of the decision boundary, the gradients vanish.
  prefs: []
  type: TYPE_NORMAL
- en: 'This prevents the generator from having enough motivation to improve the quality
    of the generated fake data. Fake samples far from the decision boundary will no
    longer attempt to move closer to the true samples'' distribution. Using the least
    squares loss function, the gradients do not vanish as long as the fake samples
    distribution is far from the real samples'' distribution. The generator will strive
    to improve its estimate of real density distribution even if the fake samples
    are already on the correct side of the decision boundary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Least-squares GAN (LSGAN)](img/B08956_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2.1: Both real and fake samples distributions divided by respective
    decision boundaries: Sigmoid and Least squares'
  prefs: []
  type: TYPE_NORMAL
- en: '| Network | Loss Functions | Equation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GAN | ![Least-squares GAN (LSGAN)](img/B08956_05_083.jpg)![Least-squares
    GAN (LSGAN)](img/B08956_05_084.jpg) | 4.1.14.1.5 |'
  prefs: []
  type: TYPE_TB
- en: '| LSGAN | ![Least-squares GAN (LSGAN)](img/B08956_05_085.jpg)![Least-squares
    GAN (LSGAN)](img/B08956_05_086.jpg) | 5.2.15.2.2 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5.2.1: A comparison between the loss functions of GAN and LSGAN'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding table shows the comparison of the loss functions between GAN and
    LSGAN. Minimizing *Equation 5.2.1* or the discriminator loss function implies
    that the MSE between real data classification and true label 1.0 should be close
    to zero. In addition, the MSE between the fake data classification and the true
    label 0.0 should be close to zero.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to GANs, the LSGAN discriminator is trained to classify real from fake
    data samples. Minimizing *Equation 5.2.2* means fooling the discriminator to think
    that the generated fake sample data are real with label 1.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing LSGAN using the DCGAN code in the previous chapter as the basis requires
    few changes only. As shown in *Listing* *5.2.1*, the discriminator sigmoid activation
    is removed. The discriminator is built by calling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The generator is similar to the original DCGAN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Both the discriminator and adversarial loss functions are replaced by `mse`.
    All the network parameters are the same as in DCGAN. The network model of LSGAN
    in Keras is similar to *Figure 4.2.1* except that there is no linear or output
    activation. The training process is similar to that seen in DCGAN and is provided
    by the utility function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5.2.1, `lsgan-mnist-5.2.1.py` shows how the discriminator and generator
    are the same in DCGAN except for the discriminator output activation and the use
    of MSE loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Following figure shows generated samples after training LSGAN using the MNIST
    dataset for 40,000 training steps. The output images have better perceptual quality
    compared to *Figure 4.2.1* in DCGAN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Least-squares GAN (LSGAN)](img/B08956_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2.2: Sample outputs of LSGAN vs. training steps'
  prefs: []
  type: TYPE_NORMAL
- en: 'I encourage you to run the trained generator model to see the new synthesized
    MNIST digits images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Auxiliary classifier GAN (ACGAN)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ACGAN is similar in principle to the **Conditional GAN** (**CGAN**) that we
    discussed in the previous chapter. We''re going to compare both CGANand ACGAN.
    For both CGAN and ACGAN, the generator inputs are noise and its label. The output
    is a fake image belonging to the input class label. For CGAN, the inputs to the
    discriminator are an image (fake or real) and its label. The output is the probability
    that the image is real. For ACGAN, the input to the discriminator is an image,
    whilst the output is the probability that the image is real and its class label.
    Following figure highlights the difference between CGAN and ACGAN during generator
    training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Auxiliary classifier GAN (ACGAN)](img/B08956_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3.1: CGAN vs. ACGAN generator training. The main difference is the
    input and output of the discriminator.'
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, in CGAN we feed the network with side information (label). In ACGAN,
    we try to reconstruct the side information using an auxiliary class decoder network.
    ACGAN argued that forcing the network to do additional tasks is known to improve
    the performance of the original task. In this case, the additional task is image
    classification. The original task is the generation of fake images.
  prefs: []
  type: TYPE_NORMAL
- en: '| Network | Loss Functions | Number |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CGAN | ![Auxiliary classifier GAN (ACGAN)](img/B08956_05_087.jpg)![Auxiliary
    classifier GAN (ACGAN)](img/B08956_05_088.jpg) | 4.3.14.3.2 |'
  prefs: []
  type: TYPE_TB
- en: '| ACGAN | ![Auxiliary classifier GAN (ACGAN)](img/B08956_05_089.jpg)![Auxiliary
    classifier GAN (ACGAN)](img/B08956_05_090.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5.3.1: A comparison between the loss functions of CGAN and ACGAN'
  prefs: []
  type: TYPE_NORMAL
- en: '| 5.3.15.3.2 |'
  prefs: []
  type: TYPE_TB
- en: Preceding table shows the ACGAN loss functions as compared to CGAN. The ACGAN
    loss functions are the same as CGAN except for the additional classifier loss
    functions. Apart from the original task of identifying real from fake images (![Auxiliary
    classifier GAN (ACGAN)](img/B08956_05_091.jpg)), *Equation 5.3.1* of the discriminator
    has the additional task of correctly classifying real and fake images (![Auxiliary
    classifier GAN (ACGAN)](img/B08956_05_092.jpg)). *Equation* *5.3.2* of the generator
    means that apart from trying to fool the discriminator with fake images (![Auxiliary
    classifier GAN (ACGAN)](img/B08956_05_093.jpg)), it is asking the discriminator
    to correctly classify those fake images (![Auxiliary classifier GAN (ACGAN)](img/B08956_05_094.jpg)).
  prefs: []
  type: TYPE_NORMAL
- en: Starting with the CGAN code, only the discriminator and the training function
    are modified to implement ACGAN. The discriminator and generator builder functions
    are also provided by `gan.py`. To see the changes made on the discriminator, following
    listing shows the builder function where the auxiliary decoder network that performs
    image classification and the dual outputs are highlighted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 5.3.1, `gan.py` shows how the discriminator model builder is the same
    as in DCGAN predicting if an image is real, the first output. An auxiliary decoder
    network is added to perform the image classification and produce the second output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The discriminator is then built by calling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The generator is the same as the one in ACGAN. To recall, the generator builder
    is shown in the following listing. We should note that both *Listings* *5.3.1*
    and *5.3.2* are the same builder functions used by WGAN and LSGAN in the previous
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 5.3.2, `gan.py` shows the generator model builder is the same as in
    CGAN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In ACGAN, the generator is instantiated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Following figure shows the network model of ACGAN in Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Auxiliary classifier GAN (ACGAN)](img/B08956_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3.2: The Keras model of ACGAN'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Listing* *5.3.3*, the discriminator and adversarial models are
    modified to accommodate the changes in the discriminator network. We now have
    two loss functions. The first is the original binary cross-entropy to train the
    discriminator in estimating the probability if the input image is real. The second
    is the image classifier predicting the class label. The output is a one-hot vector
    of 10 dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Referring to Listing 5.3.3, `acgan-mnist-5.3.1.py`, where highlighted are the
    changes implemented in the discriminator and adversarial models to accommodate
    the image classifier of the discriminator network. The two loss functions correspond
    to the two outputs of the discriminator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In *Listing* *5.3.4*, we highlight the changes implemented in the training routine.
    The main difference compared to CGAN code is that the output label must be supplied
    during discriminator and adversarial training.
  prefs: []
  type: TYPE_NORMAL
- en: 'As seen in Listing 5.3.4, `acgan-mnist-5.3.1.py`, the changes implemented in
    the train function are highlighted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In turned out that with the additional task, the performance improvement in
    ACGAN is significant compared to all GANs that we have discussed previously. ACGAN
    training is stable as shown in *Figure 5.3.3* sample outputs of ACGAN for the following
    labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Unlike CGAN, the sample outputs appearance does not vary widely during training.
    The MNIST digit image perceptive quality is also better. *Figure 5.3.4* shows a
    side by side comparison of every MNIST digit produced by both CGAN and ACGAN.
    Digits 2-6 are of better quality in ACGAN than in CGAN.
  prefs: []
  type: TYPE_NORMAL
- en: 'I encourage you to run the trained generator model to see new synthesized MNIST
    digits images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, a specific digit (for example, 3) to be generated can also be
    requested:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![Auxiliary classifier GAN (ACGAN)](img/B08956_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3.3: The sample outputs generated by the ACGAN as a function of train
    steps for labels [0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5]'
  prefs: []
  type: TYPE_NORMAL
- en: '![Auxiliary classifier GAN (ACGAN)](img/B08956_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3.4: A side by side comparison of outputs of CGAN and ACGAN conditioned
    with digits 0 to 9'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've presented various improvements in the original algorithm
    of GAN, first introduced in the previous chapter. WGAN proposed an algorithm to
    improve the stability of training by using the EMD or Wassertein 1 loss. LSGAN
    argued that the original cross-entropy function of GAN is prone to vanishing gradients,
    unlike least squares loss. LSGAN proposed an algorithm to achieve stable training
    and quality outputs. ACGAN convincingly improved the quality of the conditional
    generation of MNIST digits by requiring the discriminator to perform classification
    task on top of determining whether the input image is fake or real.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll study how to control the attributes of generator
    outputs. Whilst CGAN and ACGAN are able to indicate the desired digits to produce;
    we have not analyzed GANs that can specify the attributes of outputs. For example,
    we may want to control the writing style of the MNIST digits such as roundness,
    tilt angle, and thickness. Therefore, the goal will be to introduce GANs with
    disentangled representations to control the specific attributes of the generator
    outputs.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ian Goodfellow and others. *Generative Adversarial Nets*. Advances in neural
    information processing systems, 2014([http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Martin Arjovsky, Soumith Chintala, and Léon Bottou, *Wasserstein GAN*. arXiv
    preprint, 2017([https://arxiv.org/pdf/1701.07875.pdf](https://arxiv.org/pdf/1701.07875.pdf)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Xudong Mao and others. *Least Squares Generative Adversarial Networks*. 2017
    IEEE International Conference on Computer Vision (ICCV). IEEE 2017([http://openaccess.thecvf.com/content_ICCV_2017/papers/Mao_Least_Squares_Generative_ICCV_2017_paper.pdf](http://openaccess.thecvf.com/content_ICCV_2017/papers/Mao_Least_Squares_Generative_ICCV_2017_paper.pdf)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Augustus Odena, Christopher Olah, and Jonathon Shlens. *Conditional Image Synthesis
    with Auxiliary Classifier GANs*. ICML, 2017([http://proceedings.mlr.press/v70/odena17a/odena17a.pdf](http://proceedings.mlr.press/v70/odena17a/odena17a.pdf)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
