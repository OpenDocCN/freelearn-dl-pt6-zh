["```py\nfind -type f -name '*.sph' | awk '{printf \"sox -t sph %s -b 16 -t wav %s\\n\", $0, $0\".wav\" }' | bash\n```", "```py\npython preprocess.py\n```", "```py\npython train.py ( <== Use all available GPUs )\n```", "```py\nCUDA_VISIBLE_DEVICES=0,1 python train.py ( <== Use only GPU 0, 1 )\n```", "```py\npython test.py --set train|valid|test --frac 1.0(0.01~1.0)\n```", "```py\npython recognize.py --file \n```", "```py\n\npython recognize.py --file asset/data/LibriSpeech/test-clean/1089/134686/1089-134686-0000.flac\npython recognize.py --file asset/data/LibriSpeech/test-clean/1089/134686/1089-134686-0001.flac\npython recognize.py --file asset/data/LibriSpeech/test-clean/1089/134686/1089-134686-0002.flac\npython recognize.py --file asset/data/LibriSpeech/test-clean/1089/134686/1089-134686-0003.flac\npython recognize.py --file asset/data/LibriSpeech/test-clean/1089/134686/1089-134686-0004.flac\n```", "```py\npython export_wave_pb.py\n```", "```py\nbatch_size = 1 # batch size\nvoca_size = data.voca_size\nx = tf.placeholder(dtype=tf.sg_floatx, shape=(batch_size, None, 20))\n# sequence length except zero-padding\nseq_len = tf.not_equal(x.sg_sum(axis=2), 0.).sg_int().sg_sum(axis=1)\n# encode audio feature\nlogit = get_logit(x, voca_size)\n# ctc decoding\ndecoded, _ = tf.nn.ctc_beam_search_decoder(logit.sg_transpose(perm=[1, 0, 2]), seq_len, merge_repeated=False)\n# to dense tensor\ny = tf.add(tf.sparse_to_dense(decoded[0].indices, decoded[0].dense_shape, decoded[0].values), 1, name=\"output\")\n\nwith tf.Session() as sess:\n tf.sg_init(sess)\n saver = tf.train.Saver()\n saver.restore(sess, tf.train.latest_checkpoint('asset/train'))\n\ngraph = tf.get_default_graph()\ninput_graph_def = graph.as_graph_def()\n\nwith tf.Session() as sess:\n tf.sg_init(sess)\n saver = tf.train.Saver()\n saver.restore(sess, tf.train.latest_checkpoint('asset/train'))\n # Output model's graph details for reference.\n tf.train.write_graph(sess.graph_def, '/root/speech-to-text-wavenet/asset/train', 'graph.txt', as_text=True)\n # Freeze the output graph.\n output_graph_def = graph_util.convert_variables_to_constants(sess,input_graph_def,\"output\".split(\",\"))\n # Write it into .pb file.\n with tfw.gfile.GFile(\"/root/speech-to-text-wavenet/asset/train/wavenet_model.pb\", \"wb\") as f:\n f.write(output_graph_def.SerializeToString())\n```", "```py\nbazel build tensorflow/tools/graph_transforms:transform_graph\n bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\n --in_graph=/your/.pb/file \\\n --outputs=\"output_node_name\" \\\n --out_graph=/the/quantized/.pb/file \\\n --transforms='quantize_weights'\n```", "```py\nbazel build tensorflow/python/tools:print_selective_registration_header && \\\n bazel-bin/tensorflow/python/tools/print_selective_registration_header \\\n --graphs=path/to/graph.pb > ops_to_register.h\n```", "```py\nbazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" \\\n --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" \\\n //tensorflow/contrib/android:libtensorflow_inference.so \\\n --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\n --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a\n```", "```py\nModify BUILD in /tensorflow/tensorflow/core/kernels/\n```", "```py\ngrep \"op: \" PATH/TO/mygraph.txt | sort | uniq | sed -E 's/^.+\"(.+)\".?$/\\1/g'\n```", "```py\nbazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so \\\n --crosstool_top=//external:android/crosstool \\\n --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\n --cpu=armeabi-v7a\n```", "```py\nbazel-bin/tensorflow/contrib/android/libtensorflow_inference.so\n```", "```py\nallprojects {\n repositories {\n     jcenter()\n     }\n }\n\ndependencies {\n compile 'org.tensorflow:tensorflow-android:+'\n }\n\n```", "```py\nbazel build //tensorflow/contrib/android:android_tensorflow_inference_java\n```", "```py\nbazel-bin/tensorflow/contrib/android/libandroid_tensorflow_inference_java.jar\n\n```", "```py\n<manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    package=\"com.mlmobileapps.speech\">\n\n    <uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\"/>\n    <uses-permission android:name=\"android.permission.RECORD_AUDIO\" />\n\n    <uses-sdk\n        android:minSdkVersion=\"25\"\n        android:targetSdkVersion=\"25\" />\n\n    <application android:allowBackup=\"true\"\n        android:debuggable=\"true\"\n        android:label=\"@string/app_name\"\n        android:icon=\"@drawable/ic_launcher\"\n        android:theme=\"@style/MaterialTheme\">\n\n        <activity android:name=\"org.tensorflow.demo.SpeechActivity\"\n            android:screenOrientation=\"portrait\"\n            android:label=\"@string/activity_name_speech\">\n            <intent-filter>\n               <action android:name=\"android.intent.action.MAIN\" />\n               <category android:name=\"android.intent.category.LAUNCHER\" />\n            </intent-filter>\n        </activity>\n    </application>\n</manifest>\n```", "```py\n<FrameLayout\n    xmlns:android=\"http://schemas.android.com/apk/res/android\"\n    xmlns:app=\"http://schemas.android.com/apk/res-auto\"\n    xmlns:tools=\"http://schemas.android.com/tools\"\n    android:layout_width=\"match_parent\"\n    android:layout_height=\"match_parent\"\n    android:background=\"#6200EE\"\n    tools:context=\"org.tensorflow.demo.SpeechActivity\">\n\n    <LinearLayout\n        android:layout_width=\"match_parent\"\n        android:orientation=\"vertical\"\n        android:layout_height=\"wrap_content\">\n        <TextView\n            android:id=\"@+id/textView\"\n            android:layout_width=\"wrap_content\"\n            android:layout_height=\"wrap_content\"\n\n            android:layout_gravity=\"top\"\n            android:textColor=\"#fff\"\n            android:layout_marginLeft=\"10dp\"\n            android:layout_marginTop=\"30dp\"\n            android:text=\"Talk within 5 seconds\"\n            android:textAlignment=\"center\"\n            android:textSize=\"24dp\" />\n\n        <TextView\n            android:id=\"@+id/output_text\"\n            android:layout_width=\"wrap_content\"\n            android:layout_height=\"wrap_content\"\n            android:textColor=\"#fff\"\n            android:layout_gravity=\"top\"\n            android:layout_marginLeft=\"10dp\"\n            android:layout_marginTop=\"10dp\"\n            android:textAlignment=\"center\"\n            android:textSize=\"24dp\" />\n    </LinearLayout>\n\n    <Button\n        android:id=\"@+id/start\"\n        android:background=\"#ff0266\"\n        android:textColor=\"#fff\"\n        android:layout_width=\"wrap_content\"\n        android:padding=\"20dp\"\n        android:layout_height=\"wrap_content\"\n        android:layout_gravity=\"bottom|center_horizontal\"\n        android:layout_marginBottom=\"50dp\"\n        android:text=\"Record Voice\" />\n\n</FrameLayout>\n\n```", "```py\n@Override\nprotected void onCreate(Bundle savedInstanceState) {\n  // Set up the UI.\n  super.onCreate(savedInstanceState);\n  setContentView(R.layout.activity_speech);\n  startButton = (Button) findViewById(R.id.start);\n  startButton.setOnClickListener(\n      new View.OnClickListener() {\n        @Override\n        public void onClick(View view) {\n            startRecording();\n        }\n      });\n  outputText = (TextView) findViewById(R.id.output_text);\n  // Load the Pretrained WaveNet model.\n  inferenceInterface = new TensorFlowInferenceInterface(getAssets(), MODEL_FILENAME);\n  requestMicrophonePermission();\n}\n```", "```py\npublic synchronized void startRecording() {\n  if (recordingThread != null) {\n    return;\n  }\n  shouldContinue = true;\n  recordingThread =\n      new Thread(\n          new Runnable() {\n            @Override\n            public void run() {\n              record();\n            }\n          });\n  recordingThread.start();\n}\n```", "```py\nprivate void record() {\n  android.os.Process.setThreadPriority(android.os.Process.THREAD_PRIORITY_AUDIO);\n\n  // Estimate the buffer size we'll need for this device.\n  int bufferSize =\n      AudioRecord.getMinBufferSize(\n              SAMPLE_RATE, AudioFormat.CHANNEL_IN_MONO, AudioFormat.ENCODING_PCM_16BIT);\n  if (bufferSize == AudioRecord.ERROR || bufferSize == AudioRecord.ERROR_BAD_VALUE) {\n    bufferSize = SAMPLE_RATE * 2;\n  }\n  short[] audioBuffer = new short[bufferSize / 2];\n\n  AudioRecord record =\n      new AudioRecord(\n          MediaRecorder.AudioSource.DEFAULT,\n          SAMPLE_RATE,\n          AudioFormat.CHANNEL_IN_MONO,\n          AudioFormat.ENCODING_PCM_16BIT,\n          bufferSize);\n\n  if (record.getState() != AudioRecord.STATE_INITIALIZED) {\n    Log.e(LOG_TAG, \"Audio Record can't initialize!\");\n    return;\n  }\n\n  record.startRecording();\n\n  Log.v(LOG_TAG, \"Start recording\");\n\n  while (shouldContinue) {\n    int numberRead = record.read(audioBuffer, 0, audioBuffer.length);\n      Log.v(LOG_TAG, \"read: \" + numberRead);\n    int maxLength = recordingBuffer.length;\n    recordingBufferLock.lock();\n    try {\n        if (recordingOffset + numberRead < maxLength) {\n            System.arraycopy(audioBuffer, 0, recordingBuffer, recordingOffset, numberRead);\n        } else {\n            shouldContinue = false;\n        }\n      recordingOffset += numberRead;\n    } finally {\n      recordingBufferLock.unlock();\n    }\n  }\n  record.stop();\n  record.release();\n  startRecognition();\n}\n```", "```py\npublic synchronized void startRecognition() {\n  if (recognitionThread != null) {\n    return;\n  }\n  shouldContinueRecognition = true;\n  recognitionThread =\n      new Thread(\n          new Runnable() {\n            @Override\n            public void run() {\n              recognize();\n            }\n          });\n  recognitionThread.start();\n}\n\nprivate void recognize() {\n  Log.v(LOG_TAG, \"Start recognition\");\n\n  short[] inputBuffer = new short[RECORDING_LENGTH];\n  double[] doubleInputBuffer = new double[RECORDING_LENGTH];\n  long[] outputScores = new long[157];\n  String[] outputScoresNames = new String[]{OUTPUT_SCORES_NAME};\n\n    recordingBufferLock.lock();\n    try {\n      int maxLength = recordingBuffer.length;\n        System.arraycopy(recordingBuffer, 0, inputBuffer, 0, maxLength);\n    } finally {\n      recordingBufferLock.unlock();\n    }\n\n    // We need to feed in float values between -1.0 and 1.0, so divide the\n    // signed 16-bit inputs.\n    for (int i = 0; i < RECORDING_LENGTH; ++i) {\n      doubleInputBuffer[i] = inputBuffer[i] / 32767.0;\n    }\n\n    //MFCC java library.\n    MFCC mfccConvert = new MFCC();\n    float[] mfccInput = mfccConvert.process(doubleInputBuffer);\n    Log.v(LOG_TAG, \"MFCC Input======> \" + Arrays.toString(mfccInput));\n\n    // Run the model.\n    inferenceInterface.feed(INPUT_DATA_NAME, mfccInput, 1, 157, 20);\n    inferenceInterface.run(outputScoresNames);\n    inferenceInterface.fetch(OUTPUT_SCORES_NAME, outputScores);\n    Log.v(LOG_TAG, \"OUTPUT======> \" + Arrays.toString(outputScores));\n\n    //Output the result.\n    String result = \"\";\n    for (int i = 0;i<outputScores.length;i++) {\n        if (outputScores[i] == 0)\n            break;\n        result += map[(int) outputScores[i]];\n    }\n    final String r = result;\n    this.runOnUiThread(new Runnable() {\n        @Override\n        public void run() {\n            outputText.setText(r);\n        }\n    });\n    Log.v(LOG_TAG, \"End recognition: \" +result);\n  }\n```"]