["```py\nIf Γu = 0:\nct = ( 0 x c ̴t ) + ((1 - 0) x ct-1 )\n   = 0 + ct-1\nTherefore, ct = ct-1\n```", "```py\nIf Γu = 1:\nct = ( 1 x c ̴t ) + ((1 - 1) x ct-1 )\n   = c ̴t+ (0 x ct-1)\nTherefore, ct = c ̴t\n```", "```py\nfrom __future__ import print_function\nimport sys\nimport numpy as np\nimport re\nimport random\nimport pickle\n\nfrom nltk.corpus import gutenberg\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Bidirectional, Dropout\nfrom keras.layers import SimpleRNN, GRU, BatchNormalization\n\nfrom keras.callbacks import LambdaCallback\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils.data_utils import get_file\nfrom keras.utils.data_utils import get_file\n```", "```py\nfrom nltk.corpus import gutenberg\nhamlet = gutenberg.words('shakespeare-hamlet.txt')\ntext =''\nfor word in hamlet:            # For each word\ntext+=str(word).lower()        # Convert to lower case and add to string variable\ntext+= ' '                     # Add space   \nprint('Corpus length, Hamlet only:', len(text))\n\n-----------------------------------------------------------------------\nOutput:\nCorpus length, Hamlet only: 166765\n\n```", "```py\ncharacters = sorted(list(set(text)))\nprint('Total characters:', len(characters))\nchar_indices = dict((l, i) for i, l in enumerate(characters))\nindices_char = dict((i, l) for i, l in enumerate(characters))\n-----------------------------------------------------------------------\nOutput:\nTotal characters= 65\n```", "```py\n'''\nBreak text into :\nFeatures  -    Character-level sequences of fixed length        \nLabels    -    The next character in sequence     \n'''\ntraining_sequences = []          # Empty list to collect each sequence\n\nnext_chars = []                  # Empty list to collect next character in sequence\n\nseq_len, stride = 35, 1    # Define lenth of each input sequence & stride to move before sampling next sequence\n\nfor i in range(0, len(text) - seq_len, stride):     # Loop over text with window of 35 characters, moving 1 stride at a time\n\ntraining_sequences.append(text[i: i + seq_len]) # Append sequences to traning_sequences\n\nnext_chars.append(text[i + seq_len])            # Append following character in sequence to next_chars\n```", "```py\n# Print out sequences and labels to verify\n\nprint('Number of sequences:', len(training_sequences))\nprint('First sequences:', training_sequences[:1])\nprint('Next characters in sequence:', next_chars[:1])\nprint('Second sequences:', training_sequences[1:2])\nprint('Next characters in sequence:', next_chars[1:2])\n-----------------------------------------------------------------------\nOutput \nNumber of sequences: 166730\nFirst sequences: ['[ the tragedie of hamlet by william']\nNext characters in sequence: [' ']\nSecond sequences: [' the tragedie of hamlet by william ']\nNext characters in sequence: ['s']\n```", "```py\n#Create a Matrix of zeros\n# With dimensions : (training sequences, length of each sequence, total unique characters)\nx = np.zeros((len(training_sequences), seq_len, len(characters)), dtype=np.bool)\ny = np.zeros((len(training_sequences), len(characters)), dtype=np.bool)\nfor index, sequence in enumerate(training_sequences):     #Iterate over training sequences\nfor sub_index, chars in enumerate(sequence):          #Iterate over characters per sequence\nx[index, sub_index, char_indices[chars]] = 1      #Update character position in feature matrix to 1\ny[index, char_indices[next_chars[index]]] = 1         #Update character position in label matrix to 1\nprint('Data vectorization completed.')\nprint('Feature vectors shape', x.shape)\nprint('Label vectors shape', y.shape)\n\n-----------------------------------------------------------------------\nData vectorization completed. \nFeature vectors shape (166730, 35, 43) \nLabel vectors shape (166730, 43)\n\n```", "```py\ndef sample(softmax_predictions, sample_threshold=1.0):   \nsoftmax_preds = np.asarray(softmax_predictions).astype('float64')    \n# Make array of predictions, convert to float\n\nlog_preds = np.log(softmax_preds) / sample_threshold                 \n# Log normalize and divide by threshold\n\nexp_preds = np.exp(log_preds)                                        \n# Compute exponents of log normalized terms\n\nnorm_preds = exp_preds / np.sum(exp_preds)                           \n# Normalize predictions\n\nprob = np.random.multinomial(1, norm_preds, 1)                       \n# Draw sample from multinomial distribution\n\nreturn np.argmax(prob)                                               #Return max value\n```", "```py\ndef on_epoch_end(epoch, _):\nglobal model, model_name\nprint('----- Generating text after Epoch: %d' % epoch)\nstart_index = random.randint(0, len(text) - seq_len - 1)    \n# Random index position to start sample input sequence\nend_index = start_index + seq_len                           \n# End of sequence, corresponding to training sequence length\nsampling_range = [0.3, 0.5, 0.7, 1.0, 1.2]                  \n# Sampling entropy threshold\nfor threshold in sampling_range:print('----- *Sampling Threshold* :', threshold)\ngenerated = ''                                          \n# Empty string to collect sequence\nsentence = text[start_index: end_index]                 \n# Random input sequence taken from Hamlet\ngenerated += sentence                                  \n # Add input sentence to generated\nprint('Input sequence to generate from : \"' + sentence + '\"')     \nsys.stdout.write(generated)                            \n# Print out buffer instead of waiting till the end\nfor i in range(400):                                   \n# Generate 400 next characters in the sequence\nx_pred = np.zeros((1, seq_len, len(characters)))   \n# Matrix of zeros for input sentence\nfor n, char in enumerate(sentence):                \n# For character in sentence\nx_pred[0, n, char_indices[char]] = 1\\.          \n# Change index position for character to 1.\npreds = model.predict(x_pred, verbose=0)[0]        \n# Make prediction on input vector\nnext_index = sample(preds, threshold)              \n# Get index position of next character using sample function\nnext_char = indices_char[next_index]               \n# Get next character using index\ngenerated += next_char                             \n# Add generated character to sequence\nsentence = sentence[1:] + next_char\nsys.stdout.write(next_char)\nsys.stdout.flush()\n-----------------------------------------------------------------------\nOutput: \nprint_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n```", "```py\ndef test_models(list, epochs=10):\n    global model, model_name\n\n    for network in list:   \n        print('Initiating compilation...')\n\n        # Initialize model\n        model = network()\n        # Get model name\n        model_name = re.split(' ', str(network))[1]  \n\n        #Filepath to save model with name, epoch and loss \n        filepath = \"C:/Users/npurk/Desktop/Ch5RNN/all_models/versions/%s_epoch-{epoch:02d}-loss-{loss:.4f}.h5\"%model_name\n\n        #Checkpoint callback object \n        checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n\n        # Compile model\n        model.compile(loss='categorical_crossentropy', optimizer='adam')\n        print('Compiled:', str(model_name))\n\n        # Initiate training\n        network = model.fit(x, y,\n              batch_size=100,\n              epochs=epochs,\n              callbacks=[print_callback, checkpoint])\n\n        # Print model configuration\n        model.summary()\n\n        #Save model history object for later analysis\n        with open('C:/Users/npurk/Desktop/Ch5RNN/all_models/history/%s.pkl'%model_name, 'wb') as file_pi:\n            pickle.dump(network.history, file_pi)\n\ntest_models(all_models, epochs=5)\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Bidirectional, Dropout\nfrom keras.layers import SimpleRNN, GRU, BatchNormalization\nfrom keras.optimizers import RMSprop\n'''Fun part: Construct a bunch of functions returning different kinds of RNNs, from simple to more complex'''\ndef SimpleRNN_stacked_model():\n    model = Sequential()\n    model.add(SimpleRNN(128, input_shape=(seq_len, len(characters)), return_sequences=True))\n    model.add(SimpleRNN(128))\n    model.add(Dense(len(characters), activation='softmax'))\n    return model\n```", "```py\ndef SimpleRNN_stacked_model():\n    model = Sequential()\n    model.add(SimpleRNN(128, input_shape=(seq_len, len(characters)), return_sequences=True))\n    model.add(SimpleRNN(128))\n    model.add(Dense(len(characters), activation='softmax'))\n    return model\n```", "```py\ndef GRU_stacked_model():\n    model = Sequential()\n    model.add(GRU(128, input_shape=(seq_len, len(characters)), return_sequences=True))\n    model.add(GRU(128))\n    model.add(Dense(len(characters), activation='softmax'))\n    return model\n```", "```py\ndef Bi_directional_GRU():\n    model = Sequential()\n    model.add(Bidirectional(GRU(128, return_sequences=True), input_shape=(seq_len, len(characters))))\n    model.add(Bidirectional(GRU(128)))\n    model.add(Dense(len(characters), activation='softmax'))\n    return model\n```", "```py\ndef larger_GRU():\n    model = Sequential()\n    model.add(GRU(128, input_shape=(seq_len, len(characters)),\n                       dropout=0.2,\n                       recurrent_dropout=0.2,\n                       return_sequences=True))\n    model.add(GRU(128, dropout=0.2,\n                  recurrent_dropout=0.2,\n                  return_sequences=True))\n    model.add(GRU(128, dropout=0.2,\n                  recurrent_dropout=0.2))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(len(characters), activation='softmax'))\n    return model\n# All defined models\nall_models = [SimpleRNN_model,\n              SimpleRNN_stacked_model,\n              GRU_stacked_model,\n              Bi_directional_GRU, \n              Bi_directional_GRU,\n              larger_GRU]\n```"]