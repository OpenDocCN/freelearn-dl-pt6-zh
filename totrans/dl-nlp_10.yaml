- en: '*Appendix*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: About
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section is included to assist the learners to perform the activities present
    in the book. It includes detailed steps that are to be performed by the learners
    to complete and achieve the objectives of the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 1: Introduction to Natural Language Processing'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 1: Generating word embeddings from a corpus using Word2Vec.'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Solution:**'
  prefs: []
  type: TYPE_NORMAL
- en: Upload the text corpus from the link aforementioned.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the word2vec from gensim models
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Store the corpus in a variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Fit the word2vec model on the corpus.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Find the most similar word to 'man'.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.29: Output for similar word embeddings](img/C13783_01_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.29: Output for similar word embeddings'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: '''Father'' is to ''girl'', ''x'' is to boy. Find the top 3 words for x.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.30: Output for top three words for ‘x’'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_01_30.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.30: Output for top three words for ''x'''
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Chapter 2: Applications of Natural Language Processing'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 2: Building and training your own POS tagger'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Solution:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing to do is pick a corpus that we want to train our tagger on.
    Import the necessary Python packages. Here, we use the `nltk` `treebank` corpus
    to work on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we need to determine what features our tagger will take into consideration
    when determining what tag to assign to a word. These can include whether the word
    is all capitalized, is in lowercase, or has one capital letter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function to strip the tagged words of their tags so that we can feed
    them into our tagger:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now we need to build our training set. Our tagger needs to take features individually
    for each word, but our corpus is actually in the form of sentences, so we need
    to do a little transforming. Split the data into training and testing sets. Apply
    this function on the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Apply this function on the training set. Now we can train our tagger. It's basically
    a classifier since it's categorizing words into classes, so we can use a classification
    algorithm. You can use any that you like or try out a bunch of them to see which
    works best. Here, we'll use the decision tree classifier. Import the classifier,
    initialize it, and fit the model on the training data. Print the accuracy score.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C13783_02_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.19: Accuracy score'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Activity 3: Performing NER on a Tagged Corpus'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Solution:**'
  prefs: []
  type: TYPE_NORMAL
- en: Import the necessary Python packages and classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Print the `nltk.corpus.treebank.tagged_sents()` to see the tagged corpus that
    you need extract named entities from.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Store the first sentence of the tagged sentences in a variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Use `nltk.ne_chunk` to perform NER on the sentence. Set binary to True and print
    the named entities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C13783_02_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.20: NER on tagged corpus'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Chapter 3: Introduction to Neural Networks'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 4: Sentiment Analysis of Reviews'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Solution:**'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new `Jupyter` notebook. Import `numpy`, `pandas` and `matplotlib.pyplot`.
    Load the dataset into a dataframe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next step is to clean and prepare the data. Import `re` and `nltk`. From `nltk.corpus`
    import `stopwords`. From `nltk.stem.porter`, import `PorterStemmer`. Create an
    array for your cleaned text to be stored in.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Using a for loop, iterate through every instance (every review). Replace all
    non-alphabets with a ' ' (whitespace). Convert all alphabets into lowercase. Split
    each review into individual words. Initiate the `PorterStemmer`. If the word is
    not a stopword, perform stemming on the word. Join all the individual words back
    together to form a cleaned review. Append this cleaned review to the array you
    created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Import `CountVectorizer`. Convert the reviews into word count vectors using
    `CountVectorizer`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Create an array to store each unique word as its own column, hence making them
    independent variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Import `LabelEncoder` from `sklearn.preprocessing`. Use the `LabelEncoder` on
    the target output (`y`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Import `train_test_split`. Divide the dataset into a training set and a validation
    set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Import `StandardScaler` from `sklearn.preprocessing`. Use the `StandardScaler`
    on the features of both the training set and the validation set (`X`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now the next task is to create the neural network. Import `keras`. Import `Sequential`
    from `keras.models` and `Dense` from Keras layers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Initialize the neural network. Add the first hidden layer with '`relu`' as the
    activation function. Repeat step for the second hidden layer. Add the output layer
    with '`softmax`' as the activation function. Compile the neural network, using
    '`adam`' as the optimizer, '`binary_crossentropy`' as the loss function and '`accuracy`'
    as the performance metric.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now we need to train the model. Fit the neural network on the training dataset
    with a `batch_size` of 3 and a `nb_epoch` of 5.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Validate the model. Evaluate the neural network and print the accuracy scores
    to see how it's doing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: (Optional) Print the confusion matrix by importing `confusion_matrix` from `sklearn.metrics`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Your output should look similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C13783_03_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.21: Accuracy score for sentiment analysis'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Chapter 4: Introduction to convolutional networks'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 5: Sentiment Analysis on a real-life dataset'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Solution:**'
  prefs: []
  type: TYPE_NORMAL
- en: Import the necessary classes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Define your variables and parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Import the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Printing this out on a `Jupyter` notebook should display:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.27: Labelled dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_04_271.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.27: Labelled dataset'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Select the '`sentence`' and '`label`' columns
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Split your data into training and test set
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Tokenize
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Pad in order to ensure that all sequences have the same length
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Create the model. Note that we use a sigmoid activation function on the last
    layer and the binary cross entropy for calculating loss. This is because we are
    doing a binary classification.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The above code should yield
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C13783_04_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.28: Model summary'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The model can be visualized as follows as well:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.29: Model visualization'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_04_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.29: Model visualization'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Train and test the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The accuracy output should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.30: Accuracy score](img/C13783_04_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.30: Accuracy score'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Chapter 5: Foundations of Recurrent Neural Network'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 6: Solve a problem with RNN – Author Attribution'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Solution:**'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We begin by setting up the data pre-processing pipeline. For each one of the
    authors, we aggregate all the known papers into a single long text. We assume
    that style does not change across the various papers, hence a single text is equivalent
    to multiple small ones yet it is much easier to deal with programmatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each paper of each author we perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert all text into lower-case (ignoring the fact that capitalization may
    be a stylistic property)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Converting all newlines and multiple whitespaces into single whitespaces
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove any mention of the authors' names, otherwise we risk data leakage (authors
    names are *hamilton* and *madison*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do the above steps in a function as it is needed for predicting the unknown
    papers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output for this should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.34: Text length count'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_05_341.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.34: Text length count'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The next step is to break the long text for each author into many small sequences.
    As described above, we empirically choose a length for the sequence and use it
    throughout the model's lifecycle. We get our full dataset by labeling each sequence
    with its author.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To break the long texts into smaller sequences we use the `Tokenizer` class
    from the `keras` framework. In particular, note that we set it up to tokenize
    according to characters and not words.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Choose `SEQ_LEN` hyper parameter, this might have to be changed if the model
    doesn't fit well to training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a function `make_subsequences` to turn each document into sequences of
    length SEQ_LEN and give it a correct label.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use Keras `Tokenizer` with `char_level=True`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the tokenizer on all the texts
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use this tokenizer to convert all texts into sequences using `texts_to_sequences()`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `make_subsequences()` to turn these sequences into appropriate shape and
    length
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.35: Character count of sequences'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_05_351.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.35: Character count of sequences'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Compare the number of raw characters to the number of labeled sequences for
    each author. Deep Learning requires many examples of each input. The following
    code calculates the number of total and unique words in the texts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.36: Total word count and unique word count'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13783_05_36.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.36: Total word count and unique word count'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: We now proceed to create our train, validation sets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Stack `x` data together and y data together.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `train_test_split` to split the dataset into 80% training and 20% validation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reshape the data to make sure that they are sequences of correct length.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.37: Testing and training datasets](img/C13783_05_37.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.37: Testing and training datasets'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Finally, we construct the model graph and perform the training procedure.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Create a model using `RNN` and `Dense` layers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since its a binary classification problem, the output layer should be `Dense`
    with `sigmoid` activation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compile the model with `optimizer`, appropriate loss function and metrics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the summary of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C13783_05_38.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.38: Model summary'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Decide upon the batch size, epochs and train the model using training data and
    validate with validation data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on the results, go back to model above, change it if needed (use more
    layers, use regularization, dropout, etc., use different optimizer, or a different
    learning rate, etc.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change `Batch_size`, `epochs` if needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C13783_05_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.39: Epoch training'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Applying the Model to the Unknown Papers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Do this all the papers in the Unknown folder
  prefs: []
  type: TYPE_NORMAL
- en: Preprocess them same way as training set (lower case, removing white lines,
    etc.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `tokenizer` and `make_subsequences` function above to turn them into sequences
    of required size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the model to predict on these sequences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Count the number of sequences assigned to author **A** and the ones assigned
    to author **B**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on the count, pick the author with highest votes/count
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.40: Output for author attribution](img/C13783_05_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.40: Output for author attribution'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Chapter 6: Foundations of GRUs'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 7: Develop a sentiment classification model using Simple RNN'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Solution:**'
  prefs: []
  type: TYPE_NORMAL
- en: Load the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Pad sequences so that each sequence has the same number characters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Define and compile model using `SimpleRNN` with 32 hidden units.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Plot the validation and training accuracy and losses.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Plot the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.29: Training and validation accuracy loss'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_06_29.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.29: Training and validation accuracy loss'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Activity 8: Train your own character generation model with a dataset of your
    choice'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Solution:**'
  prefs: []
  type: TYPE_NORMAL
- en: Load the text file and import the necessary Python packages and classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.30: Sonnets from Shakespeare](img/C13783_06_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.30: Sonnets from Shakespeare'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Create dictionaries mapping characters to indices and vice-versa.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.31: Distinct character count](img/C13783_06_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.31: Distinct character count'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Create sequences from the text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.32: nb sequence count](img/C13783_06_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.32: nb sequence count'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Make input and output arrays to feed the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Build and train the model using GRU and save the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Define sampling and generation functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Generate text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.33: Generated text output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_06_33.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.33: Generated text output'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Chapter 7: Foundations of LSTM'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 9: Build a Spam or Ham classifier using a Simple RNN'
  prefs: []
  type: TYPE_NORMAL
- en: '**Solution:**'
  prefs: []
  type: TYPE_NORMAL
- en: Import required Python packages
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Read the input file containing a column that contains text and another column
    that contains the label for the text depicting whether the text is spam or not.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.35: Input data file](img/C13783_07_351.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 7.35: Input data file'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Label the columns in the input data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.36: Labelled input data](img/C13783_07_361.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 7.36: Labelled input data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Count spam, ham characters in the `v1` column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.37: Value counts for spam or ham](img/C13783_07_371.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 7.37: Value counts for spam or ham'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Get `X` as feature and `Y` as target.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Convert to sequences and pad the sequences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.38: Tokenized data](img/C13783_07_381.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 7.38: Tokenized data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Train the sequences
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Build the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Predict the mail category on new test data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.39: Output for new test data](img/C13783_07_391.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.39: Output for new test data'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Activity 10: Create a French to English translation model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Solution:**'
  prefs: []
  type: TYPE_NORMAL
- en: Import the necessary Python packages and classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Read the file in sentence pairs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Remove `\u202f` character
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Append '**BEGIN_** ' and ' **_END**' words to target sequences. Map words to
    integers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Define encoder-decoder inputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Build the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Initiate encoder training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Initiate decoder training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Define the final model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Provide inferences to encoder and decoder
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Reverse-lookup token index to decode sequences
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Encode input as a state vector
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Generate empty target sequence of length 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Populate the first character of target sequence with the start character.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Sampling loop for a batch of sequences
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Sample a token.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Exit condition: either hit max length or find stop character.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Update the target sequence (of length 1).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Update states
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Inference for user input: take in a word sequence, convert the sequence word
    by word into encoded.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.47: French to English translator](img/C13783_07_47.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.47: French to English translator'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Chapter 8: State of the art in Natural Language Processing'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activity 11: Build a Text Summarization Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Solution:**'
  prefs: []
  type: TYPE_NORMAL
- en: Import the necessary Python packages and classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Load the dataset and read the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Make vocab dictionary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Convert lowercase to standardize.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run the previous code snippet to load data, get vocab dictionaries and define
    some utility functions to be used later. Define length of input characters and
    output characters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Use `repeator` to repeat `s_prev` to be of shape (`m`, `Tx`, `n_s`) so that
    you can concatenate it with all hidden states "`a`"
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Use `densor1` to propagate `concat` through a small fully-connected neural network
    to compute the "intermediate energies" variable e.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Use `densor2` to propagate e through a small fully-connected neural network
    to compute the "`energies`" variable energies.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Use "`activator`" on "`energies`" to compute the attention weights "`alphas`"
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Use `dotor` together with "`alphas`" and "`a`" to compute the context vector
    to be given to the next (post-attention) LSTM-cell
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Define the inputs of your model with a shape (`Tx`,)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define `s0` and `c0`, initial hidden state for the decoder LSTM of shape (`n_s`,)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Initialize empty list of outputs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Apply the post-attention LSTM cell to the "`context`" vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Apply `Dense` layer to the hidden state output of the post-attention LSTM
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Append "out" to the "outputs" list
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Create model instance taking three inputs and returning the list of outputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.18: Text summarization model output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13783_08_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8.18: Text summarization model output'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Chapter 9: A practical NLP project workflow in an organisation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Code for LSTM model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Check if GPU is detected
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Setting up collar notebook
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Import necessary Python packages and classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Load the data file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Initialize tokenization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Fit tokenizer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Pad sequences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Get target variable
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Fit the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Save model and tokenizer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Code for Flask
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Import the necessary Python packages and classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Define the input files and load in dataframe
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define preprocessing functions similar to the training code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a Flask app instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define an endpoint that displays a fixed message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll have a prediction endpoint, to which we can send our review strings.
    The kind of HTTP request we will use is a ''`POST`'' request:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Start the web server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save this file as `app.py` (any name could be used). Run this code from the
    terminal using `app.py`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.31: Output for flask](img/C13783_09_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.31: Output for flask'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
