- en: Detecting and Localizing Objects in Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the chapters on building a deep convolutional neural network and transfer
    learning, we have learned about detecting the class that an image belongs to using
    deep CNN and also by leveraging transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: While object classification works, in the real world, we will also be encountering
    a scenario where we would have to locate the object within an image.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the case of a self-driving car, we would not only have to detect
    that a pedestrian is in the view point of a car, but also be able to detect how
    far the pedestrian is located away from the car so that an appropriate action
    can then be taken.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be discussing the various techniques of detecting
    objects in an image. The case studies we will be covering in this chapter are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating the training dataset of bounding box
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating region proposals within an image using selective search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating an intersection over a union between two images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting objects using region proposal-based CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing non-max suppression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting a person using the anchor box-based algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the rise of autonomous cars, facial detection, smart video surveillance,
    and people counting solutions, fast and accurate object detection systems are
    in great demand. These systems include not only object recognition and classification
    in an image, but can also locate each one of them by drawing appropriate boxes
    around them. This makes object detection a harder task than its traditional computer
    vision predecessor, image classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how the output of object detection looks like, let''s go through
    the following picture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/310b9e9a-7372-4f0d-ba7d-00ce7a848ffd.png)'
  prefs: []
  type: TYPE_IMG
- en: So far, in the previous chapters, we have learned about classification.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn about having a tight bounding box around the
    object in the picture, which is the localization task.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we will also learn about detecting the multiple objects in the
    picture, which is the object detection task.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the dataset for a bounding box
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have learned that object detection gives us the output where a bounding box
    surrounds the object of interest in an image. For us to build an algorithm that
    detects the bounding box surrounding the object in an image, we would have to
    create the input–output mapping, where the input is the image and the output is
    the bounding boxes surrounding the objects in the given image.
  prefs: []
  type: TYPE_NORMAL
- en: Note that when we detect the bounding box, we are detecting the pixel locations
    of the top-left corner of the bounding box surrounding the image, and the corresponding
    width and height of the bounding box.
  prefs: []
  type: TYPE_NORMAL
- en: To train a model that provides the bounding box, we need the image, and also
    the corresponding bounding-box coordinates of all the objects in an image.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will highlight one of the ways to create the training dataset
    where the image shall be given as input and the corresponding bounding boxes are
    stored in an XML file.
  prefs: []
  type: TYPE_NORMAL
- en: We shall be using the `labelImg` package to annotate the bounding boxes and
    the corresponding classes.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Bounding boxes around objects in image can be prepared as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Windows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Download the executable file of `labelImg` from the link here: [https://github.com/tzutalin/labelImg/files/2638199/windows_v1.8.1.zip](https://github.com/tzutalin/labelImg/files/2638199/windows_v1.8.1.zip).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Extract and open the `labelImg.exe` GUI, shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/93cd65bd-cb08-4193-a83f-b4c592fa6e51.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Specify all the possible labels in an image in the `predefined_classes.txt`
    file in the `data` folder. We need to ensure that all the classes are listed in
    a separate line, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/77c07038-7b8a-4a11-b2a9-1f14354f96eb.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Open an image by clicking Open in the GUI and annotate the image by clicking
    on Create RectBox, which will pop up the classes that will be selected as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9f98e69a-7ac4-42c9-85e3-d0adecbbe359.png)'
  prefs: []
  type: TYPE_IMG
- en: Click on Save and save the XML file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inspect the XML file. A snapshot of the XML file after drawing the rectangular
    bounding box looks as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/74ad33e9-c30c-4f97-a457-25d976da745d.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding screenshot, you should note that the `bndbox` contains the
    coordinates of the minimum and maximum values of the *x* and *y* coordinates corresponding
    to the objects of interest in the image. Additionally, we should also be in a
    position to extract the classes corresponding to the objects in image.
  prefs: []
  type: TYPE_NORMAL
- en: Ubuntu
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In Ubuntu, the same steps as preceding ones can be executed by keying in the
    following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The script `labelImg.py` can be found in the GitHub link here: [https://github.com/tzutalin/labelImg](https://github.com/tzutalin/labelImg).
  prefs: []
  type: TYPE_NORMAL
- en: Once we execute the preceding code, we should be in a position to perform the
    same analysis as we have seen in the *Windows* section.
  prefs: []
  type: TYPE_NORMAL
- en: MacOS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In macOS, the same preceding steps can be executed by keying in the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The script `labelImg.py` can be found in the GitHub link here: [https://github.com/tzutalin/labelImg](https://github.com/tzutalin/labelImg).
  prefs: []
  type: TYPE_NORMAL
- en: Once we execute the preceding script, we should be in a position to perform
    the same analysis as we have seen in the *Windows* section.
  prefs: []
  type: TYPE_NORMAL
- en: Generating region proposals within an image, using selective search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand what a region proposal is, let's break the term into its constituents—region
    and proposal.
  prefs: []
  type: TYPE_NORMAL
- en: A **region** is a portion of the total image where the pixels in that portion
    have very similar values.
  prefs: []
  type: TYPE_NORMAL
- en: A **region proposal** is the smaller portion of the total image, where there
    is a higher chance of the portion belonging to a particular object.
  prefs: []
  type: TYPE_NORMAL
- en: A region proposal is useful, as we generate candidates from the image where
    the chances of an object being located in one of those regions is high. This comes
    in handy in the object localization tasks, where we need to have a bounding box
    around the object that is similar to what we have in the picture in the previous
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look into a way of generating a bounding box within
    an image of a person.
  prefs: []
  type: TYPE_NORMAL
- en: Selective search is a region proposal algorithm used in object detection. It
    is designed to be fast with a very high recall. It is based on computing a hierarchical
    grouping of similar regions based on color, texture, size, and shape compatibility.
  prefs: []
  type: TYPE_NORMAL
- en: Region proposals can be generated using a Python package named `selectivesearch`
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: The selective search starts by over-segmenting the image (generating thousands
    of region proposals) based on intensity of the pixels using a graph-based segmentation
    method by Felzenszwalb and Huttenlocher.
  prefs: []
  type: TYPE_NORMAL
- en: 'Selective search algorithm takes these over-segments as the initial input and
    performs the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Add all bounding boxes corresponding to segmented parts to the list of regional
    proposals
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Group adjacent segments based on similarity
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to step one
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At each iteration, larger segments are formed and added to the list of region
    proposals. Hence, we create region proposals from smaller segments to larger segments
    in a bottom-up approach.
  prefs: []
  type: TYPE_NORMAL
- en: Selective search uses four similarity measures based on color, texture, size,
    and shape compatibility to come up with the region proposals.
  prefs: []
  type: TYPE_NORMAL
- en: Region proposals help identify the possible objects of interest in an image.
    Thus, we could potentially convert the exercise of localization into a classification
    exercise where we shall classify each region as whether it contains the object
    of interest.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will demonstrate the extracting of region proposals, as
    follows (The code file is available as `Selective_search.ipynb` in GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install `selectivesearch` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the relevant packages, shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the image, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the region proposals:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The parameter `min_size` provides a constraint that the region proposal should
    be at least 2,000 pixels in size, and the parameter scale effectively sets a scale
    of observation, in that a larger scale causes a preference for larger components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check the resulting number of regions and store them in a list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding step, we have stored all the regions that are more than 2,000
    pixels in size (area) into a set of candidates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the resulting image with candidates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/eb1ac059-09d8-4f87-bb46-23877f68f432.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding screenshot, we see that there are multiple regions that are
    extracted from the image.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating an intersection over a union between two images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand how accurate the proposed regions are, we use a metric named
    **Intersection over Union** (**IoU**). IoU can be visualized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c167b0ff-23eb-4dd7-8e62-f3c265fe7b82.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that, in the preceding picture, the blue box (lower one) is the ground
    truth and the red box (the upper rectangle) is the region proposal.
  prefs: []
  type: TYPE_NORMAL
- en: The intersection over the union of the region proposal is calculated as the
    ratio of the intersection of the proposal and the ground truth over the union
    of the region proposal and the ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'IoU is calculated as follows (the code file is available as `Selective_search.ipynb`
    in GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the IoU extraction function, demonstrated in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding function, we take the candidate, actual object region, and
    image shape as input.
  prefs: []
  type: TYPE_NORMAL
- en: Further, we initialized two zero-value arrays of the same shape for the candidate
    image and the actual object location image.
  prefs: []
  type: TYPE_NORMAL
- en: We have over-written the candidate image and the actual object location images
    with one, wherever the image and object are located.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we calculated the intersection over the union of the candidate image
    with the actual object location image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the image of interest:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the image and verify the actual location of the object of interest:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/77532f5a-d1fd-4883-a1f0-92c9aa15685c.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the region of interest is ~50 pixels from bottom left extending to
    ~290th pixel of the image. Additionally, on the *y* axis, it starts from ~50th
    pixel too till the end of the image.
  prefs: []
  type: TYPE_NORMAL
- en: So, the actual location of object is (50, 50, 290, 500), which is in the format
    of (`xmin`, `ymin`, `xmax`, `ymax`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the region proposals:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The regions extracted from the `selectivesearch` method are in the format of
    (`xmin`, `ymin`, `width`, `height`). Hence, before extracting the IoU of the regions,
    we should ensure that the candidate and the actual location of image are in the
    same format that is, (`xmin`, `ymin`, `xmax`, `ymax`)
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply the IoU extraction function to the image of interest. Note that the function
    takes the actual object''s location and the candidate image shape as input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Identify the region that has the highest overlap with the actual object of
    interest (ground truth bounding box):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The preceding output for this specific image is the tenth candidate where the
    coordinates are 0, 0, 299, 515.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s print the actual bounding box and the candidate bounding box. For this,
    we have to convert the (`xmin`, `ymin`, `xmax`, `ymax`) format of output into
    (`xmin`, `ymin`, `width`, `height`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s append the actual and the bounding box with the highest IoU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will loop through the preceding list and assign a bigger line width
    for actual location of object in image, so that we are able to distinguish between
    candidate and the actual object''s location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/46f2426e-a451-4087-bce0-45d0c1b1d303.png)'
  prefs: []
  type: TYPE_IMG
- en: In this way, we are in a position to identify each candidate's IoU with the
    actual location of an object in the image. Additionally, we are also in a position
    to identify the candidate that has the highest IoU with the actual location of
    an object in the image.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting objects, using region proposal-based CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we have learned about generating region proposals from
    an image. In this section, we will leverage the region proposals to come up with
    object detection and localization within an image.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The strategy we shall be adopting to perform region proposal-based object detection
    and localization is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: For the current exercise, we'll build the model based on images that contain
    only one object
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We'll extract the various region proposals (candidates) within the image
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will calculate how close the candidate is to the actual object location:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Essentially, we calculate the intersection over the union of the candidate with
    the actual location of the object
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the intersection over the union is greater than a certain threshold—the
    candidate is considered to contain the object of interest—or else, it doesn''t:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This creates the label for each candidate where the candidate's image is input
    and the intersection over the union threshold provides the output
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll resize and pass each candidate image through the VGG16 model (which we
    have learned in the previous chapter) to extract features of the candidates
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Additionally, we will create the training data of the bounding-box correction
    by comparing the location of the candidate and the actual location of an object
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a classification model that maps the features of the candidate to the
    output of whether the region contains an object
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the regions that contain an image (as per the model), build a regression
    model that maps the input features of candidate to the correction required to
    extract the accurate bounding box of an object
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Perform non-max suppression on top of the resulting bounding boxes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Non-max suppression ensures that the candidates that overlap a lot are reduced
    to 1, where only the candidate that has the highest probability of containing
    an object is left
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: By performing a non-max suppression, we would be in a position to replicate
    the model that we built for images that contain multiple objects within the image
    too
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A schematic of the preceding is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cffb313a-6080-4b13-a921-c383a7830bb1.png)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will code up the algorithm that we have discussed in the
    previous section (the code file and corresponding recommended dataset link is
    available as `Region_proposal_based_object_detection.ipynb` in GitHub along with
    the recommended dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: Download the dataset that contains a set of images, the objects contained in
    them, and the corresponding bounding boxes of objects in the image. The dataset
    and the corresponding code files that you can work on are provided in GitHub.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A sample image and the corresponding bounding box co-ordinates and class of
    object in image are available as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98a29fdc-3138-4c23-b471-afd9c7b39b9b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The class of object and bounding box co-ordinates would be available in an
    XML file (Details of how to obtain the XML file are available in code file in
    GitHub) and can be extracted from the XML file as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If  `xml["annotation"]["object"]`  is a list, it indicates that there multiple
    objects present in the same image.
  prefs: []
  type: TYPE_NORMAL
- en: '`xml["annotation"]["object"]["bndbox"]`  extracts the bounding box of object
    present in image where the bounding box is available as "xmin","ymin","xmax" and
    "ymax" co-ordinates of object in image.'
  prefs: []
  type: TYPE_NORMAL
- en: '`xml["annotation"]["object"]["name"]`  extracts the class of object present
    in image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant packages as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the IoU extraction function, shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the candidate extraction function, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Note that, in the preceding function, we are excluding all candidates which
    occupy less than 5% of the area of image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the pre-trained VGG16 model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the input and output mapping for the images that contain only one object
    within them. Initialize multiple lists that will be populated as we go through
    the images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll loop through the images and shall work on only those that contain a
    single object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are extracting the `xml` attributes of an image and
    checking whether the image contains multiple objects (if the output of `xml["annotation"]["object"]`
    is a list, then the image contains multiple objects).
  prefs: []
  type: TYPE_NORMAL
- en: 'Normalize the object location coordinates so that we can work on the normalized
    bounding box. This is done so that the normalized bounding box does not change,
    even if the image shape is changed for further processing. For example, if the
    object''s `xmin` is at 20% of *x* axis and 50% of *y* axis, it would be the same,
    even when the image is reshaped (however, had we been dealing with pixel values,
    the `xmin` value would be changed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we have normalized the bounding-box coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract candidates of the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are using the `extract_candidates` function to extract
    the region proposals of the resized image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Loop through the candidates to calculate the intersection over union of each
    candidate with the actual bounding box of the object in the image and also the
    corresponding delta between the actual bounding box and the candidate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the VGG16 features of each region proposal and assign the target
    based on the IoU of the region proposal with the actual bounding box:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Create input and output arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We utilize the `get_dummies` method as the classes are categorical text values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Build and compile the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the model, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The preceding results in a classification accuracy of 97% on a test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We are dividing the input array by `x_train.max()`, as the maximum value in
    an input array is ~11\. Typically, it is a good idea to have input values between
    zero and one, and given that the VGG16 prediction on the scaled input has a maximum
    value of ~11, we divide the input array by `x_train.max()`—which is equal to ~11.
  prefs: []
  type: TYPE_NORMAL
- en: Make a prediction of the class from the dataset (ensure that we do not consider
    an image that was used for training).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pick an image that was not used for testing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Build a function that performs image preprocessing to extract candidates, performs
    model prediction on resized candidates, filters out the predicted background class
    regions, and, finally, plots the region (candidate) that has the highest probability
    of containing a class that is other than the background class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we are resizing an input image and are extracting candidates
    from it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, are plotting an image and are initializing the predicted
    probability and predicted class lists that will be populated in subsequent steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we are looping through the candidates, resizing them,
    and passing them through the VGG16 model. Furthermore, we are passing the VGG16
    output through our model, which provides the probability of the image belonging
    to various classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we are extracting the candidate that has the highest
    probability of containing an object that is non-background (the predicted class
    of one corresponds to the background):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are plotting the image along with rectangular patch
    of the bounding box.
  prefs: []
  type: TYPE_NORMAL
- en: 'Call the function defined with a new image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/3dbde263-8f5e-4c2e-8c25-2cd98af80476.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the model accurately figured the class of the object in the image.
    Additionally, the bounding box (candidate) that has the highest probability of
    containing a person needs a little bit of correction.
  prefs: []
  type: TYPE_NORMAL
- en: In the next step, we will correct the bounding box further.
  prefs: []
  type: TYPE_NORMAL
- en: 'Build and compile a model that takes the VGG16 features of image as input and
    predicts the bounding-box corrections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Build the model to predict bounding-box corrections. However, we need to ensure
    that we predict bounding-box corrections only for those regions that are likely
    to contain an image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are looping through the input array dataset and creating
    a new dataset that is only for those regions that are likely to contain a non-background.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we are repeating the preceding step 1,000 different times to fine
    tune the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Build a function that takes an image path as input and predicts the class of
    an image, along with correcting the bounding box:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the test images that contain only one object (as we have built the
    model on images that contain a single object):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are looping through the image annotations and identifying
    the images that contain a single object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Predict on the single object image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/3ae32885-779f-4d76-997b-a69f51db9f4e.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the second model was able to correct the bounding box to fit the person;
    however, the bounding box still needs to be corrected a little more. This could
    potentially be achieved when trained on more data points.
  prefs: []
  type: TYPE_NORMAL
- en: Performing non-max suppression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, in the previous section, we have only considered the candidates that
    do not have a background, and further considered the candidate that has the highest
    probability of the object of interest. However, this fails in the scenario where
    there are multiple objects present in an image.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will discuss the ways to shortlist the candidate region
    proposals so that we are in a position to extract as many objects as possible
    within the image.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The strategy we adopt to perform NMS is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract the region proposals from an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reshape the region proposals and predict the object that is contained in the
    image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the object is non-background, we shall keep the candidate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For all the non-background class candidates, we'll order them by the probability
    that they contain an object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first candidate (post-rank ordering by decreasing probability of any class)
    will be compared with all the rest of candidates in terms of the IoU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there is considerable overlap between any other region with the first candidate,
    they shall be discarded
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Among the candidates that remain, we'll again consider the candidate that has
    the highest probability of containing an object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll repeat the comparison of first candidate (among the filtered list that
    has limited overlap with the first candidate in the previous step) with the rest
    of the candidates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This process continues until there are no candidates left for comparison
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll plot the candidates for the candidates that remain after the preceding
    steps as the final bounding boxes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Non-max suppression is coded in Python as follows. We'll continue from step
    14 in the previous recipe (The code file and corresponding recommended dataset
    link is available as `Region_proposal_based_object_detectionn.ipynb` in GitHub).
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract all the regions from an image where there is a high confidence of containing
    an object that is of a non-background class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The image that we are considering is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad993779-4983-499c-841a-61bc9b3c2692.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Pre-process the candidates—pass them through VGG16 model, and then predict
    the class of each region proposal, as well as the bounding boxes of the regions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the non-background class predictions and their corresponding bounding-box
    corrections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding step, we have filtered all the probabilities, classes, and
    bounding-box corrections for the regions that are non-background (predicted class
    `1` belongs to background class in our data preparation process).
  prefs: []
  type: TYPE_NORMAL
- en: 'Correct the candidates using the bounding-box correction values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Additionally, we have also ensured that the `xmax` and `ymax` coordinates cannot
    be greater than `224`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we need to ensure that the width and height of bounding boxes
    cannot be negative:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the image along with the bounding boxes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/509c6bb7-d6d8-4e13-b9d7-7c10a5da93ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Perform non-max suppression on top of the bounding boxes. For this, we''ll
    define a function that performs NMS by taking the minimum possible intersection
    that two bounding boxes can have (threshold), bounding-box coordinates, and the
    probability score associated with each bounding box in the following steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the `x`, `y`, `w`, and `h` values of each bounding box, their corresponding
    areas, and, also, their probability order:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the intersection over union of the candidate with highest probability
    with the rest of the candidates:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Identify the candidates that have an IoU that is less than the threshold:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding step, we are ensuring that we have the next set of candidates
    (other than the first candidate) that are to be looped through the same steps
    (notice the `while` loop at the start of the function).
  prefs: []
  type: TYPE_NORMAL
- en: 'Return the index of candidates that need to be kept:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the preceding function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot those bounding boxes that were left from the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/415d8874-7c1a-4f5a-af2e-6e66958d87e6.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding screenshot, we see that we removed all the other bounding
    boxes that were generated as region proposals.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting a person using an anchor box-based algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the drawbacks of region proposal based CNN is that it does not enable
    a real-time object recognition, as selective search takes considerable time to
    propose regions. This results in region proposal-based object detection algorithms
    not being useful in cases like self-driving car, where real-time detection is
    very important.
  prefs: []
  type: TYPE_NORMAL
- en: In order to achieve real-time detection, we will build a model that is inspired
    by the **You Only Look Once** (**YOLO**) algorithm from scratch that looks at
    the images that contain a person in image and draws a bounding box around the
    person in image.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand how YOLO overcomes the drawback of consuming considerable time
    in generating region proposals, let us break the term YOLO into its constituent
    terms—we shall make all the predictions (class of image and also the bounding
    box) from a single forward pass of the neural network. Compare this with what
    we did with region proposal based CNN, where a selective search algorithm gave
    us the region proposals, and then we built a classification algorithm on top of
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'To figure out the working details of YOLO, let''s go through a toy example.
    Let''s say the input image looks as follows—where the image is divided into a
    3 x 3 grid:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d767d1cb-cba0-4129-af5e-a1ce782a7733.png)'
  prefs: []
  type: TYPE_IMG
- en: The output of our neural network model shall be of 3 x 3 x 5 in size, where
    the first 3 x 3 correspond to the number of grids we have in the image, and the
    first output of the five channels corresponds to the probability of the grid containing
    an object, and the other four constituents are the *delta of x, y, w, h* coordinates
    corresponding to the grid in the image.
  prefs: []
  type: TYPE_NORMAL
- en: One other lever that we use is the anchor boxes. Essentially, we already know
    that there are certain shapes within the set of images we have. For example, a
    car will have a shape where the width is greater than the height and a standing
    person would generally have a higher height when compared to the width.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, we shall cluster all the height and width values we have in our image
    set into five clusters and that shall result in the five anchor boxes' height
    and width that we shall use to identify bounding boxes around the objects in our
    image.
  prefs: []
  type: TYPE_NORMAL
- en: If there are five anchor boxes working on an image, the output then shall be
    3 x 3 x 5 x 5, where the 5 x 5 corresponds to the five constituents (one probability
    of the object and the four delta along *x*, *y*, *w*, and *h*) of each of the
    five anchor boxes.
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding, we can see that the 3 x 3 x 5 x 5 output can be generated
    from a single forward pass through the neural network model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following section, the pseudo code to understand the ways to generate
    the size of anchors:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract the width and height of all images in a dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run a k-means clustering with five clusters to identify the clusters of width
    and height present in the image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The five cluster centers correspond to the width and height of the five anchor
    boxes to build the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Furthermore, in the following section, we will understand how the YOLO algorithm
    works:'
  prefs: []
  type: TYPE_NORMAL
- en: Divide the image into a fixed number of grid cells
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The grid that corresponds to the center of the ground truth of the bounding
    box of the image shall be the grid that is responsible in predicting the bounding
    box
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The center of anchor boxes shall be the same as the center of the grid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Create the training dataset:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the grid that contains the center of object, the dependent variable is one and
    the delta of *x*, *y*, *w*, and *h* need to be calculated for each anchor box
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For the grids that do not contain the center of object, the dependent variable
    is zero and the delta of *x*, *y*, *w*, and *h* do not matter
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In the first model, we shall predict the anchor box and grid cell combination
    that contain the center of the image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the second model, we predict the bounding box corrections of the anchor box
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will build the code to perform person detection (The code file and corresponding
    recommended dataset link is available as `Anchor_box_based_person_detection.ipynb` in
    GitHub along with the recommended dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: Download the dataset that contains a set of images, the objects contained in
    them, and the corresponding bounding boxes of the objects in the images. The recommended
    dataset and the corresponding code files that you can work on are provided in
    GitHub.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A sample image and its corresponding bounding box location output would look
    similar to the one that we saw in step 1 of "*Object detection using region proposal
    based CNN*" recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant packages, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the IoU extraction function, shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the anchor boxes'' width and height as a percentage of total image''s
    width and height:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Identify all the possible widths and heights of the person in the bounding
    boxes:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are looping through all the images that have only
    one object within them, and are then calculating the width and height of bounding
    box, if the image contains a person.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fit k-means clustering with five centers:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding results in cluster centers, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the training dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Initialize empty lists so that they can be appended with data in further processing:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Loop through the dataset so that we work on images that contain only one object
    in it, and also the object is a person:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code appends the location of the object (post-normalized for the
    width and height of the image).
  prefs: []
  type: TYPE_NORMAL
- en: 'Resize the image of person so that all images are of the same shape. Additionally,
    scale the image so that the values are between zero and one:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the object bounding box location, and also the normalized bounding-box
    coordinates:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the VGG16 features of the input image:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: By this step, we have created the input features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create the output features—in this case, we shall have 5 x 5 x 5 outputs
    for class label and 5 x 5 x 20 labels for the bounding-box correction labels:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding step, we have initialized zero arrays for the target class
    and bounding-box corrections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding step, we have defined a function that contains the center
    of the object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code helps us assign a class of `1` to the grid that contains
    the center of object and every other grid shall have a label of zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, let''s define a function that finds the anchor that is closest
    to the shape of the object of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code compares the width and height of the object of interest in
    an image, with all the possible anchors, and identifies the anchor that is closest
    to the width and height of the actual object in the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we shall also define a function that calculates the bounding-box corrections
    of anchor, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'We are all set to create the target data now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we have assigned a target class of `1` when the anchor
    considered is the closest anchor that matches the shape of the object in an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have also stored the bounding-box corrections in another list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Build a model to identify the grid cell and anchor that is most likely to contain
    an object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the input and output array for classification:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile and fit the model for classification:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding model, we are in a position to identify the grid cell and
    anchor box combination that is most likely to have a person. In this step, we
    shall build a dataset where we correct the bounding box for the predictions that
    are most likely to contain an object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding step, we have prepared the input (which is the VGG16 features
    of original image) and the output bounding-box corrections for the predictions
    that are most likely to contain an object.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we are multiplying `coord` by a factor of four, as for each grid cell
    and anchor box combination, there are four possible values of corrections for
    *x*, *y*, *w*, and *h*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Build a model that predicts the correction in *x*, *y*, *w*, and *h* coordinates:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create input and output arrays, and normalize the four bounding-box correction
    values so that all four values have a similar range:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Build a model that predicts the bounding-box corrections, given the VGG16 features:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile and fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict the bounding box on a new image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Extract the position that is most likely to contain the object:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding step, we are picking an image that contains a person within
    it, and resizing it, so that it can be further processed to extract the VGG16
    features. Finally, we are identifying the anchor that is most likely to contain
    the location of a person.
  prefs: []
  type: TYPE_NORMAL
- en: Extract the grid cell and anchor box combination that is most likely to contain
    an image.
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The preceding predicts the grid cell and anchor box combination that is most
    likely to contain the object of interest, which is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, `a2` and `b2` would be the grid cell (the *x* axis and
    *y* axis combination) that is most likely to contain an object, and `c2` is the
    anchor box that is possibly of the same shape as the object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Predict the corrections in *x*, *y*, *w*, and *h* coordinates:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'De-normalize the predicted bounding-box corrections:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the final corrected *x*, *y*, *w*, and *h* coordinates:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the image along with the bounding box:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/e075ef65-23d0-48bd-b284-2b3b3e9ff3b0.png)'
  prefs: []
  type: TYPE_IMG
- en: One of the drawbacks with this approach is that it gets difficult when we are
    detecting objects that are very small when compared to the image size.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's consider a scenario where the object to be detected is small in size.
    If we passed this image through a pre-trained network, this object would be detected
    in earlier layers, as, in the last few layers, the image would be passed through
    multiple pooling layers, resulting in the object to be detected shrinking to a
    very small space.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if the object to be detected is large in size, that object will be
    detected in the last layers of the pre-trained network.
  prefs: []
  type: TYPE_NORMAL
- en: 'A single shot detector uses a pre-trained network where different layers of
    the network work toward detecting different types of images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/363ff5ac-cf10-4007-8109-73940627fee0.png)'
  prefs: []
  type: TYPE_IMG
- en: Source: https://arxiv.org/pdf/1512.02325.pdf
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, you should note that features from different layers
    are passed through a dense layer, and, finally, concatenated together so that
    a model can be built and fine-tuned.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, YOLO can also be implemented based on the tutorial available here: [https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/).
  prefs: []
  type: TYPE_NORMAL
