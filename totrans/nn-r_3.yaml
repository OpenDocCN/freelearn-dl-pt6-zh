- en: Deep Learning Using Multilayer Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning is the recent hot trend in machine learning/AI. It is all about
    building advanced neural networks. By making multiple hidden layers work in a
    neural network model, we can work with complex nonlinear representations of data.
    We create deep learning using base neural networks. Deep learning has numerous
    use cases in real life, such as, driverless cars, medical diagnostics, computer
    vision, speech recognition, **Natural Language Processing** (**NLP**), handwriting
    recognition, language translation, and many other fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will deal with the deep learning process: how to train,
    test, and deploy a **Deep Neural Network** (**DNN**). We will look at the different
    packages available in R to handle DNNs. We will understand how to build and train
    a DNN with the `neuralnet` package. Finally, we will analyze an example of training
    and modeling a DNN using h2o, the scalable open-memory learning platform, to create
    models with large datasets and implement prediction with high-precision methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the topics covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Types of DNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R packages for deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and modeling a DNN with `neuralnet`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `h2o` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of the chapter, we will understand the basic concepts of deep learning
    and how to implement it in the R environment. We will discover different types
    of DNNs. We will learn how to train, test, and deploy a model. We will know how
    to train and model a DNN using `h2o`.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction of DNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the advent of big data processing infrastructure, GPU, and GP-GPU, we are
    now able to overcome the challenges with shallow neural networks, namely overfitting
    and vanishing gradient, using various activation functions and L1/L2 regularization
    techniques. Deep learning can work on large amounts of labeled and unlabeled data
    easily and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned, deep learning is a class of machine learning wherein learning
    happens on multiple levels of neuron networks. The standard diagram depicting
    a DNN is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00068.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: From the analysis of the previous figure, we can notice a remarkable analogy
    with the neural networks we have studied so far. We can then be quiet, unlike
    what it might look like, deep learning is simply an extension of the neural network.
    In this regard, most of what we have seen in the previous chapters is valid. In
    short, a DNN is a multilayer neural network that contains two or more hidden layers.
    Nothing very complicated here. By adding more layers and more neurons per layer,
    we increase the specialization of the model to train data but decrease the performance
    on the test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we anticipated, DNN are derivatives of ANN. By making the number of hidden
    layers more than one, we build DNNs. There are many variations of DNNs, as illustrated
    by the different terms shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep Belief Network** (**DBN**): It is typically a feed-forward network in
    which data flows from one layer to another without looping back. There is at least
    one hidden layer and there can be multiple hidden layers, increasing the complexity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Restricted Boltzmann Machine** (**RBM**): It has a single hidden layer and
    there is no connection between nodes in a group. It is a simple MLP model of neural
    networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recurrent Neural Networks** (**RNN**) and **Long Short Term Memory** (**LSTM**):
    These networks have data flowing in any direction within groups and across groups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As with any machine learning algorithm, even DNNs require building, training,
    and evaluating processes. A basic workflow for deep learning in shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00069.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The workflow we have seen in the previous figure remembers very closely that
    typical of a supervised learning algorithm. But what makes it different from other
    machine learning algorithms?
  prefs: []
  type: TYPE_NORMAL
- en: Almost all machine learning algorithms demonstrate their limits in identifying
    the characteristics of raw input data, especially when they are complex and lacking
    an apparent order, such as in images. Usually, this limit is exceeded through
    the help of humans, who are concerned with identifying what the machine can not
    do. Deep learning removes this step, relying on the training process to find the
    most useful models through input examples. Also in that case human intervention
    is necessary in order to make choices before starting training, but automatic
    discovery of features makes life much easier. What makes the neural networks particularly
    advantageous, compared to the other solutions offered by machine learning, is
    the great generalization ability of the model.
  prefs: []
  type: TYPE_NORMAL
- en: These features have made deep learning very effective for almost all tasks that
    require automatic learning; although it is particularly effective in a case of
    complex hierarchical data. Its underlying ANN forms highly nonlinear representations;
    these are usually composed of multiple layers together with nonlinear transformations
    and custom architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, deep learning works really well with messy data from the real world,
    making it a key instrument in several technological fields of the next few years.
    Until recently, it was a dark and daunting area to know, but its success has brought
    many great resources and projects that make it easier than ever to start.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know what the DNNs are, let's see what tools the R development environment
    offers us to deal with this particular topic.
  prefs: []
  type: TYPE_NORMAL
- en: R for DNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we clarified some key concepts that are at the deep
    learning base. We also understood the features that make the use of deep learning
    particularly convenient. Moreover, its rapid diffusion is also due to the great
    availability of a wide range of frameworks and libraries for various programming
    languages.
  prefs: []
  type: TYPE_NORMAL
- en: The R programming language is widely used by scientists and programmers, thanks
    to its extreme ease of use. Additionally, there is an extensive collection of
    libraries that allow professional data visualization and analysis with the most
    popular algorithms. The rapid diffusion of deep learning algorithms has led to
    the creation of an ever-increasing number of packages available for deep learning,
    even in R.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows the various packages/interfaces available for deep
    learning using R:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **CRAN package** | **Supported taxonomy of neural network** | **Underlying
    language/** **vendor** |'
  prefs: []
  type: TYPE_TB
- en: '| `MXNet` | Feed-forward, CNN | C/C++/CUDA |'
  prefs: []
  type: TYPE_TB
- en: '| `darch` | RBM, DBN | C/C++ |'
  prefs: []
  type: TYPE_TB
- en: '| `deepnet` | Feed-forward, RBM, DBN, autoencoders | R |'
  prefs: []
  type: TYPE_TB
- en: '| `h2o` | Feed-forward network, autoencoders | Java |'
  prefs: []
  type: TYPE_TB
- en: '| `nnet` and `neuralnet` | Feed-forward | R |'
  prefs: []
  type: TYPE_TB
- en: '| `Keras` | Variety of DNNs | Python/keras.io |'
  prefs: []
  type: TYPE_TB
- en: '| `TensorFlow` | Variety of DNNs | C++, Python/Google |'
  prefs: []
  type: TYPE_TB
- en: '`MXNet` is a modern, portable, deep learning library that can support multiple
    machines. The world''s largest companies and universities have adopted `MXNet`
    as a machine learning framework. These include Amazon, Intel, Data, Baidu, Microsoft,
    Wolfram Research, Carnegie Mellon, MIT, University of Washington, and Hong Kong
    University of Science and Technology.'
  prefs: []
  type: TYPE_NORMAL
- en: '`MXNet` is an open source framework that allows for fast modeling, and supports
    a flexible programming model in multiple programming languages (C ++, Python,
    Julia, MATLAB, JavaScript, Go, R, Scala, Perl, and Wolfram Language).'
  prefs: []
  type: TYPE_NORMAL
- en: The `MXNet` framework supports R programming language. The `MXNet` R package
    provides flexible and efficient GPU computing and a state-of-the-art deepening
    at R. It allows us to write a seamless tensorial/ matrix calculation with multiple
    GPUs in R. It also allows us to build and customize the state-of-the-art deep
    learning models in R and apply them to activities such as image classification
    and data science challenges.
  prefs: []
  type: TYPE_NORMAL
- en: The `darch` framework is based on the code written by G. E. Hinton and R. R.
    Salakhutdinov, and is available in the MATLAB environment for DBN. This package
    can generate neural networks with many levels (deep architectures) and form them
    with an innovative method developed by the authors. This method provides a pre-formation
    with the contrasting divergence method published by G. Hinton (2002) and fine-tuning
    with common training algorithms known as backpropagation or conjugated gradients.
    In addition, fine-tuning supervision can be improved with maxout and dropout,
    two recently developed techniques to improve fine-tuning for deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `deepnet` library is a relatively small, yet quite powerful package with
    variety of architectures to pick from. This library implements some deep learning
    architectures and neural network algorithms, including backpropagation, RBM, DBN,
    deep autoencoder, and so on. Unlike the other libraries we have analyzed, it was
    specifically written for R. It has several functions, including:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nn.train`: For training single or multiple hidden layers neural network by
    BP'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nn.predict`: For predicting new samples by trained neural network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dbn.dnn.train`: For training a DNN with weights initialized by DBN'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rbm.train`: For training an RBM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `h2o` R package has functions for building general linear regression, K-means,
    Naive Bayes, **Principal Component Analysis** (**PCA**), forests, and deep learning
    (multilayer `neuralnet` models). `h2o` is an external package to CRAN and is built
    using Java, and is available for a variety of platforms. It is an open source
    math engine for big data that computes parallel distributed machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The packages `nnet` and `neuralnet` have been widely discussed in the previous
    chapters. These are two packages for the management of neural networks in R. They
    are also able to build and train multicore neural networks, so they rely on deep
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: '`Keras` is an open source neural network library written in Python. Designed
    to enable fast experimentation with DNNs, it focuses on being minimal, modular,
    and extensible. The library contains numerous implementations of commonly used
    neural network building blocks, such as layers, objectives, activation functions,
    optimizers, and a host of tools to make working with image and text data easier.
    The code is hosted on GitHub, and community support forums include the GitHub
    issues page, a Gitter channel, and a Slack channel.'
  prefs: []
  type: TYPE_NORMAL
- en: '`TensorFlow` is an open source software library for machine learning. It contains
    a system for building and training neural networks to detect and decipher patterns
    and correlations, with methods similar to those adopted by human learning. It
    is used both for search and for Google production.'
  prefs: []
  type: TYPE_NORMAL
- en: Multilayer neural networks with neuralnet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After understanding the basics of deep learning, it's time to apply the skills
    acquired to a practical case. We've seen in the previous section that two libraries
    we know are listed in packages available in *R for DNNs* section. I refer to the
    `nnet` and `neuralnet` packages that we learned to use in the previous chapters
    through practical examples. Since we have some practice with the `neuralnet` library,
    I think we should start our practical exploration of the amazing world of deep
    learning from here.
  prefs: []
  type: TYPE_NORMAL
- en: To start, we introduce the dataset we will use to build and train the network.
    It is named the `College` dataset, and it contains statistics for a large number
    of US colleges, collected from the 1995 issue of *US News and World Report*. This
    dataset was taken from the `StatLib` library, which is maintained at Carnegie
    Mellon University, and was used in the *ASA Section on Statistical Graphics*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Things for us are further simplified because we do not have to retrieve the
    data and then import it into R, as these data are contained in a R package. I
    refer to the `ISLR` package. We just have to install the package and load the
    relative library. But we will see this later, when we explain the codices in detail.
    Now let''s just look at the content of the dataset `College`. It is a dataframe
    with `777` observations on the following `18` variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Private`: A factor with levels `No` and `Yes` indicating private or public
    university'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Apps`: Number of applications received'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Accept`: Number of applications accepted'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Enroll`: Number of new students enrolled'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Top10perc`: Percentage of new students from top 10 percent of H.S. class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Top25perc`: Percentage of new students from top 25 percent of H.S. class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`F.Undergrad`: Number of full time undergraduates'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`P.Undergrad`: Number of part time undergraduates'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Outstate`: Out-of-state tuition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Room.Board`: Room and board costs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Books`: Estimated book costs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Personal`: Estimated personal spending'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PhD`: Percentage of faculty with Ph.D.s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Terminal`: Percentage of faculty with terminal degree'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`S.F.Ratio`: Student-faculty ratio'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`perc.alumni`: Percentage of alumni who donate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Expend`: Instructional expenditure per student'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Grad.Rate`: Graduation rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our aim will be to build a multilayer neural network capable of predicting
    whether the school is public or private, based on the values assumed by the other
    `17` variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As usual, we will analyze the code line-by-line, by explaining in detail all
    the features applied to capture the results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As usual, the first two lines of the initial code are used to load the libraries
    needed to run the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, to install a library that is not present in the initial distribution
    of R, you must use the `install.package` function. This is the main function to
    install packages. It takes a vector of names and a destination library, downloads
    the packages from the repositories and installs them. This function should be
    used only once and not every time you run the code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This command loads the `College` dataset, which as we anticipated is contained
    in the `ISLR` library, and saves it in a given dataframe. Use the `View` function
    to view a compact display of the structure of an arbitrary R object. The following
    figure shows some of the data contained in the `College` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00070.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'How it is possible to note for each college are listed a series of statistics;
    the rows represent the observations on the columns instead are present the detected
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this snippet of code we need to normalize the data.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, it is good practice to normalize the data before training a neural
    network. With normalization, data units are eliminated, allowing you to easily
    compare data from different locations.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we will use the **min-max method** (usually called feature
    **scaling**) to get all the scaled data in the range *[0,1]*. Before applying
    the method chosen for normalization, you must calculate the minimum and maximum
    values of each database column. This procedure has already been adopted in the
    example we analyzed in [Chapter 2](part0056.html#1LCVG0-263fb608a19f4bb5955f37a7741ba5c4),
    *Learning Process in Neural Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last line scales the data by adopting the expected normalization rule.
    Note that we performed normalization only on the last *17* rows (from *2* to *18*),
    excluding the first column, `Private`, that contains a factor with levels `No`
    and `Yes`, indicating private or public university. This variable will be our
    target in the network we are about to build. To get a confirmation of what we
    say, check the typologies of the variables contained in the dataset. To do this,
    we will use the function `str` to view a compactly display the structure of an
    arbitrary R object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As anticipated, the first variable is of the `Factor` type, with two `levels`:
    `No` and `Yes`. For the remaining `17` variables, these are of the numeric type.
    As anticipated in [Chapter 1](part0021.html#K0RQ0-263fb608a19f4bb5955f37a7741ba5c4),
    *Neural Network and Artificial Intelligence Concepts*, only numeric data can be
    used in the model, as neural network is a mathematical model with approximation
    functions. So we have a problem with the first variable, `Private`. Do not worry,
    the problem can be easily resolved; just turn it into a numeric variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In this regard, the first line transforms the `Private` variable into numeric,
    while the second line of code is used to reconstruct the dataset with that variable
    and the remaining *17* appropriately normalized variables. To do this, we use
    the `cbind` function, that takes a sequence of vector, matrix, or dataframe arguments
    and combines by columns or rows, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The time has come to split the data for training and testing of the network.
    In the first line of the code just suggested, the dataset is split into *70:30*,
    with the intention of using *70* percent of the data at our disposal to train
    the network and the remaining *30* percent to test the network. In the third line,
    the data of the dataframe named data is subdivided into two new dataframes, called
    `train_data` and `test_data`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In this piece of code, we first recover all the variable names using the `names`
    function. This function gets or sets the name of an object. Next, we build the
    formula that we will use to build the network, so we use the `neuralnet` function
    to build and train the network. Everything so far has only been used to prepare
    the data. Now it is time to build the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This is the key line of the code. Here the network is built and trained; let's
    analyze it in detail. We had anticipated that we would use the `neuralnet` library
    to build our DNN. But what has changed with respect to the cases which we have
    built a single hidden layer network? Everything is played in the `hidden` argument
    setting.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the `hidden` argument must contain a vector of integers specifying
    the number of hidden neurons (vertices) in each layer.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we set the hidden layer to contain the vector *(5,3)*, which corresponds
    to two hidden levels with respective five neurons in the first hidden layer and
    three neurons in the second.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous line simply plots the network diagram, as shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00071.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, the network is built and trained, and we only have to verify
    its ability to predict:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To predict the data reserved for testing, we can use the `compute` method.
    This is a method for objects of class `nn`, typically produced by the `neuralnet`
    function. Computes the outputs of all the neurons for specific arbitrary covariate
    vectors given a trained neural network. It''s crucial to make sure that the order
    of the covariates is the same in the new matrix or dataframe as in the original
    neural network. Subsequently, to visualize, the first lines of the prediction
    result is used the `print` function, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As can be seen, the forecast results are provided in the form of decimal numbers,
    which approach the values of the two expected classes (one and zero), but do not
    exactly assume these values. We need to assume these values precisely, so we can
    make a comparison with the current values. To do this, we will use the `sapply()`
    function to round these off to either zero or one class, so we can evaluate them
    against the test labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As anticipated, the `sapply()` function has rounded the prediction results
    in the two available classes. Now we have everything we need to make a comparison
    in order to assess the DNN as a prediction tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: To make a comparison, we rely on the confusion matrix. To build it, just use
    the `table` function. Indeed, the `table` function uses the cross-classifying
    factors to construct a contingency table of counts at each combination of factor
    levels.
  prefs: []
  type: TYPE_NORMAL
- en: The confusion matrix is a specific table layout that allows visualization of
    the performance of an algorithm. Each row represents the instances in an actual
    class, while each column represents the instances in a predicted class. The term
    confusion matrix results from the fact that it makes it easy to see if the system
    is confusing two classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see then the results obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Let us understand the results. Let us first remember that in a confusion matrix,
    the terms on the main diagonal represent the number of correct predictions, that
    is, the number of instances of the predicted class that coincide with the instances
    of the actual class. It seems that in our simulation, things have gone well. In
    fact, we got `49` occurrences of class `0` (`No`) and `167` of class `1` (`Yes`).
    But let's now analyze the other two terms, these represent the mistakes made by
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'As defined in [Chapter 2](part0056.html#1LCVG0-263fb608a19f4bb5955f37a7741ba5c4),
    *Learning Process in Neural Networks*, `8` are FN and `9` are FP. In this regard,
    we recall that FN means the number of negative predictions that are positive in
    actual data, while FPs are the number of positive predictions that are negative
    in actual data. We can check this by again using the `table` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'These represent the actual results, in particular, `57` results belonging to
    class `0` and `176` to class `1`. By summing the data contained in the rows of
    the confusion matrix, we get exactly those values in fact results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we again use the `table` function to obtain the occurrences in the predicted
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'These represent the results of the prediction, in particular, `58` results
    belonging to class `0` and `175` to class `1`. By summing the data contained in
    the columns of the confusion matrix, we get exactly those values in fact results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we calculate the accuracy of the simulation by using the data
    contained in the confusion matrix. Let''s remember that accuracy is defined by
    the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00072.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '*TP = TRUE POSITIVE*'
  prefs: []
  type: TYPE_NORMAL
- en: '*TN = TRUE NEGATIVE*'
  prefs: []
  type: TYPE_NORMAL
- en: '*FP = FALSE POSITIVE*'
  prefs: []
  type: TYPE_NORMAL
- en: '*FN = TRUE NEGATIVE*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us take a look at this in the following code sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We've got an accuracy of around 93 percent, confirming that our model is able
    to predict data with a good result.
  prefs: []
  type: TYPE_NORMAL
- en: Training and modeling a DNN using H2O
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover an example of training and modeling a DNN using
    `h2o`. `h2o` is an open source, in-memory, scalable machine learning and AI platform
    used to build models with large datasets and implement predictions with high-accuracy
    methods. The `h2o` library is adapted at a large scale in numerous organizations
    to operationalize data science and provide a platform to build data products.
    `h2o` can run on individual laptops or large clusters of high-performance scalable
    servers. It works very fast, exploiting the machine architecture advancements
    and GPU processing. It has high-accuracy implementations of deep learning, neural
    networks, and other machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: As said earlier, the `h2o` R package has functions for building general linear
    regression, K-means, Naive Bayes, PCA, forests, and deep learning (multilayer
    `neuralnet` models). The `h2o` package is an external package to CRAN and is built
    using Java. It is available for a variety of platforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will install `h2o` in R using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We obtain the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: To test the package, let's go through the following example that uses the popular
    dataset named `Irisdataset`. I'm referring to the Iris flower dataset, a multivariate
    dataset introduced by the British statistician and biologist Ronald Fisher in
    his 1936 paper *The use of multiple measurements in taxonomic problems* as an
    example of linear discriminant analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset contains *50* samples from each of the three species of Iris (Iris
    `setosa`, Iris `virginica`, and Iris `versicolor`). Four features were measured
    from each sample: the length and the width of the sepals and petals, in centimeters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following variables are contained:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Sepal.Length` in centimeter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Sepal.Width` in centimeter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Petal.Length` in centimeter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Petal.Width` in centimeter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Class: `setosa`, `versicolour`, `virginica`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure shows a compactly display the structure of the `iris`
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00073.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We want to build a classifier that, depending on the size of the sepal and
    petal, is able to classify the flower species:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's go through the code to understand how to apply the `h2o` package
    to solve a pattern recognition problem.
  prefs: []
  type: TYPE_NORMAL
- en: Before proceeding, it is necessary to specify that running `h2o` on R requires
    Java 8 runtime. Verify the Java version installed on your machine beforehand and
    eventually download Java version 8 from [https://www.java.com/en/download/win10.jsp](https://www.java.com/en/download/win10.jsp).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the Java download page from Oracle''s site:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00074.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Furthermore, the `h2o` package is built with some required packages; so in
    order to properly install the `h2o` package, remember to install the following
    dependencies, all of which are available in CRAN:'
  prefs: []
  type: TYPE_NORMAL
- en: '`RCurl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bitops`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rjson`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`jsonlite`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`statmod`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tools`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After we have successfully installed the `h2o` package, we can proceed with
    loading the library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This command loads the library in R environment. The following messages are
    returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We follow the directions on the R prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The `h20.init` function initiates the `h2o` engine with a maximum memory size
    of 2 GB and two parallel cores. The console for `h2o` is initialized and we get
    the following messages once we run this script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Once `h2o` is initiated, the console can be viewed from any browser by pointing
    to `localhost:54321`. The `h2o` library runs on a JVM and the console allows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00075.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The console is intuitive and provides an interface to interact with the h[2]o
    engine. We can train and test models and run predictions on top of them. The first
    textbox, labeled CS, allows us to enter routines for execution. The `assist` command
    gives the list of the routines available. Let us continue to analyze the following
    sample code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The first command loads the `iris` dataset, which is contained in the datasets
    library, and saves it in a given dataframe. Then we use the `summary` function
    to produce result summaries of the results of the dataset. The function invokes
    particular methods which depend on the class of the first argument. The results
    are shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s analyze the next lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The `h2o.deeplearning` function is an important function within `h2o` and can
    be used for variety of operations. This function builds a DNN model using CPUs
    builds a feed-forward multilayer ANN on an `H2OFrame`. The `hidden` argument is
    used to set the number of hidden layers and the number of neurons for each hidden
    layer. In our case, we have set up a DNN with two hidden layers and `5` neurons
    for each hidden layer. Finally, the parameter `export_weights_and_biases` tells
    us that the weights and biases can be stored in `H2OFrame` and can be accessed
    like other dataframes for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before proceeding with the code analysis, a clarification should be made. The
    attentive reader can ask that on the basis of which evaluation we have chosen
    the number of hidden layers and the number of neurons for each hidden layer. Unfortunately,
    there is no precise rule or even a mathematical formula that allows us to determine
    which numbers are appropriate for that specific problem. This is because every
    problem is different from each other and each network approximates a system differently.
    So what makes the difference between one model and another? The answer is obvious
    and once again very clear: the researcher''s experience.'
  prefs: []
  type: TYPE_NORMAL
- en: The advice I can give, which stems from the vast experience in data analysis,
    is to try, try, and try again. The secret to experimental activity is just that.
    In the case of neural networks, this results in trying to set up different networks
    and then verifying their performance. For example, in our case, we could have
    started from a network with two hidden layers and *100* neurons per hidden layer,
    then progressively reduced those values, and then arrive at those that I proposed
    in the example. This procedure can be automated with the use of the iterative
    structures that R owns.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, some things can be said, for example, for the optimum choice of the
    number of neurons we need to know that:'
  prefs: []
  type: TYPE_NORMAL
- en: Small number of neurons will lead to high error for your system, as the predictive
    factors might be too complex for a small number of neurons to capture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large number of neurons will overfit to your training data and not generalize
    well
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of neurons in each hidden layer should be somewhere between the size
    of the input and the output layer, potentially the mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of neurons in each hidden layer shouldn't exceed twice the number
    of input neurons, as you are probably grossly overfit at this point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'That said, we return to the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'At the R prompt, this command prints a brief description of the features of
    the model we just created, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![](img/00076.jpeg)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'By carefully analyzing the previous figure, we can clearly distinguish the
    details of the model along with the confusion matrix. Let''s now look at how the
    training process went on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The `plot` method dispatches on the type of h2o model to select the correct
    scoring history. The arguments are restricted to what is available in the scoring
    history for a particular type of model, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00077.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'In this plot are shown the training classification errors versus epochs, as
    we can see that the gradient descents and the errors decrease as we progress in
    the epochs. How many times the dataset should be iterated (streamed) can be fractional.
    It defaults to `10`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The last six lines of the code simply print a short summary of the weights
    and biases for the three species of iris flower, as is shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We have restricted ourselves to seeing weights and biases only for the `setosa`
    species, for space reasons. In the following code we use the plot function again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This command plots the weights of the first hidden layer neurons versus sepal
    lengths, as is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00078.gif)'
  prefs: []
  type: TYPE_IMG
- en: Now, let us dedicate some time to the analysis of the results; in particular,
    we recover the confusion matrix that we just glimpsed in the model summary screen,
    shown earlier. To invoke the confusion matrix, we can use the `h2o.confusionMatrix`
    function as shown in the following code sample, which retrieves either single
    or multiple confusion matrices from the `h2o` objects.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'From the analysis of the confusion matrix, it can be seen that the model manages
    to correctly classify the three floral species by committing only four errors.
    These errors are fairly divided among only two species: `versicolor`, and `virginica`.
    However, the `setosa` species is correctly classified in all `50` occurrences.
    But why is this happening? To understand, let''s take a look at the starting data.
    In the case of multidimensional data, the best way to do this is to plot a scatterplot
    matrix of selected variables in a dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00079.gif)'
  prefs: []
  type: TYPE_IMG
- en: Let's analyze in detail the plot just proposed. The variables are written in
    a diagonal line from top left to bottom right. Then each variable is plotted against
    each other. For example, the second box in the first column is an individual scatterplot
    of `Sepal.Length` versus `Sepal.Width`, with `Sepal.Length` as the *X* axis and
    `Sepal.Width` as the *Y* axis. This same plot is replicated in the first plot
    of the second column. In essence, the boxes on the upper right hand side of the
    whole scatterplot are mirror images of the plots on the lower left hand.
  prefs: []
  type: TYPE_NORMAL
- en: From the analysis of the figure just seen, it can be seen that the `versicolor`
    and `virginica` species show overlapping boundaries. This makes us understand
    that the model's attempt to classify it when it falls into that area can cause
    errors. We can see what happens for the `setosa` species, which instead has far
    distant borders from other floral species without any classification error.
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, we evaluate the accuracy of the model in classifying floral species
    on the basis of the size of petals and sepals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The results show that the simulation, based on the first hypothesis, ranked
    the species with *97* percent accuracy. I would say that is a good result; the
    model fit the data very well. But how can we measure this feature? One method
    to find a better fit is to calculate the coefficient of determination (R-squared).
    To calculate R-squared in `h2o`, we can use the `h2o.r2` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Now let's understand what we've calculated and how to read the result. R-squared
    measures how well a model can predict the data, and falls between zero and one.
    The higher the value of coefficient of determination, the better the model is
    at predicting the data.
  prefs: []
  type: TYPE_NORMAL
- en: We got a value of *0.96*, so according to what we have said, this is a great
    result. To get a confirmation of this, we have to compare it with the result of
    another simulation model. So, we build a linear regression model based on the
    same data, that is the `iris` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build a linear regression model, we can use the `glm` function. This function
    is used to fit generalized linear models, specified by giving a symbolic description
    of the linear predictor and a description of the error distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we calculate the model''s coefficient of determination:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Now we can make a comparison between the model based on DNNs and the linear
    regression model. DNN had provided a R-squared value of *0.96*, while the resulting
    regression model provided a R-squared value of *0.87*. It is clear that DNN provides
    much better performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, it may be useful to analyze the parameters that are important for
    a neural network specialist, as shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Argument** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `x` | A vector containing the names or indices of the predictor variables
    to use in building a model. If `x` is missing, then all columns except `y` are
    used. |'
  prefs: []
  type: TYPE_TB
- en: '| `y` | The name of the response variable in a model. If the data does not
    contain a header, this is the first column index, increasing from left to right
    (the response must be either an integer or a categorical variable). |'
  prefs: []
  type: TYPE_TB
- en: '| `model_id` | This is the destination `id` for a model; it is autogenerated
    if not specified. |'
  prefs: []
  type: TYPE_TB
- en: '| `standardize` | It is a logical function. If enabled, it automatically standardizes
    the data. If disabled, the user must provide properly scaled input data. It defaults
    to `TRUE`. |'
  prefs: []
  type: TYPE_TB
- en: '| `activation` | It is an activation function. It must be one of `Tanh`, `TanhWithDropout`,
    `Rectifier`, `RectifierWithDropout`, `Maxout`, or `MaxoutWithDropout`. It defaults
    to `Rectifier`. |'
  prefs: []
  type: TYPE_TB
- en: '| `hidden` | This argument specifies hidden layer sizes (for example, `[100,
    100]`). It defaults to `[200, 200]`. |'
  prefs: []
  type: TYPE_TB
- en: '| `epochs` | How many times the dataset should be iterated (streamed) can be
    fractional. It defaults to `10`. |'
  prefs: []
  type: TYPE_TB
- en: '| `adaptive_rate` | It is a logical argument specifying the Adaptive learning
    rate. It defaults to `TRUE`. |'
  prefs: []
  type: TYPE_TB
- en: '| `rho` | This describes the adaptive learning rate time decay factor (similar
    to prior updates). It defaults to `0.99`. |'
  prefs: []
  type: TYPE_TB
- en: '| `rate_annealing` | Learning rate annealing is given by `rate/(1 + rate_annealing
    * samples`). It defaults to `1e- 06`. |'
  prefs: []
  type: TYPE_TB
- en: '| `rate_decay` | This is the learning rate decay factor between layers (*N^(th)*
    layer: `rate * rate_decay ^ (n - 1`). It defaults to `1`. |'
  prefs: []
  type: TYPE_TB
- en: '| `input_dropout_ratio` | Input layer dropout ratio (can improve generalization,
    try `0.1` or `0.2`). It defaults to `0`. |'
  prefs: []
  type: TYPE_TB
- en: '| `hidden_dropout_ratios` | Hidden layer dropout ratios can improve generalization.
    Specify one value per hidden layer. It defaults to `0.5`. |'
  prefs: []
  type: TYPE_TB
- en: '| `l1` | L1 regularization can add stability and improve generalization, and
    it causes many weights to become `0`. It defaults to `0`. |'
  prefs: []
  type: TYPE_TB
- en: '| `l2` | L2 regularization can add stability and improve generalization, and
    it causes many weights to be small. It defaults to `0`. |'
  prefs: []
  type: TYPE_TB
- en: '| `initial_weights` | This is a list of the `H2OFrame` IDs to initialize the
    weight matrices of this model with. |'
  prefs: []
  type: TYPE_TB
- en: '| `initial_biases` | It is a list of the `H2OFrame` IDs to initialize the bias
    vectors of this model with. |'
  prefs: []
  type: TYPE_TB
- en: '| `loss` | The loss function must be one of `Automatic`, `CrossEntropy`, `Quadratic`,
    `Huber`, `Absolute`, or `Quantile`. It defaults to `Automatic`. |'
  prefs: []
  type: TYPE_TB
- en: '| `distribution` | The distribution function must be one of `AUTO`, `bernoulli`,
    `multinomial`, `gaussian`, `poisson`, `gamma`, `tweedie`, `laplace`, `quantile`,
    or `huber`. It defaults to `AUTO`. |'
  prefs: []
  type: TYPE_TB
- en: '| `score_training_samples` | It is the number of training set samples for scoring
    (0 for all). It defaults to `10000`. |'
  prefs: []
  type: TYPE_TB
- en: '| `score_validation_samples` | It is the number of validation set samples for
    scoring (0 for all). It defaults to `0`. |'
  prefs: []
  type: TYPE_TB
- en: '| `classification_stop` | The stopping criterion for the classification error
    fraction on training data (-1 to disable). It defaults to `0`. |'
  prefs: []
  type: TYPE_TB
- en: '| `regression_stop` | It is the stopping criterion for the regression error
    (MSE) on training data (`-1` to disable). It defaults to `1e-06`. |'
  prefs: []
  type: TYPE_TB
- en: '| `stopping_rounds` | Early stopping based on convergence of `stopping_metric`.
    Stop if simple moving average of length `k` of the `stopping_metric` does not
    improve for `k:=stopping_rounds` scoring events (0 to disable) It defaults to
    `5`. |'
  prefs: []
  type: TYPE_TB
- en: '| `max_runtime_secs` | It is maximum allowed runtime in seconds for model training.
    Use `0` to disable it. It defaults to `0`. |'
  prefs: []
  type: TYPE_TB
- en: '| `diagnostics` | It enables diagnostics for hidden layers. It defaults to
    `TRUE`. |'
  prefs: []
  type: TYPE_TB
- en: '| `fast_mode` | It enables fast mode (minor approximation in backpropagation).
    It defaults to `TRUE`. |'
  prefs: []
  type: TYPE_TB
- en: '| `replicate_training_data` | It replicates the entire training dataset on
    every node for faster training on small datasets. It defaults to `TRUE`. |'
  prefs: []
  type: TYPE_TB
- en: '| `single_node_mode` | It runs on a single node for fine-tuning of model parameters.
    It defaults to `FALSE`. |'
  prefs: []
  type: TYPE_TB
- en: '| `shuffle_training_data` | It enables shuffling of training data (recommended
    if training data is replicated and `train_samples_per_iteration` is close to `#nodes`
    `x` `#rows`, or if using `balance_classes`). It defaults to `FALSE`. |'
  prefs: []
  type: TYPE_TB
- en: '| `missing_values_handling` | Handling of missing values must be one of `MeanImputation`
    or `Skip`. It defaults to `MeanImputation`. |'
  prefs: []
  type: TYPE_TB
- en: '| `quiet_mode` | It enables quiet mode for less output to standard output.
    It defaults to `FALSE`. |'
  prefs: []
  type: TYPE_TB
- en: '| `verbose` | It prints scoring history to the console (metrics per tree for
    GBM, DRF, and XGBoost; metrics per epoch for deep learning. It defaults to `False`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `autoencoder` | Logical autoencoder defaults to `FALSE` |'
  prefs: []
  type: TYPE_TB
- en: '| `export_weights_and_biases` | Whether to export neural network weights and
    biases to `H2OFrame`. It defaults to `FALSE`. |'
  prefs: []
  type: TYPE_TB
- en: '| `mini_batch_size` | Mini-batch size (smaller leads to better fit, whereas
    larger can speed up and generalize better). It defaults to `1`. |'
  prefs: []
  type: TYPE_TB
- en: 'There are other functions related to R with `h2o` for deep learning. Some useful
    ones are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Function** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `predict.H2Omodel` | Returns an `H2OFrame` object with probabilities and
    default predictions. |'
  prefs: []
  type: TYPE_TB
- en: '| `h2o.deepwater` | Builds a deep learning model using multiple native GPU
    backends. Builds DNN on `H2OFrame` containing various data sources. |'
  prefs: []
  type: TYPE_TB
- en: '| `as.data.frame.H2OFrame` | Converts `H2OFrame` to a dataframe. |'
  prefs: []
  type: TYPE_TB
- en: '| `h2o.confusionMatrix` | Displays the confusion matrix for a classification
    model. |'
  prefs: []
  type: TYPE_TB
- en: '| `print.H2OFrame` | Prints `H2OFrame`. |'
  prefs: []
  type: TYPE_TB
- en: '| `h2o.saveModel` | Saves an `h2o` model object to disk. |'
  prefs: []
  type: TYPE_TB
- en: '| `h2o.importFile` | Imports a file into h2o. |'
  prefs: []
  type: TYPE_TB
- en: Deep autoencoders using H2O
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Autoencoders are unsupervised learning methods on neural networks. We''ll see
    more of this in [Chapter 7](part0123.html#3L9L60-263fb608a19f4bb5955f37a7741ba5c4),
    *Use Cases of Neural Networks – Advanced Topics*. `h2o` can be used to detect
    an anomaly by using deep autoencoders. To train such a model, the same function,
    `h2o.deeplearning()`, is used, with some changes in the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The `autoencoder=TRUE` sets the `deeplearning` method to use the autoencoder
    technique unsupervised learning method. We are using only the training data, without
    the test set and the labels. The fact that we need a deep `autoencoder` instead
    of a feed-forward network is specified by the `autoencoder` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: We can choose the number of hidden units to be present in different layers.
    If we choose an integer value, what we get is called a **naive autoencoder**.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning is a subject of importance right from image detection to speech
    recognition and AI-related activity. There are numerous products and packages
    in the market for deep learning. Some of these are `Keras`, `TensorFlow`, `h2o`,
    and many others.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we learned the basics of deep learning, many variations of
    DNNs, the most important deep learning algorithms, and the basic workflow for
    deep learning. We explored the different packages available in R to handle DNNs.
  prefs: []
  type: TYPE_NORMAL
- en: To understand how to build and train a DNN, we analyzed a practical example
    of DNN implementation with the `neuralnet` package. We learned how to normalize
    data across the various available techniques, to remove data units, allowing you
    to easily compare data from different locations. We saw how to split the data
    for the training and testing of the network. We learned to use the `neuralnet`
    function to build and train a multilayered neural network. So we understood how
    to use the trained network to make predictions and we learned to use the confusion
    matrix to evaluate model performance.
  prefs: []
  type: TYPE_NORMAL
- en: We saw some basics of the h2o package. Overall, The `h2o` package is a highly
    user-friendly package that can be used to train feed-forward networks or deep
    autoencoders. It supports distributed computations and provides a web interface.
    By including the `h2o` package like any other package in R, we can do all kinds
    of modeling and processing of DNNs. The power of h2o can be utilized by the various
    features available in the package.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will understand what a perceptron is and what are the
    applications that can be built using the basic perceptron. We will learn a simple
    perceptron implementation function in R environment. We will also learn how to
    train and model a MLP . We will discover the linear separable classifier.
  prefs: []
  type: TYPE_NORMAL
