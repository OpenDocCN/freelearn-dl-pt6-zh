["```py\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Activation\nfrom keras.layers.recurrent import SimpleRNN\nfrom keras.layers import LSTM\nimport numpy as np\nfin=open('alice.txt',encoding='utf-8-sig')\nlines=[]\nfor line in fin:\n  line = line.strip().lower()\n  if(len(line)==0):\n    continue\n  lines.append(line)\nfin.close()\ntext = \" \".join(lines)\n```", "```py\nimport re\ntext = text.lower()\ntext = re.sub('[^0-9a-zA-Z]+',' ',text)\n```", "```py\nfrom collections import Counter\ncounts = Counter()\ncounts.update(text.split())\nwords = sorted(counts, key=counts.get, reverse=True)\nnb_words = len(text.split())\nword2index = {word: i for i, word in enumerate(words)}\nindex2word = {i: word for i, word in enumerate(words)}\n```", "```py\nSEQLEN = 10\nSTEP = 1\ninput_words = []\nlabel_words = []\ntext2=text.split()\nfor i in range(0,nb_words-SEQLEN,STEP):\n     x=text2[i:(i+SEQLEN)]\n     y=text2[i+SEQLEN]\n     input_words.append(x)\n     label_words.append(y)\n```", "```py\ntotal_words = len(set(words))\nX = np.zeros((len(input_words), SEQLEN, total_words), dtype=np.bool)\ny = np.zeros((len(input_words), total_words), dtype=np.bool)\n```", "```py\n# Create encoded vectors for the input and output values\nfor i, input_word in enumerate(input_words):\n     for j, word in enumerate(input_word):\n         X[i, j, word2index[word]] = 1\n     y[i,word2index[label_words[i]]]=1\n```", "```py\nHIDDEN_SIZE = 128\nBATCH_SIZE = 32\nNUM_ITERATIONS = 100\nNUM_EPOCHS_PER_ITERATION = 1\nNUM_PREDS_PER_EPOCH = 100\n\nmodel = Sequential()\nmodel.add(LSTM(HIDDEN_SIZE,return_sequences=False,input_shape=(SEQLEN,total_words)))\nmodel.add(Dense(total_words, activation='softmax'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy')\nmodel.summary()\n```", "```py\nfor iteration in range(50):\n     print(\"=\" * 50)\n     print(\"Iteration #: %d\" % (iteration))\n     model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION, validation_split = 0.1)\n     test_idx = np.random.randint(int(len(input_words)*0.1)) * (-1)\n     test_words = input_words[test_idx]\n     print(\"Generating from seed: %s\" % (test_words))\n     for i in range(NUM_PREDS_PER_EPOCH): \n         Xtest = np.zeros((1, SEQLEN, total_words))\n         for i, ch in enumerate(test_words):\n             Xtest[0, i, word2index[ch]] = 1\n         pred = model.predict(Xtest, verbose=0)[0]\n         ypred = index2word[np.argmax(pred)]\n         print(ypred,end=' ')\n         test_words = test_words[1:] + [ypred]\n```", "```py\nimport numpy as np\nimport pandas as pd\nratings = pd.read_csv('..') # Path to the file containing required fields\n```", "```py\nratings = ratings[ratings['rating']>3]\nratings = ratings.sort_values(by='timestamp')\nratings.reset_index(inplace=True)\nratings = ratings.drop(['index'],axis=1)\n```", "```py\nuser_movie_count =ratings.groupby('User').agg({'Movies':'nunique'}).reset_index()\nuser_movie_count.columns = ['User','Movie_count']\nratings2 = ratings.merge(user_movie_count,on='User',how='inner')\nmovie_count = ratings2[ratings2['Movie_count']>5]\nmovie_count = movie_count.sort_values('timestamp')\nmovie_count.reset_index(inplace=True)\nmovie_count = movie_count.drop(['index'],axis=1)\n```", "```py\nratings = movie_count\nusers = ratings.User.unique()\narticles = ratings.Movies.unique()\nuserid2idx = {o:i for i,o in enumerate(users)}\narticlesid2idx = {o:i for i,o in enumerate(articles)}\nidx2userid = {i:o for i,o in enumerate(users)}\nidx2articlesid = {i:o for i,o in enumerate(articles)}\n\nratings['Movies2'] = ratings.Movies.apply(lambda x: articlesid2idx[x])\nratings['User2'] = ratings.User.apply(lambda x: userid2idx[x])\n```", "```py\nuser_list = movie_count['User2'].unique()\nhistorical5_watched = []\nmovie_to_predict = []\nfor i in range(len(user_list)):\n     total_user_movies = movie_count[movie_count['User2']==user_list[i]].copy()\n     total_user_movies.reset_index(inplace=True)\n     total_user_movies = total_user_movies.drop(['index'],axis=1)\n     for j in range(total_user_movies.shape[0]-6):\n         historical5_watched.append(total_user_movies.loc[j:(j+4),'Movies2'].tolist())                                                                          movie_to_predict.append(total_user_movies.loc[(j+5),'Movies2'].tolist())\n```", "```py\nmovie_to_predict2 = to_categorical(y, num_classes = max(y)+1)\ntrainX = np.array(historical5_watched[:40000])\ntestX = np.array(historical5_watched[40000:])\ntrainY = np.array(movie_to_predict2[:40000])\ntestY = np.array(movie_to_predict2[40000:])\n```", "```py\nsrc_vocab = ratings['Movies2'].nunique()\nn_units = 32\nsrc_timesteps = 5\ntar_vocab = len(set(y))\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Embedding\nfrom keras.layers import LSTM, RepeatVector, TimeDistributed, Dense, Bidirectional\n\nmodel = Sequential()\nmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps))\nmodel.add((LSTM(100)))\nmodel.add(Dense(1000,activation='relu'))\nmodel.add(Dense(max(y)+1,activation='softmax'))\nmodel.summary()\n```", "```py\nmodel.fit(trainX, trainY, epochs=5, batch_size=32, validation_data=(testX, testY), verbose = 1)\n```", "```py\npred = model.predict(testX)\n```", "```py\ncount = 0\nfor i in range(testX.shape[0]):\n    rank = np.argmax(np.argsort(pred[i])[::-1]==np.argmax(testY[i]))\n    if rank<12:\n        count+=1\ncount/testX.shape[0]\n# 0.104\n```", "```py\nhistorically_watched = []\nfor i in range(len(user_list)):\n     total_user_movies = movie_count[movie_count['User2']==user_list[i]].copy()\n     total_user_movies.reset_index(inplace=True)\n     total_user_movies = total_user_movies.drop(['index'],axis=1)\n     for j in range(total_user_movies.shape[0]-6):\n         historically_watched.append(total_user_movies.loc[0:(j+4),'Movies2'].tolist())\n```", "```py\nfor j in range(pred.shape[0]):\n  for i in range(pred.shape[1]):\n    pred[j][i]= np.where(i in historically_watched[j], 0 , pred[j][i])\n```", "```py\ncount = 0\nfor i in range(testX.shape[0]):\n  rank = np.argmax(np.argsort(pred[i])[::-1]==np.argmax(testY[i]))\n  if rank<12:\n    count+=1\ncount/testX.shape[0]\n#12.6\n```", "```py\nuser_list = movie_count['User2'].unique()\nuser_movies = []\nfor i in range(len(user_list)):\n     total_user_movies = movie_count[movie_count['User2']==user_list[i]].copy()\n     total_user_movies.reset_index(inplace=True)\n     total_user_movies = total_user_movies.drop(['index'],axis=1)\n     total_user_movies['Movies3'] = total_user_movies['Movies2'].astype(str)\n     user_movies.append(total_user_movies['Movies3'].tolist())\n```", "```py\nfrom gensim.models import Word2Vec\nw2v_model = Word2Vec(user_movies,size=100,window=5,min_count=5, iter = 500)\n```", "```py\nfrom sklearn.manifold import TSNE\ntsne_model = TSNE(n_components=2, verbose=1, random_state=0)\ntsne_img_label = tsne_model.fit_transform(w2v_model.wv.syn0)\ntsne_df = pd.DataFrame(tsne_img_label, columns=['x', 'y'])\ntsne_df['image_label'] = list(w2v_model.wv.vocab.keys())\n\nfrom ggplot import *\nchart = ggplot(tsne_df, aes(x='x', y='y'))+geom_point(size=70,alpha=0.5)\nchart\n```", "```py\nidx2movie = pd.DataFrame([idx2moviesid.keys(), idx2moviesid.values()]).T\nidx2movie.columns = ['image_label','movieId']\n```", "```py\ntsne_df['image_label'] = tsne_df['image_label'].astype(int)\ntsne_df2 = pd.merge(tsne_df, idx2movie, on='image_label', how='right')\n```", "```py\nmovies = pd.read_csv('...') # Path to movies dataset\n```", "```py\ntsne_df3 = pd.merge(tsne_df2, movies, left_on='movieId', right_on = 0, how='inner')\ntsne_df4 = tsne_df3.drop([2,3,4],axis=1)\ntsne_df4.rename(columns={1:'movie_name'}, inplace=True)\n```", "```py\ntsne_df5 = tsne_df4.loc[~np.isnan(tsne_df4['x']),]\n```", "```py\nX = tsne_df5.loc[:,['x','y']]\ninertia = []\nfor i in range(10):\n      km = KMeans((i+1)*10)\n      km.fit(X)\n      inertia.append(km.inertia_)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.plot((np.arange(10)+1)*10,inertia)\nplt.title('Variation of inertia over different number of clusters')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\n```", "```py\nkm = KMeans(40)\nkm.fit(X)\ntsne_df5['clusterlabel'] = km.labelstsne_df5[tsne_df5['cluster_label']==0].head()\n```", "```py\nfor j in range(pred.shape[0]):\n     for i in range(pred.shape[1]):\n         pred[j][i]= np.where(i in historically_watched[j], 0 , pred[j][i])\n```", "```py\nmovie_cluster_id = tsne_df5[['image_label','cluster_label']]\ncount = 0\nfor j in range(pred.shape[0]):\n      t = movie_cluster_id.copy()\n      t['pred']=pred[j,list(movie_cluster_id['image_label'])]\n      t2= t.sort_values(by='pred',ascending=False).groupby('cluster_label').first().reset_index()\n      t3 = t2.sort_values(by='pred',ascending=False).reset_index()\n      final_top_preds = t3.loc[:11]['image_label'].values\n      if (np.argmax(testY[j]) in final_top_preds):\n            count+=1\n\n```", "```py\nimport pandas as pd\ndata2 = pd.read_csv('/content/stock_data.csv')\n```", "```py\nx= []\ny = []\nfor i in range(data2.shape[0]-5):\n     x.append(data2.loc[i:(i+4)]['Close'].values)\n     y.append(data2.loc[i+5]['Close'])\n\nimport numpy as np\nx = np.array(x)\ny = np.array(y)\n```", "```py\nx = x.reshape(x.shape[0],x.shape[1],1)\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.30,random_state=10)\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(100, input_shape = (5,1), activation = 'relu'))\nmodel.add((LSTM(100)))\nmodel.add(Dense(1000,activation='relu'))\nmodel.add(Dense(1,activation='linear'))\nmodel.summary()\n```", "```py\nfrom keras.optimizers import Adam\nadam = Adam(lr=0.0001)\nmodel.compile(optimizer=adam, loss='mean_squared_error')\n```", "```py\nmodel.fit(X_train, y_train, epochs=400, batch_size=64, validation_data=(X_test, y_test), verbose = 1)\n```", "```py\npred = model.predict(X_test)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.figure(figsize=(20,10))\nplt.plot(y_test,'r')\nplt.plot(pred,'--')\n\nplt.title('Variation of actual and predicted stock price')\nplt.ylabel('Stock price')\n```", "```py\nX_train = x[:2100,:,:]\ny_train = y[:2100]\nX_test = x[2100:,:,:]\ny_test = y[2100:]\n```", "```py\nplt.plot(data2['Close'])\n```", "```py\nweights = np.arange(X_train.shape[0]).reshape((X_train.shape[0]),1)/2100\n```", "```py\nimport numpy as np\nfrom keras.layers import Dense, Input\nfrom keras import Model\nimport keras.backend as K\nfrom functools import partial\n\ndef custom_loss(y_true, y_pred, weights):\n     return K.square(K.abs(y_true - y_pred) * weights)\ncl = partial(custom_loss, weights=weights_tensor)\n```", "```py\ninput_layer = Input(shape=(5,1))\nweights_tensor = Input(shape=(1,))\n\ni1 = Dense(100, activation='relu')(input_layer)\ni2 = LSTM(100)(i1)\ni3 = Dense(1000, activation='relu')(i2)\nout = Dense(1, activation='linear')(i3)\nmodel = Model([input_layer, weights_tensor], out)\n```", "```py\nfrom keras.optimizers import Adam\nadam = Adam(lr=0.0001)\nmodel = Model([input_layer, weights_tensor], out)\nmodel.compile(adam, cl)\n```", "```py\nmodel.fit(x=[X_train, weights], y=y_train, epochs=300,batch_size = 32, validation_data = ([X_test, test_weights], y_test))\n```", "```py\nfrom bs4 import BeautifulSoup\nfrom bs4 import BeautifulSoup\nimport urllib, json\n\ndates = []\ntitles = []\nfor i in range(100):\n try:\n        url = 'https://content.guardianapis.com/search?from-date=2010-01-01&section=business&page-          size=200&order-by=newest&page='+str(i+1)+'&q=amazon&api-key=0d7'\n        response = urllib.request.urlopen(url)\n        encoding = response.info().get_content_charset('utf8')\n        data = json.loads(response.read().decode(encoding))\n        print(i)\n        for j in range(len(data['response']['results'])):\n              dates.append(data['response']['results'][j]['webPublicationDate'])\n              titles.append(data['response']['results'][j]['webTitle']) \n except:\n       break\n\nimport pandas as pd\ndata = pd.DataFrame(dates, titles)\ndata = data.reset_index()\ndata.columns = ['title','date']\ndata['date']=data['date'].str[:10]\ndata['date']=pd.to_datetime(data['date'], format = '%Y-%m-%d')\ndata = data.sort_values(by='date')\ndata_final = data.groupby('date').first().reset_index()\n```", "```py\ndata2['Date'] = pd.to_datetime(data2['Date'],format='%Y-%m-%d')\ndata3 = pd.merge(data2,data_final, left_on = 'Date', right_on = 'date', how='left')\n```", "```py\nimport nltk\nimport re\nnltk.download('stopwords')\nstop = nltk.corpus.stopwords.words('english')\ndef preprocess(text):\n     text = str(text)\n     text=text.lower()\n     text=re.sub('[^0-9a-zA-Z]+',' ',text)\n     words = text.split()\n     words2=[w for w in words if (w not in stop)]\n     words3=' '.join(words2)\n     return(words3)\ndata3['title'] = data3['title'].apply(preprocess)\ndata3['title']=np.where(data3['title'].isnull(),'-','-'+data3['title'])\ndocs = data3['title'].values\nfrom collections import Counter\ncounts = Counter()\nfor i,review in enumerate(docs):\n     counts.update(review.split())\nwords = sorted(counts, key=counts.get, reverse=True)\nvocab_size=len(words)\nword_to_int = {word: i for i, word in enumerate(words, 1)}\nencoded_docs = []\nfor doc in docs:\n     encoded_docs.append([word_to_int[word] for word in doc.split()])\nmax_length = 20\nfrom keras.preprocessing.sequence import pad_sequences\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length,padding='pre')\n```", "```py\nx1 = []\nx2 = []\ny = []\nfor i in range(data3.shape[0]-5):\n     x1.append(data3.loc[i:(i+4)]['Close'].values)\n     x2.append(padded_docs[i+5])\n     y.append(data3.loc[i+5]['Close'])\n\nx1 = np.array(x1)\nx2 = np.array(x2)\ny = np.array(y)\nx1 = x1.reshape(x1.shape[0],x1.shape[1],1)\nX1_train = x1[:2100,:,:]\nX2_train = x2[:2100,:]\ny_train = y[:2100]\n\nX1_test = x1[2100:,:,:]\nX2_test = x2[2100:,:]\ny_test = y[2100:]\n```", "```py\ninput1 = Input(shape=(20,))\nmodel = Embedding(input_dim=vocab_size+1, output_dim=32, input_length=20)(input1)\nmodel = (LSTM(units=100))(model)\nmodel = (Dense(1, activation='tanh'))(model)\n\ninput2 = Input(shape=(5,1))\nmodel2 = Dense(100, activation='relu')(input2)\nmodel2 = LSTM(units=100)(model2)\nmodel2 = (Dense(1000, activation=\"relu\"))(model2)\nmodel2 = (Dense(1, activation=\"linear\"))(model2)\n\nfrom keras.layers import multiply\nconc = multiply([model, model2])\nconc2 = (Dense(1000, activation=\"relu\"))(conc)\nout = (Dense(1, activation=\"linear\"))(conc2)\n```", "```py\nmodel = Model([input1, input2, weights_tensor], out)\n```", "```py\ndef custom_loss(y_true, y_pred, weights):\n return K.square(K.abs(y_true - y_pred) * weights)\ncl = partial(custom_loss, weights=weights_tensor)\n\nmodel = Model([input1, input2, weights_tensor], out)\nmodel.compile(adam, cl)\n```", "```py\nmodel.fit(x=[X2_train, X1_train, weights], y=y_train, epochs=300,batch_size = 32, validation_data = ([X2_test, X1_test, test_weights], y_test))\n```", "```py\npred = model.predict([X2_test, X1_test, test_weights])\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.figure(figsize=(20,10))\nplt.plot(y_test,'r',label='actual')\nplt.plot(pred,'--', label = 'predicted')\nplt.title('Variation of actual and predicted stock price')\nplt.ylabel('Stock price')\nplt.legend()\n```"]