- en: Chapter 7. Other Important Deep Learning Libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll talk about other deep learning libraries, especially
    libraries with programming languages other than Java. The following are the most
    famous, well-developed libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: Theano
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caffe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You'll briefly learn about each of them. Since we'll mainly implement them using
    Python here, you can skip this chapter if you are not a Python developer. All
    the libraries introduced in this chapter support GPU implementations and have
    other special features, so let's dig into them.
  prefs: []
  type: TYPE_NORMAL
- en: Theano
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Theano was developed for deep learning, but it is not actually a deep learning
    library; it is a Python library for scientific computing. The documentation is
    available at [http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/).
    There are several characteristics introduced on the page such as the use of a
    GPU, but the most striking feature is that Theano supports **computational differentiation**
    or **automatic differentiation**, which ND4J, the Java scientific computing library,
    doesn't support. This means that, with Theano, we don't have to calculate the
    gradients of model parameters by ourselves. Theano automatically does this instead.
    Since Theano undertakes the most complicated parts of the algorithm, implementations
    of math expressions can be less difficult.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how Theano computes gradients. To begin with, we need to install
    Theano on the machine. Installation can be done just by using `pip install Theano`
    or `easy_install Theano`. Then, the following are the lines to import and use
    Theano:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: With Theano, all variables are processed as tensors. For example, we have `scalar`,
    `vector`, and `matrix`, `d` for double, `l` for long, and so on. Generic functions
    such as `sin`, `cos`, `log`, and `exp` are also defined under `theano.tensor`.
    Therefore, as shown previously, we often use the alias of tensor, `T`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step to briefly grasp Theano implementations, consider the very
    simple parabola curve. The implementation is saved in `DLWJ/src/resources/theano/1_1_parabola_scalar.py`
    so that you can reference it. First, we define `x` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This definition is unique with Python because `x` doesn''t have a value; it''s
    just a symbol. In this case, `x` is `scalar` of the type `d` (double). Then we
    can define `y` and its gradient very intuitively. The implementation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'So, `dy` should have `2x` within it. Let''s check whether we can get the correct
    answers. What we need to do additionally is to register the `math` function with
    Theano:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you can easily compute the value of the gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Very simple! This is the power of Theano. We have `x` of scalar here, but you
    can easily implement vector (and matrix) calculations as well just by defining
    `x` as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We won't go further here, but you can find the completed codes in `DLWJ/src/resources/theano/1_2_parabola_vector.py`
    and `DLWJ/src/resources/theano/1_3_parabola_matrix.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we consider implementing deep learning algorithms with Theano, we can
    find some very good examples on GitHub in *Deep Learning Tutorials* ([https://github.com/lisa-lab/DeepLearningTutorials](https://github.com/lisa-lab/DeepLearningTutorials)).
    In this chapter, we''ll look at an overview of the standard MLP implementation
    so you understand more about Theano. The forked repository as a snapshot is available
    at [https://github.com/yusugomori/DeepLearningTutorials](https://github.com/yusugomori/DeepLearningTutorials).
    First, let''s take a look at `mlp.py`. The model parameters of the hidden layer
    are the weight and bias:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Both parameters are defined using `theano.shared` so that they can be accessed
    and updated through the model. The activation can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Theano](img/B04779_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This denotes the activation function, that is, the hyperbolic tangent in this
    code. Therefore, the corresponding code is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, linear activation is also supported. Likewise, parameters `W` and `b`
    of the output layer, that is, logistic regression layer, are defined and initialized
    in `logistic_sgd.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The activation function of multi-class logistic regression is the `softmax`
    function and we can just write and define the output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can write the predicted values as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In terms of training, since the equations of the backpropagation algorithm
    are computed from the loss function and its gradient, what we need to do is just
    define the function to be minimized, that is, the negative log likelihood function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here, the mean values, not the sum, are computed to evaluate across the mini-batch.
  prefs: []
  type: TYPE_NORMAL
- en: 'With these preceding values and definitions, we can implement MLP. Here again,
    what we need to do is define the equations and symbols of MLP. The following is
    an extraction of the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you can build and train the model. Let''s look at the code in `test_mlp()`.
    Once you load the dataset and construct MLP, you can evaluate the model by defining
    the cost:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'With this cost, we get the gradients of the model parameters with just a single
    line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the equation to update the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The code in the first bracket follows this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Theano](img/B04779_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, finally, we define the actual function for the training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Each indexed input and label corresponds to `x`, `y` in *givens*, so when `index`
    is given, the parameters are updated with `updates`. Therefore, we can train the
    model with iterations of training epochs and mini-batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The original code has the test and validation part, but what we just mentioned
    is the rudimentary structure. With Theano, equations of gradients will no longer
    be derived.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TensorFlow is the library for machine learning and deep learning developed
    by Google. The project page is [https://www.tensorflow.org/](https://www.tensorflow.org/)
    and all the code is open to the public on GitHub at [https://github.com/tensorflow/tensorflow](https://github.com/tensorflow/tensorflow).
    TensorFlow itself is written with C++, but it provides a Python and C++ API. We
    focus on Python implementations in this book. The installation can be done with
    `pip`, `virtualenv`, or `docker`. The installation guide is available at [https://www.tensorflow.org/versions/master/get_started/os_setup.html](https://www.tensorflow.org/versions/master/get_started/os_setup.html).
    After the installation, you can import and use TensorFlow by writing the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'TensorFlow recommends you implement deep learning code with the following three
    parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '`inference()`: This makes predictions using the given data, which defines the
    model structure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loss()`: This returns the error values to be optimized'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training()`: This applies the actual training algorithms by computing gradients'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll follow this guideline. A tutorial on MNIST classifications for beginners
    is introduced on [https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html](https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html)
    and the code for this tutorial can be found in `DLWJ/src/resources/tensorflow/1_1_mnist_simple.py`.
    Here, we consider refining the code introduced in the tutorial. You can see all
    the code in `DLWJ/src/resources/tensorflow/1_2_mnist.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, what we have to consider is fetching the MNIST data. Thankfully, TensorFlow
    also provides the code to fetch the data in [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/input_data.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/input_data.py)
    and we put the code into the same directory. Then, by writing the following code,
    you can import the MNIST data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'MNIST data can be imported using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to Theano, we define the variable with no actual values as the placeholder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `784` is the number of units in the input layer and `10` is the number
    in the output layer. We do this because the values in the placeholder change in
    accordance with the mini-batches. Once you define the placeholder you can move
    on to the model building and training. We set the non-linear activation with the
    `softmax` function in `inference()` here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `W` and `b` are the parameters of the model. The `loss` function, that
    is, the `cross_entropy` function, is defined in `loss()` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'With the definition of `inference()` and `loss()`, we can train the model by
    writing the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`GradientDescentOptimizer()` applies the gradient descent algorithm. But be
    careful, as this method just defines the method of training and the actual training
    has not yet been executed. TensorFlow also supports `AdagradOptimizer()`, `MemontumOptimizer()`,
    and other major optimizing algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code and methods explained previously are to define the model. To execute
    the actual training, you need to initialize a session of TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we train the model with mini-batches. All the data in a mini-batch is
    stored in `feed_dict` and then used in `sess.run()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s it for the model training. It''s very simple, isn''t it? You can show
    the result by writing the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '`TensorFlow` makes it super easy to implement deep learning and it is very
    useful. Furthermore, `TensorFlow` has another powerful feature, `TensorBoard`,
    to visualize deep learning. By adding a few lines of code to the previous code
    snippet, we can use this useful feature.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how the model is visualized first. The code is in `DLWJ/src/resources/tensorflow/1_3_mnist_TensorBoard.py`,
    so simply run it. After you run the program, type the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `<ABSOLUTE_PATH>` is the absolute path of the program. Then, if you access
    `http://localhost:6006/` in your browser, you can see the following page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![TensorFlow](img/B04779_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This shows the process of the value of `cross_entropy`. Also, when you click
    **GRAPH** in the header menu, you see the visualization of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![TensorFlow](img/B04779_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'When you click on **inference** on the page, you can see the model structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![TensorFlow](img/B04779_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s look inside the code. To enable visualization, you need to wrap
    the whole area with the scope: *with* `tf.Graph().as_default()`. By adding this
    scope, all the variables declared in the scope will be displayed in the graph.
    The displayed name can be set by including the `name` label as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Defining other scopes will create nodes in the graph and this is where the
    division, `inference()`, `loss()`, and `training()` reveal their real values.
    You can define the respective scope without losing any readability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '`tf.scalar_summary()` in `loss()` makes the variable show up in the **EVENTS**
    menu. To enable visualization, we need the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Then the process of variables can be added with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This feature of visualization will be much more useful when we're using more
    complicated models.
  prefs: []
  type: TYPE_NORMAL
- en: Caffe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Caffe is a library famous for its speed. The official project page is [http://caffe.berkeleyvision.org/](http://caffe.berkeleyvision.org/)
    and the GitHub page is [https://github.com/BVLC/caffe](https://github.com/BVLC/caffe).
    Similar to TensorFlow, Caffe has been developed mainly with C++, but it provides
    a Python and MATLAB API. In addition, what is unique to Caffe is that you don't
    need any programming experience, you just write the configuration or protocol
    files, that is `.prototxt` files, to perform experiments and research with deep
    learning. Here, we focus on the protocol-based approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Caffe is a very powerful library that enables quick model building, training,
    and testing; however, it''s a bit difficult to install the library to get a lot
    of benefits from it. As you can see from the installation guide at [http://caffe.berkeleyvision.org/installation.html](http://caffe.berkeleyvision.org/installation.html),
    you need to install the following in advance:'
  prefs: []
  type: TYPE_NORMAL
- en: CUDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BLAS (ATLAS, MKL, or OpenBLAS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenCV
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Others: snappy, leveldb, gflags, glog, szip, lmdb, protobuf, and hdf5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, clone the repository from the GitHub page and create the `Makefile.config`
    file from `Makefile.config.example`. You may need Anaconda, a Python distribution,
    beforehand to run the `make` command. You can download this from [https://www.continuum.io/downloads](https://www.continuum.io/downloads).
    After you run the `make`, `make test`, and `make runtest` commands (you may want
    to run the commands with a `-jN` option such as `make -j4` or `make -j8` to speed
    up the process) and pass the test, you''ll see the power of Caffe. So, let''s
    look at an example. Go to `$CAFFE_ROOT`, the path where you cloned the repository,
    and type the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s all you need to solve the standard MNIST classification problem with
    CNN. So, what happened here? When you have a look at `train_lenet.sh`, you will
    see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'It simply runs the `caffe` command with the protocol file `lenet_solver.prototxt`.
    This file configures the hyper parameters of the model such as the learning rate
    and the momentum. The file also references the network configuration file, in
    this case, `lenet_train_test.prototxt`. You can define each layer with a JSON-like
    description:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'So, basically, the protocol file is divided into two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Net**: This defines the detailed structure of the model and gives a description
    of each layer, hence whole neural networks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Solver**: This defines the optimization settings such as the use of a CPU/GPU,
    the number of iterations, and the hyper parameters of the model such as the learning
    rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caffe can be a great tool when you need to apply deep learning to a large dataset
    with principal approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to implement deep learning algorithms and models
    using Theano, TensorFlow, and Caffe. All of them have special and powerful features
    and each of them is very useful. If you are interested in other libraries and
    frameworks, you can have *Chainer* ([http://chainer.org/](http://chainer.org/)),
    *Torch* ([http://torch.ch/](http://torch.ch/)), *Pylearn2* ([http://deeplearning.net/software/pylearn2/](http://deeplearning.net/software/pylearn2/)),
    *Nervana* ([http://neon.nervanasys.com/](http://neon.nervanasys.com/)), and so
    on. You can also reference some benchmark tests ([https://github.com/soumith/convnet-benchmarks](https://github.com/soumith/convnet-benchmarks)
    and [https://github.com/soumith/convnet-benchmarks/issues/66](https://github.com/soumith/convnet-benchmarks/issues/66))
    when you actually consider building your application with one of the libraries
    mentioned earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, you learned the fundamental theories and algorithms of
    machine learning and deep learning and how deep learning is applied to study/business
    fields. With the knowledge and techniques you've acquired here, you should be
    able to cope with any problems that confront you. While it is true that you still
    need more steps to realize AI, you now have the greatest opportunity to achieve
    innovation.
  prefs: []
  type: TYPE_NORMAL
