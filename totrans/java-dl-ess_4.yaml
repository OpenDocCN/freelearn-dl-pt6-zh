- en: Chapter 4. Dropout and Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we continue to look through the algorithms of deep learning.
    The pre-training that was taken into both DBN and SDA is indeed an innovative
    method, but deep learning also has other innovative methods. Among these methods,
    we''ll go into the details of the particularly eminent algorithms, which are the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: The dropout learning algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both algorithms are necessary to understand and master deep learning, so make
    sure you keep up.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning algorithms without pre-training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned that layer-wise training with pre-training
    was a breakthrough for DBN and SDA. The reason why these algorithms need pre-training
    is because an issue occurs where an output error gradually vanishes and doesn't
    work well in neural networks with simple piled-up layers (we call this the vanishing
    gradient problem). The deep learning algorithm needs pre-training whether you
    want to improve the existing method or reinvent it—you might think of it like
    that.
  prefs: []
  type: TYPE_NORMAL
- en: However, actually, the deep learning algorithms in this chapter don't have a
    phase of pre-training, albeit in the deep learning algorithm without pre-training,
    we can get a result with higher precision and accuracy. Why is such a thing possible?
    Here is a brief reason. Let's think about why the vanishing gradient problem occurs—remember
    the equation of backpropagation? A delta in a layer is distributed to all the
    units of a previous layer by literally propagating networks backward. This means
    that in the network where all units are tied densely, the value of an error backpropagated
    to each unit becomes small. As you can see from the equations of backpropagation,
    the gradients of the weight are obtained by the multiplication of the weights
    and deltas among the units. Hence, the more terms we have, the more dense the
    networks are and the more possibilities we have for underflow. This causes the
    vanishing gradient problem.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we can say that if the preceding problems can be avoided without
    pre-training, a machine can learn properly with deep neural networks. To achieve
    this, we need to arrange how to connect the networks. The deep learning algorithm
    in this chapter is a method that puts this contrivance into practice using various
    approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If there''s a problem with the network being tied densely, just force it to
    be sparse. Then the vanishing gradient problem won''t occur and learning can be
    done properly. The algorithm based on such an idea is the **dropout** algorithm.
    Dropout for deep neural networks was introduced in *Improving neural networks
    by preventing co adaptation of feature detectors* (Hinton, et. al. 2012, [http://arxiv.org/pdf/1207.0580.pdf](http://arxiv.org/pdf/1207.0580.pdf))
    and refined in *Dropout: A Simple Way to Prevent Neural Networks from Overfitting*
    (Srivastava, et. al. 2014, [https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)).
    In dropout, some of the units are, literally, forcibly dropped while training.
    What does this mean? Let''s look at the following figures—firstly, neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dropout](img/B04779_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'There is nothing special about this figure. It is a standard neural network
    with one input layer, two hidden layers, and one output layer. Secondly, the graphical
    model can be represented as follows by applying dropout to this network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dropout](img/B04779_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Units that are dropped from the network are depicted with cross signs. As you
    can see in the preceding figure, dropped units are interpreted as non-existent
    in the network. This means we need to change the structure of the original neural
    network while the dropout learning algorithm is being applied. Thankfully, applying
    dropout to the network is not difficult from a computational standpoint. You can
    simply build a general deep neural network first. Then the dropout learning algorithm
    can be applied just by adding a dropout mask—a simple binary mask—to all the units
    in each layer. Units with the value of 0 in the binary mask are the ones that
    are dropped from the network.
  prefs: []
  type: TYPE_NORMAL
- en: This may remind you of DA (or SDA) discussed in the previous chapter because
    DA and dropout look similar at first glance. Corrupting input data in DA also
    adds binary masks to the data when implemented. However, there are two remarkably
    different points between them. First, while it is true that both methods have
    the process of adding masks to neurons, DA applies the mask only to units in the
    input layer, whereas dropout applies it to units in the hidden layer. Some of
    the dropout algorithms apply masks to both the input layer and the hidden layer,
    but this is still different from DA. Second, in DA, once the corrupt input data
    is generated, the data will be used throughout the whole training epochs, but
    in dropout, the data with different masks will be used in each training epoch.
    This indicates that a neural network of a different shape is trained in each iteration.
    Dropout masks will be generated in each layer in each iteration according to the
    probability of dropout.
  prefs: []
  type: TYPE_NORMAL
- en: You might have a question—can we train the model even if the shape of the network
    is different in every step? The answer is yes. You can think of it this way—the
    network is well trained with dropout because it puts more weights on the existing
    neurons to reflect the characteristics of the input data. However, dropout has
    a single demerit, that is, it requires more training epochs than other algorithms
    to train and optimize the model, which means it takes more time until it is optimized.
    Another technique is introduced here to reduce this problem. Although the dropout
    algorithm itself was invented earlier, it was not enough for deep neural networks
    to gain the ability to generalize and get high precision rates just by using this
    method. With one more technique that makes the network even more sparse, we achieve
    deep neural networks to get higher accuracy. This technique is the improvement
    of the activation function, which we can say is a simple yet elegant solution.
  prefs: []
  type: TYPE_NORMAL
- en: All of the methods of neural networks explained so far utilize the sigmoid function
    or hyperbolic tangent as an activation function. You might get great results with
    these functions. However, as you can see from the shape of them, these curves
    saturate and kill the gradients when the input values or error values at a certain
    layer are relatively large or small.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the activation functions introduced to solve this problem is the **rectifier**.
    A unit-applied rectifier is called a **Rectified Linear Unit** (**ReLU**). We
    can call the activation function itself ReLU. This function is described in the
    following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dropout](img/B04779_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The function can be represented by the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dropout](img/B04779_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The broken line in the figure is the function called a **softplus function**,
    the derivative of it is logistic function, which can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dropout](img/B04779_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is just for your information: we have the following relations that a smooth
    approximation to the rectifier. As you can see from the figure above, since the
    rectifier is far simpler than the sigmoid function and hyperbolic tangent, you
    can easily guess that the time cost will reduce when it is applied to the deep
    learning algorithm. In addition, because the derivative of the rectifier—which
    is necessary when calculating backpropagation errors—is also simple, we can, additionally,
    shorten the time cost. The equation of the derivative can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dropout](img/B04779_04_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Since both the rectifier and the derivative of it are very sparse, we can easily
    imagine that the neural networks will be also sparse through training. You may
    have also noticed that we no longer have to worry about gradient saturations because
    we don't have the causal curves that the sigmoid function and hyperbolic tangent
    contain anymore.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the technique of dropout and the rectifier, a simple deep neural network
    can learn a problem without pre-training. In terms of the equations used to implement
    the dropout algorithm, they are not difficult because they are just simple methods
    of adding dropout masks to multi-layer perceptrons. Let''s look at them in order:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dropout](img/B04779_04_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Dropout](img/B04779_04_19.jpg) denotes the activation function, which
    is, in this case, the rectifier. You see, the previous equation is for units in
    the hidden layer without dropout. What the dropout does is just apply the mask
    to them. It can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dropout](img/B04779_04_20.jpg)![Dropout](img/B04779_04_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Dropout](img/B04779_04_22.jpg) denotes the probability of dropout,
    which is generally set to 0.5\. That''s all for forward activation. As you can
    see from the equations, the term of the binary mask is the only difference from
    the ones of general neural networks. In addition, during backpropagation, we also
    have to add masks to the delta. Suppose we have the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dropout](img/B04779_04_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'With this, we can define the delta as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dropout](img/B04779_04_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Dropout](img/B04779_04_25.jpg) denotes the evaluation function (these
    equations are the same as we mentioned in [Chapter 2](ch02.html "Chapter 2. Algorithms
    for Machine Learning – Preparing for Deep Learning"), *Algorithms for Machine
    Learning – Preparing for Deep Learning*). We get the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dropout](img/B04779_04_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the delta can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dropout](img/B04779_04_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we have all the equations necessary for implementation, let''s dive into
    the implementation. The package structure is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dropout](img/B04779_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'First, what we need to have is the rectifier. Like other activation functions,
    we implement it in `ActivationFunction.java` as `ReLU`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, we define `dReLU` as the derivative of the rectifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Accordingly, we updated the constructor of `HiddenLayer.java` to support `ReLU`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s have a look at `Dropout.java`. In the source code, we''ll build
    the neural networks of two hidden layers, and the probability of dropout is set
    to 0.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The constructor of `Dropout.java` can be written as follows (since the network
    is just a simple deep neural network, the code is also simple):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As explained, now we have the `HiddenLayer` class with `ReLU` support, we can
    use `ReLU` as the activation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once a model is built, what we do next is train the model with dropout. The
    method for training is simply called `train`. Since we need some layer inputs
    when calculating the backpropagation errors, we define the variable called `layerInputs`
    first to cache their respective input values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `X` is the original training data. We also need to cache the dropout
    masks for each layer for backpropagation, so let''s define it as `dropoutMasks`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Training begins in a forward activation fashion. Look how we apply the dropout
    masks to the value; we merely multiply the activated values and binary masks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The dropout method is defined in `Dropout.java` as well. As explained in the
    equation, this method returns the values following the Bernoulli distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'After forward propagation through the hidden layers, training data is forward
    propagated in the output layer of the logistic regression. Then, in the same way
    as the other neural networks algorithm, the deltas of each layer are going back
    through the network. Here, we apply the cached masks to the delta so that its
    values are backpropagated in the same network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'After the training comes the test phase. But before we apply the test data
    to the tuned model, we need to configure the weights of the network. Dropout masks
    can''t be simply applied to the test data because when masked, the shape of each
    network will be differentiated, and this may return different results because
    a certain unit may have a significant effect on certain features. Instead, what
    we do is smooth the weights of the network, which means we simulate the network
    where whole units are equally masked. This can be done using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dropout](img/B04779_04_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see from the equation, all the weights are multiplied by the probability
    of non-dropout. We define the method for this as `pretest`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We have to call this method once before the test. Since the network is a general
    multi-layered neural network, what we need to do for the prediction is just perform
    forward activation through the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Compared to DBN and SDA, the dropout MLP is far simpler and easier to implement.
    It suggests the possibility that with a mixture of two or more techniques, we
    can get higher precision.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the machine learning/deep learning algorithms you have learned about imply
    that the type of input data is one-dimensional. When you look at a real-world
    application, however, data is not necessarily one-dimensional. A typical case
    is an image. Though we can still convert two-dimensional (or higher-dimensional)
    data into a one-dimensional array from the standpoint of implementation, it would
    be better to build a model that can handle two-dimensional data as it is. Otherwise,
    some information embedded in the data, such as positional relationships, might
    be lost when flattened to one dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve this problem, an algorithm called **Convolutional Neural Networks**
    (**CNN**) was proposed. In CNN, features are extracted from two-dimensional input
    data through convolutional layers and pooling layers (this will be explained later),
    and then these features are put into general multi-layer perceptrons. This preprocessing
    for MLP is inspired by human visual areas and can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Segment the input data into several domains. This process is equivalent to a
    human's receptive fields.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract the features from the respective domains, such as edges and position
    aberrations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these features, MLP can classify data accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The graphical model of CNN is not similar to that of other neural networks.
    Here is a briefly outlined example of CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolutional neural networks](img/B04779_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You may not fully understand what CNN is just from the figure. Moreover, you
    might feel that CNN is relatively complicated and difficult to understand. But
    you don't have to worry about that. It is a fact that CNN has a complicated graphical
    model and has unfamiliar terminologies such as convolution and pooling, which
    you don't hear about in other deep learning algorithms. However, when you look
    at the model step by step, there's nothing too difficult to understand. CNN consists
    of several types of layers specifically adjusted for image recognition. Let's
    look at each layer one by one in the next subsection. In the preceding figure,
    there are two convolution and pooling (**Subsampling**) layers and fully connected
    multi-layer perceptrons in the network. We'll see what the convolutional layers
    do first.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Convolutional layers literally perform convolution, which means applying several
    filters to the image to extract features. These filters are called **kernels**,
    and convolved images are called **feature maps**. Let''s see the following image
    (decomposed to color values) and kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolution](img/B04779_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'With these, what is done with convolution is illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolution](img/B04779_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The kernel slides across the image and returns the summation of its values
    within the kernel as a multiplication filter. You might have noticed that you
    can extract many kinds of features by changing kernel values. Suppose you have
    kernels with values as described here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolution](img/B04779_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You see that the kernel on the left extracts the edges of the image because
    it accentuates the color differences, and the one on the right blurs the image
    because it degrades the original values. The great thing about CNN is that in
    convolutional layers, you don't have to set these kernel values manually. Once
    initialized, CNN itself will learn the proper values through the learning algorithm
    (which means parameters trained in CNN are the weights of kernels) and can classify
    images very precisely in the end.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's think about why neural networks with convolutional layers (kernels)
    can predict with higher precision rates. The key here is the **local receptive
    field**. In most layers in neural networks except CNN, all neurons are fully connected.
    This even causes slightly different data, for example, one-pixel parallel data
    would be regarded as completely different data in the network because this data
    is propagated to different neurons in hidden layers, whereas humans can easily
    understand they are the same. With fully connected layers, it is true that neural
    networks can recognize more complicated patterns, but at the same time they lack
    the ability to generalize and lack flexibility. In contrast, you can see that
    connections among neurons in convolutional layers are limited to their kernel
    size, making the model more robust to translated images. Thus, neural networks
    with their receptive fields limited locally are able to acquire **translation
    invariance** when kernels are optimized.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each kernel has its own values and extracts respective features from the image.
    Please bear in mind that the number of feature maps and the number of kernels
    are always the same, which means if we have 20 kernels, we have also twenty feature
    maps, that is, convolved images. This can be confusing, so let''s explore another
    example. Given a gray-scaled **image** and twenty **kernels**, how many **feature
    maps** are there? The answer is twenty. These twenty images will be propagated
    to the next layer. This is illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolution](img/B04779_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, how about this: suppose we have a 3-channeled image (for example, an RGB
    image) and the number of kernels is twenty, how many feature maps will there be?
    The answer is, again, twenty. But this time, the process of convolution is different
    from the one with gray-scaled, that is 1-channeled, images. When the image has
    multiple channels, kernels will be adapted separately for each channel. Therefore,
    in this case, we will have a total of 60 convolved images first, composed of twenty
    mapped images for each of the 3 channels. Then, all the convolved images originally
    from the same image will be combined into one feature map. As a result, we will
    have twenty feature maps. In other words, images are decomposed into different
    channeled data, applied kernels, and then combined into mixed-channeled images
    again. You can easily imagine from the flow in the preceding diagram that when
    we apply a kernel to a multi-channeled image to make decomposed images, the same
    kernel should be applied. This flow can be seen in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolution](img/B04779_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Computationally, the number of kernels is represented with the dimension of
    the weights' tensor. You'll see how to implement this later.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What pooling layers do is rather simple compared to convolutional layers. They
    actually do not train or learn by themselves but just downsample images propagated
    from convolutional layers. Why should we bother to do downsampling? You might
    think it may lose some significant information from the data. But here, again,
    as with convolutional layers, this process is necessary to make the network keep
    its translation invariance.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several ways of downsampling, but among them, max-pooling is the
    most famous. It can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pooling](img/B04779_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In a max-pooling layer, the input image is segmented into a set of non-overlapping
    sub-data and the maximum value is output from each data. This process not only
    keeps its translation invariance but also reduces the computation for the upper
    layers. With convolution and pooling, CNN can acquire robust features from the
    input.
  prefs: []
  type: TYPE_NORMAL
- en: Equations and implementations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we know what convolution and max-pooling are, let''s describe the whole
    model with equations. We''ll use the figure of convolution below in equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equations and implementations](img/B04779_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the figure, if we have an image with a size of ![Equations and
    implementations](img/B04779_04_29.jpg) and kernels with a size of ![Equations
    and implementations](img/B04779_04_30.jpg), the convolution can be represented
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equations and implementations](img/B04779_04_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Equations and implementations](img/B04779_04_32.jpg) is the weight
    of the kernel, that is, the model parameter. Just bear in mind we''ve described
    each summation from 0, not from 1, so you get a better understanding. The equation,
    however, is not enough when we think about multi-convolutional layers because
    it does not have the information from the channel. Fortunately, it''s not difficult
    because we can implement it just by adding one parameter to the kernel. The extended
    equation can be shown as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equations and implementations](img/B04779_04_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Equations and implementations](img/B04779_04_34.jpg) denotes the channel
    of the image. If the number of kernels is ![Equations and implementations](img/B04779_04_35.jpg)
    and the number of channels is ![Equations and implementations](img/B04779_04_36.jpg),
    we have ![Equations and implementations](img/B04779_04_37.jpg). Then, you can
    see from the equation that the size of the convolved image is ![Equations and
    implementations](img/B04779_04_38.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: 'After the convolution, all the convolved values will be activated by the activation
    function. We''ll implement CNN with the rectifier—the most popular function these
    days—but you may use the sigmoid function, the hyperbolic tangent, or any other
    activation functions available instead. With the activation, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equations and implementations](img/B04779_04_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Equations and implementations](img/B04779_04_40.jpg) denotes the bias,
    the other model parameter. You can see that ![Equations and implementations](img/B04779_04_40.jpg)
    doesn't have subscripts of ![Equations and implementations](img/B04779_04_41.jpg)
    and ![Equations and implementations](img/B04779_04_42.jpg), that is, we have ![Equations
    and implementations](img/B04779_04_43.jpg), a one-dimensional array. Thus, we
    have forward-propagated the values of the convolutional layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next comes the max-pooling layer. The propagation can simply be written as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equations and implementations](img/B04779_04_44.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Equations and implementations](img/B04779_04_45.jpg) and ![Equations
    and implementations](img/B04779_04_46.jpg) are the size of pooling filter and
    ![Equations and implementations](img/B04779_04_47.jpg). Usually, ![Equations and
    implementations](img/B04779_04_45.jpg) and ![Equations and implementations](img/B04779_04_46.jpg)
    are set to the same value of 2 ~ 4.
  prefs: []
  type: TYPE_NORMAL
- en: These two layers, the convolutional layer and the max-pooling layer, tend to
    be arrayed in this order, but you don't necessarily have to follow it. You can
    put two convolutional layers before max-pooling, for example. Also, while we put
    the activation right after the convolution, sometimes it is set after the max-pooling
    instead of the convolution. For simplicity, however, we'll implement CNN with
    the order and sequence of convolution–activation–max-pooling.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One important note here is that although the kernel weights will be learned
    from the data, the architecture, the size of kernel, and the size of pooling are
    all parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The simple MLP follows after convolutional layers and max-pooling layers to
    classify the data. Here, since MLP can only accept one-dimensional data, we need
    to flatten the downsampled data as preprocessing to adapt it to the input layer
    of MLP. The extraction of features was completed before MLP, so formatting the
    data into one dimension won't be a problem. Thus, CNN can classify the image data
    once the model is optimized. To do this, as with other neural networks, the backpropagation
    algorithm is applied to CNN to train the model. We won't mention the equation
    related to MLP here.
  prefs: []
  type: TYPE_NORMAL
- en: 'The error from the input layer of MLP is backpropagated to the max-pooling
    layer, and this time it is unflattened to two dimensions to be adapted properly
    to the model. Since the max-pooling layer doesn''t have model parameters, it simply
    backpropagates the error to the previous layer. The equation can be described
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equations and implementations](img/B04779_04_48.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Equations and implementations](img/B04779_04_49.jpg) denotes the evaluation
    function. This error is then backpropagated to the convolutional layer, and with
    it we can calculate the gradients of the weight and the bias. Since the activation
    with the bias comes before the convolution when backpropagating, let''s see the
    gradient of the bias first, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equations and implementations](img/B04779_04_50.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To proceed with this equation, we define the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equations and implementations](img/B04779_04_51.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We also define:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equations and implementations](img/B04779_04_52.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'With these, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equations and implementations](img/B04779_04_53.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can calculate the gradient of the weight (kernel) in the same way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equations and implementations](img/B04779_04_54.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, we can update the model parameters. If we have just one convolutional
    and max-pooling layer, the equations just given are all that we need. When we
    think of multi-convolutional layers, however, we also need to calculate the error
    of the convolutional layers. This can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equations and implementations](img/B04779_04_55.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equations and implementations](img/B04779_04_56.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, the error can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equations and implementations](img/B04779_04_57.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We have to be careful when calculating this because there's a possibility of
    ![Equations and implementations](img/B04779_04_58.jpg) or ![Equations and implementations](img/B04779_04_59.jpg),
    where there's no element in between the feature maps. To solve this, we need to
    add zero paddings to the top-left edges of them. Then, the equation is simply
    a convolution with the kernel flipped along both axes. Though the equations in
    CNN might look complicated, they are just a pile of summations of each parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'With all the previous equations, we can now implement CNN, so let''s see how
    we do it. The package structure is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equations and implementations](img/B04779_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '`ConvolutionNeuralNetworks.java` is used to build the model outline of CNN,
    and the exact algorithms for training in the convolutional layers and max-pooling
    layers, forward propagations, and backpropagations are written in `ConvolutionPoolingLayer.java`.
    In the demo, we have the original image size of `12` ![Equations and implementations](img/B04779_04_60.jpg)
    ` 12` with one channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The image will be propagated through two `ConvPoolingLayer` (convolutional
    layers and max-pooling layers). The number of kernels in the first layer is set
    to `10` with the size of `3` ![Equations and implementations](img/B04779_04_60.jpg)
    ` 3` and `20` with the size of `2 ` ![Equations and implementations](img/B04779_04_60.jpg)
    ` 2` in the second layer. The size of the pooling filters are both set to `2 `
    ![Equations and implementations](img/B04779_04_60.jpg) ` 2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'After the second max-pooling layer, there are `20` feature maps with the size
    of `2` ![Equations and implementations](img/B04779_04_60.jpg) ` 2`. These maps
    are then flattened to `80` units and will be forwarded to the hidden layer with
    `20` neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We then create simple demo data of three patterns with a little noise. We''ll
    leave out the code to create demo data here. If we illustrate the data, here is
    an example of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equations and implementations](img/B04779_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s build the model. The constructor is similar to other deep learning
    models and rather simple. We construct multi `ConvolutionPoolingLayers` first.
    The size for each layer is calculated in the method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'When you look at the constructor of the `ConvolutionPoolingLayer` class, you
    can see how the kernel and the bias are defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next comes the construction of MLP. Don''t forget to flatten the downsampled
    data when passing through them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the model is built, we need to train it. In the `train` method, we cache
    all the forward-propagated data so that we can utilize it when backpropagating:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '`preActivated_X` is defined for convolved feature maps, `activated_X` for activated
    features, and `downsampled_X` for downsampled features. We put and cache the original
    data into `downsampled_X`. The actual training begins with forward propagation
    through convolution and max-pooling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `forward` method of `ConvolutionPoolingLayer` is simple and consists of
    `convolve` and `downsample`. The `convolve` function does the convolution, and
    `downsample` does the max-pooling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The values of `preActivated_X` and `activated_X` are set inside the convolve
    method. You can see that the method simply follows the equations explained previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `downsample` method follows the equations as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: You might think we've made some mistake here because there are so many `for`
    loops in these methods, but actually there's nothing wrong. As you can see from
    the equations of CNN, the algorithm requires many loops because it has many parameters.
    The code here works well, but practically, you could define and move the part
    of the innermost loops to other methods. Here, to get a better understanding,
    we've implemented CNN with many nested loops so that we can compare the code with
    equations. You can see now that CNN requires a lot of time to get results.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we downsample the data, we need to flatten it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The data is then forwarded to the hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Multi-class logistic regression is used in the output layer and the delta is
    then backpropagated to the hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The `backward` method of `ConvolutionPoolingLayer` is the same as `forward`,
    also simple. Backpropagation of max-pooling is written in `upsample` and that
    of convolution is in `deconvolve`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'What `upsample` does is just transfer the delta to the convolutional layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'In `deconvolve`, we need to update the model parameter. Since we train the
    model with mini-batches, we calculate the summation of the gradients first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, update the weight and the bias using these gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Unlike other algorithms, we have to calculate the parameters and delta discretely
    in CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we train the model, so let''s go on to the test part. The method for testing
    or prediction simply does the forward propagation, just like the other algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations! That's all for CNN. Now you can run the code and see how it
    works. Here, we have CNN with two-dimensional data as input, but CNN can also
    have three-dimensional data if we expand the model. We can expect its application
    in medical fields, for example, finding malignant tumors from 3D-scanned data
    of human brains.
  prefs: []
  type: TYPE_NORMAL
- en: The process of convolution and pooling was originally invented by LeCun et al.
    in 1998 ([http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf)),
    yet as you can see from the codes, it requires much calculation. We can assume
    that this method might not have been suitable for practical applications with
    computers at the time, not to mention making it deep. The reason CNN has gained
    more attention recently is probably because the power and capacity of computers
    has greatly developed. But still, we can't deny the problem. Therefore, it seems
    practical to use GPU, not CPU, when we have CNN with certain amounts of data.
    Since the implementation to optimize the algorithm to GPU is complicated, we won't
    write the codes here. Instead, in [Chapter 5](ch05.html "Chapter 5. Exploring
    Java Deep Learning Libraries – DL4J, ND4J, and More"), *Exploring Java Deep Learning
    Libraries – DL4J, ND4J, and More* and [Chapter 7](ch07.html "Chapter 7. Other
    Important Deep Learning Libraries"), *Other Important Deep Learning Libraries*,
    you'll see the library of deep learning that is capable of utilizing GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you learned about two deep learning algorithms that don''t
    require pre-training: deep neural networks with dropout and CNN. The key to high
    precision rates is how we make the network sparse, and dropout is one technique
    to achieve this. Another technique is the rectifier, the activation function that
    can solve the problem of saturation that occurred in the sigmoid function and
    the hyperbolic tangent. CNN is the most popular algorithm for image recognition
    and has two features: convolution and max-pooling. Both of these attribute the
    model to acquire translation invariance. If you are interested in how dropout,
    rectifier, and other activation functions contribute to the performance of neural
    networks, the following could be good references: *Deep Sparse Rectifier Neural
    Networks* (Glorot, et. al. 2011, [http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf](http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf)),
    *ImageNet Classification with Deep Convolutional Neural Networks* (Krizhevsky
    et. al. 2012, [https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)),
    and *Maxout Networks* (Goodfellow et al. 2013, [http://arxiv.org/pdf/1302.4389.pdf](http://arxiv.org/pdf/1302.4389.pdf)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'While you now know the popular and useful deep learning algorithms, there are
    still many of them that have not been mentioned in this book. This field of study
    is getting more and more active, and more and more new algorithms are appearing.
    But don''t worry, as all the algorithms are based on the same root: neural networks.
    Now you know the way of thinking required to grasp or implement the model, you
    can fully understand whatever models you encounter.'
  prefs: []
  type: TYPE_NORMAL
- en: We've implemented deep learning algorithms from scratch so you fully understand
    them. In the next chapter, you'll see how we can implement them with deep learning
    libraries to facilitate our research or applications.
  prefs: []
  type: TYPE_NORMAL
