["```py\nfinal int train_N = 1000;  // number of training data\nfinal int test_N = 200;   // number of test data\nfinal int nIn = 2;        // dimensions of input data\n\ndouble[][] train_X = new double[train_N][nIn];  // input data for training\nint[] train_T = new int[train_N];               // output data (label) for training\n\ndouble[][] test_X = new double[test_N][nIn];  // input data for test\nint[] test_T = new int[test_N];               // label of inputs\nint[] predicted_T = new int[test_N];          // output data predicted by the model\n\nfinal int epochs = 2000;   // maximum training epochs\nfinal double learningRate = 1.;  // learning rate can be 1 in perceptrons\n```", "```py\nGaussianDistribution g1 = new GaussianDistribution(-2.0, 1.0, rng);\nGaussianDistribution g2 = new GaussianDistribution(2.0, 1.0, rng);\n```", "```py\nPerceptrons classifier = new Perceptrons(nIn);\n\n```", "```py\npublic Perceptrons(int nIn) {\n\n   this.nIn = nIn;\n   w = new double[nIn];\n\n}\n```", "```py\nwhile (true) {\n   int classified_ = 0;\n\n   for (int i=0; i < train_N; i++) {\n       classified_ += classifier.train(train_X[i], train_T[i], learningRate);\n   }\n\n   if (classified_ == train_N) break;  // when all data classified correctly\n\n   epoch++;\n   if (epoch > epochs) break;\n}\n```", "```py\npublic int train(double[] x, int t, double learningRate) {\n\n   int classified = 0;\n   double c = 0.;\n\n   // check whether the data is classified correctly\n   for (int i = 0; i < nIn; i++) {\n       c += w[i] * x[i] * t;\n   }\n\n   // apply gradient descent method if the data is wrongly classified\n   if (c > 0) {\n       classified = 1;\n   } else {\n       for (int i = 0; i < nIn; i++) {\n           w[i] += learningRate * x[i] * t;\n       }\n   }\n\n   return classified;\n}\n```", "```py\nfor (int i = 0; i < test_N; i++) {\n   predicted_T[i] = classifier.predict(test_X[i]);\n}\n```", "```py\npublic int predict (double[] x) {\n\n   double preActivation = 0.;\n\n   for (int i = 0; i < nIn; i++) {\n       preActivation += w[i] * x[i];\n   }\n\n   return step(preActivation);\n}\n```", "```py\nint[][] confusionMatrix = new int[2][2];\ndouble accuracy = 0.;\ndouble precision = 0.;\ndouble recall = 0.;\n\nfor (int i = 0; i < test_N; i++) {\n\n   if (predicted_T[i] > 0) {\n       if (test_T[i] > 0) {\n           accuracy += 1;\n           precision += 1;\n           recall += 1;\n           confusionMatrix[0][0] += 1;\n       } else {\n           confusionMatrix[1][0] += 1;\n       }\n   } else {\n       if (test_T[i] > 0) {\n           confusionMatrix[0][1] += 1;\n       } else {\n           accuracy += 1;\n           confusionMatrix[1][1] += 1;\n       }\n   }\n\n}\n\naccuracy /= test_N;\nprecision /= confusionMatrix[0][0] + confusionMatrix[1][0];\nrecall /= confusionMatrix[0][0] + confusionMatrix[0][1];\n\nSystem.out.println(\"----------------------------\");\nSystem.out.println(\"Perceptrons model evaluation\");\nSystem.out.println(\"----------------------------\");\nSystem.out.printf(\"Accuracy:  %.1f %%\\n\", accuracy * 100);\nSystem.out.printf(\"Precision: %.1f %%\\n\", precision * 100);\nSystem.out.printf(\"Recall:    %.1f %%\\n\", recall * 100);\n```", "```py\nint minibatchSize = 50;  //  number of data in each minibatch\nint minibatch_N = train_N / minibatchSize; //  number of minibatches\n\ndouble[][][] train_X_minibatch = new double[minibatch_N][minibatchSize][nIn];  // minibatches of training data\nint[][][] train_T_minibatch = new int[minibatch_N][minibatchSize][nOut];       // minibatches of output data for training\n```", "```py\nList<Integer> minibatchIndex = new ArrayList<>();  // data index for minibatch to apply SGD\nfor (int i = 0; i < train_N; i++) minibatchIndex.add(i);\nCollections.shuffle(minibatchIndex, rng);  // shuffle data index for SGD\n```", "```py\ntrain_T[i] = new int[]{1, 0, 0};\ntest_T[i] = new Integer[]{1, 0, 0};\n```", "```py\nfor (int i = 0; i < minibatch_N; i++) {\n   for (int j = 0; j < minibatchSize; j++) {\n       train_X_minibatch[i][j] = train_X[minibatchIndex.get(i * minibatchSize + j)];\n       train_T_minibatch[i][j] = train_T[minibatchIndex.get(i * minibatchSize + j)];\n   }\n}\n```", "```py\nLogisticRegression classifier = new LogisticRegression(nIn, nOut);\n```", "```py\npublic LogisticRegression(int nIn, int nOut) {\n\n   this.nIn = nIn;\n   this.nOut = nOut;\n\n   W = new double[nOut][nIn];\n   b = new double[nOut];\n\n}\n```", "```py\nfor (int epoch = 0; epoch < epochs; epoch++) {\n   for (int batch = 0; batch < minibatch_N; batch++) {\n       classifier.train(train_X_minibatch[batch], train_T_minibatch[batch], minibatchSize, learningRate);\n   }\n   learningRate *= 0.95;\n}\n```", "```py\n    // 1\\. calculate gradient of W, b\n    for (int n = 0; n < minibatchSize; n++) {\n\n       double[] predicted_Y_ = output(X[n]);\n\n       for (int j = 0; j < nOut; j++) {\n           dY[n][j] = predicted_Y_[j] - T[n][j];\n\n           for (int i = 0; i < nIn; i++) {\n               grad_W[j][i] += dY[n][j] * X[n][i];\n           }\n\n           grad_b[j] += dY[n][j];\n       }\n    }\n\n    // 2\\. update params\n    for (int j = 0; j < nOut; j++) {\n       for (int i = 0; i < nIn; i++) {\n           W[j][i] -= learningRate * grad_W[j][i] / minibatchSize;\n       }\n       b[j] -= learningRate * grad_b[j] / minibatchSize;\n    }\n\n    return dY;\n    ```", "```py\nfor (int i = 0; i < test_N; i++) {\n   predicted_T[i] = classifier.predict(test_X[i]);\n}\n```", "```py\npublic Integer[] predict(double[] x) {\n\n   double[] y = output(x);  // activate input data through learned networks\n   Integer[] t = new Integer[nOut]; // output is the probability, so cast it to label\n\n   int argmax = -1;\n   double max = 0.;\n\n   for (int i = 0; i < nOut; i++) {\n       if (max < y[i]) {\n           max = y[i];\n           argmax = i;\n       }\n   }\n\n   for (int i = 0; i < nOut; i++) {\n       if (i == argmax) {\n           t[i] = 1;\n       } else {\n           t[i] = 0;\n       }\n   }\n\n   return t;\n}\n\npublic double[] output(double[] x) {\n\n   double[] preActivation = new double[nOut];\n\n   for (int j = 0; j < nOut; j++) {\n\n       for (int i = 0; i < nIn; i++) {\n           preActivation[j] += W[j][i] * x[i];\n       }\n\n       preActivation[j] += b[j];  // linear output\n   }\n\n   return softmax(preActivation, nOut);\n}\n```", "```py\nint[][] confusionMatrix = new int[patterns][patterns];\ndouble accuracy = 0.;\ndouble[] precision = new double[patterns];\ndouble[] recall = new double[patterns];\n\nfor (int i = 0; i < test_N; i++) {\n   int predicted_ = Arrays.asList(predicted_T[i]).indexOf(1);\n   int actual_ = Arrays.asList(test_T[i]).indexOf(1);\n\n   confusionMatrix[actual_][predicted_] += 1;\n}\n\nfor (int i = 0; i < patterns; i++) {\n   double col_ = 0.;\n   double row_ = 0.;\n\n   for (int j = 0; j < patterns; j++) {\n\n       if (i == j) {\n           accuracy += confusionMatrix[i][j];\n           precision[i] += confusionMatrix[j][i];\n           recall[i] += confusionMatrix[i][j];\n       }\n\n       col_ += confusionMatrix[j][i];\n       row_ += confusionMatrix[i][j];\n   }\n   precision[i] /= col_;\n   recall[i] /= row_;\n}\n\naccuracy /= test_N;\n\nSystem.out.println(\"------------------------------------\");\nSystem.out.println(\"Logistic Regression model evaluation\");\nSystem.out.println(\"------------------------------------\");\nSystem.out.printf(\"Accuracy: %.1f %%\\n\", accuracy * 100);\nSystem.out.println(\"Precision:\");\nfor (int i = 0; i < patterns; i++) {\n   System.out.printf(\" class %d: %.1f %%\\n\", i+1, precision[i] * 100);\n}\nSystem.out.println(\"Recall:\");\nfor (int i = 0; i < patterns; i++) {\n   System.out.printf(\" class %d: %.1f %%\\n\", i+1, recall[i] * 100);\n```", "```py\npublic MultiLayerPerceptrons(int nIn, int nHidden, int nOut, Random rng) {\n\n   this.nIn = nIn;\n   this.nHidden = nHidden;\n   this.nOut = nOut;\n\n   if (rng == null) rng = new Random(1234);\n   this.rng = rng;\n\n   // construct hidden layer with tanh as activation function\n   hiddenLayer = new HiddenLayer(nIn, nHidden, null, null, rng, \"tanh\");  // sigmoid or tanh\n\n   // construct output layer i.e. multi-class logistic layer\n   logisticLayer = new LogisticRegression(nHidden, nOut);\n\n}\n```", "```py\npublic HiddenLayer(int nIn, int nOut, double[][] W, double[] b, Random rng, String activation) {\n\n   if (rng == null) rng = new Random(1234);  // seed random\n\n   if (W == null) {\n\n       W = new double[nOut][nIn];\n       double w_ = 1\\. / nIn;\n\n       for(int j = 0; j < nOut; j++) {\n           for(int i = 0; i < nIn; i++) {\n               W[j][i] = uniform(-w_, w_, rng);  // initialize W with uniform distribution\n           }\n       }\n\n   }\n\n   if (b == null) b = new double[nOut];\n\n   this.nIn = nIn;\n   this.nOut = nOut;\n   this.W = W;\n   this.b = b;\n   this.rng = rng;\n\n   if (activation == \"sigmoid\" || activation == null) {\n\n       this.activation = (double x) -> sigmoid(x);\n       this.dactivation = (double x) -> dsigmoid(x);\n\n   } else if (activation == \"tanh\") {\n\n       this.activation = (double x) -> tanh(x);\n       this.dactivation = (double x) -> dtanh(x);\n\n   }  else {\n       throw new IllegalArgumentException(\"activation function not supported\");\n   }\n\n}\n```", "```py\npublic void train(double[][] X, int T[][], int minibatchSize, double learningRate) {\n\n   double[][] Z = new double[minibatchSize][nIn];  // outputs of hidden layer (= inputs of output layer)\n   double[][] dY;\n\n   // forward hidden layer\n   for (int n = 0; n < minibatchSize; n++) {\n       Z[n] = hiddenLayer.forward(X[n]);  // activate input units\n   }\n\n   // forward & backward output layer\n   dY = logisticLayer.train(Z, T, minibatchSize, learningRate);\n\n   // backward hidden layer (backpropagate)\n   hiddenLayer.backward(X, Z, dY, logisticLayer.W, minibatchSize, learningRate);\n}\n```", "```py\npublic double[][] backward(double[][] X, double[][] Z, double[][] dY, double[][] Wprev, int minibatchSize, double learningRate) {\n\n   double[][] dZ = new double[minibatchSize][nOut];  // backpropagation error\n\n   double[][] grad_W = new double[nOut][nIn];\n   double[] grad_b = new double[nOut];\n\n   // train with SGD\n   // calculate backpropagation error to get gradient of W, b\n   for (int n = 0; n < minibatchSize; n++) {\n\n       for (int j = 0; j < nOut; j++) {\n\n           for (int k = 0; k < dY[0].length; k++) {  // k < ( nOut of previous layer )\n               dZ[n][j] += Wprev[k][j] * dY[n][k];\n           }\n           dZ[n][j] *= dactivation.apply(Z[n][j]);\n\n           for (int i = 0; i < nIn; i++) {\n               grad_W[j][i] += dZ[n][j] * X[n][i];\n           }\n\n           grad_b[j] += dZ[n][j];\n       }\n   }\n\n   // update params\n   for (int j = 0; j < nOut; j++) {\n       for(int i = 0; i < nIn; i++) {\n           W[j][i] -= learningRate * grad_W[j][i] / minibatchSize;\n       }\n       b[j] -= learningRate * grad_b[j] / minibatchSize;\n   }\n\n   return dZ;\n}\n```", "```py\ndouble[] y = output(x);  // activate input data through learned networks\nInteger[] t = new Integer[nOut]; // output is the probability, so cast it to label\n\nSystem.out.println(  Arrays.toString(y) );\n```"]