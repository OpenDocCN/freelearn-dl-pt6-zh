- en: Chapter 5. Exploring Java Deep Learning Libraries – DL4J, ND4J, and More
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, you learned the core theories of deep learning algorithms
    and implemented them from scratch. While we can now say that implementations of
    deep learning are not so difficult, we can't deny the fact that it still takes
    some time to implement models. To mitigate this situation, you'll learn how to
    write code with the Java library of deep learning in this chapter so that we can
    focus more on the critical part of data analysis rather than the trivial part.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics you''ll learn about in this chapter are:'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to the deep learning library of Java
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example code and how to write your own code with the library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some additional ways to optimize the model to get a higher precision rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing from scratch versus a library/framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We implemented the machine learning algorithms of neural networks in [Chapter
    2](ch02.html "Chapter 2. Algorithms for Machine Learning – Preparing for Deep
    Learning"), *Algorithms for Machine Learning – Preparing for Deep Learning*, and
    many deep learning algorithms from scratch in [Chapter 3](ch03.html "Chapter 3. Deep
    Belief Nets and Stacked Denoising Autoencoders"), *Deep Belief Nets and Stacked
    Denoising Autoencoders* and [Chapter 4](ch04.html "Chapter 4. Dropout and Convolutional
    Neural Networks"), *Dropout and Convolutional Neural Networks*. Of course, we
    can apply our own code to practical applications with some customizations, but
    we have to be careful when we want to utilize them because we can''t deny the
    possibility that they might cause several problems in the future. What could they
    be? Here are the possible situations:'
  prefs: []
  type: TYPE_NORMAL
- en: The code we wrote has some missing parameters for better optimization because
    we implemented just the essence of the algorithms for simplicity and so you better
    understand the concepts. While you can still train and optimize the model with
    them, you could get higher precision rates by adding another parameter of your
    own implementation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As mentioned in the previous chapter, there are still many useful deep learning
    algorithms not explained in this book. While you now have the core components
    of the deep learning algorithms, you might need to implement additional classes
    or methods to get the desired results in your fields and applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The assumed time consumption will be very critical to the application, especially
    when you think of analyzing huge amounts of data. It is true that Java has a better
    performance in terms of speed compared to other popular languages such as Python
    and R, but you may still need to consider the time cost. One plausible approach
    to solve the problem is using GPU instead of CPU, but this requires complex implementations
    to adjust the code for GPU computing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are the main causal issues, and you might also need to take into consideration
    that we don't handle exceptions in the code.
  prefs: []
  type: TYPE_NORMAL
- en: This does not mean that implementing from scratch would have fatal errors. The
    code we wrote can be used substantially as an application for certain scaled data;
    however, you need to take into consideration that you require further coding for
    the fundamental parts you have implemented if you use large-scale data mining,
    where, generally, deep learning is required. This means you need to bear in mind
    that implementation from scratch has more flexibility as you can change the code
    if required, but at the same time it has a negative side in that the algorithm's
    tuning and maintenance also has to be done independently.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, how can we solve the problems just mentioned? This is where a library (or
    framework) comes in. Thanks to active research into deep learning globally, there
    are many libraries developed and published using various programming languages
    all over the world. Of course, each library has its respective features but the
    features that every library commonly has can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A model's training can be done just by defining a layer structure of deep learning.
    You can focus on parameter setting and tuning, and you don't need to think about
    the algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of the libraries are open to the public as open source projects and are
    actively updated daily. Therefore, if there are bugs, there's a high possibility
    that these bugs will be fixed quickly (and, of course, committing to a project
    by fixing it yourself should be welcomed).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's easy to switch between running the program on CPU or on GPU. As a library
    supplements the cumbersome coding element of GPU computing, you can just focus
    on the implementation without considering CPU or GPU, if a machine supports GPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long story short, you can leave out all the parts that could be brutal when
    you implement to a library from scratch. Thanks to this, you can take more time
    on the essential data mining section, hence if you want to utilize practical applications,
    there's a high possibility that you can perform data analysis more efficiently
    using a library.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, depending too much on a library isn''t good. Using a library is convenient,
    but on the flip side, it has some demerits, as listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: Since you can build various deep learning models easily, you can implement without
    having a concrete understanding of what theory the model is supported by. This
    might not be a problem if we only consider implementations related to a specific
    model, but there will be a risk you can't deal with when you want to combine other
    methods or consider other methods when applying the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can't use algorithms not supported by a library, hence there might be a
    case where you can't choose a model you would like to use. This can be solved
    by a version upgrade, but on the other hand, there's a possibility that some part
    of a past implementation might be deprecated due to a change of specification
    by the upgrade. Moreover, we can't deny the possibility that the development of
    a library is suddenly terminated or utilization turns out to be chargeable due
    to a sudden change in its license. In these cases, there's a risk that the code
    you have developed up to this point cannot be used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The precision rate you can get from experimentation depends on how a library
    is implemented. For example, if we conduct an experiment with the same neural
    network model in two different libraries, the results we obtain can be hugely
    changed. This is because neural network algorithms include a stochastic operation,
    and the calculation accuracy of a machine is limited, that is, calculated values
    during the process could have fluctuations based on the method of implementation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because you well understand the fundamental concepts and theories of deep learning
    algorithms thanks to the previous chapters, we don't need to worry about the first
    point. However, we need to be careful about the remaining two points. From the
    next section on, implementation using a library is introduced and we'll be more
    conscious of the merits and demerits we just discussed.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing DL4J and ND4J
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lot of the libraries of deep learning have been developed all over the world.
    In November 2015, **TensorFlow** ([http://www.tensorflow.org/](http://www.tensorflow.org/)),
    a machine learning/deep learning library developed by Google, became open to the
    public and attracted great attention.
  prefs: []
  type: TYPE_NORMAL
- en: When we look at the programming language with which libraries are being developed,
    most of them open to the public are developed by Python or use the Python API.
    TensorFlow is developed with C++ on the backend but it's also possible to write
    code with Python. This book focuses on Java to learn deep learning, hence the
    libraries developed by other languages will be briefly introduced in [Chapter
    7](ch07.html "Chapter 7. Other Important Deep Learning Libraries"), *Other Important
    Deep Learning Libraries*.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what Java-based libraries do we have? Actually, there are a few cases that
    are actively developed (perhaps there are also some projects not open to public).
    However, there is only one library we can use practically: **Deeplearning4j**
    (**DL4J**). The official project page URL is [http://deeplearning4j.org/](http://deeplearning4j.org/).
    This library is also open source and the source code is all published on GitHub.
    The URL is [https://github.com/deeplearning4j/deeplearning4j](https://github.com/deeplearning4j/deeplearning4j).
    This library was developed by Skymind ([http://www.skymind.io/](http://www.skymind.io/)).
    What kind of library is this? If you look at the project page, it''s introduced
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"Deeplearning4j is the first commercial-grade, open-source, distributed deep-learning
    library written for Java and Scala. Integrated with Hadoop and Spark, DL4J is
    designed to be used in business environments, rather than as a research tool.
    Skymind is its commercial support arm.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Deeplearning4j aims to be cutting-edge plug and play, more convention than
    configuration, which allows for fast prototyping for non-researchers. DL4J is
    customizable at scale. Released under the Apache 2.0 license, all derivatives
    of DL4J belong to their authors."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When you read this, you will see that the biggest feature of DL4J is that it
    was designed on the premise of being integrated with Hadoop. This indicates that
    DL4J suits the processing of large-scale data and is more scalable than other
    libraries. Moreover, DL4J supports GPU computing, so it's possible to process
    data even faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, DL4J uses a library called **N-Dimensional Arrays for Java** (**ND4J**)
    internally. The project page is [http://nd4j.org/](http://nd4j.org/). The same
    as DL4J, this library is also published on GitHub as an open source project: [https://github.com/deeplearning4j/nd4j](https://github.com/deeplearning4j/nd4j).
    The developer of the library is the same as DL4J, Skymind. As you can see from
    the name of the library, this is a scientific computing library that enables us
    to handle versatile *n*-dimensional array objects. If you are a Python developer,
    it might be easier for you to understand this if you imagine NumPy, as ND4J is
    a library inspired by NumPy. ND4J also supports GPU computing and the reason why
    DL4J is able to do GPU integration is because it uses ND4J internally.'
  prefs: []
  type: TYPE_NORMAL
- en: What good can come from working with them on GPUs? Let's briefly look at this
    point. The biggest difference between CPU and GPU is the difference in the number
    of cores. GPU is, as represented in its name, a graphical processing unit, originally
    an integrated circuit for image processing. This is why GPU is well optimized
    to handle the same commands simultaneously. Parallel processing is its forte.
    On the other hand, as CPU needs to process various commands, these tasks are basically
    made to be processed in order. Compared to CPU, GPU is good at processing huge
    numbers of simple tasks, therefore calculations such as training iterations of
    deep learning is its field of expertise.
  prefs: []
  type: TYPE_NORMAL
- en: Both ND4J and DL4J are very useful for research and data mining with deep learning.
    From the next section on, we'll see how these are used for deep learning in simple
    examples. You can easily understand the contents because you should already understand
    the core theories of deep learning by now. Hopefully, you can make use of this
    for your fields of study or business.
  prefs: []
  type: TYPE_NORMAL
- en: Implementations with ND4J
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As there are many cases where ND4J alone can be used conveniently, let''s briefly
    grasp how to use ND4J before looking into the explanation of DL4J. If you would
    like to use ND4J alone, once you create a new Maven project, then you can use
    ND4J by adding the following code to `pom.xml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `<nd4j.version>` describes the latest version of ND4J, but please check
    whether it is updated when you actually implement the code. Also, switching from
    CPU to GPU is easy while working with ND4J. If you have CUDA installed with version
    7.0, then what you do is just define `artifactId` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You can replace the version of `<artifactId>` depending on your configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a simple example of what calculations are possible with ND4J.
    The type we utilize with ND4J is `INDArray`, that is, an extended type of `Array`.
    We begin by importing the following dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define `INDArray` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`Nd4j.create` takes two arguments. The former defines the actual values within
    `INDArray`, and the latter defines the shape of the vector (matrix). By running
    this code, you get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Since `INDArray` can output its values with `System.out.print`, it''s easy
    to debug. Calculation with scalar can also be done with ease. Add 1 to `x` as
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, the calculation within `INDArray` can be done easily, as shown in the
    following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, basic arithmetic operations can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'These will return the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, ND4J has destructive arithmetic operators. When you write the `x.addi(y)`
    command, `x` changes its own values so that `System.out.println(x);` will return
    the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Likewise, `subi`, `muli`, and `divi` are also destructive operators. There are
    also many other methods that can conveniently perform calculations between vectors
    or matrices. For more information, you can refer to [http://nd4j.org/documentation.html](http://nd4j.org/documentation.html),
    [http://nd4j.org/doc/](http://nd4j.org/doc/) and `http://nd4j.org/apidocs/`.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at one more example to see how machine learning algorithms can be
    written with ND4J. We'll implement the easiest example, perceptrons, based on
    the source code written in [Chapter 2](ch02.html "Chapter 2. Algorithms for Machine
    Learning – Preparing for Deep Learning"), *Algorithms for Machine Learning – Preparing
    for Deep Learning*. We set the package name `DLWJ.examples.ND4J` and the file
    (class) name `Perceptrons.java`.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s add these two lines to import from ND4J:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The model has two parameters: `num` of the input layer and the weight. The
    former doesn''t change from the previous code; however, the latter isn''t `Array`
    but `INDArray`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see from the constructor that since the weight of the perceptrons is
    represented as a vector, the number of rows is set to the number of units in the
    input layer and the number of columns to 1\. This definition is written here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, because we define the model parameter as `INDArray`, we also define the
    demo data, training data, and test data as `INDArray`. You can see these definitions
    at the beginning of the main method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'When we substitute a value into `INDArray`, we use `put`. Please be careful
    that any value we can set with `put` is only the values of the `scalar` type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The flow from a model building and training is the same as the previous code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Each piece of training data is given to the `train` method by `getRow()`. First,
    let''s see the entire content of the `train` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We first focus our attention on the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the part that checks whether the data is classified correctly by perceptions,
    as shown in the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementations with ND4J](img/B04779_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'You can see from the code that `.mmul()` is for the multiplication between
    vectors or matrices. We wrote this part of the calculation in [Chapter 2](ch02.html
    "Chapter 2. Algorithms for Machine Learning – Preparing for Deep Learning"), *Algorithms
    for Machine Learning – Preparing for Deep Learning*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: By comparing both codes, you can see that multiplication between vectors or
    matrices can be written easily with `INDArray`, and so you can implement the algorithm
    intuitively just by following the equations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation to update the model parameters is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, again, you can implement the code like you write a math equation. The
    equation is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementations with ND4J](img/B04779_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The last time we implemented this part, we wrote it with a `for` loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore, the prediction after the training is also the standard forward
    activation, shown as the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementations with ND4J](img/B04779_05_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementations with ND4J](img/B04779_05_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can simply define the `predict` method with just a single line inside, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: When you run the program, you can see its precision and accuracy, and the recall
    is the same as we get with the previous code.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, it'll greatly help that you implement the algorithms analogous to mathematical
    equations. We only implement perceptrons here, but please try other algorithms
    by yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Implementations with DL4J
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ND4J is the library that helps you to implement deep learning easily and conveniently.
    However, you have to implement the algorithms yourself, which is not too different
    from its implementation in the previous chapters. In other words, ND4J is just
    a library that makes calculating numerical values easier and is not a library
    that is optimized for deep learning algorithms. One library that makes deep learning
    easier to handle is DL4J. Fortunately, as for DL4J, some example code with typical
    methods is published on GitHub ([https://github.com/deeplearning4j/dl4j-0.4-examples](https://github.com/deeplearning4j/dl4j-0.4-examples)).
    These examples are used on the premise that you are using DL4J's version 0.4-*.
    When you actually clone this repository, please check the latest version again.
    In this section, we'll extract the fundamental part from these sample programs
    and take a look at it. We'll reference the forked repository on [https://github.com/yusugomori/dl4j-0.4-examples](https://github.com/yusugomori/dl4j-0.4-examples)
    as a screenshot in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s first set up the environments from our cloned repository. If you''re
    using IntelliJ, you can import the project from **File** | **New** | **Project**
    from existing sources and select the path of the repository. Then, choose **Import
    project from external model** and select **Maven** as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setup](img/B04779_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'You don''t have to do anything special for the other steps except click **Next**.
    Please be careful that the supported versions of JDK are 1.7 or above. This may
    not be a problem because we needed version 1.8 or above in the previous chapters.
    Once you have set it up without a problem, you can confirm the structure of the
    directories as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setup](img/B04779_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once you have set up the project, let''s first look at `pom.xml`. You can see
    that the description of the packages related to DL4J is written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, you can see from the following lines that DL4J depends on ND4J:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'If you would like to run a program on GPU, what you have to do is just change
    this written section. As mentioned in the previous section, this can be written
    as follows if you have CUDA installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Here, `XXX` is the version of CUDA and depends on your machine's preference.
    It's great to adopt GPU computing only using this. We don't have to do anything
    special and we can focus on implementations of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other characteristic library that DL4J develops and uses is **Canova**.
    The part that corresponds to `pom.xml` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Canova is also, of course, an open source library and its source code can be
    seen on GitHub at [https://github.com/deeplearning4j/Canova](https://github.com/deeplearning4j/Canova).
    As explained on that page, Canova is the library used to vectorize raw data into
    usable vector formats across the machine learning tools. This also helps us focus
    on the more important part of data mining because data formatting is indispensable
    in whatever research or experiment we're performing.
  prefs: []
  type: TYPE_NORMAL
- en: Build
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's look at the source code in the examples and see how to build a deep learning
    model. During the process, the terms of deep learning that you haven't yet learned
    are also briefly explained. The examples are implemented with various models such
    as MLP, DBN, and CNN, but there is one problem here. As you can see when looking
    at `README.md`, some methods don't generate good precision. This is because, as
    explained in the previous section, the calculation precision a machine has is
    limited and fluctuation occurring with calculated values during the process depends
    completely on the difference of implementation. Hence, practically, learning can't
    be done properly, although theoretically it should be done well. You can get better
    results by, for example, changing the seed values or adjusting the parameters,
    but as we would like to focus on how to use a library, we'll use a model that
    gets higher precision as an example.
  prefs: []
  type: TYPE_NORMAL
- en: DBNIrisExample.java
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's first look at `DBNIrisExample.java` in the package of `deepbelief`. Iris,
    contained in the filename, is one of the benchmark datasets often used when measuring
    the precision or accuracy of a machine learning method. The dataset contains 150
    pieces of data out of 3 classes of 50 instances each, and each class refers to
    a type of Iris plant. The number of inputs is 4 and the number of outputs is therefore
    3\. One class is linearly separable from the other two; the latter are not linearly
    separable from each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation begins by setting up the configuration. Here are the variables
    that need setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In DL4J, input data can be up to two-dimensional data, hence you need to assign
    the number of rows and columns of the data. As Iris is one-dimensional data, `numColumns`
    is set as `1`. Here `numSamples` is the total data and `batchSize` is the amount
    of data in each mini-batch. Since the total data is 150 and it is relatively small,
    `batchSize` is set at the same number. This means that learning is done without
    splitting the data into mini-batches. `splitTrainNum` is the variable that decides
    the allocation between the training data and test data. Here, 80% of all the dataset
    is training data and 20% is the test data. In the previous section, `listenerFreq`
    decides how often we see loss function's value for logging is seen in the process.
    This value is set to 1 here, which means the value is logged after each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Subsequently, we need to fetch the dataset. In DL4J, a class that can easily
    fetch data with respect to a typical dataset, such as Iris, MINST, and LFW, is
    prepared. Therefore, you can just write the following line if you would like to
    fetch the Iris dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The following two lines are to format data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This code splits the data into training data and test data and stores them
    respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it makes data handling easier by treating all the data DL4J
    prepares with the `DataSet` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s actually build a model. The basic structure is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The code begins by defining the model configuration and then builds and initializes
    the actual model with the definition. Let''s take a look at the configuration
    details. At the beginning, the whole network is set up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The configuration setup is self-explanatory. However, since you haven't learned
    about regularization before now, let's briefly check it out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regularization prevents the neural networks model from overfitting and makes
    the model more generalized. To achieve this, the evaluation function ![DBNIrisExample.java](img/B04779_05_07.jpg)
    is rewritten with the penalty term as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![DBNIrisExample.java](img/B04779_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![DBNIrisExample.java](img/B04779_05_09.jpg) denotes the vector norm.
    The regularization is called L1 regularization when ![DBNIrisExample.java](img/B04779_05_10.jpg)
    and L2 regularization when ![DBNIrisExample.java](img/B04779_05_11.jpg). The norm
    is called L1 norm and L2 norm, respectively. That's why we have `.l1()` and `.l2()`
    in the code. ![DBNIrisExample.java](img/B04779_05_12.jpg) is the hyper parameter.
    These regularization terms make the model more sparse. L2 regularization is also
    called weight decay and is used to prevent the vanishing gradient problem.
  prefs: []
  type: TYPE_NORMAL
- en: The `.useDropConnect()` command is used to enable dropout and `.list()` to define
    the number of layers, excluding the input layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you set up a whole model, then the next step is to configure each layer.
    In this sample code, the model is not defined as deep neural networks. One single
    RBM layer is defined as a hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Here, the value of `0` in the first line is the layer's index and `.k()` is
    for contrastive divergence. Since Iris' data is of float values, we can't use
    binary RBM. That's why we have `RBM.VisibleUnit.GAUSSIAN` here, enabling the model
    to handle continuous values. Also, as for the definition of this layer, what should
    be especially mentioned is the role of `Updater.ADAGRAD`. This is used to optimize
    the learning rate. For now, we go on to the model structure, and a detailed explanation
    of the optimizer will be introduced at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The subsequent output layer is very simple and self-explanatory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus, the neural networks have been built with three layers : input layer,
    hidden layer, and output layer. The graphical model of this example can be illustrated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![DBNIrisExample.java](img/B04779_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'After the model building, we need to train the networks. Here, again, the code
    is super simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Because the first line is to log the process, what we need to do to train the
    model is just to write `model.fit()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Testing or evaluating the model is also easy with DL4J. First, the variables
    for evaluation are set up as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can get the values of the feature matrix using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'By running the code, we will have the result as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '`F1 Score`, also called `F-Score` or `F-measure`, is the harmonic means of
    precision and recall, and is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![DBNIrisExample.java](img/B04779_05_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This value is often calculated to measure the model''s performance as well.
    Also, as written in the example, you can see the actual values and predicted values
    by writing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s it for the whole training and test process. The neural networks in
    the preceding code are not deep, but you can easily build deep neural networks
    just by changing the configuration as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, building deep neural networks requires just simple implementations
    with DL4J. Once you set up the model, what you need to do is adjust the parameters.
    For example, increasing the iterations value or changing the seed value would
    return a better result.
  prefs: []
  type: TYPE_NORMAL
- en: CSVExample.java
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous example, we train the model with the dataset used as a benchmark
    indicator. When you would like to train and test the model with your own prepared
    data, you can easily import it from CSV. Let''s look at `CSVExample.java` in the
    CSV package. The first step is to initialize the CSV reader as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'In DL4J, a class called `CSVRecordReader` is prepared and you can easily import
    data from a CSV file. The value of the first argument in the `CSVRecordReader`
    class represents how many lines should be skipped in the file. This is convenient
    when the file contains header rows. The second argument is the delimiter. To actually
    read a file and import data, the code can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'With this code, the file in `resources/iris.txt` will be imported to the model.
    The values in the file here are the same as ones as in the Iris dataset. To use
    this initialized data for model training, we define the iterator as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: In the previous example, we used the `IrisDataSetIterator` class, but here the
    `RecordReaderDataSetIterator` class is used because we use our own prepared data.
    The values `4` and `3` are the number of features and labels, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Building and training a model can be done in almost the same way as the process
    explained in the previous example. In this example, we build deep neural networks
    of two hidden layers with the dropout and the rectifier, that is, we have an input
    layer - hidden layer - hidden layer - output layer, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We can run the model using the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The graphical model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![CSVExample.java](img/B04779_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This time, however, the way to code for training is slightly different from
    the previous example. Before, we split the data into training data and test data
    using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows that we shuffle the data within the `.splitTestAndTrain()` method.
    In this example, we set up training data with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, here the data is first shuffled and then split into training
    data and test data. Be careful that the types of the arguments in `.splitTestAndTrain()`
    are different from each other. This will be beneficial because we don''t have
    to count the exact amount of data or training data. The actual training is done
    using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The way to evaluate the model is just the same as the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'With the preceding code, we get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: In addition to the dataset of a benchmark indicator, you can now analyze whatever
    data you have.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the model even deeper, you just need to add another layer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: CNNMnistExample.java/LenetMnistExample.java
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CNN is rather complicated compared to other models because of its structure,
    but we don't need to worry about these complications because we can easily implement
    CNN with DL4J. Let's take a look at `CNNMnistExample.java` in the package of convolution.
    In this example, we train the model with the MNIST dataset ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)),
    one of the most famous benchmark indicators. As mentioned in [Chapter 1](ch01.html
    "Chapter 1. Deep Learning Overview"), *Deep Learning Overview*, this dataset contains
    70,000 handwritten numbers data from 0 to 9, with both a height and width of 28
    pixels each.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define the values necessary for the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Since images in MNIST are all grayscale data, the number of channels is set
    to `1`. In this example, we use `2,000` data of `70,000` and split it into training
    data and test data. The size of the mini-batch is `500` here, so the training
    data is divided into 4 mini-batches. Furthermore, the data in each mini-batch
    is split into training data and test data, and each piece of test data is stored
    in `ArrayList`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We didn''t have to set `ArrayList` in the previous examples because we had
    just one batch. For the `MnistDataSetIterator` class, we can set the MNIST data
    just by using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we build the model with a convolutional layer and subsampling layer.
    Here, we have one convolutional layer and one max-pooling layer, directly followed
    by an output layer. The structure of the configurations for CNN is slightly different
    from the other algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The difference is that we can''t build a model directly from the configuration
    because we need to tell the builder to set up a convolutional layer using `ConvolutionLayerSetup()`
    in advance. Each `.layer()` requires just the same method of coding. The convolutional
    layer is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Here, the value of `10` in `ConvolutionLayer.Builder()` is the size of the kernels,
    and the value of `6` in `.nOut()` is the number of kernels. Also, `.stride()`
    defines the size of the strides of the kernels. The code we implemented from scratch
    in [Chapter 4](ch04.html "Chapter 4. Dropout and Convolutional Neural Networks"),
    *Dropout and Convolutional Neural Networks* has a functionality equivalent only
    to `.stride(1, 1)`. The larger the number is, the less time it takes because it
    decreases the number of calculations necessary for convolutions, but we have to
    be careful at the same time that it might also decrease the model's precision.
    Anyway, we can implement convolutions with more flexibility now.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `subsampling` layer is described as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Here, `{2, 2}` is the size of the pooling windows. You may have noticed that
    we don't have to set the size of the inputs for each layer, including the output
    layer. These values are automatically set once you set up the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output layer can be written just the same as in the other models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The graphical model of this example is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![CNNMnistExample.java/LenetMnistExample.java](img/B04779_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'After the building comes the training. Since we have multiple mini-batches,
    we need to iterate training through all the batches. This can be achieved easily
    using `.hasNext()` on `DataSetIterator` and `mnistIter` in this case. The whole
    training process can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Here, the test data and test labels are stocked for further use.
  prefs: []
  type: TYPE_NORMAL
- en: 'During the test, again, we need to iterate the evaluation process of the test
    data because we have more than one mini-batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we use the same as in the other examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'This will return the result as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The example just given is the model with one convolutional layer and one subsampling
    layer, but you have deep convolutional neural networks with `LenetMnistExample.java`.
    In this example, there are two convolutional layers and subsampling layers, followed
    by fully connected multi-layer perceptrons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the first convolutional layer, dropout can easily be applied
    to CNN with DL4J.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this model, we get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: You can see from the MNIST dataset page ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/))
    that the state-of-the-art result is much better than the one above. Here, again,
    you would realize how important the combination of parameters, activation functions,
    and optimization algorithms are.
  prefs: []
  type: TYPE_NORMAL
- en: Learning rate optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have learned various deep learning algorithms so far; you may have noticed
    that they have one parameter in common: the learning rate. The learning rate is
    defined in the equations to update the model parameters. So, why not think of
    algorithms to optimize the learning rate? Originally, these equations were described
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning rate optimization](img/B04779_05_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning rate optimization](img/B04779_05_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Learning rate optimization](img/B04779_05_19.jpg) is the number of steps
    and ![Learning rate optimization](img/B04779_05_20.jpg) is the learning rate.
    It is well known that decreasing the value of the learning rate with each iteration
    lets the model have better precision rates, but we should determine the decline
    carefully because a sudden drop in the value would collapse the model. The learning
    rate is one of the model parameters, so why not optimize it? To do so, we need
    to know what the best rate could be.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest way of setting the rate is using the momentum, represented as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning rate optimization](img/B04779_05_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Learning rate optimization](img/B04779_05_22.jpg), called the **momentum
    coefficient**. This hyper parameter is often set to be 0.5 or 0.9 first and then
    fine-tuned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Momentum is actually a simple but effective way of adjusting the learning rate
    but **ADAGRAD**, proposed by Duchi et al. ([http://www.magicbroom.info/Papers/DuchiHaSi10.pdf](http://www.magicbroom.info/Papers/DuchiHaSi10.pdf)),
    is known to be a better way. The equation is described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning rate optimization](img/B04779_05_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning rate optimization](img/B04779_05_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Theoretically, this works well, but practically, we often use the following
    equations to prevent divergence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning rate optimization](img/B04779_05_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Or we use:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning rate optimization](img/B04779_05_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ADAGRAD is easier to use than momentum because the value is set automatically
    and we don't have to set additional hyper parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'ADADELTA, suggested by Zeiler ([http://arxiv.org/pdf/1212.5701.pdf](http://arxiv.org/pdf/1212.5701.pdf)),
    is known to be an even better optimizer. This is an algorithm-based optimizer
    and cannot be written in a single equation. Here is a description of ADADELTA:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialization:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initialize accumulation variables:![Learning rate optimization](img/B04779_05_27.jpg)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Learning rate optimization](img/B04779_05_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Iteration ![Learning rate optimization](img/B04779_05_29.jpg):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute:![Learning rate optimization](img/B04779_05_30.jpg)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Accumulate gradient:![Learning rate optimization](img/B04779_05_31.jpg)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute update:![Learning rate optimization](img/B04779_05_32.jpg)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Accumulate updates:![Learning rate optimization](img/B04779_05_33.jpg)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply update:![Learning rate optimization](img/B04779_05_17.jpg)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, ![Learning rate optimization](img/B04779_05_34.jpg) and ![Learning rate
    optimization](img/B04779_05_35.jpg) are the hyper parameters. You may think ADADELTA
    is rather complicated but you don't need to worry about this complexity when implementing
    with DL4J.
  prefs: []
  type: TYPE_NORMAL
- en: There are still other optimizers supported in DL4J such as **RMSProp**, **RMSProp**
    + momentum, and **Nesterov's Accelerated Gradient** **Descent**. However, we won't
    dig into them because, practically, momentum, ADAGRAD, and ADADELTA are enough
    to optimize the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to implement deep learning models with the
    libraries ND4J and DL4J. Both support GPU computing and both give us the ability
    to implement them without any difficulties. ND4J is a library for scientific computing
    and enables vectorization, which makes it easier to implement a calculation among
    arrays because we don't need to write iterations within them. Since machine learning
    and deep learning algorithms have many equations with vector calculations, such
    as inner products and element-wise multiplication, ND4J also helps implement them.
  prefs: []
  type: TYPE_NORMAL
- en: 'DL4J is a library for deep learning, and by following some examples with the
    library, you saw that we can easily build, train, and evaluate various types of
    deep learning models. Additionally, while building the model, you learned why
    regularization is necessary to get better results. You also got to know some optimizers
    of the learning rate: momentum, ADAGRAD, and ADADELTA. All of these can be implemented
    easily with DL4J.'
  prefs: []
  type: TYPE_NORMAL
- en: You gained knowledge of the core theories and implementations of deep learning
    algorithms and you now know how to implement them with little difficulty. We can
    say that we've completed the theoretical part of this book. Therefore, in the
    next chapter, we'll look at how deep learning algorithms are adapted to practical
    applications first and then look into other possible fields and ideas to apply
    the algorithms.
  prefs: []
  type: TYPE_NORMAL
