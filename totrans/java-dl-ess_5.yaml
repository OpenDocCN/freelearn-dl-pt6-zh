- en: Chapter 5. Exploring Java Deep Learning Libraries – DL4J, ND4J, and More
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章 探索Java深度学习库——DL4J、ND4J及更多
- en: In the previous chapters, you learned the core theories of deep learning algorithms
    and implemented them from scratch. While we can now say that implementations of
    deep learning are not so difficult, we can't deny the fact that it still takes
    some time to implement models. To mitigate this situation, you'll learn how to
    write code with the Java library of deep learning in this chapter so that we can
    focus more on the critical part of data analysis rather than the trivial part.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，你学习了深度学习算法的核心理论，并从零开始实现了它们。虽然我们现在可以说，深度学习的实现并不那么困难，但我们不能否认，实施模型仍然需要一些时间。为了解决这个问题，在本章中，你将学习如何使用Java深度学习库编写代码，这样我们就可以更多地专注于数据分析的关键部分，而不是琐碎的细节。
- en: 'The topics you''ll learn about in this chapter are:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中你将学习的主题包括：
- en: An introduction to the deep learning library of Java
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java深度学习库简介
- en: Example code and how to write your own code with the library
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例代码以及如何使用该库编写你自己的代码
- en: Some additional ways to optimize the model to get a higher precision rate
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些优化模型以提高精度率的附加方法
- en: Implementing from scratch versus a library/framework
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头实现与使用库/框架
- en: 'We implemented the machine learning algorithms of neural networks in [Chapter
    2](ch02.html "Chapter 2. Algorithms for Machine Learning – Preparing for Deep
    Learning"), *Algorithms for Machine Learning – Preparing for Deep Learning*, and
    many deep learning algorithms from scratch in [Chapter 3](ch03.html "Chapter 3. Deep
    Belief Nets and Stacked Denoising Autoencoders"), *Deep Belief Nets and Stacked
    Denoising Autoencoders* and [Chapter 4](ch04.html "Chapter 4. Dropout and Convolutional
    Neural Networks"), *Dropout and Convolutional Neural Networks*. Of course, we
    can apply our own code to practical applications with some customizations, but
    we have to be careful when we want to utilize them because we can''t deny the
    possibility that they might cause several problems in the future. What could they
    be? Here are the possible situations:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第2章](ch02.html "第2章 机器学习算法——为深度学习做准备")中实现了神经网络的机器学习算法，*机器学习算法——为深度学习做准备*，并在[第3章](ch03.html
    "第3章 深度信念网络与堆叠去噪自编码器")，*深度信念网络与堆叠去噪自编码器*和[第4章](ch04.html "第4章 Dropout和卷积神经网络")，*Dropout和卷积神经网络*中从头实现了许多深度学习算法。当然，我们可以通过一些定制将自己的代码应用于实际应用，但在我们想要利用它们时必须小心，因为我们不能否认它们未来可能引发的问题。可能是什么问题呢？以下是可能的情况：
- en: The code we wrote has some missing parameters for better optimization because
    we implemented just the essence of the algorithms for simplicity and so you better
    understand the concepts. While you can still train and optimize the model with
    them, you could get higher precision rates by adding another parameter of your
    own implementation.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们编写的代码缺少一些更好的优化参数，因为我们只是为了简化问题并帮助你更好地理解概念而实现了算法的核心部分。虽然你仍然可以使用这些代码来训练和优化模型，但通过添加你自己实现的额外参数，你可以获得更高的精度率。
- en: As mentioned in the previous chapter, there are still many useful deep learning
    algorithms not explained in this book. While you now have the core components
    of the deep learning algorithms, you might need to implement additional classes
    or methods to get the desired results in your fields and applications.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如前章所述，本书中仍有许多有用的深度学习算法未被解释。虽然你现在已经掌握了深度学习算法的核心组件，但你可能需要实现额外的类或方法，以在你的领域和应用中获得期望的结果。
- en: The assumed time consumption will be very critical to the application, especially
    when you think of analyzing huge amounts of data. It is true that Java has a better
    performance in terms of speed compared to other popular languages such as Python
    and R, but you may still need to consider the time cost. One plausible approach
    to solve the problem is using GPU instead of CPU, but this requires complex implementations
    to adjust the code for GPU computing.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假定的时间消耗对应用非常关键，尤其是当你考虑分析大量数据时。确实，相较于Python和R等其他流行语言，Java在速度方面有更好的性能，但你仍然需要考虑时间成本。一个可行的解决方案是使用GPU代替CPU，但这需要复杂的实现来调整代码以适应GPU计算。
- en: These are the main causal issues, and you might also need to take into consideration
    that we don't handle exceptions in the code.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是主要的因果问题，你可能还需要考虑到我们没有在代码中处理异常。
- en: This does not mean that implementing from scratch would have fatal errors. The
    code we wrote can be used substantially as an application for certain scaled data;
    however, you need to take into consideration that you require further coding for
    the fundamental parts you have implemented if you use large-scale data mining,
    where, generally, deep learning is required. This means you need to bear in mind
    that implementation from scratch has more flexibility as you can change the code
    if required, but at the same time it has a negative side in that the algorithm's
    tuning and maintenance also has to be done independently.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不意味着从头实现会有致命的错误。我们编写的代码可以作为处理某些规模数据的应用程序；然而，如果你使用大规模数据挖掘，通常需要深度学习，那么你需要考虑，你所实现的基础部分需要进一步编码。这意味着，你需要记住，从头实现具有更大的灵活性，因为在必要时你可以更改代码，但同时它也有负面影响，即算法的调优和维护必须独立完成。
- en: 'So, how can we solve the problems just mentioned? This is where a library (or
    framework) comes in. Thanks to active research into deep learning globally, there
    are many libraries developed and published using various programming languages
    all over the world. Of course, each library has its respective features but the
    features that every library commonly has can be summarized as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何解决刚才提到的问题呢？这就是库（或框架）派上用场的地方。得益于全球对深度学习的积极研究，世界各地有许多使用各种编程语言开发并发布的库。当然，每个库都有各自的特点，但每个库共同具备的特点可以总结如下：
- en: A model's training can be done just by defining a layer structure of deep learning.
    You can focus on parameter setting and tuning, and you don't need to think about
    the algorithms.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只需定义深度学习的层结构，就可以完成模型的训练。你可以专注于参数设置和调优，而无需考虑算法。
- en: Most of the libraries are open to the public as open source projects and are
    actively updated daily. Therefore, if there are bugs, there's a high possibility
    that these bugs will be fixed quickly (and, of course, committing to a project
    by fixing it yourself should be welcomed).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数库作为开源项目向公众开放，并且每天都在积极更新。因此，如果出现bug，很有可能这些bug会被迅速修复（当然，如果你自己修复它并提交到项目中，这也是受欢迎的）。
- en: It's easy to switch between running the program on CPU or on GPU. As a library
    supplements the cumbersome coding element of GPU computing, you can just focus
    on the implementation without considering CPU or GPU, if a machine supports GPU.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在CPU和GPU之间切换程序运行非常容易。由于库补充了GPU计算中的繁琐编码部分，你可以专注于实现，而无需考虑CPU或GPU，只要机器支持GPU。
- en: Long story short, you can leave out all the parts that could be brutal when
    you implement to a library from scratch. Thanks to this, you can take more time
    on the essential data mining section, hence if you want to utilize practical applications,
    there's a high possibility that you can perform data analysis more efficiently
    using a library.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，你可以省去所有在从头实现库时可能会遇到的麻烦部分。得益于此，你可以将更多时间花在核心数据挖掘部分，因此，如果你希望利用实际应用，使用库进行数据分析时效率更高的可能性也会大大增加。
- en: 'However, depending too much on a library isn''t good. Using a library is convenient,
    but on the flip side, it has some demerits, as listed here:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，过度依赖库并不好。使用库虽然方便，但也有一些缺点，如下所列：
- en: Since you can build various deep learning models easily, you can implement without
    having a concrete understanding of what theory the model is supported by. This
    might not be a problem if we only consider implementations related to a specific
    model, but there will be a risk you can't deal with when you want to combine other
    methods or consider other methods when applying the model.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于你可以轻松构建各种深度学习模型，你可以在没有具体理解模型所依赖的理论的情况下进行实现。如果我们只考虑与特定模型相关的实现，这可能不是问题，但当你想结合其他方法或在应用模型时考虑其他方法时，可能会遇到无法处理的风险。
- en: You can't use algorithms not supported by a library, hence there might be a
    case where you can't choose a model you would like to use. This can be solved
    by a version upgrade, but on the other hand, there's a possibility that some part
    of a past implementation might be deprecated due to a change of specification
    by the upgrade. Moreover, we can't deny the possibility that the development of
    a library is suddenly terminated or utilization turns out to be chargeable due
    to a sudden change in its license. In these cases, there's a risk that the code
    you have developed up to this point cannot be used.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你不能使用库中不支持的算法，因此可能会遇到无法选择自己想用的模型的情况。这个问题可以通过版本升级来解决，但另一方面，过去某些实现的部分可能由于规范更改而被弃用。此外，我们不能排除库的开发突然终止或由于许可证的突然变更，使用该库变为收费的可能性。在这些情况下，你之前开发的代码可能无法再使用。
- en: The precision rate you can get from experimentation depends on how a library
    is implemented. For example, if we conduct an experiment with the same neural
    network model in two different libraries, the results we obtain can be hugely
    changed. This is because neural network algorithms include a stochastic operation,
    and the calculation accuracy of a machine is limited, that is, calculated values
    during the process could have fluctuations based on the method of implementation.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你从实验中获得的精度取决于库的实现方式。例如，如果我们在两个不同的库中使用相同的神经网络模型进行实验，得到的结果可能会有很大的不同。这是因为神经网络算法包括随机操作，而且机器的计算精度是有限的，即在计算过程中，基于实现方法的不同，计算值可能会有波动。
- en: Because you well understand the fundamental concepts and theories of deep learning
    algorithms thanks to the previous chapters, we don't need to worry about the first
    point. However, we need to be careful about the remaining two points. From the
    next section on, implementation using a library is introduced and we'll be more
    conscious of the merits and demerits we just discussed.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你在前几章中已经很好地理解了深度学习算法的基本概念和理论，因此我们不需要担心第一个问题。然而，我们需要小心剩下的两个问题。从下一节开始，将介绍如何使用库进行实现，并且我们将更加关注刚刚讨论的优缺点。
- en: Introducing DL4J and ND4J
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍DL4J和ND4J
- en: A lot of the libraries of deep learning have been developed all over the world.
    In November 2015, **TensorFlow** ([http://www.tensorflow.org/](http://www.tensorflow.org/)),
    a machine learning/deep learning library developed by Google, became open to the
    public and attracted great attention.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 全球范围内已经开发了很多深度学习库。2015年11月，**TensorFlow**（[http://www.tensorflow.org/](http://www.tensorflow.org/)），由Google开发的机器学习/深度学习库，公开发布并引起了广泛关注。
- en: When we look at the programming language with which libraries are being developed,
    most of them open to the public are developed by Python or use the Python API.
    TensorFlow is developed with C++ on the backend but it's also possible to write
    code with Python. This book focuses on Java to learn deep learning, hence the
    libraries developed by other languages will be briefly introduced in [Chapter
    7](ch07.html "Chapter 7. Other Important Deep Learning Libraries"), *Other Important
    Deep Learning Libraries*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们看一下开发库所使用的编程语言时，大多数公开的库都是用Python开发的或使用Python API。TensorFlow的后端是用C++开发的，但也可以用Python编写代码。本书重点讲解使用Java学习深度学习，因此其他语言开发的库将在[第7章](ch07.html
    "第7章. 其他重要的深度学习库")中简要介绍，*其他重要的深度学习库*。
- en: 'So, what Java-based libraries do we have? Actually, there are a few cases that
    are actively developed (perhaps there are also some projects not open to public).
    However, there is only one library we can use practically: **Deeplearning4j**
    (**DL4J**). The official project page URL is [http://deeplearning4j.org/](http://deeplearning4j.org/).
    This library is also open source and the source code is all published on GitHub.
    The URL is [https://github.com/deeplearning4j/deeplearning4j](https://github.com/deeplearning4j/deeplearning4j).
    This library was developed by Skymind ([http://www.skymind.io/](http://www.skymind.io/)).
    What kind of library is this? If you look at the project page, it''s introduced
    as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '*"Deeplearning4j is the first commercial-grade, open-source, distributed deep-learning
    library written for Java and Scala. Integrated with Hadoop and Spark, DL4J is
    designed to be used in business environments, rather than as a research tool.
    Skymind is its commercial support arm.*'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Deeplearning4j aims to be cutting-edge plug and play, more convention than
    configuration, which allows for fast prototyping for non-researchers. DL4J is
    customizable at scale. Released under the Apache 2.0 license, all derivatives
    of DL4J belong to their authors."*'
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When you read this, you will see that the biggest feature of DL4J is that it
    was designed on the premise of being integrated with Hadoop. This indicates that
    DL4J suits the processing of large-scale data and is more scalable than other
    libraries. Moreover, DL4J supports GPU computing, so it's possible to process
    data even faster.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, DL4J uses a library called **N-Dimensional Arrays for Java** (**ND4J**)
    internally. The project page is [http://nd4j.org/](http://nd4j.org/). The same
    as DL4J, this library is also published on GitHub as an open source project: [https://github.com/deeplearning4j/nd4j](https://github.com/deeplearning4j/nd4j).
    The developer of the library is the same as DL4J, Skymind. As you can see from
    the name of the library, this is a scientific computing library that enables us
    to handle versatile *n*-dimensional array objects. If you are a Python developer,
    it might be easier for you to understand this if you imagine NumPy, as ND4J is
    a library inspired by NumPy. ND4J also supports GPU computing and the reason why
    DL4J is able to do GPU integration is because it uses ND4J internally.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: What good can come from working with them on GPUs? Let's briefly look at this
    point. The biggest difference between CPU and GPU is the difference in the number
    of cores. GPU is, as represented in its name, a graphical processing unit, originally
    an integrated circuit for image processing. This is why GPU is well optimized
    to handle the same commands simultaneously. Parallel processing is its forte.
    On the other hand, as CPU needs to process various commands, these tasks are basically
    made to be processed in order. Compared to CPU, GPU is good at processing huge
    numbers of simple tasks, therefore calculations such as training iterations of
    deep learning is its field of expertise.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Both ND4J and DL4J are very useful for research and data mining with deep learning.
    From the next section on, we'll see how these are used for deep learning in simple
    examples. You can easily understand the contents because you should already understand
    the core theories of deep learning by now. Hopefully, you can make use of this
    for your fields of study or business.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Implementations with ND4J
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As there are many cases where ND4J alone can be used conveniently, let''s briefly
    grasp how to use ND4J before looking into the explanation of DL4J. If you would
    like to use ND4J alone, once you create a new Maven project, then you can use
    ND4J by adding the following code to `pom.xml`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here, `<nd4j.version>` describes the latest version of ND4J, but please check
    whether it is updated when you actually implement the code. Also, switching from
    CPU to GPU is easy while working with ND4J. If you have CUDA installed with version
    7.0, then what you do is just define `artifactId` as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can replace the version of `<artifactId>` depending on your configuration.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a simple example of what calculations are possible with ND4J.
    The type we utilize with ND4J is `INDArray`, that is, an extended type of `Array`.
    We begin by importing the following dependencies:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we define `INDArray` as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`Nd4j.create` takes two arguments. The former defines the actual values within
    `INDArray`, and the latter defines the shape of the vector (matrix). By running
    this code, you get the following result:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Since `INDArray` can output its values with `System.out.print`, it''s easy
    to debug. Calculation with scalar can also be done with ease. Add 1 to `x` as
    shown here:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, you will get the following output:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Also, the calculation within `INDArray` can be done easily, as shown in the
    following example:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, basic arithmetic operations can be represented as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'These will return the following result:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Also, ND4J has destructive arithmetic operators. When you write the `x.addi(y)`
    command, `x` changes its own values so that `System.out.println(x);` will return
    the following output:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Likewise, `subi`, `muli`, and `divi` are also destructive operators. There are
    also many other methods that can conveniently perform calculations between vectors
    or matrices. For more information, you can refer to [http://nd4j.org/documentation.html](http://nd4j.org/documentation.html),
    [http://nd4j.org/doc/](http://nd4j.org/doc/) and `http://nd4j.org/apidocs/`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at one more example to see how machine learning algorithms can be
    written with ND4J. We'll implement the easiest example, perceptrons, based on
    the source code written in [Chapter 2](ch02.html "Chapter 2. Algorithms for Machine
    Learning – Preparing for Deep Learning"), *Algorithms for Machine Learning – Preparing
    for Deep Learning*. We set the package name `DLWJ.examples.ND4J` and the file
    (class) name `Perceptrons.java`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s add these two lines to import from ND4J:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The model has two parameters: `num` of the input layer and the weight. The
    former doesn''t change from the previous code; however, the latter isn''t `Array`
    but `INDArray`:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You can see from the constructor that since the weight of the perceptrons is
    represented as a vector, the number of rows is set to the number of units in the
    input layer and the number of columns to 1\. This definition is written here:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, because we define the model parameter as `INDArray`, we also define the
    demo data, training data, and test data as `INDArray`. You can see these definitions
    at the beginning of the main method:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'When we substitute a value into `INDArray`, we use `put`. Please be careful
    that any value we can set with `put` is only the values of the `scalar` type:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The flow from a model building and training is the same as the previous code:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Each piece of training data is given to the `train` method by `getRow()`. First,
    let''s see the entire content of the `train` method:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We first focus our attention on the following code:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This is the part that checks whether the data is classified correctly by perceptions,
    as shown in the following equation:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementations with ND4J](img/B04779_05_06.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
- en: 'You can see from the code that `.mmul()` is for the multiplication between
    vectors or matrices. We wrote this part of the calculation in [Chapter 2](ch02.html
    "Chapter 2. Algorithms for Machine Learning – Preparing for Deep Learning"), *Algorithms
    for Machine Learning – Preparing for Deep Learning*, as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: By comparing both codes, you can see that multiplication between vectors or
    matrices can be written easily with `INDArray`, and so you can implement the algorithm
    intuitively just by following the equations.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation to update the model parameters is as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here, again, you can implement the code like you write a math equation. The
    equation is represented as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementations with ND4J](img/B04779_05_13.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: 'The last time we implemented this part, we wrote it with a `for` loop:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Furthermore, the prediction after the training is also the standard forward
    activation, shown as the following equation:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementations with ND4J](img/B04779_05_15.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
- en: 'Here:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementations with ND4J](img/B04779_05_16.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: 'We can simply define the `predict` method with just a single line inside, as
    follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: When you run the program, you can see its precision and accuracy, and the recall
    is the same as we get with the previous code.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Thus, it'll greatly help that you implement the algorithms analogous to mathematical
    equations. We only implement perceptrons here, but please try other algorithms
    by yourself.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Implementations with DL4J
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ND4J is the library that helps you to implement deep learning easily and conveniently.
    However, you have to implement the algorithms yourself, which is not too different
    from its implementation in the previous chapters. In other words, ND4J is just
    a library that makes calculating numerical values easier and is not a library
    that is optimized for deep learning algorithms. One library that makes deep learning
    easier to handle is DL4J. Fortunately, as for DL4J, some example code with typical
    methods is published on GitHub ([https://github.com/deeplearning4j/dl4j-0.4-examples](https://github.com/deeplearning4j/dl4j-0.4-examples)).
    These examples are used on the premise that you are using DL4J's version 0.4-*.
    When you actually clone this repository, please check the latest version again.
    In this section, we'll extract the fundamental part from these sample programs
    and take a look at it. We'll reference the forked repository on [https://github.com/yusugomori/dl4j-0.4-examples](https://github.com/yusugomori/dl4j-0.4-examples)
    as a screenshot in this section.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s first set up the environments from our cloned repository. If you''re
    using IntelliJ, you can import the project from **File** | **New** | **Project**
    from existing sources and select the path of the repository. Then, choose **Import
    project from external model** and select **Maven** as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '![Setup](img/B04779_05_01.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
- en: 'You don''t have to do anything special for the other steps except click **Next**.
    Please be careful that the supported versions of JDK are 1.7 or above. This may
    not be a problem because we needed version 1.8 or above in the previous chapters.
    Once you have set it up without a problem, you can confirm the structure of the
    directories as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '![Setup](img/B04779_05_02.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
- en: 'Once you have set up the project, let''s first look at `pom.xml`. You can see
    that the description of the packages related to DL4J is written as:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Also, you can see from the following lines that DL4J depends on ND4J:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'If you would like to run a program on GPU, what you have to do is just change
    this written section. As mentioned in the previous section, this can be written
    as follows if you have CUDA installed:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Here, `XXX` is the version of CUDA and depends on your machine's preference.
    It's great to adopt GPU computing only using this. We don't have to do anything
    special and we can focus on implementations of deep learning.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'The other characteristic library that DL4J develops and uses is **Canova**.
    The part that corresponds to `pom.xml` is as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: DL4J 开发和使用的另一个特征库是 **Canova**。对应于 `pom.xml` 的部分如下：
- en: '[PRE26]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Canova is also, of course, an open source library and its source code can be
    seen on GitHub at [https://github.com/deeplearning4j/Canova](https://github.com/deeplearning4j/Canova).
    As explained on that page, Canova is the library used to vectorize raw data into
    usable vector formats across the machine learning tools. This also helps us focus
    on the more important part of data mining because data formatting is indispensable
    in whatever research or experiment we're performing.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，Canova 也是一个开源库，其源代码可以在 GitHub 上查看，地址是 [https://github.com/deeplearning4j/Canova](https://github.com/deeplearning4j/Canova)。正如该页面所述，Canova
    是一个用于将原始数据向量化为机器学习工具可以使用的向量格式的库。这也有助于我们专注于数据挖掘中的更重要部分，因为数据格式化在任何研究或实验中都是不可或缺的。
- en: Build
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建
- en: Let's look at the source code in the examples and see how to build a deep learning
    model. During the process, the terms of deep learning that you haven't yet learned
    are also briefly explained. The examples are implemented with various models such
    as MLP, DBN, and CNN, but there is one problem here. As you can see when looking
    at `README.md`, some methods don't generate good precision. This is because, as
    explained in the previous section, the calculation precision a machine has is
    limited and fluctuation occurring with calculated values during the process depends
    completely on the difference of implementation. Hence, practically, learning can't
    be done properly, although theoretically it should be done well. You can get better
    results by, for example, changing the seed values or adjusting the parameters,
    but as we would like to focus on how to use a library, we'll use a model that
    gets higher precision as an example.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看示例中的源代码，了解如何构建一个深度学习模型。在这个过程中，你还会看到一些你还未学习的深度学习术语，并得到简要解释。这些示例实现了各种模型，如
    MLP、DBN 和 CNN，但这里有一个问题。正如在 `README.md` 中看到的那样，一些方法没有生成良好的精度。这是因为，正如前一部分所解释的那样，机器的计算精度是有限的，在过程中的计算值波动完全依赖于实现的差异。因此，从实际情况来看，学习无法顺利进行，尽管理论上应该是可行的。你可以通过改变种子值或调整参数来获得更好的结果，但由于我们希望专注于如何使用这个库，我们将以一个精度更高的模型为例。
- en: DBNIrisExample.java
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DBNIrisExample.java
- en: Let's first look at `DBNIrisExample.java` in the package of `deepbelief`. Iris,
    contained in the filename, is one of the benchmark datasets often used when measuring
    the precision or accuracy of a machine learning method. The dataset contains 150
    pieces of data out of 3 classes of 50 instances each, and each class refers to
    a type of Iris plant. The number of inputs is 4 and the number of outputs is therefore
    3\. One class is linearly separable from the other two; the latter are not linearly
    separable from each other.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先来看看位于 `deepbelief` 包中的 `DBNIrisExample.java`。文件名中的 "Iris" 是一个常用于衡量机器学习方法精度或准确性的基准数据集之一。该数据集包含来自
    3 个类别的 150 条数据，每个类别包含 50 个实例，每个类别对应一种鸢尾花的类型。输入特征数为 4，因此输出的类别数为 3。一个类别可以与另外两个类别线性可分；而后两个类别之间则不是线性可分的。
- en: 'The implementation begins by setting up the configuration. Here are the variables
    that need setting:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 实现从设置配置开始。以下是需要设置的变量：
- en: '[PRE27]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In DL4J, input data can be up to two-dimensional data, hence you need to assign
    the number of rows and columns of the data. As Iris is one-dimensional data, `numColumns`
    is set as `1`. Here `numSamples` is the total data and `batchSize` is the amount
    of data in each mini-batch. Since the total data is 150 and it is relatively small,
    `batchSize` is set at the same number. This means that learning is done without
    splitting the data into mini-batches. `splitTrainNum` is the variable that decides
    the allocation between the training data and test data. Here, 80% of all the dataset
    is training data and 20% is the test data. In the previous section, `listenerFreq`
    decides how often we see loss function's value for logging is seen in the process.
    This value is set to 1 here, which means the value is logged after each epoch.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'Subsequently, we need to fetch the dataset. In DL4J, a class that can easily
    fetch data with respect to a typical dataset, such as Iris, MINST, and LFW, is
    prepared. Therefore, you can just write the following line if you would like to
    fetch the Iris dataset:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The following two lines are to format data:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This code splits the data into training data and test data and stores them
    respectively:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As you can see, it makes data handling easier by treating all the data DL4J
    prepares with the `DataSet` class.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s actually build a model. The basic structure is as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The code begins by defining the model configuration and then builds and initializes
    the actual model with the definition. Let''s take a look at the configuration
    details. At the beginning, the whole network is set up:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The configuration setup is self-explanatory. However, since you haven't learned
    about regularization before now, let's briefly check it out.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'Regularization prevents the neural networks model from overfitting and makes
    the model more generalized. To achieve this, the evaluation function ![DBNIrisExample.java](img/B04779_05_07.jpg)
    is rewritten with the penalty term as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '![DBNIrisExample.java](img/B04779_05_08.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: Here, ![DBNIrisExample.java](img/B04779_05_09.jpg) denotes the vector norm.
    The regularization is called L1 regularization when ![DBNIrisExample.java](img/B04779_05_10.jpg)
    and L2 regularization when ![DBNIrisExample.java](img/B04779_05_11.jpg). The norm
    is called L1 norm and L2 norm, respectively. That's why we have `.l1()` and `.l2()`
    in the code. ![DBNIrisExample.java](img/B04779_05_12.jpg) is the hyper parameter.
    These regularization terms make the model more sparse. L2 regularization is also
    called weight decay and is used to prevent the vanishing gradient problem.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: The `.useDropConnect()` command is used to enable dropout and `.list()` to define
    the number of layers, excluding the input layer.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'When you set up a whole model, then the next step is to configure each layer.
    In this sample code, the model is not defined as deep neural networks. One single
    RBM layer is defined as a hidden layer:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Here, the value of `0` in the first line is the layer's index and `.k()` is
    for contrastive divergence. Since Iris' data is of float values, we can't use
    binary RBM. That's why we have `RBM.VisibleUnit.GAUSSIAN` here, enabling the model
    to handle continuous values. Also, as for the definition of this layer, what should
    be especially mentioned is the role of `Updater.ADAGRAD`. This is used to optimize
    the learning rate. For now, we go on to the model structure, and a detailed explanation
    of the optimizer will be introduced at the end of this chapter.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'The subsequent output layer is very simple and self-explanatory:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Thus, the neural networks have been built with three layers : input layer,
    hidden layer, and output layer. The graphical model of this example can be illustrated
    as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![DBNIrisExample.java](img/B04779_05_03.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: 'After the model building, we need to train the networks. Here, again, the code
    is super simple:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Because the first line is to log the process, what we need to do to train the
    model is just to write `model.fit()`.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'Testing or evaluating the model is also easy with DL4J. First, the variables
    for evaluation are set up as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then, we can get the values of the feature matrix using:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'By running the code, we will have the result as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '`F1 Score`, also called `F-Score` or `F-measure`, is the harmonic means of
    precision and recall, and is represented as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![DBNIrisExample.java](img/B04779_05_14.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: 'This value is often calculated to measure the model''s performance as well.
    Also, as written in the example, you can see the actual values and predicted values
    by writing the following:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'That''s it for the whole training and test process. The neural networks in
    the preceding code are not deep, but you can easily build deep neural networks
    just by changing the configuration as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: As you can see, building deep neural networks requires just simple implementations
    with DL4J. Once you set up the model, what you need to do is adjust the parameters.
    For example, increasing the iterations value or changing the seed value would
    return a better result.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: CSVExample.java
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous example, we train the model with the dataset used as a benchmark
    indicator. When you would like to train and test the model with your own prepared
    data, you can easily import it from CSV. Let''s look at `CSVExample.java` in the
    CSV package. The first step is to initialize the CSV reader as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'In DL4J, a class called `CSVRecordReader` is prepared and you can easily import
    data from a CSV file. The value of the first argument in the `CSVRecordReader`
    class represents how many lines should be skipped in the file. This is convenient
    when the file contains header rows. The second argument is the delimiter. To actually
    read a file and import data, the code can be written as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'With this code, the file in `resources/iris.txt` will be imported to the model.
    The values in the file here are the same as ones as in the Iris dataset. To use
    this initialized data for model training, we define the iterator as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: In the previous example, we used the `IrisDataSetIterator` class, but here the
    `RecordReaderDataSetIterator` class is used because we use our own prepared data.
    The values `4` and `3` are the number of features and labels, respectively.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'Building and training a model can be done in almost the same way as the process
    explained in the previous example. In this example, we build deep neural networks
    of two hidden layers with the dropout and the rectifier, that is, we have an input
    layer - hidden layer - hidden layer - output layer, as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We can run the model using the following lines of code:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The graphical model is as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![CSVExample.java](img/B04779_05_04.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: 'This time, however, the way to code for training is slightly different from
    the previous example. Before, we split the data into training data and test data
    using the following:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This shows that we shuffle the data within the `.splitTestAndTrain()` method.
    In this example, we set up training data with the following code:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'As you can see, here the data is first shuffled and then split into training
    data and test data. Be careful that the types of the arguments in `.splitTestAndTrain()`
    are different from each other. This will be beneficial because we don''t have
    to count the exact amount of data or training data. The actual training is done
    using:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The way to evaluate the model is just the same as the previous example:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'With the preceding code, we get the following result:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: In addition to the dataset of a benchmark indicator, you can now analyze whatever
    data you have.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the model even deeper, you just need to add another layer as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: CNNMnistExample.java/LenetMnistExample.java
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CNN is rather complicated compared to other models because of its structure,
    but we don't need to worry about these complications because we can easily implement
    CNN with DL4J. Let's take a look at `CNNMnistExample.java` in the package of convolution.
    In this example, we train the model with the MNIST dataset ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)),
    one of the most famous benchmark indicators. As mentioned in [Chapter 1](ch01.html
    "Chapter 1. Deep Learning Overview"), *Deep Learning Overview*, this dataset contains
    70,000 handwritten numbers data from 0 to 9, with both a height and width of 28
    pixels each.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define the values necessary for the model:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Since images in MNIST are all grayscale data, the number of channels is set
    to `1`. In this example, we use `2,000` data of `70,000` and split it into training
    data and test data. The size of the mini-batch is `500` here, so the training
    data is divided into 4 mini-batches. Furthermore, the data in each mini-batch
    is split into training data and test data, and each piece of test data is stored
    in `ArrayList`:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We didn''t have to set `ArrayList` in the previous examples because we had
    just one batch. For the `MnistDataSetIterator` class, we can set the MNIST data
    just by using:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Then, we build the model with a convolutional layer and subsampling layer.
    Here, we have one convolutional layer and one max-pooling layer, directly followed
    by an output layer. The structure of the configurations for CNN is slightly different
    from the other algorithms:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The difference is that we can''t build a model directly from the configuration
    because we need to tell the builder to set up a convolutional layer using `ConvolutionLayerSetup()`
    in advance. Each `.layer()` requires just the same method of coding. The convolutional
    layer is defined as:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Here, the value of `10` in `ConvolutionLayer.Builder()` is the size of the kernels,
    and the value of `6` in `.nOut()` is the number of kernels. Also, `.stride()`
    defines the size of the strides of the kernels. The code we implemented from scratch
    in [Chapter 4](ch04.html "Chapter 4. Dropout and Convolutional Neural Networks"),
    *Dropout and Convolutional Neural Networks* has a functionality equivalent only
    to `.stride(1, 1)`. The larger the number is, the less time it takes because it
    decreases the number of calculations necessary for convolutions, but we have to
    be careful at the same time that it might also decrease the model's precision.
    Anyway, we can implement convolutions with more flexibility now.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'The `subsampling` layer is described as:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Here, `{2, 2}` is the size of the pooling windows. You may have noticed that
    we don't have to set the size of the inputs for each layer, including the output
    layer. These values are automatically set once you set up the model.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'The output layer can be written just the same as in the other models:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The graphical model of this example is as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![CNNMnistExample.java/LenetMnistExample.java](img/B04779_05_05.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: 'After the building comes the training. Since we have multiple mini-batches,
    we need to iterate training through all the batches. This can be achieved easily
    using `.hasNext()` on `DataSetIterator` and `mnistIter` in this case. The whole
    training process can be written as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Here, the test data and test labels are stocked for further use.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'During the test, again, we need to iterate the evaluation process of the test
    data because we have more than one mini-batch:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Then, we use the same as in the other examples:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This will return the result as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The example just given is the model with one convolutional layer and one subsampling
    layer, but you have deep convolutional neural networks with `LenetMnistExample.java`.
    In this example, there are two convolutional layers and subsampling layers, followed
    by fully connected multi-layer perceptrons:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: As you can see in the first convolutional layer, dropout can easily be applied
    to CNN with DL4J.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'With this model, we get the following result:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: You can see from the MNIST dataset page ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/))
    that the state-of-the-art result is much better than the one above. Here, again,
    you would realize how important the combination of parameters, activation functions,
    and optimization algorithms are.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Learning rate optimization
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have learned various deep learning algorithms so far; you may have noticed
    that they have one parameter in common: the learning rate. The learning rate is
    defined in the equations to update the model parameters. So, why not think of
    algorithms to optimize the learning rate? Originally, these equations were described
    as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning rate optimization](img/B04779_05_17.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
- en: 'Here:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning rate optimization](img/B04779_05_18.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: Here, ![Learning rate optimization](img/B04779_05_19.jpg) is the number of steps
    and ![Learning rate optimization](img/B04779_05_20.jpg) is the learning rate.
    It is well known that decreasing the value of the learning rate with each iteration
    lets the model have better precision rates, but we should determine the decline
    carefully because a sudden drop in the value would collapse the model. The learning
    rate is one of the model parameters, so why not optimize it? To do so, we need
    to know what the best rate could be.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest way of setting the rate is using the momentum, represented as
    follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning rate optimization](img/B04779_05_21.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: Here, ![Learning rate optimization](img/B04779_05_22.jpg), called the **momentum
    coefficient**. This hyper parameter is often set to be 0.5 or 0.9 first and then
    fine-tuned.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'Momentum is actually a simple but effective way of adjusting the learning rate
    but **ADAGRAD**, proposed by Duchi et al. ([http://www.magicbroom.info/Papers/DuchiHaSi10.pdf](http://www.magicbroom.info/Papers/DuchiHaSi10.pdf)),
    is known to be a better way. The equation is described as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning rate optimization](img/B04779_05_23.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
- en: 'Here:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning rate optimization](img/B04779_05_24.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
- en: 'Theoretically, this works well, but practically, we often use the following
    equations to prevent divergence:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning rate optimization](img/B04779_05_25.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
- en: 'Or we use:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning rate optimization](img/B04779_05_26.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
- en: ADAGRAD is easier to use than momentum because the value is set automatically
    and we don't have to set additional hyper parameters.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'ADADELTA, suggested by Zeiler ([http://arxiv.org/pdf/1212.5701.pdf](http://arxiv.org/pdf/1212.5701.pdf)),
    is known to be an even better optimizer. This is an algorithm-based optimizer
    and cannot be written in a single equation. Here is a description of ADADELTA:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialization:'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initialize accumulation variables:![Learning rate optimization](img/B04779_05_27.jpg)
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And:'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Learning rate optimization](img/B04779_05_28.jpg)'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Iteration ![Learning rate optimization](img/B04779_05_29.jpg):'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute:![Learning rate optimization](img/B04779_05_30.jpg)
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Accumulate gradient:![Learning rate optimization](img/B04779_05_31.jpg)
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute update:![Learning rate optimization](img/B04779_05_32.jpg)
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Accumulate updates:![Learning rate optimization](img/B04779_05_33.jpg)
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply update:![Learning rate optimization](img/B04779_05_17.jpg)
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, ![Learning rate optimization](img/B04779_05_34.jpg) and ![Learning rate
    optimization](img/B04779_05_35.jpg) are the hyper parameters. You may think ADADELTA
    is rather complicated but you don't need to worry about this complexity when implementing
    with DL4J.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: There are still other optimizers supported in DL4J such as **RMSProp**, **RMSProp**
    + momentum, and **Nesterov's Accelerated Gradient** **Descent**. However, we won't
    dig into them because, practically, momentum, ADAGRAD, and ADADELTA are enough
    to optimize the learning rate.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to implement deep learning models with the
    libraries ND4J and DL4J. Both support GPU computing and both give us the ability
    to implement them without any difficulties. ND4J is a library for scientific computing
    and enables vectorization, which makes it easier to implement a calculation among
    arrays because we don't need to write iterations within them. Since machine learning
    and deep learning algorithms have many equations with vector calculations, such
    as inner products and element-wise multiplication, ND4J also helps implement them.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'DL4J is a library for deep learning, and by following some examples with the
    library, you saw that we can easily build, train, and evaluate various types of
    deep learning models. Additionally, while building the model, you learned why
    regularization is necessary to get better results. You also got to know some optimizers
    of the learning rate: momentum, ADAGRAD, and ADADELTA. All of these can be implemented
    easily with DL4J.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: You gained knowledge of the core theories and implementations of deep learning
    algorithms and you now know how to implement them with little difficulty. We can
    say that we've completed the theoretical part of this book. Therefore, in the
    next chapter, we'll look at how deep learning algorithms are adapted to practical
    applications first and then look into other possible fields and ideas to apply
    the algorithms.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
